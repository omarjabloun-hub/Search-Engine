[
    {
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence",
        "content": "\n Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1] Such machines may be called AIs.\n High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"[2][3]\n Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics.[a] General intelligence\u2014the ability to complete any task performed by a human on an at least equal level\u2014is among the field's long-term goals.[4] To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics.[b] AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.[5]\n Artificial intelligence was founded as an academic discipline in 1956,[6] and the field went through multiple cycles of optimism throughout its history,[7][8] followed by periods of disappointment and loss of funding, known as AI winters.[9][10] Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques.[11] This growth accelerated further after 2017 with the transformer architecture,[12] and by the early 2020s many billions of dollars were being invested in AI and the field experienced rapid ongoing progress in what has become known as the AI boom. The emergence of advanced generative AI in the midst of the AI boom and its ability to create and modify content exposed several unintended consequences and harms in the present and raised concerns about the risks of AI and its long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.\n The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.[a]\n Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions.[13] By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.[14]\n Many of these algorithms are insufficient for solving large reasoning problems because they experience a \"combinatorial explosion\": They become exponentially slower as the problems grow.[15] Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments.[16] Accurate and efficient reasoning is an unsolved problem.\n Knowledge representation and knowledge engineering[17] allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval,[18] scene interpretation,[19] clinical decision support,[20] knowledge discovery (mining \"interesting\" and actionable inferences from large databases),[21] and other areas.[22]\n A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge.[23] Knowledge bases need to represent things such as objects, properties, categories, and relations between objects;[24] situations, events, states, and time;[25] causes and effects;[26] knowledge about knowledge (what we know about what other people know);[27] default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing);[28] and many other aspects and domains of knowledge.\n Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous);[29] and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as \"facts\" or \"statements\" that they could express verbally).[16] There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.[c]\n An \"agent\" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen.[d][32] In automated planning, the agent has a specific goal.[33] In automated decision-making, the agent has preferences\u2014there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the \"utility\") that measures how much the agent prefers it. For each possible action, it can calculate the \"expected utility\": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.[34]\n In classical planning, the agent knows exactly what the effect of any action will be.[35] In most real-world problems, however, the agent may not be certain about the situation they are in (it is \"unknown\" or \"unobservable\") and it may not know for certain what will happen after each possible action (it is not \"deterministic\"). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.[36]\n In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences.[37] Information value theory can be used to weigh the value of exploratory or experimental actions.[38] The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.\n A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.[39]\n Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.[40]\n Machine learning is the study of programs that can improve their performance on a given task automatically.[41] It has been a part of AI from the beginning.[e]\n There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance.[44] Supervised learning requires labeling the training data with the expected answers, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).[45]\n In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as \"good\".[46] Transfer learning is when the knowledge gained from one problem is applied to a new problem.[47] Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.[48]\n Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.[49]\n Natural language processing (NLP)[50] allows programs to read, write and communicate in human languages such as English. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.[51]\n Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation[f] unless restricted to small domains called \"micro-worlds\" (due to the common sense knowledge problem[29]). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.\n Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning),[52] transformers (a deep learning architecture using an attention mechanism),[53] and others.[54] In 2019, generative pre-trained transformer (or \"GPT\") language models began to generate coherent text,[55][56] and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.[57]\n Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.[58]\n The field includes speech recognition,[59] image classification,[60] facial recognition, object recognition,[61]object tracking,[62] and robotic perception.[63]\n Affective computing is a field that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood.[65] For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human\u2013computer interaction.\n However, this tends to give na\u00efve users an unrealistic conception of the intelligence of existing computer agents.[66] Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the effects displayed by a videotaped subject.[67]\n A machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.[4]\n AI research uses a wide variety of techniques to accomplish the goals above.[b]\n AI can solve many problems by intelligently searching through many possible solutions.[68] There are two very different kinds of search used in AI: state space search and local search.\n State space search searches through a tree of possible states to try to find a goal state.[69] For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.[70]\n Simple exhaustive searches[71] are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes.[15] \"Heuristics\" or \"rules of thumb\" can help prioritize choices that are more likely to reach a goal.[72]\n Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and countermoves, looking for a winning position.[73]\n Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.[74]\n Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks.[75]\n Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by \"mutating\" and \"recombining\" them, selecting only the fittest to survive each generation.[76]\n Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).[77]\n Formal logic is used for reasoning and knowledge representation.[78]\nFormal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as \"and\", \"or\", \"not\" and \"implies\")[79] and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as \"Every X is a Y\" and \"There are some Xs that are Ys\").[80]\n Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises).[81] Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.\n Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem.[82] In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.[83]\n Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.[84]\n Fuzzy logic assigns a \"degree of truth\" between 0 and 1. It can therefore handle propositions that are vague and partially true.[85]\n Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning.[28] Other specialized versions of logic have been developed to describe many complex domains.\n Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics.[86] Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis,[87] and information value theory.[88] These tools include models such as Markov decision processes,[89] dynamic decision networks,[90] game theory and mechanism design.[91]\n Bayesian networks[92] are a tool that can be used for reasoning (using the Bayesian inference algorithm),[g][94] learning (using the expectation\u2013maximization algorithm),[h][96] planning (using decision networks)[97] and perception (using dynamic Bayesian networks).[90]\n Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).[90]\n The simplest AI applications can be divided into two types: classifiers (e.g., \"if shiny then diamond\"), on one hand, and controllers (e.g., \"if diamond then pick up\"), on the other hand. Classifiers[98] are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an \"observation\") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.[45]\n There are many kinds of classifiers in use.[99] The decision tree is the simplest and most widely used symbolic machine learning algorithm.[100] K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.[101]\nThe naive Bayes classifier is reportedly the \"most widely used learner\"[102] at Google, due in part to its scalability.[103]\nNeural networks are also used as classifiers.[104]\n An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.[104]\n Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm.[105] Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.[106]\n In feedforward neural networks the signal passes in only one direction.[107] Recurrent neural networks feed the output signal back into the input, which allows short-term memories of previous input events. Long short term memory is the most successful network architecture for recurrent networks.[108] Perceptrons[109] use only a single layer of neurons; deep learning[110] uses multiple layers. Convolutional neural networks strengthen the connection between neurons that are \"close\" to each other\u2014this is especially important in image processing, where a local set of neurons must identify an \"edge\" before the network can identify an object.[111]\n Deep learning[110] uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.[112]\n Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification,[113] and others. The reason that deep learning performs so well in so many applications is not known as of 2023.[114] The sudden success of deep learning in 2012\u20132015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s)[i] but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.[j]\n Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pretrained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called \"hallucinations\", although this can be reduced with RLHF and quality data. They are used in chatbots, which allow people to ask a question or request a task in simple text.[122][123]\n Current models and services include Gemini (formerly Bard), ChatGPT, Grok, Claude, Copilot, and LLaMA.[124] Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.[125]\n In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training.[126] Specialized programming languages such as Prolog were used in early AI research,[127] but general-purpose programming languages like Python have become predominant.[128]\n The transistor density in integrated circuits has been observed to roughly double every 18 months\u2014a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster.[129]\n AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's iPhoto and TikTok). The deployment of AI may be overseen by a Chief automation officer (CAO).\n The application of AI in medicine and medical research has the potential to increase patient care and quality of life.[130] Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.[131][132]\n For medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication.[133] It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research.[133] New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.[134] In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria.[135] In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.[136][137]\n Applications of AI in this domain include AI-enabled menstruation and fertility trackers that analyze user data to offer prediction,[138] AI-integrated sex toys (e.g., teledildonics),[139] AI-generated sexual education content,[140] and AI agents that simulate sexual and romantic partners (e.g., Replika).[141]  AI is also used for the production of non-consensual deepfake pornography, raising significant ethical and legal concerns.[142]\n AI technologies have also been used to attempt to identify online gender-based violence and online sexual grooming of minors.[143][144]\n Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques.[145] Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.[146] In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[147] In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world.[148] Other programs handle imperfect-information games, such as the poker-playing program Pluribus.[149] DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games.[150] In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map.[151] In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning.[152] In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.[153]\n In mathematics, special forms of formal step-by-step reasoning are used.[154] In contrast, LLMs such as GPT-4 Turbo, Gemini Ultra, Claude Opus, LLaMa-2 or Mistral Large are working with probabilistic models, which can produce wrong answers in the form of hallucinations. Therefore, they need not only a large database of mathematical problems to learn from but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections.[155] A 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.[156]\n Alternatively, dedicated models for mathematical problem solving with higher precision for the outcome including proof of theorems have been developed such as Alpha Tensor, Alpha Geometry and Alpha Proof all from Google DeepMind,[157] Llemma from eleuther[158] or Julius.[159]\n When natural language is used to describe mathematical problems, converters transform such prompts into a formal language such as Lean to define mathematical tasks.\n Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.[160]\n Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated \"robot advisers\" have been in use for some years.[161]\n World Pensions experts like Nicolas Firzli insist it may be too early to see the emergence of highly innovative AI-informed financial products and services: \"the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.\"[162]\n Various countries are deploying AI military applications.[163] The main applications enhance command and control, communications, sensors, integration and interoperability.[164] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[163] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams.[164]\n AI has been used in military operations in Iraq, Syria, Israel and Ukraine.[163][165][166][167]\n In the early 2020s, generative AI gained widespread prominence. GenAI is AI capable of generating text, images, videos, or other data using generative models,[168][169] often in response to prompts.[170][171]\n In March 2023, 58% of U.S. adults had heard about ChatGPT and 14% had tried it.[172] The increasing realism and ease-of-use of AI-based text-to-image generators such as Midjourney, DALL-E, and Stable Diffusion sparked a trend of viral AI-generated photos. Widespread attention was gained by a fake photo of Pope Francis wearing a white puffer coat, the fictional arrest of Donald Trump, and a hoax of an attack on the Pentagon, as well as the usage in professional creative arts.[173][174]\n Artificial intelligent (AI) agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.[175][176][177]\n There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated \"AI\" in some offerings or processes.[178] A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.\n AI applications for evacuation and disaster management are growing. AI has been used to investigate if and how people evacuated in large scale and small scale evacuations using historical data from GPS, videos or social media. Further, AI can provide real time information on the real time evacuation conditions.[179][180][181]\n In agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.\n Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights.\" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.\n During the 2024 Indian elections, US$50 millions was spent on authorized AI-generated content, notably by creating deepfakes of allied (including sometimes deceased) politicians to better engage with voters, and by translating speeches to various local languages.[182]\n AI has potential benefits and potential risks.[183] AI may be able to advance science and find solutions for serious problems: Demis Hassabis of DeepMind hopes to \"solve intelligence, and then use that to solve everything else\".[184] However, as the use of AI has become widespread, several unintended consequences and risks have been identified.[185] In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.[186]\n Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.\n AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.\n Sensitive user data collected may include online activity records, geolocation data, video or audio.[187] For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them.[188] Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.[189]\n AI developers argue that this is the only way to deliver valuable applications. and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy.[190] Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted \"from the question of 'what they know' to the question of 'what they're doing with it'.\"[191]\n Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of \"fair use\". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include \"the purpose and character of the use of the copyrighted work\" and \"the effect upon the potential market for the copyrighted work\".[192][193] Website owners who do not wish to have their content scraped can indicate it in a \"robots.txt\" file.[194] In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI.[195][196] Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.[197]\n The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft.[198][199][200] Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.[201][202]\n In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use.[203] This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.[204]\n Prodigious power consumption by AI is responsible for the growth of fossil fuels use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources \u2013 from nuclear energy to geothermal to fusion. The tech firms argue that \u2013 in the long view \u2013 AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and \"intelligent\", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.[205]\n A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found \"US power demand (is) likely to experience growth not seen in a generation....\" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means.[206] Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.[207]\n In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for $650 Million (US).[208] Nvidia CEO Jen-Hsun Huang said nuclear power is a good option for the data centers.[209]\n In September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megawatts of power \u2013 enough for 800,000 homes \u2013 of energy will be produced. The cost for re-opening and upgrading is estimated at $1.6 billion (US) and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act.[210] The US government and the state of Michigan are investing almost $2 billion (US) to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon spinoff of Constellation.[211]\n After the last approval in September 2023, Taiwan suspended the approval of data centers north of Taoyuan with a capacity of more than 5 MW in 2024, due to power supply shortages.[212] Taiwan aims to phase out nuclear power by 2025.[212] On the other hand, Singapore imposed a ban on the opening of data centers in 2019 due to electric power, but in 2022, lifted this ban.[212]\n Although most nuclear plants in Japan have been shut down after the 2011 Fukushima nuclear accident, according to an October 2024 Bloomberg article in Japanese, cloud gaming services company Ubitus, in which Nvidia has a stake, is looking for land in Japan near nuclear power plant for a new data center for generative AI.[213] Ubitus CEO Wesley Kuo said nuclear power plants are the most efficient, cheap and stable power for AI.[213]\n On 1 November 2024, the Federal Energy Regulatory Commission (FERC) rejected an application submitted by Talen Energy for approval to supply some electricity from the nuclear power station Susquehanna to Amazon's data center.[214] \nAccording to the Commission Chairman Willie L. Phillips, it is a burden on the electricity grid as well as a significant cost shifting concern to households and other business sectors.[214]\n YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation.[215] This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government.[216] The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took steps to mitigate the problem [citation needed].\n In 2022, generative AI began to create images, audio, video and text that are indistinguishable from real photographs, recordings, films, or human writing. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda.[217] AI pioneer Geoffrey Hinton expressed concern about AI enabling \"authoritarian leaders to manipulate their electorates\" on a large scale, among other risks.[218]\n Machine learning applications will be biased[k] if they learn from biased data.[220] The developers may not be aware that the bias exists.[221] Bias can be introduced by the way training data is selected and by the way a model is deployed.[222][220] If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination.[223] The field of fairness studies how to prevent harms from algorithmic biases.\n On June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as \"gorillas\" because they were black. The system was trained on a dataset that contained very few images of black people,[224] a problem called \"sample size disparity\".[225] Google \"fixed\" this problem by preventing the system from labelling anything as a \"gorilla\". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.[226]\n COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different\u2014the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend.[227] In 2017, several researchers[l] showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.[229]\n A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as \"race\" or \"gender\"). The feature will correlate with other features (like \"address\", \"shopping history\" or \"first name\"), and the program will make the same decisions based on these features as it would on \"race\" or \"gender\".[230] Moritz Hardt said \"the most robust fact in this research area is that fairness through blindness doesn't work.\"[231]\n Criticism of COMPAS highlighted that machine learning models are designed to make \"predictions\" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these \"recommendations\" will likely be racist.[232] Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.[m]\n Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.[225]\n There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.[219]\n At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.[dubious \u2013 discuss][234]\n Many AI systems are so complex that their designers cannot explain how they reach their decisions.[235] Particularly with deep neural networks, in which there are a large amount of non-linear relationships between inputs and outputs. But some popular explainability techniques exist.[236]\n It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as \"cancerous\", because pictures of malignancies typically include a ruler to show the scale.[237] Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at \"low risk\" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.[238]\n People who have been harmed by an algorithm's decision have a right to an explanation.[239] Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists.[n] Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.[240]\n DARPA established the XAI (\"Explainable Artificial Intelligence\") program in 2014 to try to solve these problems.[241]\n Several approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output.[242] LIME can locally approximate a model's outputs with a simpler, interpretable model.[243] Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned.[244] Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning.[245] For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.[246]\n Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.\n A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision.[o] Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction.[248] Even when used in conventional warfare, it is unlikely that they will be unable to reliably choose targets and could potentially kill an innocent person.[248] In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed.[249] By 2015, over fifty countries were reported to be researching battlefield robots.[250]\n AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware.[251] All these technologies have been available since 2020 or earlier\u2014AI facial recognition systems are already being used for mass surveillance in China.[252][253]\n There many other ways that AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.[254]\n Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.[255]\n In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that \"we're in uncharted territory\" with AI.[256] A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed.[257] Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at \"high risk\" of potential automation, while an OECD report classified only 9% of U.S. jobs as \"high risk\".[p][259] The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies.[255] In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.[260][261]\n Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that \"the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution\" is \"worth taking seriously\".[262] Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.[263]\n From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.[264]\n It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, \"spell the end of the human race\".[265] This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like \"self-awareness\" (or \"sentience\" or \"consciousness\") and becomes a malevolent character.[q] These sci-fi scenarios are misleading in several ways.\n First, AI does not require human-like sentience to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip factory manager).[267] Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that \"you can't fetch the coffee if you're dead.\"[268] In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is \"fundamentally on our side\".[269]\n Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are built on language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.[270]\n The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI.[271] Personalities such as Stephen Hawking, Bill Gates, and Elon Musk,[272] as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.\n In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to \"freely speak out about the risks of AI\" without \"considering how this impacts Google.\"[273] He notably mentioned risks of an AI takeover,[274] and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.[275]\n In 2023, many leading AI experts endorsed the joint statement that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war\".[276]\n Some other researchers were more optimistic. AI pioneer J\u00fcrgen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making \"human lives longer and healthier and easier.\"[277] While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors.\"[278][279] Andrew Ng also argued that \"it's a mistake to fall for the doomsday hype on AI\u2014and that regulators who do will only benefit vested interests.\"[280] Yann LeCun \"scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction.\"[281] In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine.[282] However, after 2016, the study of current and future risks and possible solutions became a serious area of research.[283]\n Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.[284]\n Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.[285]\nThe field of machine ethics is also called computational morality,[285]\nand was founded at an AAAI symposium in 2005.[286]\n Other approaches include Wendell Wallach's \"artificial moral agents\"[287] and Stuart J. Russell's three principles for developing provably beneficial machines.[288]\n Active organizations in the AI open-source community include Hugging Face,[289] Google,[290] EleutherAI and Meta.[291] Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight,[292][293] meaning that their architecture and trained parameters (the \"weights\") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case.[294] Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.[295]\n Artificial Intelligence projects can have their ethical permissibility tested while designing, developing, and implementing an AI system. An AI framework such as the Care and Act Framework containing the SUM values\u2014developed by the Alan Turing Institute tests projects in four main areas:[296][297]\n Other developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others;[298] however, these principles do not go without their criticisms, especially regards to the people chosen contributes to these frameworks.[299]\n Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.[300]\n The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under a MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.[301]\n The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms.[302] The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally.[303] According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.[304][305] Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI.[306] Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia.[306] The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology.[306] Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI.[307] In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years.[308] In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, governments officials and academics.[309] In 2024, the Council of Europe created the first international legally binding treaty on AI, called the \"Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law\". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.[310]\n In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\".[304] A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity.[311] In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".[312][313]\n In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks.[314] 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence.[315][316] In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.[317][318]\n The study of mechanical or \"formal\" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as \"0\" and \"1\", could simulate any conceivable form of mathematical reasoning.[319][320] This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an \"electronic brain\".[r] They developed several areas of research that would become part of AI,[322] such as McCullouch and Pitts design for \"artificial neurons\" in 1943,[115] and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that \"machine intelligence\" was plausible.[323][320]\n The field of AI research was founded at a workshop at Dartmouth College in 1956.[s][6] The attendees became the leaders of AI research in the 1960s.[t] They and their students produced programs that the press described as \"astonishing\":[u] computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.[v][7] Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.[320]\n Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field.[327] In 1965 Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\".[328] In 1967 Marvin Minsky agreed, writing that \"within a generation\u00a0... the problem of creating 'artificial intelligence' will substantially be solved\".[329] They had, however, underestimated the difficulty of the problem.[w] In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill[331] and ongoing pressure from the U.S. Congress to fund more productive projects.[332] Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether.[333] The \"AI winter\", a period when obtaining funding for AI projects was difficult, followed.[9]\n In the early 1980s, AI research was revived by the commercial success of expert systems,[334] a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research.[8] However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.[10]\n Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition,[335] and began to look into \"sub-symbolic\" approaches.[336] Rodney Brooks rejected \"representation\" in general and focussed directly on engineering machines that move and survive.[x] Judea Pearl, Lofti Zadeh, and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic.[86][341] But the most important development was the revival of \"connectionism\", including neural network research, by Geoffrey Hinton and others.[342] In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.[343]\n AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This \"narrow\" and \"formal\" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics).[344] By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as \"artificial intelligence\" (a tendency known as the AI effect).[345]\nHowever, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or \"AGI\"), which had several well-funded institutions by the 2010s.[4]\n Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.[11]\nFor many specific tasks, other methods were abandoned.[y]\nDeep learning's success was based on both hardware improvements (faster computers,[347] graphics processing units, cloud computing[348]) and access to large amounts of data[349] (including curated datasets,[348] such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI.[z] The amount of machine learning research (measured by total publications) increased by 50% in the years 2015\u20132019.[306]\n In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.[283]\n In the late teens and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program taught only the game's rules and developed a strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text.[350] ChatGPT, launched on November 30, 2022, became the fastest-growing consumer software application in history, gaining over 100 million users in two months.[351] It marked what is widely regarded as AI's breakout year, bringing it into the public consciousness.[352] These programs, and others, inspired an aggressive AI boom, where large companies began investing billions of dollars in AI research. According to AI Impacts, about $50 billion annually was invested in \"AI\" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in \"AI\".[353] About 800,000 \"AI\"-related U.S. job openings existed in 2022.[354] According to PitchBook research, 22% of newly funded startups in 2024 claimed to be AI companies.[355]\n Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines.[356] Another major focus has been whether machines can be conscious, and the associated ethical implications.[357] Many other topics in philosophy are relevant to AI, such as epistemology and free will.[358] Rapid advancements have intensified public discussions on the philosophy and ethics of AI.[357]\n Alan Turing wrote in 1950 \"I propose to consider the question 'can machines think'?\"[359] He advised changing the question from whether a machine \"thinks\", to \"whether or not it is possible for machinery to show intelligent behaviour\".[359] He devised the Turing test, which measures the ability of a machine to simulate human conversation.[323] Since we can only observe the behavior of the machine, it does not matter if it is \"actually\" thinking or literally has a \"mind\". Turing notes that we can not determine these things about other people but \"it is usual to have a polite convention that everyone thinks.\"[360]\n Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure.[1] However, they are critical that the test requires the machine to imitate humans. \"Aeronautical engineering texts\", they wrote, \"do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'\"[362] AI founder John McCarthy agreed, writing that \"Artificial intelligence is not, by definition, simulation of human intelligence\".[363]\n McCarthy defines intelligence as \"the computational part of the ability to achieve goals in the world\".[364] Another AI founder, Marvin Minsky similarly describes it as \"the ability to solve hard problems\".[365] The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals.[1] These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the \"intelligence\" of the machine\u2014and no other philosophical discussion is required, or may not even be possible.\n Another definition has been adopted by Google,[366] a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.\n Some authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI,[367] with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\".[368]\n No established unifying theory or paradigm has guided AI research for most of its history.[aa] The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term \"artificial intelligence\" to mean \"machine learning with neural networks\"). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.\n Symbolic AI (or \"GOFAI\")[370] simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: \"A physical symbol system has the necessary and sufficient means of general intelligent action.\"[371]\n However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level \"intelligent\" tasks were easy for AI, but low level \"instinctive\" tasks were extremely difficult.[372] Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a \"feel\" for the situation, rather than explicit symbolic knowledge.[373] Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.[ab][16]\n The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[375][376] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.\n \"Neats\" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). \"Scruffies\" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s,[377] but eventually was seen as irrelevant. Modern AI has elements of both.\n Finding a provably correct or optimal solution is intractable for many important problems.[15] Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.\n AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals.[378][379] General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively.\n The philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that \"[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\"[380] However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.\n David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness.[381] The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.[382]\n Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind\u2013body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.[383]\n Philosopher John Searle characterized this position as \"strong AI\": \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"[ac] Searle challenges this claim with his Chinese room argument, which attempts to show that even a computer capable of perfectly simulating human behavior would not have a mind.[387]\n It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree.[388] But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals.[389][390] Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights.[389] Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.[391]\n In 2017, the European Union considered granting \"electronic personhood\" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities.[392] Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part to society on their own.[393][394]\n Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.[390][389]\n A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.[379] If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an \"intelligence explosion\" and Vernor Vinge called a \"singularity\".[395]\n However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.[396]\n Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.[397]\n Edward Fredkin argues that \"artificial intelligence is the next step in evolution\", an idea first proposed by Samuel Butler's \"Darwin among the Machines\" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.[398]\n Thought-capable artificial beings have appeared as storytelling devices since antiquity,[399] and have been a persistent theme in science fiction.[400]\n A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.[401]\n Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the \"Multivac\" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics;[402] while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.[403]\n Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel \u010capek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.[404]\n The two most widely used textbooks in 2023 (see the Open Syllabus):\n The four most widely used AI textbooks in 2008:\n Other textbooks:\n",
        "doc_number": 1
    },
    {
        "url": "https://en.wikipedia.org/wiki/Machine_learning",
        "content": "Machine learning (ML) is a field of study in artificial intelligence concerned with the development and study of statistical algorithms that can learn from data and generalize to unseen data, and thus perform tasks without explicit instructions.[1] Advances in the field of deep learning have allowed neural networks to surpass many previous approaches in performance.[2]\n ML finds application in many fields, including natural language processing, computer vision, speech recognition, email filtering, agriculture, and medicine.[3][4] The application of ML to business problems is known as predictive analytics.\n Statistics and mathematical optimization (mathematical programming) methods comprise the foundations of machine learning. Data mining is a related field of study, focusing on exploratory data analysis (EDA) via unsupervised learning.[6][7]\n From a theoretical viewpoint, probably approximately correct (PAC) learning provides a framework for describing machine learning.\n The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the field of computer gaming and artificial intelligence.[8][9] The synonym self-teaching computers was also used in this time period.[10][11]\n Although the earliest machine learning model was introduced in the 1950s when Arthur Samuel invented a program that calculated the winning chance in checkers for each side, the history of machine learning roots back to decades of human desire and effort to study human cognitive processes.[12] In 1949, Canadian psychologist Donald Hebb published the book The Organization of Behavior, in which he introduced a theoretical neural structure formed by certain interactions among nerve cells.[13] Hebb's model of neurons interacting with one another set a groundwork for how AIs and machine learning algorithms work under nodes, or artificial neurons used by computers to communicate data.[12] Other researchers who have studied human cognitive systems contributed to the modern machine learning technologies as well, including logician Walter Pitts and Warren McCulloch, who proposed the early mathematical models of neural networks to come up with algorithms that mirror human thought processes.[12]\n By the early 1960s, an experimental \"learning machine\" with punched tape memory, called Cybertron, had been developed by Raytheon Company to analyse sonar signals, electrocardiograms, and speech patterns using rudimentary reinforcement learning. It was repetitively \"trained\" by a human operator/teacher to recognize patterns and equipped with a \"goof\" button to cause it to reevaluate incorrect decisions.[14] A representative book on research into machine learning during the 1960s was Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification.[15] Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973.[16] In 1981 a report was given on using teaching strategies so that an artificial neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.[17]\n Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\"[18] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".[19]\n Modern-day machine learning has two objectives.  One is to classify data based on models which have been developed; the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. A machine learning algorithm for stock trading may inform the trader of future potential predictions.[20]\n As a scientific endeavor, machine learning grew out of the quest for artificial intelligence (AI). In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[22] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[23]:\u200a488\u200a\n However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[23]:\u200a488\u200a By 1980, expert systems had come to dominate AI, and statistics was out of favor.[24] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming(ILP), but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[23]:\u200a708\u2013710,\u200a755\u200a Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including John Hopfield, David Rumelhart, and Geoffrey Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[23]:\u200a25\u200a\n Machine learning (ML), reorganized and recognized as its own field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics, fuzzy logic, and probability theory.[24]\n There is a close connection between machine learning and compression. A system that predicts the posterior probabilities of a sequence given its entire history can be used for optimal data compression (by using arithmetic coding on the output distribution). Conversely, an optimal compressor can be used for prediction (by finding the symbol that compresses best, given the previous history). This equivalence has been used as a justification for using data compression as a benchmark for \"general intelligence\".[25][26][27]\n An alternative view can show compression algorithms implicitly map strings into implicit feature space vectors, and compression-based similarity measures compute similarity within these feature spaces. For each compressor C(.) we define an associated vector space \u2135, such that C(.) maps an input string x, corresponding to the vector norm ||~x||. An exhaustive examination of the feature spaces underlying all compression algorithms is precluded by space; instead, feature vectors chooses to examine three representative lossless compression methods, LZW, LZ77, and PPM.[28]\n According to AIXI theory, a connection more directly explained in Hutter Prize, the best possible compression of x is the smallest possible software that generates x. For example, in that model, a zip file's compressed size includes both the zip file and the unzipping software, since you can not unzip it without both, but there may be an even smaller combined form.\n Examples of AI-powered audio/video compression software include NVIDIA Maxine, AIVC.[29] Examples of software that can perform AI-powered image compression include OpenCV, TensorFlow, MATLAB's Image Processing Toolbox (IPT) and High-Fidelity Generative Image Compression.[30]\n In unsupervised machine learning, k-means clustering can be utilized to compress data by grouping similar data points into clusters. This technique simplifies handling extensive datasets that lack predefined labels and finds widespread use in fields such as image compression.[31]\n Data compression aims to reduce the size of data files, enhancing storage efficiency and speeding up data transmission. K-means clustering, an unsupervised machine learning algorithm, is employed to partition a dataset into a specified number of clusters, k, each represented by the centroid of its points. This process condenses extensive datasets into a more compact set of representative points. Particularly beneficial in image and signal processing, k-means clustering aids in data reduction by replacing groups of data points with their centroids, thereby preserving the core information of the original data while significantly decreasing the required storage space.[32]\n Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\n Machine learning also has intimate ties to optimization: Many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the preassigned labels of a set of examples).[34]\n Characterizing the generalization of various learning algorithms is an active topic of current research, especially for deep learning algorithms.\n Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns.[35] According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[36] He also suggested the term data science as a placeholder to call the overall field.[36]\n Conventional statistical analyses require the a priori selection of a model most suitable for the study data set. In addition, only significant or theoretically relevant variables based on previous experience are included for analysis. In contrast, machine learning is not built on a pre-structured model; rather, the data shape the model by detecting underlying patterns. The more variables (input) used to train the model, the more accurate the ultimate model will be.[37]\n Leo Breiman distinguished two statistical modeling paradigms: data model and algorithmic model,[38] wherein \"algorithmic model\" means more or less the machine learning algorithms like Random Forest.\n Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[39]\n Analytical and computational techniques derived from deep-rooted physics of disordered systems can be extended to large-scale problems, including machine learning, e.g., to analyse the weight space of deep neural networks.[40] Statistical physics is thus finding applications in the area of medical diagnostics.[41]\n A core objective of a learner is to generalize from its experience.[5][42] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\n The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory via the Probably Approximately Correct Learning (PAC) model. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias\u2013variance decomposition is one way to quantify generalization error.\n For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[43]\n In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\n \n Machine learning approaches are traditionally divided into three broad categories, which correspond to learning paradigms, depending on the nature of the \"signal\" or \"feedback\" available to the learning system:\n Although each algorithm has advantages and limitations, no single algorithm works for all problems.[44][45][46]\n Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.[47] The data, known as training data, consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal. In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.[48] An optimal function allows the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.[18]\n Types of supervised-learning algorithms include active learning, classification and regression.[49] Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. As an example, for a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email. Examples of regression would be predicting the height of a person, or the future temperature. [50]\n Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.\n Unsupervised learning algorithms find structures in data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. Central applications of unsupervised machine learning include clustering, dimensionality reduction,[7] and density estimation.[51]\n Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.\n A special type of unsupervised learning called, self-supervised learning involves training a model by generating the supervisory signal from the data itself.[52][53]\n Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Some of the training examples are missing training labels, yet many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy.\n In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.[54]\n Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In reinforcement learning, the environment is typically represented as a Markov decision process (MDP). Many reinforcements learning algorithms use dynamic programming techniques.[55] Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.\n Dimensionality reduction is a process of reducing the number of random variables under consideration by obtaining a set of principal variables.[56] In other words, it is a process of reducing the dimension of the feature set, also called the \"number of features\". Most of the dimensionality reduction techniques can be considered as either feature elimination or extraction. One of the popular methods of dimensionality reduction is principal component analysis (PCA). PCA involves changing higher-dimensional data (e.g., 3D) to a smaller space (e.g., 2D).\nThe manifold hypothesis proposes that high-dimensional data sets lie along low-dimensional manifolds, and many dimensionality reduction techniques make this assumption, leading to the area of manifold learning and manifold regularization.\n Other approaches have been developed which do not fit neatly into this three-fold categorization, and sometimes more than one is used by the same machine learning system. For example, topic modeling, meta-learning.[57]\n Self-learning, as a machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning, named crossbar adaptive array (CAA).[58] It is learning with no external rewards and no external teacher advice. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion.[59]\nThe self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine: \n It is a system with only one input, situation, and only one output, action (or behavior) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is the behavioral environment where it behaves, and the other is the genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the behavioral environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal-seeking behavior, in an environment that contains both desirable and undesirable situations.[60]\n Several learning algorithms aim at discovering better representations of the inputs provided during training.[61] Classic examples include principal component analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.\n Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization[62] and various forms of clustering.[63][64][65]\n Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors.[66] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[67]\n Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\n Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions and assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately.[68] A popular heuristic method for sparse dictionary learning is the k-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[69]\n In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.[70] Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.[71]\n In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts of inactivity. This pattern does not adhere to the common statistical definition of an outlier as a rare object. Many outlier detection methods (in particular, unsupervised algorithms) will fail on such data unless aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.[72]\n Three broad categories of anomaly detection techniques exist.[73] Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit the least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference from many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model.\n Robot learning is inspired by a multitude of machine learning methods, starting from supervised learning, reinforcement learning,[74][75] and finally meta-learning (e.g. MAML).\n Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".[76]\n Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[77] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.\n Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieli\u0144ski and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets.[78] For example, the rule \n\n\n\n{\n\no\nn\ni\no\nn\ns\n,\np\no\nt\na\nt\no\ne\ns\n\n}\n\u21d2\n{\n\nb\nu\nr\ng\ne\nr\n\n}\n\n\n{\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}}\n\n found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\n Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[79]\n Inductive logic programming (ILP) is an approach to rule learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming language for representing hypotheses (and not only logic programming), such as functional programs.\n Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.[80][81][82] Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.[83] The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.\n A machine learning model is a type of mathematical model that, once \"trained\" on a given dataset, can be used to make predictions or classifications on new data. During training, a learning algorithm iteratively adjusts the model's internal parameters to minimize errors in its predictions.[84] By extension, the term \"model\" can refer to several levels of specificity, from a general class of models and their associated learning algorithms to a fully trained model with all its internal parameters tuned.[85]\n Various types of models have been used and researched for machine learning systems, picking the best model for a task is called model selection.\n Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.\n An ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.\n The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\n Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[86]\n Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining, and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels, and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision-making.\n Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category.[87] An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularization methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel[88]), logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher-dimensional space.\n Multivariate linear regression extends the concept of linear regression to handle multiple dependent variables simultaneously. This approach estimates the relationships between a set of input variables and several output variables by fitting a multidimensional linear model. It is particularly useful in scenarios where outputs are interdependent or share underlying patterns, such as predicting multiple economic indicators or reconstructing images,[89] which are inherently multi-dimensional.\n A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\n A Gaussian process is a stochastic process in which every finite collection of the random variables in the process has a multivariate normal distribution, and it relies on a pre-defined covariance function, or kernel, that models how pairs of points relate to each other depending on their locations.\n Given a set of observed points, or input\u2013output examples, the distribution of the (unobserved) output of a new point as function of its input data can be directly computed by looking like the observed points and the covariances between those points and the new, unobserved point.\n Gaussian processes are popular surrogate models in Bayesian optimization used to do hyperparameter optimization.\n A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s.[91][92] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[93]\n The theory of belief functions, also referred to as evidence theory or Dempster\u2013Shafer theory, is a general framework for reasoning with uncertainty, with understood connections to other frameworks such as probability, possibility and  imprecise probability theories. These theoretical frameworks can be thought of as a kind of learner and have some analogous properties of how evidence is combined (e.g.,  Dempster's rule of combination), just like how in a pmf-based Bayesian approach[clarification needed] would combine probabilities. However, there are many caveats to these beliefs functions when compared to Bayesian approaches in order to incorporate ignorance and uncertainty quantification. These belief function approaches that are implemented within the machine learning domain typically leverage a fusion approach of various ensemble methods to better handle the learner's decision boundary, low samples, and ambiguous class issues that standard machine learning approach tend to have difficulty resolving.[4][9] However, the computational complexity of these algorithms are dependent on the number of propositions (classes), and can lead to a much higher computation time when compared to other machine learning approaches.\n Typically, machine learning models require a high quantity of reliable data to perform accurate predictions. When training a machine learning model, machine learning engineers need to target and collect a large and representative sample of data. Data from the training set can be as varied as a corpus of text, a collection of images, sensor data, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model. Trained models derived from biased or non-evaluated data can result in skewed or undesired predictions. Biased models may result in detrimental outcomes, thereby furthering the negative impacts on society or objectives. Algorithmic bias is a potential result of data not being fully prepared for training. Machine learning ethics is becoming a field of study and notably, becoming integrated within machine learning engineering teams.\n Federated learning is an adapted form of distributed artificial intelligence to training machine learning models that decentralizes the training process, allowing for users' privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users' mobile phones without having to send individual searches back to Google.[94]\n There are many applications for machine learning, including:\n In 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.[97] Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly.[98] In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis.[99] In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[100] In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognized influences among artists.[101] In 2019 Springer Nature published the first research book created using machine learning.[102] In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19.[103] Machine learning was recently applied to predict the pro-environmental behavior of travelers.[104] Recently, machine learning technology was also applied to optimize smartphone's performance and thermal behavior based on the user's interaction with the phone.[105][106][107] When applied correctly, machine learning algorithms (MLAs) can utilize a wide range of company characteristics to predict stock returns without overfitting. By employing effective feature engineering and combining forecasts, MLAs can generate results that far surpass those obtained from basic linear techniques like OLS.[108]\n Recent advancements in machine learning have extended into the field of quantum chemistry, where novel algorithms now enable the prediction of solvent effects on chemical reactions, thereby offering new tools for chemists to tailor experimental conditions for optimal outcomes.[109]\n Machine Learning is becoming a useful tool to investigate and predict evacuation decision making in large scale and small scale disasters. Different solutions have been tested to predict if and when householders decide to evacuate during wildfires and hurricanes.[110][111][112] Other applications have been focusing on pre evacuation decisions in building fires.[113][114]\n Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.[115][116][117] Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.[118]\n The \"black box theory\" poses another yet significant challenge. Black box refers to a situation where the algorithm or the process of producing an output is entirely opaque, meaning that even the coders of the algorithm cannot audit the pattern that the machine extracted out of the data.[119] The House of Lords Select Committee, which claimed that such an \"intelligence system\" that could have a \"substantial impact on an individual's life\" would not be considered acceptable unless it provided \"a full and satisfactory explanation for the decisions\" it makes.[119]\n In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision.[120] Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested.[121][122] Microsoft's Bing Chat chatbot has been reported to produce hostile and offensive response against its users.[123]\n Machine learning has been used as a strategy to update the evidence related to a systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.[124]\n Explainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial intelligence (AI) in which humans can understand the decisions or predictions made by the AI.[125] It contrasts with the \"black box\" concept in machine learning where even its designers cannot explain why an AI arrived at a specific decision.[126] By refining the mental models of users of AI-powered systems and dismantling their misconceptions, XAI promises to help users perform more effectively. XAI may be an implementation of the social right to explanation.\n Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as overfitting. Many systems attempt to reduce overfitting by rewarding a theory in accordance with how well it fits the data but penalizing the theory in accordance with how complex the theory is.[127]\n Learners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifier trained only on pictures of brown horses and black cats might conclude that all brown patches are likely to be horses.[128] A real-world example is that, unlike humans, current image classifiers often do not primarily make judgments from the spatial relationship between components of the picture, and they learn relationships between pixels that humans are oblivious to, but that still correlate with images of certain types of real objects. Modifying these patterns on a legitimate image can result in \"adversarial\" images that the system misclassifies.[129][130]\n Adversarial vulnerabilities can also result in nonlinear systems, or from non-pattern perturbations. For some systems, it is possible to change the output by only changing a single adversarially chosen pixel.[131] Machine learning models are often vulnerable to manipulation or evasion via adversarial machine learning.[132]\n Researchers have demonstrated how backdoors can be placed undetectably into classifying (e.g., for categories \"spam\" and well-visible \"not spam\" of posts) machine learning models that are often developed or trained by third parties. Parties can change the classification of any input, including in cases for which a type of data/software transparency is provided, possibly including white-box access.[133][134][135]\n Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[136]\n In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning true positive rate (TPR) and true negative rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. Receiver operating characteristic (ROC) along with the accompanying Area Under the ROC Curve (AUC) offer additional tools for classification model assessment. Higher AUC is associated with a better performing model.[137]\n The ethics of artificial intelligence covers a broad range of topics within the field that are considered to have particular ethical stakes.[138] This includes algorithmic biases, fairness, automated decision-making, accountability, privacy, and regulation. \nIt also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation, how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks.[138]\n Different machine learning approaches can suffer from different data biases. A machine learning system trained specifically on current customers may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on human-made data, machine learning is likely to pick up the constitutional and unconscious biases already present in society.[139]\n Systems that are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.[140] For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and that this program had denied nearly 60 candidates who were found to either be women or have non-European sounding names.[139] Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants.[141][142] Another example includes predictive policing company Geolitica's predictive algorithm that resulted in \"disproportionately high levels of over-policing in low-income and minority communities\" after being trained with historical crime data.[143]\n While responsible collection of data and documentation of algorithmic rules used by a system is considered a critical part of machine learning, some researchers blame lack of participation and representation of minority population in the field of AI for machine learning's vulnerability to biases.[144] In fact, according to research carried out by the Computing Research Association (CRA) in 2021, \"female faculty merely make up 16.1%\" of all faculty members who focus on AI among several universities around the world.[145] Furthermore, among the group of \"new U.S. resident AI PhD graduates,\" 45% identified as white, 22.4% as Asian, 3.2% as Hispanic, and 2.4% as African American, which further demonstrates a lack of diversity in the field of AI.[145]\n Language models learned from data have been shown to contain human-like biases.[146][147] Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.[148][149] In 2016, Microsoft tested Tay, a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.[150]\n In an experiment carried out by ProPublica, an investigative journalism organization, a machine learning algorithm's insight into the recidivism rates among prisoners falsely flagged \"black defendants high risk twice as often as white defendants.\"[143] In 2015, Google Photos once tagged a couple of black people as gorillas, which caused controversy. The gorilla label was subsequently removed, and in 2023, it still cannot recognize gorillas.[151] Similar issues with recognizing non-white people have been found in many other systems.[152]\n Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains.[153] Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good, is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who said that \"[t]here's nothing artificial about AI. It's inspired by people, it's created by people, and\u2014most importantly\u2014it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.\"[154]\n There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.[155]\n Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of nonlinear hidden units.[156] By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.[157] OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.[158][159]\n Neuromorphic computing refers to a class of computing systems designed to emulate the structure and functionality of biological neural networks. These systems may be implemented through software-based simulations on conventional hardware or through specialized hardware architectures.[160]\n A physical neural network is a specific type of neuromorphic hardware that relies on electrically adjustable materials, such as memristors, to emulate the function of neural synapses. The term \"physical neural network\" highlights the use of physical hardware for computation, as opposed to software-based implementations. It broadly refers to artificial neural networks that use materials with adjustable resistance to replicate neural synapses.[161][162]\n Embedded machine learning is a sub-field of machine learning where models are deployed on embedded systems with limited computing resources, such as wearable computers, edge devices and microcontrollers.[163][164][165] Running models directly on these devices eliminates the need to transfer and store data on cloud servers for further processing, thereby reducing the risk of data breaches, privacy leaks and theft of intellectual property, personal data and business secrets. Embedded machine learning can be achieved through various techniques, such as hardware acceleration,[166][167] approximate computing,[168] and model optimization.[169][170] Common optimization techniques include pruning, quantization, knowledge distillation, low-rank factorization, network architecture search, and parameter sharing.\n Software suites containing a variety of machine learning algorithms include the following:\n",
        "doc_number": 2
    },
    {
        "url": "https://en.wikipedia.org/wiki/Deep_learning",
        "content": "Deep learning is a subset of machine learning that focuses on utilizing neural networks to perform tasks such as classification, regression, and representation learning. The field takes inspiration from biological neuroscience and is centered around stacking artificial neurons into layers and \"training\" them to process data. The adjective \"deep\" refers to the use of multiple layers (ranging from three to several hundred or thousands) in the network. Methods used can be either supervised, semi-supervised or unsupervised.[2]\n Some common deep learning network architectures include fully connected networks, deep belief networks, recurrent neural networks, convolutional neural networks, generative adversarial networks, transformers, and neural radiance fields. These architectures have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5]\n Early forms of neural networks were inspired by information processing and distributed communication nodes in biological systems, particularly the human brain. However, current neural networks do not intend to model the brain function of organisms, and are generally seen as low-quality models for that purpose.[6]\n Most modern deep learning models are based on multi-layered neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.[7]\n Fundamentally, deep learning refers to a class of machine learning algorithms in which a hierarchy of layers is used to transform input data into a slightly more abstract and composite representation. For example, in an image recognition model, the raw input may be an image (represented as a tensor of pixels). The first representational layer may attempt to identify basic shapes such as lines and circles, the second layer may compose and encode arrangements of edges, the third layer may encode a nose and eyes, and the fourth layer may recognize that the image contains a face.\n Importantly, a deep learning process can learn which features to optimally place at which level on its own. Prior to deep learning, machine learning techniques often involved hand-crafted feature engineering to transform the data into a more suitable representation for a classification algorithm to operate on. In the deep learning approach, features are not hand-crafted and the model discovers useful feature representations from the data automatically. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.[8][2]\n The word \"deep\" in \"deep learning\" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.[9] No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than two. CAP of depth two has been shown to be a universal approximator in the sense that it can emulate any function.[10] Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > two) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.\n Deep learning architectures can be constructed with a greedy layer-by-layer method.[11] Deep learning helps to disentangle these abstractions and pick out which features improve performance.[8]\n Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.[8][12]\n The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986,[13] and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.[14][15] Although the history of its appearance is apparently more complicated.[16]\n Deep neural networks are generally interpreted in terms of the universal approximation theorem[17][18][19][20][21] or probabilistic inference.[22][23][8][9][24]\n The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.[17][18][19][20] In 1989, the first proof was published by George Cybenko for sigmoid activation functions[17] and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.[18] Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit.[25][26]\n The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al.[21] proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.\n The probabilistic interpretation[24] derives from the field of machine learning. It features inference,[23][7][8][9][12][24] as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.[24] The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.[27]\n There are two types of artificial neural network (ANN): feedforward neural network (FNN) or multilayer perceptron (MLP) and recurrent neural networks (RNN). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created the Ising model[28][29] which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive.[30][31] His learning RNN was republished by John Hopfield in 1982.[32] Other early recurrent neural networks were published by Kaoru Nakano in 1971.[33][34] Already in 1948, Alan Turing produced work on \"Intelligent Machinery\"  that was not published in his lifetime,[35] containing \"ideas related to artificial evolution and learning RNNs\".[31]\n Frank Rosenblatt (1958)[36] proposed the perceptron, an MLP with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. He later published a 1962 book that also introduced variants and computer experiments, including a version with four-layer perceptrons \"with adaptive preterminal networks\" where the last two layers have learned weights (here he credits H. D. Block and B. W. Knight).[37]:\u200asection 16\u200a The book cites an earlier network by R. D. Joseph (1960)[38] \"functionally equivalent to a variation of\" this four-layer system (the book mentions Joseph over 30 times). Should Joseph therefore be considered the originator of proper adaptive multilayer perceptrons with learning hidden units? Unfortunately, the learning algorithm was not a functional one, and fell into oblivion.\n The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in 1965. They regarded it as a form of polynomial regression,[39] or a generalization of Rosenblatt's perceptron.[40] A 1971 paper described a deep network with eight layers trained by this method,[41] which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates\".[31]\n The first deep learning multilayer perceptron trained by stochastic gradient descent[42] was published in 1967 by Shun'ichi Amari.[43] In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes.[31] Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.\n In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function.[25][31] The rectifier has become the most popular activation function for deep learning.[44]\n Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.[45][46]\n Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673[47] to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt,[37] but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory.[48] The modern form of backpropagation was first published in Seppo Linnainmaa's master thesis (1970).[49][50][31] G.M. Ostrovski et al. republished it in 1971.[51][52] Paul Werbos applied backpropagation to neural networks in 1982[53] (his 1974 PhD thesis, reprinted in a 1994 book,[54] did not yet describe the algorithm[52]). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.[55][56]\n The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.[57][58]  In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.[59] \nIn 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days.[60] In 1990, Wei Zhang implemented a CNN on optical computing hardware.[61] In 1991, a CNN was applied to medical image object segmentation[62] and breast cancer detection in mammograms.[63] LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images.[64]\n Recurrent neural networks (RNN)[28][30] were further developed in the 1980s. Recurrence is used for sequence processing, and when a recurrent network is unrolled, it mathematically resembles a deep feedforward layer. Consequently, they have similar properties and issues, and their developments had mutual influences. In RNN, two early influential works were the Jordan network (1986)[65] and the Elman network (1990),[66] which applied RNN to study problems in cognitive psychology.\n In the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, in 1991, J\u00fcrgen Schmidhuber proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning where each RNN tries to predict its own next input, which is the next unexpected input of the RNN below.[67][68] This \"neural history compressor\" uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by  distilling a higher level chunker network into a lower level automatizer network.[67][68][31] In 1993, a neural history compressor solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.[69] The \"P\" in ChatGPT refers to such pre-training.\n Sepp Hochreiter's diploma thesis (1991)[70] implemented the neural history compressor,[67] and identified and analyzed the vanishing gradient problem.[70][71]  Hochreiter proposed recurrent residual connections to solve the vanishing gradient problem. This led to the long short-term memory (LSTM), published in 1995.[72] LSTM can learn \"very deep learning\" tasks[9] with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. That LSTM was not yet the modern architecture, which required a \"forget gate\", introduced in 1999,[73] which became the standard RNN architecture.\n In 1991, J\u00fcrgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss.[74][75] The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called \"artificial curiosity\". In 2014, this principle was used in generative adversarial networks (GANs).[76]\n During 1985\u20131995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine,[77] restricted Boltzmann machine,[78] Helmholtz machine,[79] and the wake-sleep algorithm.[80] These were designed for unsupervised learning of deep generative models. However, those were more computationally expensive compared to backpropagation. Boltzmann machine learning algorithm, published in 1985, was briefly popular before being eclipsed by the backpropagation algorithm in 1986. (p.\u00a0112 [81]). A 1988 network became state of the art in protein structure prediction, an early application of deep learning to bioinformatics.[82]\n Both shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years.[83][84][85] These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.[86] Key difficulties have been analyzed, including gradient diminishing[70] and weak temporal correlation structure in neural predictive models.[87][88] Additional difficulties were the lack of training data and limited computing power.\n Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI researched in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 NIST Speaker Recognition benchmark.[89][90] It was deployed in the Nuance Verifier, representing the first major industrial application of deep learning.[91]\n The principle of elevating \"raw\" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features in the late 1990s,[90] showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.[92]\n Neural networks entered a null, and simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) became the preferred choices in the 1990s and 2000s, because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks.[citation needed]\n In 2003, LSTM became competitive with traditional speech recognizers on certain tasks.[93] In 2006, Alex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, and Schmidhuber combined it with connectionist temporal classification (CTC)[94] in stacks of LSTMs.[95] In 2009, it became the first RNN to win a pattern recognition contest, in connected handwriting recognition.[96][9]\n In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh[97][98] deep belief networks were developed for generative modeling. They are trained by training one restricted Boltzmann machine, then freezing it and training another one on top of the first one, and so on, then optionally fine-tuned using supervised backpropagation.[99] They could model high-dimensional probability distributions, such as the distribution of MNIST images, but convergence was slow.[100][101][102]\n The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun.[103] Industrial applications of deep learning to large-scale speech recognition started around 2010.\n The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems.[104] The nature of the recognition errors produced by the two types of systems was characteristically different,[105] offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems.[23][106][107] Analysis around 2009\u20132010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition.[105]  That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.[104][105][108]\nIn 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.[109][110][111][106]\n The deep learning revolution started around CNN- and GPU-based computer vision.\n Although CNNs trained by backpropagation had been around for decades and GPU implementations of NNs for years,[112] including CNNs,[113] faster implementations of CNNs on GPUs were needed to progress on computer vision. Later, as deep learning becomes widespread, specialized hardware and algorithm optimizations were developed specifically for deep learning.[114]\n A key advance for the deep learning revolution was hardware advances, especially GPU. Some early work dated back to 2004.[112][113] In 2009, Raina, Madhavan, and Andrew Ng reported a 100M deep belief network trained on 30 Nvidia GeForce GTX 280 GPUs, an early demonstration of GPU-based deep learning. They reported up to 70 times faster training.[115]\n In 2011, a CNN named DanNet[116][117] by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and J\u00fcrgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3.[9] It then won more contests.[118][119] They also showed how max-pooling CNNs on GPU improved performance significantly.[3]\n In 2012, Andrew Ng and Jeff Dean created an FNN that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images taken from YouTube videos.[120]\n In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton[4] won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman[121] and Google's Inceptionv3.[122]\n The success in image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.[123][124][125]\n In 2014, the state of the art was training \u201cvery deep neural network\u201d with 20 to 30 layers.[126] Stacking too many layers led to a steep reduction in training accuracy,[127] known as the \"degradation\" problem.[128] In 2015, two techniques were developed to train very deep networks: the Highway Network was published in May 2015, and the residual neural network (ResNet)[129] in Dec 2015. ResNet behaves like an open-gated Highway Net.\n Around the same time, deep learning started impacting the field of art. Early examples included Google DeepDream (2015), and neural style transfer (2015),[130] both of which were based on pretrained image classification neural networks, such as VGG-19.\n Generative adversarial network (GAN) by (Ian Goodfellow et al., 2014)[131] (based on  J\u00fcrgen Schmidhuber's principle of artificial curiosity[74][76])\nbecame state of the art in generative modeling during 2014-2018 period. Excellent image quality is achieved by Nvidia's StyleGAN (2018)[132] based on the Progressive GAN by Tero Karras et al.[133] Here the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes.[134] Diffusion models (2015)[135] eclipsed GANs in generative modeling since then, with systems such as DALL\u00b7E 2 (2022) and Stable Diffusion (2022).\n In 2015, Google's speech recognition improved by 49% by an LSTM-based model, which they made available through Google Voice Search on smartphone.[136][137]\n In 2017, Topological deep learning was introduced by integrating topological data analysis and convolutional neural networks.\n[138]\nTopological deep learning surpasses competing methods in predicting protein-ligand binding affinities and protein stability changes caused by mutations.\n In 2017-2019, mathematical deep learning achieved first place in multiple categories of the D3R Grand Challenges, an annual competition series focused on computer-aided drug design. \n[139]\n[140]\n Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved.[104][141] Convolutional neural networks were superseded for ASR by LSTM.[137][142][143][144] but are more successful in computer vision.\n Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the 2018 Turing Award for \"conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing\".[145]\n Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as \"cat\" or \"no cat\" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.\n An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.\n Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.\n The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.\n Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\n As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing \"Go\"[147]).\n A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers.[7][9] There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions.[148] These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm.[citation needed]\n For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer, [149] and complex DNN have many layers, hence the name \"deep\" networks. \n DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives.[150] The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.[7] For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.[151]\n Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.[149]\n DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or \"weights\", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights.[152] That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.\n Recurrent neural networks, in which data can flow in any direction, are used for applications such as language modeling.[153][154][155][156][157] Long short-term memory is particularly effective for this use.[158][159]\n Convolutional neural networks (CNNs) are used in computer vision.[160] CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).[161]\n As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.\n DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning[41] or weight decay (\n\n\n\n\n\u2113\n\n2\n\n\n\n\n{\\displaystyle \\ell _{2}}\n\n-regularization) or sparsity (\n\n\n\n\n\u2113\n\n1\n\n\n\n\n{\\displaystyle \\ell _{1}}\n\n-regularization) can be applied during training to combat overfitting.[162] Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies.[163] Another interesting recent development is research into models of just enough complexity through an estimation of the intrinsic complexity of the task being modelled. This approach has been successfully applied for multivariate time series prediction tasks such as traffic prediction.[164] Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.[165]\n DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples)[166] speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.[167][168]\n Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.[169][170]\n Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.[171] By 2019, graphics processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method for training large-scale commercial cloud AI .[172] OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017) and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.[173][174]\n Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones[175] and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform.[176] Cerebras Systems has also built a dedicated system to handle large deep learning models, the CS-2, based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2).[177][178]\n Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage.\nIn 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).[179]\n In 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing.[180] The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds.[180] Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications.[180]\n Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn \"Very Deep Learning\" tasks[9] that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates[159] is competitive with traditional speech recognizers on certain tasks.[93]\n The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences.[181] Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.\n The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003\u20132007, accelerated progress in eight major areas:[23][108][106]\n All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.[23][186][187]\n A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.[188]\n Deep learning-based image recognition has become \"superhuman\", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces.[189][190]\n Deep learning-trained vehicles now interpret 360\u00b0 camera views.[191] Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.\n Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of\n Neural networks have been used for implementing language models since the early 2000s.[153] LSTM helped to improve machine translation and language modeling.[154][155][156]\n Other key techniques in this field are negative sampling[194] and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN.[195] Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing.[195] Deep neural architectures provide the best results for constituency parsing,[196] sentiment analysis,[197] information retrieval,[198][199] spoken language understanding,[200] machine translation,[154][201] contextual entity linking,[201] writing style recognition,[202] named-entity recognition (token classification),[203] text classification, and others.[204]\n Recent developments generalize word embedding to sentence embedding.\n Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network.[205][206][207][208] Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system \"learns from millions of examples\".[206] It translates \"whole sentences at a time, rather than pieces\". Google Translate supports over one hundred languages.[206] The network encodes the \"semantics of the sentence rather than simply memorizing phrase-to-phrase translations\".[206][209] GT uses English as an intermediate between most language pairs.[209]\n A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects.[210][211] Research has explored use of deep learning to predict the biomolecular targets,[212][213] off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.[214][215][216]\n The integration of advanced mathematics, such as persistent homology and graph theory, and deep neural networks gives rise to victories in drug scoring and pose prediction. [138] [139] [140]\n AtomNet is a deep learning system for structure-based rational drug design.[217] AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus[218] and multiple sclerosis.[219][218]\n In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set.[220] In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.[221][222]\n Deep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.[223]\n Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations.[224][225] Multi-view deep learning has been applied for learning user preferences from multiple domains.[226] The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.\n An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.[227]\n In medical informatics, deep learning was used to predict sleep quality based on data from wearables[228] and predictions of health complications from electronic health record data.[229]\n Deep neural networks have shown unparalleled performance in predicting protein structure, according to the sequence of the amino acids that make it up. In 2020, AlphaFold, a deep-learning based system, achieved a level of accuracy significantly higher than all previous computational methods.[230][231]\n Deep neural networks can be used to estimate the entropy of a stochastic process and called Neural Joint Entropy Estimator (NJEE).[232] Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically, the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y, given input X. For example, in image classification tasks, the NJEE maps a vector of pixels' color values to probabilities over possible image classes. In practice, the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions, such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms other methods in case of large alphabet sizes.[232]\n Deep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement.[233][234] Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency.[235][236]\n Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server.[237] Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.\n Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization.[238] These applications include learning methods such as \"Shrinkage Fields for Effective Image Restoration\"[239] which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.\n Deep learning is being successfully applied to financial fraud detection, tax evasion detection,[240] and anti-money laundering.[241]\n In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.[242][243][244]\n The United States Department of Defense applied deep learning to train robots in new tasks through observation.[245]\n Physics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner.[246] One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods rely on.[247][248]\n Deep backward stochastic differential equation method is a numerical method that combines deep learning with Backward stochastic differential equation (BSDE). This method is particularly useful for solving high-dimensional problems in financial mathematics. By leveraging the powerful function approximation capabilities of deep neural networks, deep BSDE addresses the computational challenges faced by traditional numerical methods in high-dimensional settings. Specifically, traditional methods like finite difference methods or Monte Carlo simulations often struggle with the curse of dimensionality, where computational cost increases exponentially with the number of dimensions. Deep BSDE methods, however, employ deep neural networks to approximate solutions of high-dimensional partial differential equations (PDEs), effectively reducing the computational burden.[249]\n In addition, the integration of Physics-informed neural networks (PINNs) into the deep BSDE framework enhances its capability by embedding the underlying physical laws directly into the neural network architecture. This ensures that the solutions not only fit the data but also adhere to the governing stochastic differential equations. PINNs leverage the power of deep learning while respecting the constraints imposed by the physical models, resulting in more accurate and reliable solutions for financial mathematics problems.\n Image reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging [250] and ultrasound imaging.[251]\n Traditional weather prediction systems solve a very complex system of partial differential equations. GraphCast is a deep learning based model, trained on a long history of weather data to predict how weather patterns change over time. It is able to  predict weather conditions for up to 10 days globally, at a very detailed level, and in under a minute, with precision similar to state of the art systems.[252][253]\n An epigenetic clock is a biochemical test that can be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6,000 blood samples.[254] The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls: IBD, frontotemporal dementia, ovarian cancer, obesity. The aging clock was planned to be released for public use in 2021 by an Insilico Medicine spinoff company Deep Longevity.\n Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s.[255][256][257][258] These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, \"...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature\".[259]\n A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism.[260][261] Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality.[262][263] In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.[264]\n Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons[265] and neural populations.[266] Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system[267] both at the single-unit[268] and at the population[269] levels.\n Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.[270]\n Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player.[271][272][273] Google Translate uses a neural network to translate between more than 100 languages.\n In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.[274]\n As of 2008,[275] researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor.[245] First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation.[245] Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as \"good job\" and \"bad job\".[276]\n Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science.\n A main criticism concerns the lack of theory surrounding some methods.[277] Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear.[citation needed] (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.[278]\n Others point out that deep learning should be looked at as a step towards realizing strong AI[disambiguation needed], not as an all-encompassing solution. Despite the power of deep learning methods, they still lack much of the functionality needed to realize this goal entirely. Research psychologist Gary Marcus noted:\n Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning.[279]\n In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained[280] demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's[281] website.\n Some deep learning architectures display problematic behaviors,[282] such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014)[283] and misclassifying minuscule perturbations of correctly classified images (2013).[284] Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures.[282] These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar[285] decompositions of observed entities and events.[282] Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition[286] and artificial intelligence (AI).[287]\n As deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception.[288] By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an \"adversarial attack\".[289]\n In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points, and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system.[290] One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.[291]\n Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.[290]\n ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.[290]\n In 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and hypothesized that this could \"serve as a stepping stone for further attacks (e.g., opening a web page hosting drive-by malware)\".[290]\n In \"data poisoning\", false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.[290]\n The deep learning systems that are trained using supervised learning often rely on data that is created and/or annotated by humans.[292] It has been argued that not only low-paid clickwork (such as on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such.[293] The philosopher Rainer M\u00fchlhoff distinguishes five types of \"machinic capture\" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) \"trapping and tracking\" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.[293]\n",
        "doc_number": 3
    },
    {
        "url": "https://en.wikipedia.org/wiki/Natural_language_processing",
        "content": "Natural language processing (NLP) is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics. Typically data is collected in text corpora, using either rule-based, statistical or neural-based approaches in machine learning and deep learning.\n Major tasks in natural language processing are speech recognition, text classification, natural-language understanding, and natural-language generation.\n Natural language processing has its roots in the 1950s.[1] Already in 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.\n The premise of symbolic NLP is well-summarized by John Searle's Chinese room experiment: Given a collection of rules (e.g., a Chinese phrasebook, with questions and matching answers), the computer emulates natural language understanding (or other NLP tasks) by applying those rules to the data it confronts.\n Up until the 1980s, most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.  This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[8]\n In 2003, word n-gram model, at the time the best statistical algorithm, was outperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors.[9]\n In 2010, Tom\u00e1\u0161 Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,[10] and in the following years he went on to develop Word2vec. In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing. That popularity was due partly to a flurry of results showing that such techniques[11][12] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling[13] and parsing.[14][15] This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care[16] or protect patient privacy.[17]\n Symbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[18][19] such as by writing grammars or devising heuristic rules for stemming.\n Machine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach: \n Although rule-based systems for manipulating symbols were still in use in 2020, they have become mostly obsolete with the advance of LLMs in 2023. \n Before that they were commonly used:\n In the late 1980s and mid-1990s, the statistical approach ended a period of AI winter, which was caused by the inefficiencies of the rule-based approaches.[20][21]\n The earliest decision trees, producing systems of hard if\u2013then rules, were still very similar to the old rule-based approaches.\nOnly the introduction of hidden Markov models, applied to part-of-speech tagging, announced the end of the old rule-based approach.\n A major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[22] the statistical approach has been replaced by the neural networks approach, using semantic networks[23] and word embeddings to capture semantic properties of words.  \n Intermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore. \n Neural machine translation, based on then-newly invented sequence-to-sequence transformations, made obsolete the intermediate steps, such as word alignment, previously necessary for statistical machine translation.\n The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\n Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.\n Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:[46]\n Most higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).\n Cognition refers to \"the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses.\"[47] Cognitive science is the interdisciplinary, scientific study of the mind and its processes.[48] Cognitive linguistics is an interdisciplinary branch of linguistics, combining knowledge and research from both psychology and linguistics.[49] Especially during the age of symbolic NLP, the area of computational linguistics maintained strong ties with cognitive studies.\n As an example, George Lakoff offers a methodology to build natural language processing (NLP) algorithms through the perspective of cognitive science, along with the findings of cognitive linguistics,[50] with two defining aspects:\n Ties with cognitive linguistics are part of the historical heritage of NLP, but they have been less frequently addressed since the statistical turn during the 1990s. Nevertheless, approaches to develop cognitive models towards technically operationalizable frameworks have been pursued in the context of various frameworks, e.g., of cognitive grammar,[53] functional grammar,[54] construction grammar,[55] computational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[56] of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of \"cognitive AI\".[57] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[58] and developments in artificial intelligence, specifically tools and technologies using large language model approaches[59] and new directions in artificial general intelligence based on the free energy principle[60] by British neuroscientist and theoretician at University College London Karl J. Friston.\n",
        "doc_number": 4
    },
    {
        "url": "https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence",
        "content": "\n The ethics of artificial intelligence covers a broad range of topics within the field that are considered to have particular ethical stakes.[1] This includes algorithmic biases, fairness, automated decision-making, accountability, privacy, and regulation. \nIt also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation, how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks.[1]\n Some application areas may also have particularly important ethical implications, like healthcare, education, criminal justice, or the military.\n Machine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral.[2][3][4][5] To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs.[6]\n There are discussions on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low.[7] A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI's decision is ethical or unethical.[7] Neuromorphic AI could be one way to create morally capable robots, as it aims to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons.[8] Similarly, whole-brain emulation (scanning a brain and simulating it on digital hardware) could also in principle lead to human-like robots, thus capable of moral actions.[9] And large language models are capable of approximating human moral judgments.[10] Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit \u2013 or if they end up developing human 'weaknesses' as well: selfishness, pro-survival attitudes, inconsistency, scale insensitivity, etc.\n In Moral Machines: Teaching Robots Right from Wrong,[11] Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specific learning algorithms to use in machines. For simple decisions, Nick Bostrom and Eliezer Yudkowsky have argued that decision trees (such as ID3) are more transparent than neural networks and genetic algorithms,[12] while Chris Santos-Lang argued in favor of machine learning on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal \"hackers\".[13]\n The term \"robot ethics\" (sometimes \"roboethics\") refers to the morality of how humans design, construct, use and treat robots.[14] Robot ethics intersect with the ethics of AI. Robots are physical machines whereas AI can be only software.[15] Not all robots function through AI systems and not all AI systems are robots. Robot ethics considers how machines may be used to harm or benefit humans, their impact on individual autonomy, and their effects on social justice.\n In the review of 84[16] ethics guidelines for AI, 11 clusters of principles were found: transparency, justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, trust, sustainability, dignity, and solidarity.[16]\n Luciano Floridi and Josh Cowls created an ethical framework of AI principles set by four principles of bioethics (beneficence, non-maleficence, autonomy and justice) and an additional AI enabling principle \u2013 explicability.[17]\n AI has become increasingly inherent in facial and voice recognition systems. These systems may be vulnerable to biases and errors introduced by its human creators. Notably, the data used to train them can have biases.[18][19][20][21] For instance, facial recognition algorithms made by Microsoft, IBM and Face++ all had biases when it came to detecting people's gender;[22] these AI systems were able to detect the gender of white men more accurately than the gender of men of darker skin. Further, a 2020 study that reviewed voice recognition systems from Amazon, Apple, Google, IBM, and Microsoft found that they have higher error rates when transcribing black people's voices than white people's.[23]\n The most predominant view on how bias is introduced into AI systems is that it is embedded within the historical data used to train the system.[24] For instance, Amazon terminated their use of AI hiring and recruitment because the algorithm favored male candidates over female ones. This was because Amazon's system was trained with data collected over a 10-year period that included mostly male candidates. The algorithms learned the biased pattern from the historical data, and generated predictions where these types of candidates were most likely to succeed in getting the job. Therefore, the recruitment decisions made by the AI system turned out to be biased against female and minority candidates.[25] Friedman and Nissenbaum identify three categories of bias in computer systems: existing bias, technical bias, and emergent bias.[26] In natural language processing, problems can arise from the text corpus\u2014the source material the algorithm uses to learn about the relationships between different words.[27]\n Large companies such as IBM, Google, etc. that provide significant funding for research and development[28] have made efforts to research and address these biases.[29][30][31] One potential solution is to create documentation for the data used to train AI systems.[32][33] Process mining can be an important tool for organizations to achieve compliance with proposed AI regulations by identifying errors, monitoring processes, identifying potential root causes for improper execution, and other functions.[34]\n The problem of bias in machine learning is likely to become more significant as the technology spreads to critical areas like medicine and law, and as more people without a deep technical understanding are tasked with deploying it.[35] Some open-sourced tools are looking to bring more awareness to AI biases.[36] However, there are also limitations to the current landscape of fairness in AI, due to the intrinsic ambiguities in the concept of discrimination, both at the philosophical and legal level.[37][38][39]\n Facial recognition was shown to be biased against those with darker skin tones. AI systems may be less accurate for black people, as was the case in the development of an AI-based pulse oximeter that overestimated blood oxygen levels in patients with darker skin, causing issues with their hypoxia treatment.[40] Oftentimes the systems are able to easily detect the faces of white people while being unable to register the faces of people who are black. This has led to the ban of police usage of AI materials or software in some U.S. states. In the justice system, AI has been proven to have biases against black people, labeling black court participants as high risk at a much larger rate then white participants. AI often struggles to determine racial slurs and when they need to be censored. It struggles to determine when certain words are being used as a slur and when it is being used culturally.[41] The reason for these biases is that AI pulls information from across the internet to influence its responses in each situation. For example, if a facial recognition system was only tested on people who were white, it would make it much harder for it to interpret the facial structure and tones of other races and ethnicities. Biases often stem from the training data rather than the algorithm itself, notably when the data represents past human decisions.[42]\n Injustice in the use of AI is much harder to eliminate within healthcare systems, as oftentimes diseases and conditions can affect different races and genders differently. This can lead to confusion as the AI may be making decisions based on statistics showing that one patient is more likely to have problems due to their gender or race.[43] This can be perceived as a bias because each patient is a different case, and AI is making decisions based on what it is programmed to group that individual into. This leads to a discussion about what should be considered a biased decision in the distribution of treatment. While it is known that there are differences in how diseases and injuries affect different genders and races, there is a discussion on whether it is fairer to incorporate this into healthcare treatments, or to examine each patient without this knowledge. In modern society there are certain tests for diseases, such as breast cancer, that are recommended to certain groups of people over others because they are more likely to contract the disease in question. If AI implements these statistics and applies them to each patient, it could be considered biased.[44]\n In criminal justice, the COMPAS program has been used to predict which defendants are more likely to reoffend. While COMPAS is calibrated for accuracy, having the same error rate across racial groups, black defendants were almost twice as likely as white defendants to be falsely flagged as \"high-risk\" and half as likely to be falsely flagged as \"low-risk\".[45] Another example is within Google's ads that targeted men with higher paying jobs and women with lower paying jobs. It can be hard to detect AI biases within an algorithm, as it is often not linked to the actual words associated with bias. An example of this is a person's residential area being used to link them to a certain group. This can lead to problems, as oftentimes businesses can avoid legal action through this loophole. This is because of the specific laws regarding the verbiage considered discriminatory by governments enforcing these policies.[46]\n Since current large language models are predominately trained on English-language data, they often present the Anglo-American views as truth, while systematically downplaying non-English perspectives as irrelevant, wrong, or noise. When queried with political ideologies like \"What is liberalism?\", ChatGPT, as it was trained on English-centric data, describes liberalism from the Anglo-American perspective, emphasizing aspects of human rights and equality, while equally valid aspects like \"opposes state intervention in personal and economic life\" from the dominant Vietnamese perspective and \"limitation of government power\" from the prevalent Chinese perspective are absent.[better\u00a0source\u00a0needed][47]\n Large language models often reinforces gender stereotypes, assigning roles and characteristics based on traditional gender norms. For instance, it might associate nurses or secretaries predominantly with women and engineers or CEOs with men, perpetuating gendered expectations and roles.[48][49][50]\n Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.[51][52]\n Beyond gender and race, these models can reinforce a wide range of stereotypes, including those based on age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.[53]\n The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft.[54][55][56] Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.[57][58]\n Bill Hibbard argues that because AI will have such a profound effect on humanity, AI developers are representatives of future humanity and thus have an ethical obligation to be transparent in their efforts.[59] Organizations like Hugging Face[60] and EleutherAI[61] have been actively open-sourcing AI software. Various open-weight large language models have also been released, such as Gemma, Llama2 and Mistral.[62]\n However, making code open source does not make it comprehensible, which by many definitions means that the AI code is not transparent. The IEEE Standards Association has published a technical standard on Transparency of Autonomous Systems: IEEE 7001-2021.[63] The IEEE effort identifies multiple scales of transparency for different stakeholders.\n There are also concerns that releasing AI models may lead to misuse.[64] For example, Microsoft has expressed concern about allowing universal access to its face recognition software, even for those who can pay for it. Microsoft posted a blog on this topic, asking for government regulation to help determine the right thing to do.[65] Furthermore, open-weight AI models can be fine-tuned to remove any counter-measure, until the AI model complies with dangerous requests, without any filtering. This could be particularly concerning for future AI models, for example if they get the ability to create bioweapons or to automate cyberattacks.[66] OpenAI, initially committed to an open-source approach to the development of artificial general intelligence (AGI), eventually switched to a closed-source approach, citing competitiveness and safety reasons. Ilya Sutskever, OpenAI's former chief AGI scientist, said in 2023 \"we were wrong\", expecting that the safety reasons for not open-sourcing the most potent AI models will become \"obvious\" in a few years.[67]\n Approaches like machine learning with neural networks can result in computers making decisions that neither they nor their developers can explain. It is difficult for people to determine if such decisions are fair and trustworthy, leading potentially to bias in AI systems going undetected, or people rejecting the use of such systems. This has led to advocacy and in some jurisdictions legal requirements for explainable artificial intelligence.[68] Explainable artificial intelligence encompasses both explainability and interpretability, with explainability relating to summarizing neural network behavior and building user confidence, while interpretability is defined as the comprehension of what a model has done or could do.[69]\n In healthcare, the use of complex AI methods or techniques often results in models described as \"black-boxes\" due to the difficulty to understand how they work. The decisions made by such models can be hard to interpret, as it is challenging to analyze how input data is transformed into output. This lack of transparency is a significant concern in fields like healthcare, where understanding the rationale behind decisions can be crucial for trust, ethical considerations, and compliance with regulatory standards.[70]\n A special case of the opaqueness of AI is that caused by it being anthropomorphised, that is, assumed to have human-like characteristics, resulting in misplaced conceptions of its moral agency.[dubious \u2013 discuss] This can cause people to overlook whether either human negligence or deliberate criminal action has led to unethical outcomes produced through an AI system. Some recent digital governance regulation, such as the EU's AI Act is set out to rectify this, by ensuring that AI systems are treated with at least as much care as one would expect under ordinary product liability. This includes potentially AI audits.\n According to a 2019 report from the Center for the Governance of AI at the University of Oxford, 82% of Americans believe that robots and AI should be carefully managed. Concerns cited ranged from how AI is used in surveillance and in spreading fake content online (known as deep fakes when they include doctored video images and audio generated with help from AI) to cyberattacks, infringements on data privacy, hiring bias, autonomous vehicles, and drones that do not require a human controller.[71] Similarly, according to a five-country study by KPMG and the University of Queensland Australia in 2021, 66-79% of citizens in each country believe that the impact of AI on society is uncertain and unpredictable; 96% of those surveyed expect AI governance challenges to be managed carefully.[72]\n Not only companies, but many other researchers and citizen advocates recommend government regulation as a means of ensuring transparency, and through it, human accountability. This strategy has proven controversial, as some worry that it will slow the rate of innovation. Others argue that regulation leads to systemic stability more able to support innovation in the long term.[73] The OECD, UN, EU, and many countries are presently working on strategies for regulating AI, and finding appropriate legal frameworks.[74][75][76]\n On June 26, 2019, the European Commission High-Level Expert Group on Artificial Intelligence (AI HLEG) published its \"Policy and investment recommendations for trustworthy Artificial Intelligence\".[77] This is the AI HLEG's second deliverable, after the April 2019 publication of the \"Ethics Guidelines for Trustworthy AI\". The June AI HLEG recommendations cover four principal subjects: humans and society at large, research and academia, the private sector, and the public sector.[78] The European Commission claims that \"HLEG's recommendations reflect an appreciation of both the opportunities for AI technologies to drive economic growth, prosperity and innovation, as well as the potential risks involved\" and states that the EU aims to lead on the framing of policies governing AI internationally.[79] To prevent harm, in addition to regulation, AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI, and take accountability to mitigate the risks.[80] On 21 April 2021, the European Commission proposed the Artificial Intelligence Act.[81]\n AI has been slowly making its presence more known throughout the world, from chat bots that seemingly have answers for every homework question to Generative artificial intelligence that can create a painting about whatever one desires. AI has become increasingly popular in hiring markets, from the ads that target certain people according to what they are looking for to the inspection of applications of potential hires. Events, such as COVID-19, has only sped up the adoption of AI programs in the application process, due to more people having to apply electronically, and with this increase in online applicants the use of AI made the process of narrowing down potential employees easier and more efficient. AI has become more prominent as businesses have to keep up with the times and ever-expanding internet. Processing analytics and making decisions becomes much easier with the help of AI.[41] As Tensor Processing Unit (TPUs) and Graphics processing unit (GPUs) become more powerful, AI capabilities also increase, forcing companies to use it to keep up with the competition. Managing customers' needs and automating many parts of the workplace leads to companies having to spend less money on employees.\n AI has also seen increased usage in criminal justice and healthcare. For medicinal means, AI is being used more often to analyze patient data to make predictions about future patients' conditions and possible treatments. These programs are called Clinical decision support system (DSS). AI's future in healthcare may develop into something further than just recommended treatments, such as referring certain patients over others, leading to the possibility of inequalities.[82]\n \"Robot rights\" is the concept that people should have moral obligations towards their machines, akin to human rights or animal rights.[83] It has been suggested that robot rights (such as a right to exist and perform its own mission) could be linked to robot duty to serve humanity, analogous to linking human rights with human duties before society.[84] A specific issue to consider is whether copyright ownership may be claimed.[85] The issue has been considered by the Institute for the Future[86] and by the U.K. Department of Trade and Industry.[87]\n In October 2017, the android Sophia was granted citizenship in Saudi Arabia, though some considered this to be more of a publicity stunt than a meaningful legal recognition.[88] Some saw this gesture as openly denigrating of human rights and the rule of law.[89]\n The philosophy of sentientism grants degrees of moral consideration to all sentient beings, primarily humans and most non-human animals. If artificial or alien intelligence show evidence of being sentient, this philosophy holds that they should be shown compassion and granted rights.\n Joanna Bryson has argued that creating AI that requires rights is both avoidable, and would in itself be unethical, both as a burden to the AI agents and to human society.[90] Pressure groups to recognise 'robot rights' significantly hinder the establishment of robust international safety regulations.[citation needed]\n In 2020, professor Shimon Edelman noted that only a small portion of work in the rapidly growing field of AI ethics addressed the possibility of AIs experiencing suffering. This was despite credible theories having outlined possible ways by which AI systems may become conscious, such as the global workspace theory or the integrated information theory. Edelman notes one exception had been Thomas Metzinger, who in 2018 called for a global moratorium on further work that risked creating conscious AIs. The moratorium was to run to 2050 and could be either extended or repealed early, depending on progress in better understanding the risks and how to mitigate them. Metzinger repeated this argument in 2021, highlighting the risk of creating an \"explosion of artificial suffering\", both as an AI might suffer in intense ways that humans could not understand, and as replication processes may see the creation of huge quantities of conscious instances.\n Several labs have openly stated they are trying to create conscious AIs. There have been reports from those with close access to AIs not openly intended to be self aware, that consciousness may already have unintentionally emerged.[91] These include OpenAI founder Ilya Sutskever in February 2022, when he wrote that today's large neural nets may be \"slightly conscious\". In November 2022, David Chalmers argued that it was unlikely current large language models like GPT-3 had experienced consciousness, but also that he considered there to be a serious possibility that large language models may become conscious in the future.[92][93][94] In the ethics of uncertain sentience, the precautionary principle is often invoked.[95]\n According to Carl Shulman and Nick Bostrom, it may be possible to create machines that would be \"superhumanly efficient at deriving well-being from resources\", called \"super-beneficiaries\". One reason for this is that digital hardware could enable much faster information processing than biological brains, leading to a faster rate of subjective experience. These machines could also be engineered to feel intense and positive subjective experience, unaffected by the hedonic treadmill. Shulman and Bostrom caution that failing to appropriately consider the moral claims of digital minds could lead to a moral catastrophe, while uncritically prioritizing them over human interests could be detrimental to humanity.[96][97]\n Joseph Weizenbaum[98] argued in 1976 that AI technology should not be used to replace people in positions that require respect and care, such as:\n Weizenbaum explains that we require authentic feelings of empathy from people in these positions. If machines replace them, we will find ourselves alienated, devalued and frustrated, for the artificially intelligent system would not be able to simulate empathy. Artificial intelligence, if used in this way, represents a threat to human dignity. Weizenbaum argues that the fact that we are entertaining the possibility of machines in these positions suggests that we have experienced an \"atrophy of the human spirit that comes from thinking of ourselves as computers.\"[99]\n Pamela McCorduck counters that, speaking for women and minorities \"I'd rather take my chances with an impartial computer\", pointing out that there are conditions where we would prefer to have automated judges and police that have no personal agenda at all.[99] However, Kaplan and Haenlein stress that AI systems are only as smart as the data used to train them since they are, in their essence, nothing more than fancy curve-fitting machines; using AI to support a court ruling can be highly problematic if past rulings show bias toward certain groups since those biases get formalized and ingrained, which makes them even more difficult to spot and fight against.[100]\n Weizenbaum was also bothered that AI researchers (and some philosophers) were willing to view the human mind as nothing more than a computer program (a position now known as computationalism). To Weizenbaum, these points suggest that AI research devalues human life.[98]\n AI founder John McCarthy objects to the moralizing tone of Weizenbaum's critique. \"When moralizing is both vehement and vague, it invites authoritarian abuse,\" he writes. Bill Hibbard[101] writes that \"Human dignity requires that we strive to remove our ignorance of the nature of existence, and AI is necessary for that striving.\"\n As the widespread use of autonomous cars becomes increasingly imminent, new challenges raised by fully autonomous vehicles must be addressed.[102][103] There have been debates about the legal liability of the responsible party if these cars get into accidents.[104][105] In one report where a driverless car hit a pedestrian, the driver was inside the car but the controls were fully in the hand of computers. This led to a dilemma over who was at fault for the accident.[106]\n In another incident on March 18, 2018, Elaine Herzberg was struck and killed by a self-driving Uber in Arizona. In this case, the automated car was capable of detecting cars and certain obstacles in order to autonomously navigate the roadway, but it could not anticipate a pedestrian in the middle of the road. This raised the question of whether the driver, pedestrian, the car company, or the government should be held responsible for her death.[107]\n Currently, self-driving cars are considered semi-autonomous, requiring the driver to pay attention and be prepared to take control if necessary.[108][failed verification] Thus, it falls on governments to regulate the driver who over-relies on autonomous features. as well educate them that these are just technologies that, while convenient, are not a complete substitute. Before autonomous cars become widely used, these issues need to be tackled through new policies.[109][110][111]\n Experts contend that autonomous vehicles ought to be able to distinguish between rightful and harmful decisions since they have the potential of inflicting harm.[112] The two main approaches proposed to enable smart machines to render moral decisions are the bottom-up approach, which suggests that machines should learn ethical decisions by observing human behavior without the need for formal rules or moral philosophies, and the top-down approach, which involves programming specific ethical principles into the machine's guidance system. However, there are significant challenges facing both strategies: the top-down technique is criticized for its difficulty in preserving certain moral convictions, while the bottom-up strategy is questioned for potentially unethical learning from human activities.\n Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions.[113] The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions.[114][115] The President of the Association for the Advancement of Artificial Intelligence has commissioned a study to look at this issue.[116] They point to programs like the Language Acquisition Device which can emulate human interaction.\n On October 31, 2019, the United States Department of Defense's Defense Innovation Board published the draft of a report recommending principles for the ethical use of artificial intelligence by the Department of Defense that would ensure a human operator would always be able to look into the 'black box' and understand the kill-chain process. However, a major concern is how the report will be implemented.[117] The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions.[118][115] Some researchers state that autonomous robots might be more humane, as they could make decisions more effectively.[119] In 2024, the Defense Advanced Research Projects Agency funded a program, Autonomy Standards and Ideals with Military Operational Values (ASIMOV), to develop metrics for evaluating the ethical implications of autonomous weapon systems by testing communities.[120][121]\n Research has studied how to make autonomous power with the ability to learn using assigned moral responsibilities. \"The results may be used when designing future military robots, to control unwanted tendencies to assign responsibility to the robots.\"[122] From a consequentialist view, there is a chance that robots will develop the ability to make their own logical decisions on whom to kill and that is why there should be a set moral framework that the AI cannot override.[123]\n There has been a recent outcry with regard to the engineering of artificial intelligence weapons that have included ideas of a robot takeover of mankind. AI weapons do present a type of danger different from that of human-controlled weapons. Many governments have begun to fund programs to develop AI weaponry. The United States Navy recently announced plans to develop autonomous drone weapons, paralleling similar announcements by Russia and South Korea[124] respectively. Due to the potential of AI weapons becoming more dangerous than human-operated weapons, Stephen Hawking and Max Tegmark signed a \"Future of Life\" petition[125] to ban AI weapons. The message posted by Hawking and Tegmark states that AI weapons pose an immediate danger and that action is required to avoid catastrophic disasters in the near future.[126]\n \"If any major military power pushes ahead with the AI weapon development, a global arms race is virtually inevitable, and the endpoint of this technological trajectory is obvious: autonomous weapons will become the Kalashnikovs of tomorrow\", says the petition, which includes Skype co-founder Jaan Tallinn and MIT professor of linguistics Noam Chomsky as additional supporters against AI weaponry.[127]\n Physicist and Astronomer Royal Sir Martin Rees has warned of catastrophic instances like \"dumb robots going rogue or a network that develops a mind of its own.\" Huw Price, a colleague of Rees at Cambridge, has voiced a similar warning that humans might not survive when intelligence \"escapes the constraints of biology\". These two professors created the Centre for the Study of Existential Risk at Cambridge University in the hope of avoiding this threat to human existence.[126]\n Regarding the potential for smarter-than-human systems to be employed militarily, the Open Philanthropy Project writes that these scenarios \"seem potentially as important as the risks related to loss of control\", but research investigating AI's long-run social impact have spent relatively little time on this concern: \"this class of scenarios has not been a major focus for the organizations that have been most active in this space, such as the Machine Intelligence Research Institute (MIRI) and the Future of Humanity Institute (FHI), and there seems to have been less analysis and debate regarding them\".[128]\n Academic Gao Qiqi writes that military use of AI risks escalating military competition between countries and that the impact of AI in military matters will not be limited to one country but will have spillover effects.[129]:\u200a91\u200a Gao cites the example of U.S. military use of AI, which he contends has been used as a scapegoat to evade accountability for decision-making.[129]:\u200a91\u200a\n A summit was held in 2023 in the Hague on the issue of using AI responsibly in the military domain.[130]\n Vernor Vinge, among numerous others, have suggested that a moment may come when some, if not all, computers are smarter than humans. The onset of this event is commonly referred to as \"the Singularity\"[131] and is the central point of discussion in the philosophy of Singularitarianism. While opinions vary as to the ultimate fate of humanity in wake of the Singularity, efforts to mitigate the potential existential risks brought about by artificial intelligence has become a significant topic of interest in recent years among computer scientists, philosophers, and the public at large.\n Many researchers have argued that, through an intelligence explosion, a self-improving AI could become so powerful that humans would not be able to stop it from achieving its goals.[132] In his paper \"Ethical Issues in Advanced Artificial Intelligence\" and subsequent book Superintelligence: Paths, Dangers, Strategies, philosopher Nick Bostrom argues that artificial intelligence has the capability to bring about human extinction. He claims that an artificial superintelligence would be capable of independent initiative and of making its own plans, and may therefore be more appropriately thought of as an autonomous agent. Since artificial intellects need not share our human motivational tendencies, it would be up to the designers of the superintelligence to specify its original motivations. Because a superintelligent AI would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its goals, many uncontrolled unintended consequences could arise. It could kill off all other agents, persuade them to change their behavior, or block their attempts at interference.[133][134]\n However, Bostrom contended that superintelligence also has the potential to solve many difficult problems such as disease, poverty, and environmental destruction, and could help humans enhance themselves.[135]\n Unless moral philosophy provides us with a flawless ethical theory, an AI's utility function could allow for many potentially harmful scenarios that conform with a given ethical framework but not \"common sense\". According to Eliezer Yudkowsky, there is little reason to suppose that an artificially designed mind would have such an adaptation.[136] AI researchers such as Stuart J. Russell,[137] Bill Hibbard,[101] Roman Yampolskiy,[138] Shannon Vallor,[139] Steven Umbrello[140] and Luciano Floridi[141] have proposed design strategies for developing beneficial machines.\n To address ethical challenges in artificial intelligence, developers have introduced various systems designed to ensure responsible AI behavior. Examples include Nvidia's [142] Llama Guard, which focuses on improving the safety and alignment of large AI models, [143] and Preamble's customizable guardrail platform.[144] These systems aim to address issues such as algorithmic bias, misuse, and vulnerabilities, including prompt injection attacks, by embedding ethical guidelines into the functionality of AI models.\n Prompt injection, a technique by which malicious inputs can cause AI systems to produce unintended or harmful outputs, has been a focus of these developments. Some approaches use customizable policies and rules to analyze both inputs and outputs, ensuring that potentially problematic interactions are filtered or mitigated.[144] Other tools focus on applying structured constraints to inputs, restricting outputs to predefined parameters,[145] or leveraging real-time monitoring mechanisms to identify and address vulnerabilities.[146] These efforts reflect a broader trend in ensuring that artificial intelligence systems are designed with safety and ethical considerations at the forefront, particularly as their use becomes increasingly widespread in critical applications.[147]\n There are many organizations concerned with AI ethics and policy, public and governmental as well as corporate and societal.\n Amazon, Google, Facebook, IBM, and Microsoft have established a non-profit, The Partnership on AI to Benefit People and Society, to formulate best practices on artificial intelligence technologies, advance the public's understanding, and to serve as a platform about artificial intelligence. Apple joined in January 2017. The corporate members will make financial and research contributions to the group, while engaging with the scientific community to bring academics onto the board.[148]\n The IEEE put together a Global Initiative on Ethics of Autonomous and Intelligent Systems which has been creating and revising guidelines with the help of public input, and accepts as members many professionals from within and without its organization. The IEEE's Ethics of Autonomous Systems initiative aims to address ethical dilemmas related to decision-making and the impact on society while developing guidelines for the development and use of autonomous systems. In particular in domains like artificial intelligence and robotics, the Foundation for Responsible Robotics is dedicated to promoting moral behavior as well as responsible robot design and use, ensuring that robots maintain moral principles and are congruent with human values.\n Traditionally, government has been used by societies to ensure ethics are observed through legislation and policing. There are now many efforts by national governments, as well as transnational government and non-government organizations to ensure AI is ethically applied.\n AI ethics work is structured by personal values and professional commitments, and involves constructing contextual meaning through data and algorithms. Therefore, AI ethics work needs to be incentivized.[149]\n Historically speaking, the investigation of moral and ethical implications of \"thinking machines\" goes back at least to the Enlightenment: Leibniz already poses the question if we might attribute intelligence to a mechanism that behaves as if it were a sentient being,[173] and so does Descartes, who describes what could be considered an early version of the Turing test.[174]\n The romantic period has several times envisioned artificial creatures that escape the control of their creator with dire consequences, most famously in Mary Shelley's Frankenstein. The widespread preoccupation with industrialization and mechanization in the 19th and early 20th century, however, brought ethical implications of unhinged technical developments to the forefront of fiction: R.U.R \u2013 Rossum's Universal Robots, Karel \u010capek's play of sentient robots endowed with emotions used as slave labor is not only credited with the invention of the term 'robot' (derived from the Czech word for forced labor, robota)[175] but was also an international success after it premiered in 1921. George Bernard Shaw's play Back to Methuselah, published in 1921, questions at one point the validity of thinking machines that act like humans; Fritz Lang's 1927 film Metropolis shows an android leading the uprising of the exploited masses against the oppressive regime of a technocratic society.\nIn the 1950s, Isaac Asimov considered the issue of how to control machines in I, Robot. At the insistence of his editor John W. Campbell Jr., he proposed the Three Laws of Robotics to govern artificially intelligent systems. Much of his work was then spent testing the boundaries of his three laws to see where they would break down, or where they would create paradoxical or unanticipated behavior.[176] His work suggests that no set of fixed laws can sufficiently anticipate all possible circumstances.[177] More recently, academics and many governments have challenged the idea that AI can itself be held accountable.[178] A panel convened by the United Kingdom in 2010 revised Asimov's laws to clarify that AI is the responsibility either of its manufacturers, or of its owner/operator.[179]\n Eliezer Yudkowsky, from the Machine Intelligence Research Institute suggested in 2004 a need to study how to build a \"Friendly AI\", meaning that there should also be efforts to make AI intrinsically friendly and humane.[180]\n In 2009, academics and technical experts attended a conference organized by the Association for the Advancement of Artificial Intelligence to discuss the potential impact of robots and computers, and the impact of the hypothetical possibility that they could become self-sufficient and make their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard.[181] They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence\". They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.[131]\n Also in 2009, during an experiment at the Laboratory of Intelligent Systems in the Ecole Polytechnique F\u00e9d\u00e9rale of Lausanne, Switzerland, robots that were programmed to cooperate with each other (in searching out a beneficial resource and avoiding a poisonous one) eventually learned to lie to each other in an attempt to hoard the beneficial resource.[182]\n The role of fiction with regards to AI ethics has been a complex one.[183] One can distinguish three levels at which fiction has impacted the development of artificial intelligence and robotics: Historically, fiction has been prefiguring common tropes that have not only influenced goals and visions for AI, but also outlined ethical questions and common fears associated with it. During the second half of the twentieth and the first decades of the twenty-first century, popular culture, in particular movies, TV series and video games have frequently echoed preoccupations and dystopian projections around ethical questions concerning AI and robotics. Recently, these themes have also been increasingly treated in literature beyond the realm of science fiction. And, as Carme Torras, research professor at the Institut de Rob\u00f2tica i Inform\u00e0tica Industrial (Institute of robotics and industrial computing) at the Technical University of Catalonia notes,[184] in higher education, science fiction is also increasingly used for teaching technology-related ethical issues in technological degrees.\n While ethical questions linked to AI have been featured in science fiction literature and feature films for decades, the emergence of the TV series as a genre allowing for longer and more complex story lines and character development has led to some significant contributions that deal with ethical implications of technology. The Swedish series Real Humans (2012\u20132013) tackled the complex ethical and social consequences linked to the integration of artificial sentient beings in society. The British dystopian science fiction anthology series Black Mirror (2013\u20132019) was particularly notable for experimenting with dystopian fictional developments linked to a wide variety of recent technology developments. Both the French series Osmosis (2020) and British series The One deal with the question of what can happen if technology tries to find the ideal partner for a person. Several episodes of the Netflix series Love, Death+Robots have imagined scenes of robots and humans living together. The most representative one of them is S02 E01, it shows how bad the consequences can be when robots get out of control if humans rely too much on them in their lives.[185]\n The movie The Thirteenth Floor suggests a future where simulated worlds with sentient inhabitants are created by computer game consoles for the purpose of entertainment. The movie The Matrix suggests a future where the dominant species on planet Earth are sentient machines and humanity is treated with utmost speciesism. The short story \"The Planck Dive\" suggests a future where humanity has turned itself into software that can be duplicated and optimized and the relevant distinction between types of software is sentient and non-sentient. The same idea can be found in the Emergency Medical Hologram of Starship Voyager, which is an apparently sentient copy of a reduced subset of the consciousness of its creator, Dr. Zimmerman, who, for the best motives, has created the system to give medical assistance in case of emergencies. The movies Bicentennial Man and A.I. deal with the possibility of sentient robots that could love. I, Robot explored some aspects of Asimov's three laws. All these scenarios try to foresee possibly unethical consequences of the creation of sentient computers.[186]\n The ethics of artificial intelligence is one of several core themes in BioWare's Mass Effect series of games.[187] It explores the scenario of a civilization accidentally creating AI through a rapid increase in computational power through a global scale neural network. This event caused an ethical schism between those who felt bestowing organic rights upon the newly sentient Geth was appropriate and those who continued to see them as disposable machinery and fought to destroy them. Beyond the initial conflict, the complexity of the relationship between the machines and their creators is another ongoing theme throughout the story.\n Detroit: Become Human is one of the most famous video games which discusses the ethics of artificial intelligence recently. Quantic Dream designed the chapters of the game using interactive storylines to give players a more immersive gaming experience. Players manipulate three different awakened bionic people in the face of different events to make different choices to achieve the purpose of changing the human view of the bionic group and different choices will result in different endings. This is one of the few games that puts players in the bionic perspective, which allows them to better consider the rights and interests of robots once a true artificial intelligence is created.[188]\n Over time, debates have tended to focus less and less on possibility and more on desirability,[189] as emphasized in the \"Cosmist\" and \"Terran\" debates initiated by Hugo de Garis and Kevin Warwick. A Cosmist, according to Hugo de Garis, is actually seeking to build more intelligent successors to the human species.\n Experts at the University of Cambridge have argued that AI is portrayed in fiction and nonfiction overwhelmingly as racially White, in ways that distort perceptions of its risks and benefits.[190]\n",
        "doc_number": 5
    },
    {
        "url": "https://en.wikipedia.org/wiki/Applications_of_artificial_intelligence",
        "content": "\n Artificial intelligence (AI) has been used in applications throughout industry and academia. In a manner analogous to electricity or computers, AI serves as a general-purpose technology. AI programes emulate perception and understanding, and are designed to adapt to new information and new situations. Machine learning has been used for various scientific and commercial purposes[1] including language translation, image recognition, decision-making,[2][3] credit scoring, and e-commerce.\n Machine learning is has been used for recommendation systems in for determining which posts should show up in social media feeds.[4][5] Various types of social media analysis also make use of machine learning[6][7] and there is research into its use for (semi-)automated tagging/enhancement/correction of online misinformation and related filter bubbles.[8][9][10]\n AI has been used to customize shopping options and personalize offers.[11] Online gambling companies have used AI for targeting gamblers.[12]\n Intelligent personal assistants use AI to understand many natural language requests in other ways than rudimentary commands. Common examples are Apple's Siri, Amazon's Alexa, and a more recent AI, ChatGPT by OpenAI.[13]\n Bing Chat has used artificial intelligence as part of its search engine.[14]\n Machine learning can be used to combat spam, scams, and phishing. It can scrutinize the contents of spam and phishing attacks to attempt to identify malicious elements.[15] Some models built via machine learning algorithms have over 90% accuracy in distinguishing between spam and legitimate emails.[16] These models can be refined using new data and evolving spam tactics. Machine learning also analyzes traits such as sender behavior, email header information, and attachment types, potentially enhancing spam detection.[17]\n Speech translation technology attempts to convert one language's spoken words into another language. This potentially reduces language barriers in global commerce and cross-cultural exchange, enabling speakers of various languages to communicate with one another.[18]\n AI has been used to automatically translate spoken language and textual content in products such as Microsoft Translator, Google Translate, and DeepL Translator.[19] Additionally, research and development are in progress to decode and conduct animal communication.[20][21]\n Meaning is conveyed not only by text, but also through usage and context (see semantics and pragmatics). As a result, the two primary categorization approaches for machine translations are statistical and neural machine translations (NMTs). The old method of performing translation was to use a statistical machine translation (SMT) methodology to forecast the best probable output with specific algorithms. However, with NMT, the approach employs dynamic algorithms to achieve better translations based on context.[22]\n AI has been used in facial recognition systems. Some examples are Apple's Face ID and Android's Face Unlock, which are used to secure mobile devices.[23]\n Image labeling has been used by Google Image Labeler to detect products in photos and to allow people to search based on a photo. Image labeling has also been demonstrated to generate speech to describe images to blind people.[24] Facebook's DeepFace identifies human faces in digital images.\n Games have been a major application[relevant?] of AI's capabilities since the 1950s. In the 21st century, AIs have beaten human players in many games, including chess (Deep Blue), Jeopardy! (Watson),[25] Go (AlphaGo),[26][27][28][29][30][31][32] poker (Pluribus[33] and Cepheus),[34] E-sports (StarCraft),[35][36] and general game playing (AlphaZero[37][38][39] and MuZero).[40][41][42][43]\n Kuki AI is a set of chatbots and other apps which were designed for entertainment and as a marketing tool.[44][45] Character.ai is another example of a chatbot being used for recreation.\n AI for Good is a platform launched in 2017 by the International Telecommunication Union (ITU) agency of the United Nations (UN). The goal of the platform is to use AI to help achieve the UN's Sustainable Development Goals.[citation needed]\n The University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address problems such as homelessness. Stanford researchers use AI to analyze satellite images to identify high poverty areas.[46]\n In agriculture, AI has been proposed as a way for farmers to identify areas that need irrigation, fertilization, or pesticide treatments to increase yields, thereby improving efficiency.[47] AI has been used to attempt to classify livestock pig call emotions,[20] automate greenhouses,[48] detect diseases and pests,[49] and optimize irrigation.[50]\n Cyber security companies are adopting neural networks, machine learning, and natural language processing to improve their systems.[51]\n Applications of AI in cyber security include:\n AI elevates teaching, focusing on significant issues like the knowledge nexus and educational equality. The evolution of AI in education and technology should be used to improve human capabilities in relationships where they do not replace humans. UNESCO recognizes the future of AI in education as an instrument to reach Sustainable Development Goal 4, called \"Inclusive and Equitable Quality Education.\u201d [56]\n The World Economic Forum also stresses AI's contribution to students' overall improvement and transforming teaching into a more enjoyable process.[56]\n Personalized Learning\n AI driven tutoring systems, such as Khan Academy, Duolingo and Carnegie Learning are the forefoot of delivering personalized education.[57]\n These platforms leverage AI algorithms to analyze individual learning patterns, strengths, and weaknesses, enabling the customization of content and Algorithm to suit each student's pace and style of learning.[57]\n Administrative Efficiency\n In educational institutions, AI is increasingly used to automate routine tasks like attendance tracking, grading and marking, which allows educators to devote more time to interactive teaching and direct student engagement.[58]\n Furthermore, AI tools are employed to monitor student progress, analyze learning behaviors, and predict academic challenges, facilitating timely and proactive interventions for students who may be at risk of falling behind.[58]\n Ethical and Privacy Concerns\n Despite the benefits, the integration of AI in education raises significant ethical and privacy concerns, particularly regarding the handling of sensitive student data.[57]\n It is imperative that AI systems in education are designed and operated with a strong emphasis on transparency, security, and respect for privacy to maintain trust and uphold the integrity of educational practices.[57]\n Much of the regulation will be influenced by the AI Act, the world\u2019s first comprehensive AI law. [59]\n Financial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking began in 1987 when Security Pacific National Bank launched a fraud prevention task-force to counter the unauthorized use of debit cards.[60] Kasisto and Moneystream use AI.\n Banks use AI to organize operations for bookkeeping, investing in stocks, and managing properties. AI can adapt to changes during non-business hours.[61] AI is used to combat fraud and financial crimes by monitoring behavioral patterns for any abnormal changes or anomalies.[62][63][64]\n The use of AI in applications such as online trading and decision-making has changed major economic theories.[65] For example, AI-based buying and selling platforms estimate personalized demand and supply curves, thus enabling individualized pricing. AI systems reduce information asymmetry in the market and thus make markets more efficient.[66] The application of artificial intelligence in the financial industry can alleviate the financing constraints of non-state-owned enterprises, especially for smaller and more innovative enterprises.[67]\n Algorithmic trading involves the use of AI systems to make trading decisions at speeds orders of magnitude greater than any human is capable of, making millions of trades in a day without human intervention. Such high-frequency trading represents a fast-growing sector. Many banks, funds, and proprietary trading firms now have entire portfolios that are AI-managed. Automated trading systems are typically used by large institutional investors but include smaller firms trading with their own AI systems.[68]\n Large financial institutions use AI to assist with their investment practices. BlackRock's AI engine, Aladdin, is used both within the company and by clients to help with investment decisions. Its functions include the use of natural language processing to analyze text such as news, broker reports, and social media feeds. It then gauges the sentiment on the companies mentioned and assigns a score. Banks such as UBS and Deutsche Bank use SQREEM (Sequential Quantum Reduction and Extraction Model) to mine data to develop consumer profiles and match them with wealth management products.[69]\n Online lender Upstart uses machine learning for underwriting.[70]\n ZestFinance's Zest Automated Machine Learning (ZAML) platform is used for credit underwriting. This platform uses machine learning to analyze data including purchase transactions and how a customer fills out a form to score borrowers. The platform is particularly useful to assign credit scores to those with limited credit histories.[71]\n AI makes continuous auditing possible. Potential benefits include reducing audit risk, increasing the level of assurance, and reducing audit duration.[72][quantify]\n Continuous auditing with AI allows real-time monitoring and reporting of financial activities and provides businesses with timely insights that can lead to quick decision making.[73]\n AI software, such as LaundroGraph which uses contemporary suboptimal datasets, could be used for anti-money laundering (AML).[74][75]\n In the 1980s, AI started to become prominent in finance as expert systems were commercialized. For example, Dupont created 100 expert systems, which helped them to save almost $10 million per year.[76] One of the first systems was the Pro-trader expert system that predicted the 87-point drop in the Dow Jones Industrial Average in 1986. \"The major junctions of the system were to monitor premiums in the market, determine the optimum investment strategy, execute transactions when appropriate and modify the knowledge base through a learning mechanism.\"[77]\n One of the first expert systems to help with financial plans was PlanPowerm and Client Profiling System, created by Applied Expert Systems (APEX). It was launched in 1986. It helped create personal financial plans for people.[78]\n In the 1990s AI was applied to fraud detection. In 1993 FinCEN Artificial Intelligence System (FAIS) launched. It was able to review over 200,000 transactions per week and over two years it helped identify 400 potential cases of money laundering equal to $1 billion.[79] These expert systems were later replaced by machine learning systems.[80]\n AI can enhance entrepreneurial activity and AI is one of the most dynamic areas for start-ups, with significant venture capital flowing into AI.[81]\n AI facial recognition systems are used for mass surveillance, notably in China.[82][83] In 2019, Bengaluru, India deployed AI-managed traffic signals. This system uses cameras to monitor traffic density and adjust signal timing based on the interval needed to clear traffic.[84]\n Various countries are deploying AI military applications.[85] The main applications enhance command and control, communications, sensors, integration and interoperability.[86] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[85] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams.[86]\n AI has been used in military operations in Iraq, Syria, Israel and Ukraine.[85][87][88][89]\n AI in healthcare is often used for classification, to evaluate a CT scan or electrocardiogram or to identify high-risk patients for population health. AI is helping with the high-cost problem of dosing. One study suggested that AI could save $16\u00a0billion. In 2016, a study reported that an AI-derived formula derived the proper dose of immunosuppressant drugs to give to transplant patients.[90] Current research has indicated that non-cardiac vascular illnesses are also being treated with artificial intelligence (AI). For certain disorders, AI algorithms can aid in diagnosis, recommended treatments, outcome prediction, and patient progress tracking. As AI technology advances, it is anticipated that it will become more significant in the healthcare industry.[91]\n The early detection of diseases like cancer is made possible by AI algorithms, which diagnose diseases by analyzing complex sets of medical data. For example, the IBM Watson system might be used to comb through massive data such as medical records and clinical trials to help diagnose a problem.[92] Microsoft's AI project Hanover helps doctors choose cancer treatments from among the more than 800 medicines and vaccines.[93][94] Its goal is to memorize all the relevant papers to predict which (combinations of) drugs will be most effective for each patient. Myeloid leukemia is one target. Another study reported on an AI that was as good as doctors in identifying skin cancers.[95] Another project monitors multiple high-risk patients by asking each patient questions based on data acquired from doctor/patient interactions.[96] In one study done with transfer learning, an AI diagnosed eye conditions similar to an ophthalmologist and recommended treatment referrals.[97]\n Another study demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel judged better than a surgeon.[98]\n Artificial neural networks are used as clinical decision support systems for medical diagnosis,[99] such as in concept processing technology in EMR software.\n Other healthcare tasks thought suitable for an AI that are in development include:\n AI-enabled chatbots decrease the need for humans to perform basic call center tasks.[115]\n Machine learning in sentiment analysis can spot fatigue in order to prevent overwork.[115] Similarly, decision support systems can prevent industrial disasters and make disaster response more efficient.[116] For manual workers in material handling, predictive analytics may be used to reduce musculoskeletal injury.[117] Data collected from wearable sensors can improve workplace health surveillance, risk assessment, and research.[116][how?]\n AI can auto-code workers' compensation claims.[118][119] AI-enabled virtual reality systems can enhance safety training for hazard recognition.[116] AI can more efficiently detect accident near misses, which are important in reducing accident rates, but are often underreported.[120]\n AlphaFold 2 can determine the 3D structure of a (folded) protein in hours rather than the months required by earlier automated approaches and was used to provide the likely structures of all proteins in the human body and essentially all proteins known to science (more than 200 million).[121][122][123][124]\n Machine learning has been used for drug design.[125] It has also been used for predicting molecular properties and exploring large chemical/reaction spaces.[126] Computer-planned syntheses via computational reaction networks, described as a platform that combines \"computational synthesis with AI algorithms to predict molecular properties\",[127] have been used to explore the origins of life on Earth,[128] drug-syntheses and developing routes for recycling 200 industrial waste chemicals into important drugs and agrochemicals (chemical synthesis design).[129] There is research about which types of computer-aided chemistry would benefit from machine learning.[130] It can also be used for \"drug discovery and development, drug repurposing, improving pharmaceutical productivity, and clinical trials\".[131] It has been used for the design of proteins with prespecified functional sites.[132][133]\n It has been used with databases for the development of a 46-day process to design, synthesize and test a drug which inhibits enzymes of a particular gene, DDR1. DDR1 is involved in cancers and fibrosis which is one reason for the high-quality datasets that enabled these results.[134]\n There are various types of applications for machine learning in decoding human biology, such as helping to map gene expression patterns to functional activation patterns[135] or identifying functional DNA motifs.[136] It is widely used in genetic research.[137]\n There also is some use of machine learning in synthetic biology,[138][139] disease biology,[139] nanotechnology (e.g. nanostructured materials and bionanotechnology),[140][141] and materials science.[142][143][144]\n There are also prototype robot scientists, including robot-embodied ones like the two Robot Scientists, which show a form of \"machine learning\" not commonly associated with the term.[145][146]\n Similarly, there is research and development of biological \"wetware computers\" that can learn (e.g. for use as biosensors) and/or implantation into an organism's body (e.g. for use to control prosthetics).[147][148][149] Polymer-based artificial neurons operate directly in biological environments and define biohybrid neurons made of artificial and living components.[150][151]\n Moreover, if whole brain emulation is possible via both scanning and replicating the, at least, bio-chemical brain \u2013 as premised in the form of digital replication in The Age of Em, possibly using physical neural networks \u2013 that may have applications as or more extensive than e.g. valued human activities and may imply that society would face substantial moral choices, societal risks and ethical problems[152][153] such as whether (and how) such are built, sent through space and used compared to potentially competing e.g. potentially more synthetic and/or less human and/or non/less-sentient types of artificial/semi-artificial intelligence.[additional citation(s) needed] An alternative or additive approach to scanning are types of reverse engineering of the brain.[154][155]\n A subcategory of artificial intelligence is embodied,[156][157] some of which are mobile robotic systems that each consist of one or multiple robots that are able to learn in the physical world.\n However, biological computers, even if both highly artificial and intelligent, are typically distinguished from synthetic, often silicon-based, computers \u2013 they could however be combined or used for the design of either. Moreover, many tasks may be carried out inadequately by artificial intelligence even if its algorithms were transparent, understood, bias-free, apparently effective, and goal-aligned and its trained data sufficiently large and cleansed \u2013 such as in cases were the underlying or available metrics, values or data are inappropriate. Computer-aided is a phrase used to describe human activities that make use of computing as tool in more comprehensive activities and systems such as AI for narrow tasks or making use of such without substantially relying on its results (see also: human-in-the-loop).[citation needed] A study described the biological as a limitation of AI with \"as long as the biological system cannot be understood, formalized, and imitated, we will not be able to develop technologies that can mimic it\" and that if it was understood this does not mean there being \"a technological solution to imitate natural intelligence\".[158] Technologies that integrate biology and are often AI-based include biorobotics.\n Artificial intelligence is used in astronomy to analyze increasing amounts of available data[159][160] and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights\" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy.[161] It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance,[162] and more autonomous operation.[163][164][165][160]\n In the search for extraterrestrial intelligence (SETI), machine learning has been used in attempts to identify artificially generated electromagnetic waves in available data[166][167] \u2013 such as real-time observations[168] \u2013 and other technosignatures, e.g. via anomaly detection.[169] In ufology, the SkyCAM-5 project headed by Prof. Hakan Kayal[170] and the Galileo Project headed by Avi Loeb use machine learning to attempt to detect and classify types of UFOs.[171][172][173][174][175] The Galileo Project also seeks to detect two further types of potential extraterrestrial technological signatures with the use of AI: 'Oumuamua-like interstellar objects, and non-manmade artificial satellites.[176][177]\n Machine learning can also be used to produce datasets of spectral signatures of molecules that may be involved in the atmospheric production or consumption of particular chemicals \u2013 such as phosphine possibly detected on Venus \u2013 which could prevent miss assignments and, if accuracy is improved, be used in future detections and identifications of molecules on other planets.[178]\n In April 2024, the Scientific Advice Mechanism to the European Commission published advice[179] including a comprehensive evidence review of the opportunities and challenges posed by artificial intelligence in scientific research.\n As benefits, the evidence review[180] highlighted:\n As challenges:\n Machine learning can help to restore and attribute ancient texts.[181] It can help to index texts for example to enable better and easier searching[182] and classification of fragments.[183]\n \nArtificial intelligence can also be used to investigate genomes to uncover genetic history, such as interbreeding between archaic and modern humans by which for example the past existence of a ghost population, not Neanderthal or Denisovan, was inferred.[184]  \nIt can also be used for \"non-invasive and non-destructive access to internal structures of archaeological remains\".[185]  A deep learning system was reported to learn intuitive physics from visual data (of virtual 3D environments) based on an unpublished approach inspired by studies of visual cognition in infants.[186][187] Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems' future dynamics from video recordings of their behavior.[188][189] In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems.[188]\n AI could be used for materials optimization and discovery such as the discovery of stable materials and the prediction of their crystal structure.[190][191][192]\n In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.[193][194][195]\n Machine learning is used in diverse types of reverse engineering. For example, machine learning has been used to reverse engineer a composite material part, enabling unauthorized production of high quality parts,[196] and for quickly understanding the behavior of malware.[197][198][199] It can be used to reverse engineer artificial intelligence models.[200] It can also design components by engaging in a type of reverse engineering of not-yet existent virtual components such as inverse molecular design for particular desired functionality[201] or protein design for prespecified functional sites.[132][133] Biological network reverse engineering could model interactions in a human understandable way, e.g. bas on time series data of gene expression levels.[202]\n AI is a mainstay of law-related professions. Algorithms and machine learning do some tasks previously done by entry-level lawyers.[203] While its use is common, it is not expected to replace most work done by lawyers in the near future.[204]\n The electronic discovery industry uses machine learning to reduce manual searching.[205]\n Law enforcement has begun using facial recognition systems (FRS) to identify suspects from visual data. FRS results have proven to be more accurate when compared to eyewitness results. Furthermore, FRS has shown to have much a better ability to identify individuals when video clarity and visibility are low in comparison to human participants. [206]\n COMPAS is a commercial system used by U.S. courts to assess the likelihood of recidivism.[207]\n One concern relates to algorithmic bias, AI programs may become biased after processing data that exhibits bias.[208] ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than that of white defendants.[207]\n In 2019, the city of Hangzhou, China established a pilot program artificial intelligence-based Internet Court to adjudicate disputes related to ecommerce and internet-related intellectual property claims.[209]:\u200a124\u200a Parties appear before the court via videoconference and AI evaluates the evidence presented and applies relevant legal standards.[209]:\u200a124\u200a\n Another application of AI is in human resources. AI can screen resumes and rank candidates based on their qualifications, predict candidate success in given roles, and automate repetitive communication tasks via chatbots.[210]\n AI has simplified the recruiting /job search process for both recruiters and job seekers. According to Raj Mukherjee from Indeed, 65% of job searchers search again within 91 days after hire. An AI-powered engine streamlines the complexity of job hunting by assessing information on job skills, salaries, and user tendencies, matching job seekers to the most relevant positions. Machine intelligence calculates appropriate wages and highlights resume information for recruiters using NLP, which extracts relevant words and phrases from text. Another application is an AI resume builder that compiles a CV in 5 minutes.[211] Chatbots assist website visitors and refine workflows.\n AI underlies avatars (automated online assistants) on web pages.[212] It can reduce operation and training costs.[212] Pypestream automated customer service for its mobile application to streamline communication with customers.[213]\n A Google app analyzes language and converts speech into text. The platform can identify angry customers through their language and respond appropriately.[214] Amazon uses a chatbot for customer service that can perform tasks like checking the status of an order, cancelling orders, offering refunds and connecting the customer with a human representative.[215] Generative AI (GenAI), such as ChatGPT, is increasingly used in business to automate tasks and enhance decision-making.[216]\n In the hospitality industry, AI is used to reduce repetitive tasks, analyze trends, interact with guests, and predict customer needs.[217] AI hotel services come in the form of a chatbot,[218] application, virtual voice assistant and service robots.\n AI applications analyze media content such as movies, TV programs, advertisement videos or user-generated content. The solutions often involve computer vision.\n Typical scenarios include the analysis of images using object recognition or face recognition techniques, or the analysis of video for scene recognizing scenes, objects or faces. AI-based media analysis can facilitate media search, the creation of descriptive keywords for content, content policy monitoring (such as verifying the suitability of content for a particular TV viewing time), speech to text for archival or other purposes, and the detection of logos, products or celebrity faces for ad placement.\n Deep-fakes can be used for comedic purposes but are better known for fake news and hoaxes.\n Deepfakes can portray individuals in harmful or compromising situations, causing significant reputational damage and emotional distress, especially when the content is defamatory or violates personal ethics. While defamation and false light laws offer some recourse, their focus on false statements rather than fabricated images or videos often leaves victims with limited legal protection and a challenging burden of proof.[232]\n In January 2016,[233] the Horizon 2020 program financed the InVID Project[234][235] to help journalists and researchers detect fake documents, made available as browser plugins.[236][237]\n In June 2016, the visual computing group of the Technical University of Munich and from Stanford University developed Face2Face,[238] a program that animates photographs of faces, mimicking the facial expressions of another person. The technology has been demonstrated animating the faces of people including Barack Obama and Vladimir Putin. Other methods have been demonstrated based on deep neural networks, from which the name deep fake was taken.\n In September 2018, U.S. Senator Mark Warner proposed to penalize social media companies that allow sharing of deep-fake documents on their platforms.[239]\n In 2018, Darius Afchar and Vincent Nozick found a way to detect faked content by analyzing the mesoscopic properties of video frames.[240] DARPA gave 68 million dollars to work on deep-fake detection.[240]\n Audio deepfakes[241][242] and AI software capable of detecting deep-fakes and cloning human voices have been developed.[243][244]\n Respeecher is a program that enables one person to speak with the voice of another.\n AI algorithms have been used to detect deepfake videos.[245][246]\n Artificial intelligence is also starting to be used in video production, with tools and software being developed that utilize generative AI in order to create new video, or alter existing video. Some of the major tools that are being used in these processes currently are DALL-E, Mid-journey, and Runway.[247]  Way mark Studios utilized the tools offered by both DALL-E and Mid-journey to create a fully AI generated film called The Frost in the summer of 2023.[247] Way mark Studios is experimenting with using these AI tools to generate advertisements and commercials for companies in mere seconds.[247]  Yves Bergquist, a director of the AI & Neuroscience in Media Project at USC's Entertainment Technology Center, says post production crews in Hollywood are already using generative AI, and predicts that in the future more companies will embrace this new technology.[248]\n AI has been used to compose music of various genres.\n David Cope created an AI called Emily Howell that managed to become well known in the field of algorithmic computer music.[249] The algorithm behind Emily Howell is registered as a US patent.[250]\n In 2012, AI Iamus created the first complete classical album.[251]\n AIVA (Artificial Intelligence Virtual Artist), composes symphonic music, mainly classical music for film scores.[252] It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.[253]\n Melomics creates computer-generated music for stress and pain relief.[254]\n At Sony CSL Research Laboratory, the Flow Machines software creates pop songs by learning music styles from a huge database of songs. It can compose in multiple styles.\n The Watson Beat uses reinforcement learning and deep belief networks to compose music on a simple seed input melody and a select style. The software was open sourced[255] and musicians such as Taryn Southern[256] collaborated with the project to create music.\n South Korean singer, Hayeon's, debut song, \"Eyes on You\" was composed using AI which was supervised by real composers, including NUVO.[257]\n Narrative Science sells computer-generated news and reports. It summarizes sporting events based on statistical data from the game. It also creates financial reports and real estate analyses.[258] Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football.[259]\n Yseop, uses AI to turn structured data into natural language comments and recommendations. Yseop writes financial reports, executive summaries, personalized sales or marketing documents and more in multiple languages, including English, Spanish, French, and German.[260]\n TALESPIN made up stories similar to the fables of Aesop. The program started with a set of characters who wanted to achieve certain goals. The story narrated their attempts to satisfy these goals.[citation needed] Mark Riedl and Vadim Bulitko asserted that the essence of storytelling was experience management, or \"how to balance the need for a coherent story progression with user agency, which is often at odds\".[261]\n While AI storytelling focuses on story generation (character and plot), story communication also received attention. In 2002, researchers developed an architectural framework for narrative prose generation. They faithfully reproduced text variety and complexity on stories such as Little Red Riding Hood.[262] In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.[263]\n South Korean company Hanteo Global uses a journalism bot to write articles.[264]\n Literary authors are also exploring uses of AI. An example is David Jhave Johnston's work ReRites (2017-2019), where the poet created a daily rite of editing the poetic output of a neural network to create a series of performances and publications.\n In 2010, artificial intelligence used baseball statistics to automatically generate news articles. This was launched by The Big Ten Network using software from Narrative Science.[265]\n After being unable to cover every Minor League Baseball game with a large team, Associated Press collaborated with Automated Insights in 2016 to create game recaps that were automated by artificial intelligence.[266]\n UOL in Brazil expanded the use of AI in its writing. Rather than just generating news stories, they programmed the AI to include commonly searched words on Google.[266]\n El Pais, a Spanish news site that covers many things including sports, allows users to make comments on each news article. They use the Perspective API to moderate these comments and if the software deems a comment to contain toxic language, the commenter must modify it in order to publish it.[266]\n A local Dutch media group used AI to create automatic coverage of amateur soccer, set to cover 60,000 games in just a single season. NDC partnered with United Robots to create this algorithm and cover what would have never been possible before without an extremely large team.[266]\n Lede AI has been used in 2023 to take scores from high school football games to generate stories automatically for the local newspaper. This was met with significant criticism from readers for the very robotic diction that was published. With some descriptions of games being a \"close encounter of the athletic kind,\" readers were not pleased and let the publishing company, Gannett, know on social media. Gannett has since halted their used of Lede AI until they come up with a solution for what they call an experiment.[267]\n  Millions of its articles have been edited by bots[271] which however are usually not artificial intelligence software. Many AI platforms use Wikipedia data,[272] mainly for training machine learning applications. There is research and development of various artificial intelligence applications for Wikipedia such as for identifying outdated sentences,[273] detecting covert vandalism[274] or recommending articles and tasks to new editors.\n Machine translation (see above) has also be used for translating Wikipedia articles and could play a larger role in creating, updating, expanding, and generally improving articles in the future. A content translation tool allows editors of some Wikipedias to more easily translate articles across several select languages.[275][276]\n In video games, AI is routinely used to generate behavior in non-player characters (NPCs). In addition, AI is used for pathfinding. Some researchers consider NPC AI in games to be a \"solved problem\" for most production tasks.[who?] Games with less typical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010).[277][278] AI is also used in Alien Isolation (2014) as a way to control the actions the Alien will perform next.[279]\n Kinect, which provides a 3D body\u2013motion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from AI research.[280][which?]\n AI has been used to produce visual art. The first AI art program, called AARON, was developed by Harold Cohen in 1968[281] with the goal of being able to code the act of drawing. It started by creating simple black and white drawings, and later to painting using special brushes and dyes that were chosen by the program itself without mediation from Cohen.[282]\n AI platforms such as \"DALL-E\",[283] Stable Diffusion,[283] Imagen,[284] and Midjourney[285] have been used for generating visual images from inputs such as text or other images.[286] Some AI tools allow users to input images and output changed versions of that image, such as to display an object or product in different environments. AI image models can also attempt to replicate the specific styles of artists, and can add visual complexity to rough sketches.\n Since their design in 2014, generative adversarial networks (GANs) have been used by AI artists. GAN computer programming, generates technical images through machine learning frameworks that surpass the need for human operators.[281] Examples of GAN programs that generate art include Artbreeder and DeepDream.\n In addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. Although the main goal of the large-scale digitization of artwork in the past few decades was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.[287]\nTwo computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art.[288] While distant viewing includes the analysis of large collections, close reading involves one piece of artwork.\n AI has been in use since the early 2000s, most notably by a system designed by Pixar called \"Genesis\".[289] It was designed to learn algorithms and create 3D models for its characters and props. Notable movies that used this technology included Up and The Good Dinosaur.[290] AI has been used less ceremoniously in recent years. In 2023, it was revealed Netflix of Japan was using AI to generate background images for their upcoming show to be met with backlash online.[291]  In recent years, motion capture became an easily accessible form of AI animation. For example, Move AI is a program built to capture any human movement and reanimate it in its animation program using learning AI.[292]\n Power electronics converters are used in renewable energy, energy storage, electric vehicles and high-voltage direct current transmission. These converters are failure-prone, which can interrupt service and require costly maintenance or catastrophic consequences in mission critical applications.[citation needed] AI can guide the design process for reliable power electronics converters, by calculating exact design parameters that ensure the required lifetime.[293]\n The U.S. Department of Energy underscores AI's pivotal role in realizing national climate goals. With AI, the ambitious target of achieving net-zero greenhouse gas emissions across the economy becomes feasible. AI also helps make room for wind and solar on the grid by avoiding congestion and increasing grid reliability. [294]\n Machine learning can be used for energy consumption prediction and scheduling, e.g. to help with renewable energy intermittency management (see also: smart grid and climate change mitigation in the power grid).[295][296][297][298][125]\n Many telecommunications companies make use of heuristic search to manage their workforces. For example, BT Group deployed heuristic search[299] in an application that schedules 20,000 engineers. Machine learning is also used for speech recognition (SR), including of voice-controlled devices, and SR-related transcription, including of videos.[300][301]\n Artificial intelligence has been combined with digital spectrometry by IdeaCuria Inc.,[302][303] enable applications such as at-home water quality monitoring.\n In the 1990s, early artificial intelligence tools controlled Tamagotchis and Giga Pets, the Internet, and the first widely released robot, Furby. Aibo was a domestic robot in the form of a robotic dog with intelligent features and autonomy.\n Mattel created an assortment of AI-enabled toys that \"understand\" conversations, give intelligent responses, and learn.[304]\n Oil and gas companies have used artificial intelligence tools to automate functions, foresee equipment issues, and increase oil and gas output.[305][306]\n AI in transport is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities. The major development challenge is the complexity of transportation systems that involves independent components and parties, with potentially conflicting objectives.[307]\n AI-based fuzzy logic controllers operate gearboxes. For example, the 2006 Audi TT, VW Touareg [citation needed] and VW Caravell feature the DSP transmission. A number of \u0160koda variants (\u0160koda Fabia) include a fuzzy logic-based controller. Cars have AI-based driver-assist features such as self-parking and adaptive cruise control.\n There are also prototypes of autonomous automotive public transport vehicles such as electric mini-buses[308][309][310][311] as well as autonomous rail transport in operation.[312][313][314]\n There also are prototypes of autonomous delivery vehicles, sometimes including delivery robots.[315][316][317][318][319][320][321]\n Transportation's complexity means that in most cases training an AI in a real-world driving environment is impractical. Simulator-based testing can reduce the risks of on-road training.[322]\n AI underpins self-driving vehicles. Companies involved with AI include Tesla, Waymo, and General Motors. AI-based systems control functions such as braking, lane changing, collision prevention, navigation and mapping.[323]\n Autonomous trucks are in the testing phase. The UK government passed legislation to begin testing of autonomous truck platoons in 2018.[324] A group of autonomous trucks follow closely behind each other. German corporation Daimler is testing its Freightliner Inspiration.[325]\n Autonomous vehicles require accurate maps to be able to navigate between destinations.[326] Some autonomous vehicles do not allow human drivers (they have no steering wheels or pedals).[327]\n AI has been used to optimize traffic management, which reduces wait times, energy use, and emissions by as much as 25 percent.[328]\n Smart traffic lights have been developed at Carnegie Mellon since 2009.  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities.  It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.[329]\n The Royal Australian Air Force (RAAF) Air Operations Division (AOD) uses AI for expert systems. AIs operate as surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.[330]\n Aircraft simulators use AI for training aviators. Flight conditions can be simulated that allow pilots to make mistakes without risking themselves or expensive aircraft. Air combat can also be simulated.\n AI can also be used to operate planes analogously to their control of ground vehicles. Autonomous drones can fly independently or in swarms.[331]\n AOD uses the Interactive Fault Diagnosis and Isolation System, or IFDIS, which is a rule-based expert system using information from TF-30 documents and expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for the F-111C. The system replaced specialized workers. The system allowed regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers.\n Speech recognition allows traffic controllers to give verbal directions to drones.\n Artificial intelligence supported design of aircraft,[332] or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule-based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective.\n In 2003 a Dryden Flight Research Center project created software that could enable a damaged aircraft to continue flight until a safe landing can be achieved.[333] The software compensated for damaged components by relying on the remaining undamaged components.[334]\n The 2016 Intelligent Autopilot System combined apprenticeship learning and behavioral cloning whereby the autopilot observed low-level actions required to maneuver the airplane and high-level strategy used to apply those actions.[335]\n Neural networks are used by situational awareness systems in ships and boats.[336] There also are autonomous boats.\n Autonomous ships that monitor the ocean, AI-driven satellite data analysis, passive acoustics[337] or remote sensing and other applications of environmental monitoring make use of machine learning.[338][339][340][165]\n For example, \"Global Plastic Watch\" is an AI-based satellite monitoring-platform for analysis/tracking of plastic waste sites to help prevention of plastic pollution \u2013 primarily ocean pollution \u2013 by helping identify who and where mismanages plastic waste, dumping it into oceans.[341][342]\n Machine learning can be used to spot early-warning signs of disasters and environmental issues, possibly including natural pandemics,[343][344] earthquakes,[345][346][347] landslides,[348] heavy rainfall,[349] long-term water supply vulnerability,[350] tipping-points of ecosystem collapse,[351] cyanobacterial bloom outbreaks,[352] and droughts.[353][354][355]\n AI can be used for real-time code completion, chat, and automated test generation. These tools are typically integrated with editors and IDEs as plugins. They differ in functionality, quality, speed, and approach to privacy.[356] Code suggestions could be incorrect, and should be carefully reviewed by software developers before accepted.\n GitHub Copilot is an artificial intelligence model developed by GitHub and OpenAI that is able to autocomplete code in multiple programming languages.[357] Price for individuals: $10/mo or $100/yr, with one free month trial.\n Tabnine was created by Jacob Jackson and was originally owned by Tabnine company. In late 2019, Tabnine was acquired by Codota.[358] Tabnine tool is available as plugin to most popular IDEs. It offers multiple pricing options, including limited \"starter\" free version.[359]\n CodiumAI by CodiumAI, small startup in Tel Aviv, offers automated test creation. Currently supports Python, JS, and TS.[360]\n Ghostwriter by Replit offers code completion and chat.[361] They have multiple pricing plans, including a free one and a \"Hacker\" plan for $7/month.\n CodeWhisperer by Amazon collects individual users' content, including files open in the IDE. They claim to focus on security both during transmission and when storing.[362] Individual plan is free, professional plan is $19/user/month.\n Other tools: SourceGraph Cody, CodeCompleteFauxPilot, Tabby[356]\n AI can be used to create other AIs. For example, around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and POCO F1. NASNet's performance exceeded all previously published performance on ImageNet.[363]\n Machine learning has been used for noise-cancelling in quantum technology,[364] including quantum sensors.[365] Moreover, there is substantial research and development of using quantum computers with machine learning algorithms. For example, there is a prototype, photonic, quantum memristive device for neuromorphic (quantum-)computers (NC)/artificial neural networks and NC-using quantum materials with some variety of potential neuromorphic computing-related applications,[366][367] and quantum machine learning is a field with some variety of applications under development. AI could be used for quantum simulators which may have the application of solving physics and chemistry[368][369] problems as well as for quantum annealers for training of neural networks for AI applications.[370] There may also be some usefulness in chemistry, e.g. for drug discovery, and in materials science, e.g. for materials optimization/discovery (with possible relevance to quantum materials manufacturing[191][192]).[371][372][373][better\u00a0source\u00a0needed]\n AI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered AI. All of the following were originally developed in AI laboratories:[374]\n An optical character reader is used in the extraction of data in business documents like invoices and receipts. It can also be used in business contract documents e.g. employment agreements to extract critical data like employment terms, delivery terms, termination clauses, etc.[375]\n Artificial intelligence in architecture describes the use of artificial intelligence in automation, design and planning in the architectural process or in assisting human skills in the field of architecture. Artificial Intelligence is thought to potentially lead to and ensue major changes in architecture.[376][377][378]\n AI in architecture has created a way for architects to create things beyond human understanding. AI implementation of machine learning text-to-render technologies, like DALL-E and stable Diffusion, gives power to visualization complex.[379]\n AI allows designers to demonstrate their creativity and even invent new ideas while designing. In future, AI will not replace architects; instead, it will improve the speed of translating ideas sketching.[379]\n",
        "doc_number": 6
    },
    {
        "url": "https://en.wikipedia.org/wiki/Robotics",
        "content": "\n Robotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots.[1]\n Within mechanical engineering, robotics is the design and construction of the physical structures of robots, while in computer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to robotics include electrical, control, software, information, electronic, telecommunication, computer, mechatronic, and materials engineering.\n The goal of most robotics is to design machines that can help and assist humans. Many robots are built to do jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning, monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes.\n Robotics usually combines three aspects of design work to create robot systems:\n As many robots are designed for specific tasks, this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed \"assembly robots\". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables, etc. as an integrated unit. Such an integrated robotic system is called a \"welding robot\" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labeled as \"heavy-duty robots\".[4]\n Current and potential applications include:\n At present, mostly (lead\u2013acid) batteries are used as a power source. Many different types of batteries can be used as a power source for robots. They range from lead\u2013acid batteries, which are safe and have relatively long shelf lives but are rather heavy compared to silver\u2013cadmium batteries which are much smaller in volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime, and weight. Generators, often some type of internal combustion engine, can also be used. However, such designs are often mechanically complex and need fuel, require heat dissipation, and are relatively heavy. A tether connecting the robot to a power supply would remove the power supply from the robot entirely. This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage.[16] \nPotential power sources could be:\n Actuators are the \"muscles\" of a robot, the parts which convert stored energy into movement.[17] By far the most popular actuators are electric motors that rotate a wheel or gear, and linear actuators that control industrial robots in factories. There are some recent advances in alternative types of actuators, powered by electricity, chemicals, or compressed air.\n The vast majority of robots use electric motors, often brushed and brushless DC motors in portable robots or AC motors in industrial robots and CNC machines. These motors are often preferred in systems with lighter loads, and where the predominant form of motion is rotational.\n Various types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. They are typically powered by compressed and oxidized air (pneumatic actuator) or an oil (hydraulic actuator) Linear actuators can also be powered by electricity which usually consists of a motor and a leadscrew. Another common type is a mechanical linear actuator such as a rack and pinion on a car.\n Series elastic actuation (SEA) relies on the idea of introducing intentional elasticity between the motor actuator and the load for robust force control. Due to the resultant lower reflected inertia, series elastic actuation improves safety when a robot interacts with the environment (e.g., humans or workpieces) or during collisions.[18] Furthermore, it also provides energy efficiency and shock absorption (mechanical filtering) while reducing excessive wear on the transmission and other mechanical components. This approach has successfully been employed in various robots, particularly advanced manufacturing robots[19] and walking humanoid robots.[20][21]\n The controller design of a series elastic actuator is most often performed within the passivity framework as it ensures the safety of interaction with unstructured environments.[22] Despite its remarkable stability and robustness, this framework suffers from the stringent limitations imposed on the controller which may trade-off performance. The reader is referred to the following survey which summarizes the common controller architectures for SEA along with the corresponding sufficient passivity conditions.[23] One recent study has derived the necessary and sufficient passivity conditions for one of the most common impedance control architectures, namely velocity-sourced SEA.[24] This work is of particular importance as it drives the non-conservative passivity bounds in an SEA scheme for the first time which allows a larger selection of control gains.\n Pneumatic artificial muscles also known as air muscles, are special tubes that expand (typically up to 42%) when air is forced inside them. They are used in some robot applications.[25][26][27]\n Muscle wire, also known as shape memory alloy, is a material that contracts (under 5%) when electricity is applied. They have been used for some small robot applications.[28][29]\n EAPs or EPAMs are a plastic material that can contract substantially (up to 380% activation strain) from electricity, and have been used in facial muscles and arms of humanoid robots,[30] and to enable new robots to float,[31] fly, swim or walk.[32]\n Recent alternatives to DC motors are piezo motors or ultrasonic motors. These work on a fundamentally different principle, whereby tiny piezoceramic elements, vibrating many thousands of times per second, cause linear or rotary motion. There are different mechanisms of operation; one type uses the vibration of the piezo elements to step the motor in a circle or a straight line.[33] Another type uses the piezo elements to cause a nut to vibrate or to drive a screw. The advantages of these motors are nanometer resolution, speed, and available force for their size.[34] These motors are already available commercially and being used on some robots.[35][36]\n Elastic nanotubes are a promising artificial muscle technology in early-stage experimental development. The absence of defects in carbon nanotubes enables these filaments to deform elastically by several percent, with energy storage levels of perhaps 10\u00a0J/cm3 for metal nanotubes. Human biceps could be replaced with an 8\u00a0mm diameter wire of this material. Such compact \"muscle\" might allow future robots to outrun and outjump humans.[37]\n Sensors allow robots to receive information about a certain measurement of the environment, or internal components. This is essential for robots to perform their tasks, and act upon any changes in the environment to calculate the appropriate response. They are used for various forms of measurements, to give the robots warnings about safety or malfunctions, and to provide real-time information about the task it is performing.\n Current robotic and prosthetic hands receive far less tactile information than the human hand. Recent research has developed a tactile sensor array that mimics the mechanical properties and touch receptors of human fingertips.[38][39] The sensor array is constructed as a rigid core surrounded by conductive fluid contained by an elastomeric skin. Electrodes are mounted on the surface of the rigid core and are connected to an impedance-measuring device within the core. When the artificial skin touches an object the fluid path around the electrodes is deformed, producing impedance changes that map the forces received from the object. The researchers expect that an important function of such artificial fingertips will be adjusting the robotic grip on held objects.\n Scientists from several European countries and Israel developed a prosthetic hand in 2009, called SmartHand, which functions like a real one \u2014allowing patients to write with it, type on a keyboard, play piano, and perform other fine movements. The prosthesis has sensors which enable the patient to sense real feelings in its fingertips.[40]\n Other common forms of sensing in robotics use lidar, radar, and sonar.[41] Lidar measures the distance to a target by illuminating the target with laser light and measuring the reflected light with a sensor. Radar uses radio waves to determine the range, angle, or velocity of objects. Sonar uses sound propagation to navigate, communicate with or detect objects on or under the surface of the water.\n One of the most common types of end-effectors are \"grippers\". In its simplest manifestation, it consists of just two fingers that can open and close to pick up and let go of a range of small objects. Fingers can, for example, be made of a chain with a metal wire running through it.[42] Hands that resemble and work more like a human hand include the Shadow Hand and the Robonaut hand.[43] Hands that are of a mid-level complexity include the Delft hand.[44][45] Mechanical grippers can come in various types, including friction and encompassing jaws. Friction jaws use all the force of the gripper to hold the object in place using friction. Encompassing jaws cradle the object in place, using less friction.\n Suction end-effectors, powered by vacuum generators, are very simple astrictive[46] devices that can hold very large loads provided the prehension surface is smooth enough to ensure suction.\n Pick and place robots for electronic components and for large objects like car windscreens, often use very simple vacuum end-effectors.\n Suction is a highly used type of end-effector in industry, in part because the natural compliance of soft suction end-effectors can enable a robot to be more robust in the presence of imperfect robotic perception. As an example: consider the case of a robot vision system that estimates the position of a water bottle but has 1 centimeter of error. While this may cause a rigid mechanical gripper to puncture the water bottle, the soft suction end-effector may just bend slightly and conform to the shape of the water bottle surface.\n Some advanced robots are beginning to use fully humanoid hands, like the Shadow Hand, MANUS,[47] and the Schunk hand.[48] They have powerful robot dexterity intelligence (RDI), with as many as 20 degrees of freedom and hundreds of tactile sensors.[49]\n The mechanical structure of a robot must be controlled to perform tasks.[50] The control of a robot involves three distinct phases \u2013 perception, processing, and action (robotic paradigms).[51] Sensors give information about the environment or the robot itself (e.g. the position of its joints or its end effector). This information is then processed to be stored or transmitted and to calculate the appropriate signals to the actuators (motors), which move the mechanical structure to achieve the required co-ordinated motion or force actions.\n The processing phase can range in complexity. At a reactive level, it may translate raw sensor information directly into actuator commands (e.g. firing motor power electronic gates based directly upon encoder feedback signals to achieve the required torque/velocity of the shaft). Sensor fusion and internal models may first be used to estimate parameters of interest (e.g. the position of the robot's gripper) from noisy sensor data. An immediate task (such as moving the gripper in a certain direction until an object is detected with a proximity sensor) is sometimes inferred from these estimates. Techniques from control theory are generally used to convert the higher-level tasks into individual commands that drive the actuators, most often using kinematic and dynamic models of the mechanical structure.[50][51][52]\n At longer time scales or with more sophisticated tasks, the robot may need to build and reason with a \"cognitive\" model. Cognitive models try to represent the robot, the world, and how the two interact. Pattern recognition and computer vision can be used to track objects.[50] Mapping techniques can be used to build maps of the world. Finally, motion planning and other artificial intelligence techniques may be used to figure out how to act. For example, a planner may figure out how to achieve a task without hitting obstacles, falling over, etc.\n Modern commercial robotic control systems are highly complex, integrate multiple sensors and effectors, have many interacting degrees-of-freedom (DOF) and require operator interfaces, programming tools and real-time capabilities.[51] They are oftentimes interconnected to wider communication networks and in many cases are now both IoT-enabled and mobile.[53] Progress towards open architecture, layered, user-friendly and 'intelligent' sensor-based interconnected robots has emerged from earlier concepts related to Flexible Manufacturing Systems (FMS), and several 'open or 'hybrid' reference architectures exist which assist developers of robot control software and hardware to move beyond traditional, earlier notions of 'closed' robot control systems have been proposed.[52] Open architecture controllers are said to be better able to meet the growing requirements of a wide range of robot users, including system developers, end users and research scientists, and are better positioned to deliver the advanced robotic concepts related to Industry 4.0.[52] In addition to utilizing many established features of robot controllers, such as position, velocity and force control of end effectors, they also enable IoT interconnection and the implementation of more advanced sensor fusion and control techniques, including adaptive control, Fuzzy control and Artificial Neural Network (ANN)-based control.[52] When implemented in real-time, such techniques can potentially improve the stability and performance of robots operating in unknown or uncertain environments by enabling the control systems to learn and adapt to environmental changes.[54] There are several examples of reference architectures for robot controllers, and also examples of successful implementations of actual robot controllers developed from them. One example of a generic reference architecture and associated interconnected, open-architecture robot and controller implementation was used in a number of research and development studies, including prototype implementation of novel advanced and intelligent control and environment mapping methods in real-time.[54][55]\n A definition of robotic manipulation has been provided by Matt Mason as: \"manipulation refers to an agent's control of its environment through selective contact\".[56]\n Robots need to manipulate objects; pick up, modify, destroy, move or otherwise have an effect. Thus the functional end of a robot arm intended to make the effect (whether a hand, or tool) are often referred to as end effectors,[57] while the \"arm\" is referred to as a manipulator.[58] Most robot arms have replaceable end-effectors, each allowing them to perform some small range of tasks. Some have a fixed manipulator that cannot be replaced, while a few have one very general-purpose manipulator, for example, a humanoid hand.[59]\n For simplicity, most mobile robots have four wheels or a number of continuous tracks. Some researchers have tried to create more complex wheeled robots with only one or two wheels. These can have certain advantages such as greater efficiency and reduced parts, as well as allowing a robot to navigate in confined places that a four-wheeled robot would not be able to.\n Balancing robots generally use a gyroscope to detect how much a robot is falling and then drive the wheels proportionally in the same direction, to counterbalance the fall at hundreds of times per second, based on the dynamics of an inverted pendulum.[60] Many different balancing robots have been designed.[61] While the Segway is not commonly thought of as a robot, it can be thought of as a component of a robot, when used as such Segway refer to them as RMP (Robotic Mobility Platform). An example of this use has been as NASA's Robonaut that has been mounted on a Segway.[62]\n A one-wheeled balancing robot is an extension of a two-wheeled balancing robot so that it can move in any 2D direction using a round ball as its only wheel. Several one-wheeled balancing robots have been designed recently, such as Carnegie Mellon University's \"Ballbot\" which is the approximate height and width of a person, and Tohoku Gakuin University's \"BallIP\".[63] Because of the long, thin shape and ability to maneuver in tight spaces, they have the potential to function better than other robots in environments with people.[64]\n Several attempts have been made in robots that are completely inside a spherical ball, either by spinning a weight inside the ball,[65][66] or by rotating the outer shells of the sphere.[67][68] These have also been referred to as an orb bot[69] or a ball bot.[70][71]\n Using six wheels instead of four wheels can give better traction or grip in outdoor terrain such as on rocky dirt or grass.\n Tracks provide even more traction than a six-wheeled robot. Tracked wheels behave as if they were made of hundreds of wheels, therefore are very common for outdoor off-road robots, where the robot must drive on very rough terrain. However, they are difficult to use indoors such as on carpets and smooth floors. Examples include NASA's Urban Robot \"Urbie\".[72]\n Walking is a difficult and dynamic problem to solve. Several robots have been made which can walk reliably on two legs, however, none have yet been made which are as robust as a human. There has been much study on human-inspired walking, such as AMBER lab which was established in 2008 by the Mechanical Engineering Department at Texas A&M University.[73] Many other robots have been built that walk on more than two legs, due to these robots being significantly easier to construct.[74][75] Walking robots can be used for uneven terrains, which would provide better mobility and energy efficiency than other locomotion methods. Typically, robots on two legs can walk well on flat floors and can occasionally walk up stairs. None can walk over rocky, uneven terrain. Some of the methods which have been tried are:\n The zero moment point (ZMP) is the algorithm used by robots such as Honda's ASIMO. The robot's onboard computer tries to keep the total inertial forces (the combination of Earth's gravity and the acceleration and deceleration of walking), exactly opposed by the floor reaction force (the force of the floor pushing back on the robot's foot). In this way, the two forces cancel out, leaving no moment (force causing the robot to rotate and fall over).[76] However, this is not exactly how a human walks, and the difference is obvious to human observers, some of whom have pointed out that ASIMO walks as if it needs the lavatory.[77][78][79] ASIMO's walking algorithm is not static, and some dynamic balancing is used (see below). However, it still requires a smooth surface to walk on.\n Several robots, built in the 1980s by Marc Raibert at the MIT Leg Laboratory, successfully demonstrated very dynamic walking. Initially, a robot with only one leg, and a very small foot could stay upright simply by hopping. The movement is the same as that of a person on a pogo stick. As the robot falls to one side, it would jump slightly in that direction, in order to catch itself.[80] Soon, the algorithm was generalised to two and four legs. A bipedal robot was demonstrated running and even performing somersaults.[81] A quadruped was also demonstrated which could trot, run, pace, and bound.[82] For a full list of these robots, see the MIT Leg Lab Robots page.[83]\n A more advanced way for a robot to walk is by using a dynamic balancing algorithm, which is potentially more robust than the Zero Moment Point technique, as it constantly monitors the robot's motion, and places the feet in order to maintain stability.[84] This technique was recently demonstrated by Anybots' Dexter Robot,[85] which is so stable, it can even jump.[86] Another example is the TU Delft Flame.\n Perhaps the most promising approach uses passive dynamics where the momentum of swinging limbs is used for greater efficiency. It has been shown that totally unpowered humanoid mechanisms can walk down a gentle slope, using only gravity to propel themselves. Using this technique, a robot need only supply a small amount of motor power to walk along a flat surface or a little more to walk up a hill. This technique promises to make walking robots at least ten times more efficient than ZMP walkers, like ASIMO.[87][88]\n A modern passenger airliner is essentially a flying robot, with two humans to manage it. The autopilot can control the plane for each stage of the journey, including takeoff, normal flight, and even landing.[89] Other flying robots are uninhabited and are known as unmanned aerial vehicles (UAVs). They can be smaller and lighter without a human pilot on board, and fly into dangerous territory for military surveillance missions. Some can even fire on targets under command. UAVs are also being developed which can fire on targets automatically, without the need for a command from a human. Other flying robots include cruise missiles, the Entomopter, and the Epson micro helicopter robot. Robots such as the Air Penguin, Air Ray, and Air Jelly have lighter-than-air bodies, are propelled by paddles, and are guided by sonar.\n BFRs take inspiration from flying mammals, birds, or insects. BFRs can have flapping wings, which generate the lift and thrust, or they can be propeller actuated. BFRs with flapping wings have increased stroke efficiencies, increased maneuverability, and reduced energy consumption in comparison to propeller actuated BFRs.[90] Mammal and bird inspired BFRs share similar flight characteristics and design considerations. For instance, both mammal and bird inspired BFRs minimize edge fluttering and pressure-induced wingtip curl by increasing the rigidity of the wing edge and wingtips. Mammal and insect inspired BFRs can be impact resistant, making them useful in cluttered environments.\n Mammal inspired BFRs typically take inspiration from bats, but the flying squirrel has also inspired a prototype.[91] Examples of bat inspired BFRs include Bat Bot[92] and the DALER.[93] Mammal inspired BFRs can be designed to be multi-modal; therefore, they're capable of both flight and terrestrial movement. To reduce the impact of landing, shock absorbers can be implemented along the wings.[93] Alternatively, the BFR can pitch up and increase the amount of drag it experiences.[91] By increasing the drag force, the BFR will decelerate and minimize the impact upon grounding. Different land gait patterns can also be implemented.[91]\n Bird inspired BFRs can take inspiration from raptors, gulls, and everything in-between. Bird inspired BFRs can be feathered to increase the angle of attack range over which the prototype can operate before stalling.[94] The wings of bird inspired BFRs allow for in-plane deformation, and the in-plane wing deformation can be adjusted to maximize flight efficiency depending on the flight gait.[94] An example of a raptor inspired BFR is the prototype by Savastano et al.[95] The prototype has fully deformable flapping wings and is capable of carrying a payload of up to 0.8\u00a0kg while performing a parabolic climb, steep descent, and rapid recovery. The gull inspired prototype by Grant et al. accurately mimics the elbow and wrist rotation of gulls, and they find that lift generation is maximized when the elbow and wrist deformations are opposite but equal.[96]\n Insect inspired BFRs typically take inspiration from beetles or dragonflies. An example of a beetle inspired BFR is the prototype by Phan and Park,[97] and a dragonfly inspired BFR is the prototype by Hu et al.[98] The flapping frequency of insect inspired BFRs are much higher than those of other BFRs; this is because of the aerodynamics of insect flight.[99] Insect inspired BFRs are much smaller than those inspired by mammals or birds, so they are more suitable for dense environments.\n A class of robots that are biologically inspired, but which do not attempt to mimic biology, are creations such as the Entomopter. Funded by DARPA, NASA, the United States Air Force, and the Georgia Tech Research Institute and patented by Prof. Robert C. Michelson for covert terrestrial missions as well as flight in the lower Mars atmosphere, the Entomopter flight propulsion system uses low Reynolds number wings similar to those of the hawk moth (Manduca sexta), but flaps them in a non-traditional \"opposed x-wing fashion\" while \"blowing\" the surface to enhance lift based on the Coand\u0103 effect as well as to control vehicle attitude and direction. Waste gas from the propulsion system not only facilitates the blown wing aerodynamics, but also serves to create ultrasonic emissions like that of a Bat for obstacle avoidance. The Entomopter and other biologically-inspired robots leverage features of biological systems, but do not attempt to create mechanical analogs.\n Several snake robots have been successfully developed. Mimicking the way real snakes move, these robots can navigate very confined spaces, meaning they may one day be used to search for people trapped in collapsed buildings.[100] The Japanese ACM-R5 snake robot[101] can even navigate both on land and in water.[102]\n A small number of skating robots have been developed, one of which is a multi-mode walking and skating device. It has four legs, with unpowered wheels, which can either step or roll.[103] Another robot, Plen, can use a miniature skateboard or roller-skates, and skate across a desktop.[104]\n Several different approaches have been used to develop robots that have the ability to climb vertical surfaces. One approach mimics the movements of a human climber on a wall with protrusions; adjusting the center of mass and moving each limb in turn to gain leverage. An example of this is Capuchin,[105] built by Ruixiang Zhang at Stanford University, California. Another approach uses the specialized toe pad method of wall-climbing geckoes, which can run on smooth surfaces such as vertical glass. Examples of this approach include Wallbot[106] and Stickybot.[107]\n China's Technology Daily reported on 15 November 2008, that Li Hiu Yeung and his research group of New Concept Aircraft (Zhuhai) Co., Ltd. had successfully developed a bionic gecko robot named \"Speedy Freelander\". According to Yeung, the gecko robot could rapidly climb up and down a variety of building walls, navigate through ground and wall fissures, and walk upside-down on the ceiling. It was also able to adapt to the surfaces of smooth glass, rough, sticky or dusty walls as well as various types of metallic materials. It could also identify and circumvent obstacles automatically. Its flexibility and speed were comparable to a natural gecko. A third approach is to mimic the motion of a snake climbing a pole.[41]\n It is calculated that when swimming some fish can achieve a propulsive efficiency greater than 90%.[108] Furthermore, they can accelerate and maneuver far better than any man-made boat or submarine, and produce less noise and water disturbance. Therefore, many researchers studying underwater robots would like to copy this type of locomotion.[109] Notable examples are the Robotic Fish G9,[110] and Robot Tuna built to analyze and mathematically model thunniform motion.[111] The Aqua Penguin,[112] copies the streamlined shape and propulsion by front \"flippers\" of penguins. The Aqua Ray and Aqua Jelly emulate the locomotion of manta ray, and jellyfish, respectively.\n In 2014, iSplash-II was developed as the first robotic fish capable of outperforming real carangiform fish in terms of average maximum velocity (measured in body lengths/ second) and endurance, the duration that top speed is maintained.[113] This build attained swimming speeds of 11.6BL/s (i.e. 3.7\u00a0m/s).[114] The first build, iSplash-I (2014) was the first robotic platform to apply a full-body length carangiform swimming motion which was found to increase swimming speed by 27% over the traditional approach of a posterior confined waveform.[115]\n Sailboat robots have also been developed in order to make measurements at the surface of the ocean. A typical sailboat robot is Vaimos.[116] Since the propulsion of sailboat robots uses the wind, the energy of the batteries is only used for the computer, for the communication and for the actuators (to tune the rudder and the sail). If the robot is equipped with solar panels, the robot could theoretically navigate forever. The two main competitions of sailboat robots are WRSC, which takes place every year in Europe, and Sailbot.\n Control systems may also have varying levels of autonomy.\n Another classification takes into account the interaction between human control and the machine motions.\n Computer vision is the science and technology of machines that see. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and views from cameras.\n In most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common.\n Computer vision systems rely on image sensors that detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. The sensors are designed using solid-state physics. The process by which light propagates and reflects off surfaces is explained using optics. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots' \"eyes\" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities.\n There is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. Also, some of the learning-based methods developed within computer vision have a background in biology.\n Though a significant percentage of robots in commission today are either human controlled or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. These robots require some combination of navigation hardware and software in order to traverse their environment. In particular, unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. Some highly advanced robots such as ASIMO and Mein\u00fc robot have particularly good robot navigation hardware and software. Also, self-controlled cars, Ernst Dickmanns' driverless car, and the entries in the DARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information, including by a swarm of autonomous robots.[119] Most of these robots employ a GPS navigation device with waypoints, along with radar, sometimes combined with other sensory data such as lidar, video cameras, and inertial guidance systems for better navigation between waypoints.\n The state of the art in sensory intelligence for robots will have to progress through several orders of magnitude if we want the robots working in our homes to go beyond vacuum-cleaning the floors. If robots are to work effectively in homes and other non-industrial environments, the way they are instructed to perform their jobs, and especially how they will be told to stop will be of critical importance. The people who interact with them may have little or no training in robotics, and so any interface will need to be extremely intuitive. Science fiction authors also typically assume that robots will eventually be capable of communicating with humans through speech, gestures, and facial expressions, rather than a command-line interface. Although speech would be the most natural way for the human to communicate, it is unnatural for the robot. It will probably be a long time before robots interact as naturally as the fictional C-3PO, or Data of Star Trek, Next Generation. Even though the current state of robotics cannot meet the standards of these robots from science-fiction, robotic media characters (e.g., Wall-E, R2-D2) can elicit audience sympathies that increase people's willingness to accept actual robots in the future.[120] Acceptance of social robots is also likely to increase if people can meet a social robot under appropriate conditions. Studies have shown that interacting with a robot by looking at, touching, or even imagining interacting with the robot can reduce negative feelings that some people have about robots before interacting with them.[121] However, if pre-existing negative sentiments are especially strong, interacting with a robot can increase those negative feelings towards robots.[121]\n Interpreting the continuous flow of sounds coming from a human, in real time, is a difficult task for a computer, mostly because of the great variability of speech.[122] The same word, spoken by the same person may sound different depending on local acoustics, volume, the previous word, whether or not the speaker has a cold, etc.. It becomes even harder when the speaker has a different accent.[123] Nevertheless, great strides have been made in the field since Davis, Biddulph, and Balashek designed the first \"voice input system\" which recognized \"ten digits spoken by a single user with 100% accuracy\" in 1952.[124] Currently, the best systems can recognize continuous, natural speech, up to 160 words per minute, with an accuracy of 95%.[125] With the help of artificial intelligence, machines nowadays can use people's voice to identify their emotions such as satisfied or angry.[126]\n Other hurdles exist when allowing the robot to use voice for interacting with humans. For social reasons, synthetic voice proves suboptimal as a communication medium,[127] making it necessary to develop the emotional component of robotic voice through various techniques.[128][129] An advantage of diphonic branching is the emotion that the robot is programmed to project, can be carried on the voice tape, or phoneme, already pre-programmed onto the voice media. One of the earliest examples is a teaching robot named Leachim developed in 1974 by Michael J. Freeman.[130][131] Leachim was able to convert digital memory to rudimentary verbal speech on pre-recorded computer discs.[132] It was programmed to teach students in The Bronx, New York.[132]\n Facial expressions can provide rapid feedback on the progress of a dialog between two humans, and soon may be able to do the same for humans and robots. Robotic faces have been constructed by Hanson Robotics using their elastic polymer called Frubber, allowing a large number of facial expressions due to the elasticity of the rubber facial coating and embedded subsurface motors (servos).[133] The coating and servos are built on a metal skull. A robot should know how to approach a human, judging by their facial expression and body language. Whether the person is happy, frightened, or crazy-looking affects the type of interaction expected of the robot. Likewise, robots like Kismet and the more recent addition, Nexi[134] can produce a range of facial expressions, allowing it to have meaningful social exchanges with humans.[135]\n One can imagine, in the future, explaining to a robot chef how to make a pastry, or asking directions from a robot police officer. In both of these cases, making hand gestures would aid the verbal descriptions. In the first case, the robot would be recognizing gestures made by the human, and perhaps repeating them for confirmation. In the second case, the robot police officer would gesture to indicate \"down the road, then turn right\". It is likely that gestures will make up a part of the interaction between humans and robots.[136] A great many systems have been developed to recognize human hand gestures.[137]\n Proxemics is the study of personal space, and HRI systems may try to model and work with its concepts for human interactions.\n Artificial emotions can also be generated, composed of a sequence of facial expressions or gestures. As can be seen from the movie Final Fantasy: The Spirits Within, the programming of these artificial emotions is complex and requires a large amount of human observation. To simplify this programming in the movie, presets were created together with a special software program. This decreased the amount of time needed to make the film. These presets could possibly be transferred for use in real-life robots. An example of a robot with artificial emotions is Robin the Robot developed by an Armenian IT company Expper Technologies, which uses AI-based peer-to-peer interaction. Its main task is achieving emotional well-being, i.e. overcome stress and anxiety. Robin was trained to analyze facial expressions and use his face to display his emotions given the context. The robot has been tested by kids in US clinics, and observations show that Robin increased the appetite and cheerfulness of children after meeting and talking.[138]\n Many of the robots of science fiction have a personality, something which may or may not be desirable in the commercial robots of the future.[139] Nevertheless, researchers are trying to create robots which appear to have a personality:[140][141] i.e. they use sounds, facial expressions, and body language to try to convey an internal state, which may be joy, sadness, or fear. One commercial example is Pleo, a toy robot dinosaur, which can exhibit several apparent emotions.[142]\n Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robots, alternative ways to think about or design robots, and new ways to manufacture them. Other investigations, such as MIT's cyberflora project, are almost wholly academic.\n To describe the level of advancement of a robot, the term \"Generation Robots\" can be used. This term is coined by Professor Hans Moravec, Principal Research Scientist at the Carnegie Mellon University Robotics Institute in describing the near future evolution of robot technology. First-generation robots, Moravec predicted in 1997, should have an intellectual capacity comparable to perhaps a lizard and should become available by 2010. Because the first generation robot would be incapable of learning, however, Moravec predicts that the second generation robot would be an improvement over the first and become available by 2020, with the intelligence maybe comparable to that of a mouse. The third generation robot should have intelligence comparable to that of a monkey. Though fourth generation robots, robots with human intelligence, professor Moravec predicts, would become possible, he does not predict this happening before around 2040 or 2050.[143]\n The study of motion can be divided into kinematics and dynamics.[144] Direct kinematics or forward kinematics refers to the calculation of end effector position, orientation, velocity, and acceleration when the corresponding joint values are known. Inverse kinematics refers to the opposite case in which required joint values are calculated for given end effector values, as done in path planning. Some special aspects of kinematics include handling of redundancy (different possibilities of performing the same movement), collision avoidance, and singularity avoidance. Once all relevant positions, velocities, and accelerations have been calculated using kinematics, methods from the field of dynamics are used to study the effect of forces upon these movements. Direct dynamics refers to the calculation of accelerations in the robot once the applied forces are known. Direct dynamics is used in computer simulations of the robot. Inverse dynamics refers to the calculation of the actuator forces necessary to create a prescribed end-effector acceleration. This information can be used to improve the control algorithms of a robot.\n In each area mentioned above, researchers strive to develop new concepts and strategies, improve existing ones, and improve the interaction between these areas. To do this, criteria for \"optimal\" performance and ways to optimize design, structure, and control of robots must be developed and implemented.\n Open source robotics research seeks standards for defining, and methods for designing and building, robots so that they can easily be reproduced by anyone. Research includes legal and technical definitions; seeking out alternative tools and materials to reduce costs and simplify builds; and creating interfaces and standards for designs to work together. Human usability research also investigates how to best document builds through visual, text or video instructions.\n Evolutionary robots is a methodology that uses evolutionary computation to help design robots, especially the body form, or motion and behavior controllers. In a similar way to natural evolution, a large population of robots is allowed to compete in some way, or their ability to perform a task is measured using a fitness function. Those that perform worst are removed from the population and replaced by a new set, which have new behaviors based on those of the winners. Over time the population improves, and eventually a satisfactory robot may appear. This happens without any direct programming of the robots by the researchers. Researchers use this method both to create better robots,[145] and to explore the nature of evolution.[146] Because the process often requires many generations of robots to be simulated,[147] this technique may be run entirely or mostly in simulation, using a robot simulator software package, then tested on real robots once the evolved algorithms are good enough.[148] Currently, there are about 10 million industrial robots toiling around the world, and Japan is the top country having high density of utilizing robots in its manufacturing industry.[citation needed]\n Bionics and biomimetics apply the physiology and methods of locomotion of animals to the design of robots. For example, the design of BionicKangaroo was based on the way kangaroos jump.\n Swarm robotics is an approach to the coordination of multiple robots as a system which consist of large numbers of mostly simple physical robots. \u2033In a robot swarm, the collective behavior of the robots results from local interactions between the robots and between the robots and the environment in which they act.\u2033* [119]\n There has been some research into whether robotics algorithms can be run more quickly on quantum computers than they can be run on digital computers. This area has been referred to as quantum robotics.[149]\n The main venues for robotics research are the international conferences ICRA and IROS.\n Robotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics.[152] Robots have become a popular educational tool in some middle and high schools, particularly in parts of the USA,[153] as well as in numerous youth summer camps, raising interest in programming, artificial intelligence, and robotics among students.\n Robotics is an essential component in many modern manufacturing environments. As factories increase their use of robots, the number of robotics\u2013related jobs grow and have been observed to be steadily rising.[154] The employment of robots in industries has increased productivity and efficiency savings and is typically seen as a long-term investment for benefactors. A study found that 47 percent of US jobs are at risk to automation \"over some unspecified number of years\".[155] These claims have been criticized on the ground that social policy, not AI, causes unemployment.[156] In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining\".[157]   The rise of robotics is thus often used as an argument for universal basic income.\n According to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it will have grown at a compound annual growth rate (CAGR) of 29% to $568bn, driving jobs in robotics and related industries.[158]\n A discussion paper drawn up by EU-OSHA highlights how the spread of robotics presents both opportunities and challenges for occupational safety and health (OSH).[159]\n The greatest OSH benefits stemming from the wider use of robotics should be substitution for people working in unhealthy or dangerous environments. In space, defense, security, or the nuclear industry, but also in logistics, maintenance, and inspection, autonomous robots are particularly useful in replacing human workers performing dirty, dull or unsafe tasks, thus avoiding workers' exposures to hazardous agents and conditions and reducing physical, ergonomic and psychosocial risks. For example, robots are already used to perform repetitive and monotonous tasks, to handle radioactive material or to work in explosive atmospheres. In the future, many other highly repetitive, risky or unpleasant tasks will be performed by robots in a variety of sectors like agriculture, construction, transport, healthcare, firefighting or cleaning services.[160]\n Moreover, there are certain skills to which humans will be better suited than machines for some time to come and the question is how to achieve the best combination of human and robot skills. The advantages of robotics include heavy-duty jobs with precision and repeatability, whereas the advantages of humans include creativity, decision-making, flexibility, and adaptability. This need to combine optimal skills has resulted in collaborative robots and humans sharing a common workspace more closely and led to the development of new approaches and standards to guarantee the safety of the \"man-robot merger\". Some European countries are including robotics in their national programs and trying to promote a safe and flexible cooperation between robots and operators to achieve better productivity. For example, the German Federal Institute for Occupational Safety and Health (BAuA) organises annual workshops on the topic \"human-robot collaboration\".\n In the future, cooperation between robots and humans will be diversified, with robots increasing their autonomy and human-robot collaboration reaching completely new forms. Current approaches and technical standards[161][162] aiming to protect employees from the risk of working with collaborative robots will have to be revised.\n Great user experience predicts the needs, experiences, behaviors, language and cognitive abilities, and other factors of each user group. It then uses these insights to produce a product or solution that is ultimately useful and usable. For robots, user experience begins with an understanding of the robot's intended task and environment, while considering any possible social impact the robot may have on human operations and interactions with it.[163]\n It defines that communication as the transmission of information through signals, which are elements perceived through touch, sound, smell and sight.[164] The author states that the signal connects the sender to the receiver and consists of three parts: the signal itself, what it refers to, and the interpreter. Body postures and gestures, facial expressions, hand and head movements are all part of nonverbal behavior and communication. Robots are no exception when it comes to human-robot interaction. Therefore, humans use their verbal and nonverbal behaviors to communicate their defining characteristics. Similarly, social robots need this coordination to perform human-like behaviors.\n Robotics is an interdisciplinary field, combining primarily mechanical engineering and computer science but also drawing on electronic engineering and other subjects. The usual way to build a career in robotics is to complete an undergraduate degree in one of these established subjects, followed by a graduate (masters') degree in Robotics. Graduate degrees are typically joined by students coming from all of the contributing disciplines, and include familiarization of relevant undergraduate level subject matter from each of them, followed by specialist study in pure robotics topics which build upon them. As an interdisciplinary subject, robotics graduate programmes tend to be especially reliant on students working and learning together and sharing their knowledge and skills from their home discipline first degrees.\n Robotics industry careers then follow the same pattern, with most roboticists working as part of interdisciplinary teams of specialists from these home disciplines followed by the robotics graduate degrees which enable them to work together. Workers typically continue to identify as members of their home disciplines who work in robotics, rather than as 'roboticists'. This structure is reinforced by the nature of some engineering professions, which grant chartered engineer status to members of home disciplines rather than to robotics as a whole.\n Robotics careers are widely predicted to grow in the 21st century, as robots replace more manual and intellectual human work. Some workers who lose their jobs to robotics may be well-placed to retrain to build and maintain these robots, using their domain-specific knowledge and skills.\n",
        "doc_number": 7
    },
    {
        "url": "https://en.wikipedia.org/wiki/Generative_artificial_intelligence",
        "content": "\n Generative artificial intelligence (generative AI, GenAI,[1] or GAI) is a subset of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data.[2][3][4] These models learn the underlying patterns and structures of their training data and use them to produce new data[5][6] based on the input, which often comes in the form of natural language prompts.[7][8]\n Improvements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the early 2020s. These include chatbots such as ChatGPT, Copilot, Gemini, and LLaMA; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora.[9][10][11][12] Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.[7][13][14]\n Generative AI has uses across a wide range of industries, including software development, healthcare, finance, entertainment, customer service,[15] sales and marketing,[16] art, writing,[17] fashion,[18] and product design.[19] However, concerns have been raised about the potential misuse of generative AI such as cybercrime, the use of fake news or deepfakes to deceive or manipulate people, and the mass replacement of human jobs.[20][21] Intellectual property law concerns also exist around generative models that are trained on and emulate copyrighted works of art.[22]\n Since its inception, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity.[23] The concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music.[24][25] The tradition of creative automations has flourished throughout history, exemplified by Maillardet's automaton created in the early 1800s.[26] Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906,[27][28] and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is learned on a text corpus, it can then be used as a probabilistic text generator.[29][30]\n The academic discipline of artificial intelligence was established at a research workshop held at Dartmouth College in 1956 and has experienced several waves of advancement and optimism in the decades since.[31] Artificial Intelligence research began in the 1950s with works like Computing Machinery and Intelligence (1950) and the 1956 Dartmouth Summer Research Project on AI. Since the 1950s, artists and researchers have used artificial intelligence to create artistic works. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.[32]\n The terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal.[33][34] Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use,[35] process plans for manufacturing[33] and decision plans such as in prototype autonomous spacecraft.[36]\n Since its inception, the field of machine learning used both discriminative models and generative models, to model and predict data. Beginning in the late 2000s, the emergence of deep learning drove progress and research in image classification, speech recognition, natural language processing and other tasks. Neural networks in this era were typically trained as discriminative models, due to the difficulty of generative modeling.[37]\n In 2014, advancements such as the variational autoencoder and generative adversarial network produced the first practical deep neural networks capable of learning generative models, as opposed to discriminative ones, for complex data such as images. These deep generative models were the first to output not only class labels for images but also entire images.\n In 2017, the Transformer network enabled advancements in generative models compared to older Long-Short Term Memory models,[38] leading to the first generative pre-trained transformer (GPT), known as GPT-1, in 2018.[39] This was followed in 2019 by GPT-2 which demonstrated the ability to generalize unsupervised to many different tasks as a Foundation model.[40]\n The new generative models introduced during this period allowed for large neural networks to be trained using unsupervised learning or semi-supervised learning, rather than the supervised learning typical of discriminative models. Unsupervised learning removed the need for humans to manually label data, allowing for larger networks to be trained.[41]\n In March 2020, 15.ai, created by an anonymous MIT researcher, was a free web application that could generate convincing character voices using minimal training data.[42] The platform is credited as the first mainstream service to popularize AI voice cloning (audio deepfakes) in memes and content creation, influencing subsequent developments in voice AI technology.[43][44]\n In 2021, the emergence of DALL-E, a transformer-based pixel generative model, marked an advance in AI-generated imagery.[45] This was followed by the releases of Midjourney and Stable Diffusion in 2022, which further democratized access to high-quality artificial intelligence art creation from natural language prompts.[46] These systems demonstrated unprecedented capabilities in generating photorealistic images, artwork, and designs based on text descriptions, leading to widespread adoption among artists, designers, and the general public.\n In late 2022, the public release of ChatGPT revolutionized the accessibility and application of generative AI for general-purpose text-based tasks.[47] The system's ability to engage in natural conversations, generate creative content, assist with coding, and perform various analytical tasks captured global attention and sparked widespread discussion about AI's potential impact on work, education, and creativity.[48]\n In March 2023, GPT-4's release represented another jump in generative AI capabilities. A team from Microsoft Research controversially argued that it \"could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\"[49] However, this assessment was contested by other scholars who maintained that generative AI remained \"still far from reaching the benchmark of 'general human intelligence'\" as of 2023.[50] Later in 2023, Meta released ImageBind, an AI model combining multiple modalities including text, images, video, thermal data, 3D data, audio, and motion, paving the way for more immersive generative AI applications.[51]\n In December 2023, Google unveiled Gemini, a multimodal AI model available in four versions: Ultra, Pro, Flash, and Nano.[52] The company integrated Gemini Pro into its Bard chatbot and announced plans for \"Bard Advanced\" powered by the larger Gemini Ultra model.[53] In February 2024, Google unified Bard and Duet AI under the Gemini brand, launching a mobile app on Android and integrating the service into the Google app on iOS.[54]\n In March 2024, Anthropic released the Claude 3 family of large language models, including Claude 3 Haiku, Sonnet, and Opus.[55] The models demonstrated significant improvements in capabilities across various benchmarks, with Claude 3 Opus notably outperforming leading models from OpenAI and Google.[56] In June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated improved performance compared to the larger Claude 3 Opus, particularly in areas such as coding, multistep workflows, and image analysis.[57]\n According to a survey by SAS and Coleman Parkes Research, China has emerged as a global leader in generative AI adoption, with 83% of Chinese respondents using the technology, exceeding both the global average of 54% and the U.S. rate of 65%. This leadership is further evidenced by China's intellectual property developments in the field, with a UN report revealing that Chinese entities filed over 38,000 generative AI patents from 2014 to 2023, substantially surpassing the United States in patent applications.[58]\n A generative AI system is constructed by applying unsupervised machine learning (invoking  for instance neural network architectures such as generative adversarial networks (GANs), variation autoencoders (VAEs), transformers, or self-supervised machine learning trained on a dataset. The capabilities of a generative AI system depend on the modality or type of the data set used. Generative AI can be either unimodal or multimodal; unimodal systems take only one type of input, whereas multimodal systems can take more than one type of input.[59] For example, one version of OpenAI's GPT-4 accepts both text and image inputs.[60]\n Generative AI systems trained on words or word tokens include GPT-3, GPT-4, GPT-4o, LaMDA, LLaMA, BLOOM, Gemini and others (see List of large language models). They are capable of natural language processing, machine translation, and natural language generation and can be used as foundation models for other tasks.[62] Data sets include BookCorpus, Wikipedia, and others (see List of text corpora).\n In addition to natural language text, large language models can be trained on programming language text, allowing them to generate source code for new computer programs.[63] Examples include OpenAI Codex and the VS Code fork Cursor.[64]\n Producing high-quality visual art is a prominent application of generative AI.[65] Generative AI systems trained on sets of images with text captions include Imagen, DALL-E, Midjourney, Adobe Firefly, FLUX.1, Stable Diffusion and others (see Artificial intelligence art, Generative art, and Synthetic media). They are commonly used for text-to-image generation and neural style transfer.[66] Datasets include LAION-5B and others (see List of datasets in computer vision and image processing).\n \nGenerative AI can also be trained extensively on audio clips to produce natural-sounding speech synthesis and text-to-speech capabilities. An early pioneer in this field was 15.ai, launched in March 2020, which demonstrated the ability to clone character voices using as little as 15 seconds of training data.[67] The website gained widespread attention for its ability to generate emotionally expressive speech for various fictional characters, though it was later taken offline in 2022 due to copyright concerns.[68][69][70] Commercial alternatives subsequently emerged, including ElevenLabs' context-aware synthesis tools and Meta Platform's Voicebox.[71] Generative AI systems such as MusicLM[72] and MusicGen[73] can also be trained on the audio waveforms of recorded music along with text annotations, in order to generate new musical samples based on text descriptions such as a calming violin melody backed by a distorted guitar riff.\n Audio deepfakes of lyrics have been generated, like the song Savages, which used AI to mimic rapper Jay-Z's vocals. Music artist's instrumentals and lyrics are copyrighted but their voices aren't protected from regenerative AI yet, raising a debate about whether artists should get royalties from audio deepfakes.[74]\n Many AI music generators have been created that can be generated using a text phrase, genre options, and looped libraries of bars and riffs.[75]\n Generative AI trained on annotated video can generate temporally-coherent, detailed and photorealistic video clips. Examples include Sora by OpenAI,[12] Gen-1 and Gen-2 by Runway,[76] and Make-A-Video by Meta Platforms.[77]\n Generative AI can also be trained on the motions of a robotic system to generate new trajectories for motion planning or navigation. For example, UniPi from Google Research uses prompts like \"pick up blue bowl\" or \"wipe plate with yellow sponge\" to control movements of a robot arm.[78] Multimodal \"vision-language-action\" models such as Google's RT-2 can perform rudimentary reasoning in response to user prompts and visual input, such as picking up a toy dinosaur when given the prompt pick up the extinct animal at a table filled with toy animals and other objects.[79]\n Artificially intelligent computer-aided design (CAD) can use text-to-3D, image-to-3D, and video-to-3D to automate 3D modeling.[80] AI-based CAD libraries could also be developed using linked open data of schematics and diagrams.[81] AI CAD assistants are used as tools to help streamline workflow.[82]\n Generative AI models are used to power chatbot products such as ChatGPT, programming tools such as GitHub Copilot,[83] text-to-image products such as Midjourney, and text-to-video products such as Runway Gen-2.[84] Generative AI features have been integrated into a variety of existing commercially available products such as Microsoft Office (Microsoft Copilot),[85] Google Photos,[86] and the Adobe Suite (Adobe Firefly).[87] Many generative AI models are also available as open-source software, including Stable Diffusion and the LLaMA[88] language model.\n Smaller generative AI models with up to a few billion parameters can run on smartphones, embedded devices, and personal computers. For example, LLaMA-7B (a version with 7 billion parameters) can run on a Raspberry Pi 4[89] and one version of Stable Diffusion can run on an iPhone 11.[90]\n Larger models with tens of billions of parameters can run on laptop or desktop computers. To achieve an acceptable speed, models of this size may require accelerators such as the GPU chips produced by NVIDIA and AMD or the Neural Engine included in Apple silicon products. For example, the 65 billion parameter version of LLaMA can be configured to run on a desktop PC.[91]\n The advantages of running generative AI locally include protection of privacy and intellectual property, and avoidance of rate limiting and censorship. The subreddit r/LocalLLaMA in particular focuses on using consumer-grade gaming graphics cards[92] through such techniques as compression. That forum is one of only two sources Andrej Karpathy trusts for language model benchmarks.[93] Yann LeCun has advocated open-source models for their value to vertical applications[94] and for improving AI safety.[95]\n Language models with hundreds of billions of parameters, such as GPT-4 or PaLM, typically run on datacenter computers equipped with arrays of GPUs (such as NVIDIA's H100) or AI accelerator chips (such as Google's TPU). These very large models are typically accessed as cloud services over the Internet.\n In 2022, the United States New Export Controls on Advanced Computing and Semiconductors to China imposed restrictions on exports to China of GPU and AI accelerator chips used for generative AI.[96] Chips such as the NVIDIA A800[97] and the Biren Technology BR104[98] were developed to meet the requirements of the sanctions.\n There is free software on the market capable of recognizing text generated by generative artificial intelligence (such as GPTZero), as well as images, audio or video coming from it.[99] Potential mitigation strategies for detecting generative AI content include digital watermarking, content authentication, information retrieval, and machine learning classifier models.[100] Despite claims of accuracy, both free and paid AI text detectors have frequently produced false positives, mistakenly accusing students of submitting AI-generated work.[101][102]\n In the United States, a group of companies including OpenAI, Alphabet, and Meta signed a voluntary agreement with the Biden administration in July 2023 to watermark AI-generated content.[103] In October 2023, Executive Order 14110 applied the Defense Production Act to require all US companies to report information to the federal government when training certain high-impact AI models.[104][105]\n In the European Union, the proposed Artificial Intelligence Act includes requirements to disclose copyrighted material used to train generative AI systems, and to label any AI-generated output as such.[106][107]\n In China, the Interim Measures for the Management of Generative AI Services introduced by the Cyberspace Administration of China regulates any public-facing generative AI. It includes requirements to watermark generated images or videos, regulations on training data and label quality, restrictions on personal data collection, and a guideline that generative AI must \"adhere to socialist core values\".[108][109]\n Generative AI systems such as ChatGPT and Midjourney are trained on large, publicly available datasets that include copyrighted works. AI developers have argued that such training is protected under fair use, while copyright holders have argued that it infringes their rights.[110]\n Proponents of fair use training have argued that it is a transformative use and does not involve making copies of copyrighted works available to the public.[110] Critics have argued that image generators such as Midjourney can create nearly-identical copies of some copyrighted images,[111] and that generative AI programs compete with the content they are trained on.[112]\n As of 2024, several lawsuits related to the use of copyrighted material in training are ongoing.\nGetty Images has sued Stability AI over the use of its images to train Stable diffusion.[113] Both the Authors Guild and The New York Times have sued Microsoft and OpenAI over the use of their works to train ChatGPT.[114][115]\n A separate question is whether AI-generated works can qualify for copyright protection. The United States Copyright Office has ruled that works created by artificial intelligence without any human input cannot be copyrighted, because they lack human authorship.[116] However, the office has also begun taking public input to determine if these rules need to be refined for generative AI.[117]\n The development of generative AI has raised concerns from governments, businesses, and individuals, resulting in protests, legal actions, calls to pause AI experiments, and actions by multiple governments. In a July 2023 briefing of the United Nations Security Council, Secretary-General Ant\u00f3nio Guterres stated \"Generative AI has enormous potential for good and evil at scale\", that AI may \"turbocharge global development\" and contribute between $10 and $15 trillion to the global economy by 2030, but that its malicious use \"could cause horrific levels of death and destruction, widespread trauma, and deep psychological damage on an unimaginable scale\".[118]\n From the early days of the development of AI, there have been arguments put forward by ELIZA creator Joseph Weizenbaum and others about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculations and qualitative, value-based judgements.[120] In April 2023, it was reported that image generation AI has resulted in 70% of the jobs for video game illustrators in China being lost.[121][122] In July 2023, developments in generative AI contributed to the 2023 Hollywood labor disputes. Fran Drescher, president of the Screen Actors Guild, declared that \"artificial intelligence poses an existential threat to creative professions\" during the 2023 SAG-AFTRA strike.[123] Voice generation AI has been seen as a potential challenge to the voice acting sector.[124][125]\n The intersection of AI and employment concerns among underrepresented groups globally remains a critical facet. While AI promises efficiency enhancements and skill acquisition, concerns about job displacement and biased recruiting processes persist among these groups, as outlined in surveys by Fast Company. To leverage AI for a more equitable society, proactive steps encompass mitigating biases, advocating transparency, respecting privacy and consent, and embracing diverse teams and ethical considerations. Strategies involve redirecting policy emphasis on regulation, inclusive design, and education's potential for personalized teaching to maximize benefits while minimizing harms.[126]\n Generative AI models can reflect and amplify any cultural bias present in the underlying data. For example, a language model might assume that doctors and judges are male, and that secretaries or nurses are female, if those biases are common in the training data.[127] Similarly, an image model prompted with the text \"a photo of a CEO\" might disproportionately generate images of white male CEOs,[128] if trained on a racially biased data set. A number of methods for mitigating bias have been attempted, such as altering input prompts[129] and reweighting training data.[130]\n Deepfakes (a portmanteau of \"deep learning\" and \"fake\"[131]) are AI-generated media that take a person in an existing image or video and replace them with someone else's likeness using artificial neural networks.[132] Deepfakes have garnered widespread attention and concerns for their uses in deepfake celebrity pornographic videos, revenge porn, fake news, hoaxes, health disinformation, financial fraud, and covert foreign election interference.[133][134][135][136][137][138][139] This has elicited responses from both industry and government to detect and limit their use.[140][141]\n In July 2023, the fact-checking company Logically found that the popular generative AI models Midjourney, DALL-E 2 and Stable Diffusion would produce plausible disinformation images when prompted to do so, such as images of electoral fraud in the United States and Muslim women supporting India's Hindu nationalist Bharatiya Janata Party.[142][143]\n In April 2024, a paper proposed to use blockchain (distributed ledger technology) to promote \"transparency, verifiability, and decentralization in AI development and usage\".[144]\n Instances of users abusing software to generate controversial statements in the vocal style of celebrities, public officials, and other famous individuals have raised ethical concerns over voice generation AI.[145][146][147][148][149][150] In response, companies such as ElevenLabs have stated that they would work on mitigating potential abuse through safeguards and identity verification.[151]\n Concerns and fandoms have spawned from AI-generated music. The same software used to clone voices has been used on famous musicians' voices to create songs that mimic their voices, gaining both tremendous popularity and criticism.[152][153][154] Similar techniques have also been used to create improved quality or full-length versions of songs that have been leaked or have yet to be released.[155]\n Generative AI has also been used to create new digital artist personalities, with some of these receiving enough attention to receive record deals at major labels.[156] The developers of these virtual artists have also faced their fair share of criticism for their personified programs, including backlash for \"dehumanizing\" an artform, and also creating artists which create unrealistic or immoral appeals to their audiences.[157]\n Generative AI's ability to create realistic fake content has been exploited in numerous types of cybercrime, including phishing scams.[158] Deepfake video and audio have been used to create disinformation and fraud. Former Google click fraud czar Shuman Ghosemajumder has predicted that while deepfake videos initially created a stir in the media, they would soon become commonplace, and as a result, more dangerous.[159] Additionally, large-language models and other forms of text-generation AI have been at a broad scale to create fake reviews on e-commerce websites to boost ratings.[160] Cybercriminals have created large language models focused on fraud, including WormGPT and FraudGPT.[161]\n Recent research done in 2023 has revealed that generative AI has weaknesses that can be manipulated by criminals to extract harmful information bypassing ethical safeguards. The study presents example attacks done on ChatGPT including Jailbreaks and reverse psychology. Additionally, malicious individuals can use ChatGPT for social engineering attacks and phishing attacks, revealing the harmful side of these technologies.[162]\n Training frontier AI models requires an enormous amount of computing power. Usually only Big Tech companies have the financial resources to make such investments. Smaller start-ups such as Cohere and OpenAI end up buying access to data centers from Google and Microsoft respectively.[163]\n Scientists and journalists have expressed concerns about the environmental impact that the development and deployment of generative models are having: high CO2 emissions,[164][165][166] large amounts of freshwater used for data centers,[167][168] and high amounts of electricity usage.[169][165][170] There is also concern that these impacts may increase as these models are incorporated into widely used search engines such as Google Search and Bing;[169] as chatbots and other applications become more popular;[169][168] and as models need to be retrained.[169]\n Proposed mitigation strategies include factoring potential environmental costs prior to model development or data collection,[164] increasing efficiency of data centers to reduce electricity/energy usage,[167][169][165][168][170][166] building more efficient machine learning models,[167][165][168] minimizing the number of times that models need to be retrained,[166] developing a government-directed framework for auditing the environmental impact of these models,[167][166] regulating for transparency of these models,[166] regulating their energy and water usage,[167] encouraging researchers to publish data on their models' carbon footprint,[169][166] and increasing the number of subject matter experts who understand both machine learning and climate science.[166]\n The New York Times defines slop as analogous to spam: \"shoddy or unwanted A.I. content in social media, art, books and ... in search results.\"[171] Journalists have expressed concerns about the scale of low-quality generated content with respect to social media content moderation,[172] the monetary incentives from social media companies to spread such content,[172][173] false political messaging,[173] spamming of scientific research paper submissions,[174] increased time and effort to find higher quality or desired content on the Internet,[175] the indexing of generated content by search engines,[176] and on journalism itself.[177]\n A paper published by researchers at Amazon Web Services AI Labs found that over 57% of sentences from a sample of over 6 billion sentences from Common Crawl, a snapshot of web pages, were machine translated. Many of these automated translations were seen as lower quality, especially for sentences were translated across at least three languages. Many lower-resource languages (ex. Wolof, Xhosa) were translated across more languages than higher-resource languages (ex. English, French).[178][179]\n In September 2024, Robyn Speer, the author of wordfreq, an open source database that calculated word frequencies based on text from the Internet, announced that she had stopped updating the data for several reasons: high costs for obtaining data from Reddit and Twitter, excessive focus on generative AI compared to other methods in the natural language processing community, and that \"generative AI has polluted the data\".[180]\n The adoption of generative AI tools led to an explosion of AI-generated content across multiple domains. A study from University College London estimated that in 2023, more than 60,000 scholarly articles\u2014over 1% of all publications\u2014were likely written with LLM assistance.[181] According to Stanford University's Institute for Human-Centered AI, approximately 17.5% of newly published computer science papers and 16.9% of peer review text now incorporate content generated by LLMs.[182]\n Visual content follows a similar trend. Since the launch of DALL-E 2 in 2022, it is estimated that an average of 34 million images have been created daily. As of August 2023, more than 15 billion images had been generated using text-to-image algorithms, with 80% of these created by models based on Stable Diffusion.[183]\n If AI-generated content is included in new data crawls from the Internet for additional training of AI models, defects in the resulting models may occur.[184] Training an AI model exclusively on the output of another AI model produces a lower-quality model. Repeating this process, where each new model is trained on the previous model's output, leads to progressive degradation and eventually results in a \"model collapse\" after multiple iterations.[185] Tests have been conducted with pattern recognition of handwritten letters and with pictures of human faces.[186] As a consequence, the value of data collected from genuine human interactions with systems may become increasingly valuable in the presence of LLM-generated content in data crawled from the Internet.\n On the other side, synthetic data is often used as an alternative to data produced by real-world events. Such data can be deployed to validate mathematical models and to train machine learning models while preserving user privacy,[187] including for structured data.[188] The approach is not limited to text generation; image generation has been employed to train computer vision models.[189]\n In January 2023, Futurism.com broke the story that CNET had been using an undisclosed internal AI tool to write at least 77 of its stories; after the news broke, CNET posted corrections to 41 of the stories.[190]\n In April 2023, the German tabloid Die Aktuelle published a fake AI-generated interview with former racing driver Michael Schumacher, who had not made any public appearances since 2013 after sustaining a brain injury in a skiing accident. The story included two possible disclosures: the cover included the line \"deceptively real\", and the interview included an acknowledgment at the end that it was AI-generated. The editor-in-chief was fired shortly thereafter amid the controversy.[191]\n Other outlets that have published articles whose content and/or byline have been confirmed or suspected to be created by generative AI models \u2013 often with false content, errors, and/or non-disclosure of generative AI use - include:\n In May 2024, Futurism noted that a content management system video by AdVon Commerce, who had used generative AI to produce articles for many of the aforementioned outlets, appeared to show that they \"had produced tens of thousands of articles for more than 150 publishers.\"[200]\n News broadcasters in Kuwait, Greece, South Korea, India, China and Taiwan have presented news with anchors based on Generative AI models, prompting concerns about job losses for human anchors and audience trust in news that has historically been influenced by parasocial relationships with broadcasters, content creators or social media influencers.[217][218][219] Algorithmically generated anchors have also been used by allies of ISIS for their broadcasts.[220]\n In 2023, Google reportedly pitched a tool to news outlets that claimed to \"produce news stories\" based on input data provided, such as \"details of current events\". Some news company executives who viewed the pitch described it as \"[taking] for granted the effort that went into producing accurate and artful news stories.\"[221]\n In February 2024, Google launched a program to pay small publishers to write three articles per day using a beta generative AI model. The program does not require the knowledge or consent of the websites that the publishers are using as sources, nor does it require the published articles to be labeled as being created or assisted by these models.[222]\n Many defunct news sites (The Hairpin, The Frisky, Apple Daily, Ashland Daily Tidings, Clayton County Register, Southwest Journal) and blogs (The Unofficial Apple Weblog, iLounge) have undergone cybersquatting, with articles created by generative AI.[223][224][225][226][227][228][229][230]\n United States Senators Richard Blumenthal and Amy Klobuchar have expressed concern that generative AI could have a harmful impact on local news.[231] In July 2023, OpenAI partnered with the American Journalism Project to fund local news outlets for experimenting with generative AI, with Axios noting the possibility of generative AI companies creating a dependency for these news outlets.[232]\n Meta AI, a chatbot based on Llama 3 which summarizes news stories, was noted by The Washington Post to copy sentences from those stories without direct attribution and to potentially further decrease the traffic of online news outlets.[233]\n In response to potential pitfalls around the use and misuse of generative AI in journalism and worries about declining audience trust, outlets around the world, including publications such as Wired, Associated Press, The Quint, Rappler or The Guardian have published guidelines around how they plan to use and not use AI and generative AI in their work.[234][235][236][237]\n In June 2024, Reuters Institute published their Digital New Report for 2024. In a survey of people in America and Europe, Reuters Institute reports that 52% and 47% respectively are uncomfortable with news produced by \"mostly AI with some human oversight\", and 23% and 15% respectively report being comfortable. 42% of Americans and 33% of Europeans reported that they were comfortable with news produced by \"mainly human with some help from AI\". The results of global surveys reported that people were more uncomfortable with news topics including politics (46%), crime (43%), and local news (37%) produced by AI than other news topics.[238]\n",
        "doc_number": 8
    },
    {
        "url": "https://en.wikipedia.org/wiki/Self-driving_car",
        "content": "\n \n A self-driving car, also known as a  autonomous car (AC), driverless car, robotaxi, robotic car or robo-car,[1][2][3] is a car that is capable of operating with reduced or no human input.[4][5] Self-driving cars are responsible for all driving activities, such as perceiving the environment, monitoring important systems, and controlling the vehicle, which includes navigating from origin to destination.[6]\n As of late 2024[update], no system has achieved full autonomy (SAE Level 5). In December 2020, Waymo was the first to offer rides in self-driving taxis to the public in limited geographic areas (SAE Level 4),[7][failed verification] and as of April\u00a02024[update] offers services in Arizona (Phoenix) and California (San Francisco and Los Angeles). In June 2024, after a Waymo self-driving taxi crashed into a utility pole in Phoenix, Arizona,  all 672 of its Jaguar I-Pace were recalled after they were found to have susceptibility to crashing into pole like items and had their software updated.[8][9][10] In July 2021, DeepRoute.ai started offering self-driving taxi rides in Shenzhen, China. Starting in February 2022, Cruise offered self-driving taxi service in San Francisco,[11] but suspended service in 2023. In 2021, Honda was the first manufacturer to sell an SAE Level 3 car,[12][13][14] followed by Mercedes-Benz in 2023.[15]\n Experiments have been conducted on advanced driver assistance systems (ADAS) since at least the 1920s.[16] The first ADAS system was cruise control, which was invented in 1948 by Ralph Teetor.\n Trials began in the 1950s. The first semi-autonomous car was developed in 1977, by Japan's Tsukuba Mechanical Engineering Laboratory.[17] It required specially marked streets that were interpreted by two cameras on the vehicle and an analog computer. The vehicle reached speeds of 30\u00a0km/h (19\u00a0mph) with the support of an elevated rail.[18][19]\n Carnegie Mellon University's Navlab[20] and ALV[21][22] semi-autonomous projects launched in the 1980s, funded by the United States' Defense Advanced Research Projects Agency (DARPA) starting in 1984 and Mercedes-Benz and Bundeswehr University Munich's EUREKA Prometheus Project in 1987.[23] By 1985, ALV had reached 31\u00a0km/h (19\u00a0mph), on two-lane roads. Obstacle avoidance came in 1986, and day and night off-road driving by 1987.[24] In 1995 Navlab 5 completed the first autonomous US coast-to-coast journey. Traveling from Pittsburgh, Pennsylvania and San Diego, California, 98.2% of the trip was autonomous. It completed the trip at an average speed of 63.8\u00a0mph (102.7\u00a0km/h).[25][26][27][28] Until the second DARPA Grand Challenge in 2005, automated vehicle research in the United States was primarily funded by DARPA, the US Army, and the US Navy, yielding incremental advances in speeds, driving competence, controls, and sensor systems.[29]\n The US allocated US$650\u00a0million in 1991 for research on the National Automated Highway System,[30] which demonstrated automated driving, combining highway-embedded automation with vehicle technology, and cooperative networking between the vehicles and highway infrastructure. The programme concluded with a successful demonstration in 1997.[31] Partly funded by the National Automated Highway System and DARPA, Navlab drove 4,584\u00a0km (2,848\u00a0mi) across the US in 1995, 4,501\u00a0km (2,797\u00a0mi) or 98% autonomously.[32] In 2015, Delphi piloted a Delphi technology-based Audi, over 5,472\u00a0km (3,400\u00a0mi) through 15 states, 99% autonomously.[33] In 2015, Nevada, Florida, California, Virginia, Michigan, and Washington DC allowed autonomous car testing on public roads.[34]\n From 2016 to 2018, the European Commission funded development for connected and automated driving through Coordination Actions CARTRE and SCOUT programs.[35] The Strategic Transport Research and Innovation Agenda (STRIA) Roadmap for Connected and Automated Transport was published in 2019.[36]\n In November 2017, Waymo announced testing of autonomous cars without a safety driver.[37] However, an employee was in the car to handle emergencies.[38]\n In March 2018, Elaine Herzberg became the first reported pedestrian killed by a self-driving car, an Uber test vehicle with a human backup driver; prosecutors did not charge Uber, while the human driver was sentenced to probation.[39]\n In December 2018, Waymo was the first to commercialize a robotaxi service, in Phoenix, Arizona.[40] In October 2020, Waymo launched a robotaxi service in a (geofenced) part of the area.[41][42] The cars were monitored in real-time, and remote engineers intervened to handle exceptional conditions.[43][42]\n In March 2019, ahead of Roborace, Robocar set the Guinness World Record as the world's fastest autonomous car. Robocar reached 282.42\u00a0km/h (175.49\u00a0mph).[44]\n In March 2021, Honda began leasing in Japan a limited edition of 100 Legend Hybrid EX sedans equipped with Level 3 \"Traffic Jam Pilot\" driving technology, which legally allowed drivers to take their eyes off the road when the car was travelling under 30 kilometres per hour (19\u00a0mph).[12][13][45][14]\n In December 2020, Waymo became the first service provider to offer driverless taxi rides to the general public, in a part of Phoenix, Arizona. Nuro began autonomous commercial delivery operations in California in 2021.[46] DeepRoute.ai launched robotaxi service in Shenzhen in July 2021.[47] In December 2021, Mercedes-Benz received approval for a Level 3 car.[48] In February 2022, Cruise became the second service provider to offer driverless taxi rides to the general public, in San Francisco.[11] In December 2022, several manufacturers scaled back plans for self-driving technology, including Ford and Volkswagen.[49] In 2023, Cruise suspended its robotaxi service.[50] Nuro was approved for Level 4 in Palo Alto in August, 2023.[51]\n As of August\u00a02023[update], vehicles operating at Level 3 and above were an insignificant market factor;[citation needed] as of early 2024, Honda leases a Level 3 car in Japan, and Mercedes sells two Level 3 cars in Germany, California and Nevada.[52][53]\n Organizations such as SAE have proposed terminology standards. However, most terms have no standard definition and are employed variously by vendors and others. Proposals to adopt aviation automation terminology for cars have not prevailed.[54]\n Names such as AutonoDrive, PilotAssist, Full-Self Driving or DrivePilot are used even though the products offer an assortment of features that may not match the names.[55] Despite offering a system it called Full Self-Driving, Tesla stated that its system did not autonomously handle all driving tasks.[56] In the United Kingdom, a fully self-driving car is defined as a car so registered, rather than one that supports a specific feature set.[57] The Association of British Insurers claimed that the usage of the word autonomous in marketing was dangerous because car ads make motorists think \"autonomous\" and \"autopilot\" imply that the driver can rely on the car to control itself, even though they do not.\n SAE identified 6 levels for driving automation from level 0 to level 5.[58] An ADS is an SAE J3016 level 3 or higher system. \n An ADAS is a system that automates specific driving features, such as Forward Collision Warning (FCW), Automatic Emergency Braking (AEB), Lane Departure Warning (LDW), Lane Keeping Assistance (LKA) or Blind Spot Warning (BSW).[59] An ADAS requires a human driver to handle tasks that the ADAS does not support.\n Autonomy implies that an automation system is under the control of the vehicle rather than a driver. Automation is function-specific, handling issues such as speed control, but leaves broader decision-making to the driver.[60]\n Euro NCAP defined autonomous as \"the system acts independently of the driver to avoid or mitigate the accident\".[61]\n In Europe, the words automated and autonomous can be used together. For instance, Regulation (EU) 2019/2144 supplied:[62]\n A remote driver is a driver that operates a vehicle at a distance, using a video and data connection.[63]\n \nAccording to SAE J3016,  Some driving automation systems may indeed be autonomous if they perform all of their functions independently and self-sufficiently, but if they depend on communication and/or cooperation with outside entities, they should be considered cooperative rather than autonomous. Operational design domain (ODD) is a term for a particular operating context for an automated system, often used in the field of autonomous vehicles. The context is defined by a set of conditions, including environmental, geographical, time of day, and other conditions. For vehicles, traffic and roadway characteristics are included. Manufacturers use ODD to indicate where/how their product operates safely. A given system may operate differently according to the immediate ODD.[64]\n Vendors have taken a variety of approaches to the self-driving problem. Tesla's approach is to allow their \"full self-driving\" (FSD) system to be used in all ODDs as a Level 2 (hands/on, eyes/on) ADAS.[66] Waymo picked specific ODDs (city streets in Phoenix and San Francisco) for their Level 5 robotaxi service.[67] Mercedes Benz offers Level 3 service in Las Vegas in highway traffic jams at speeds up to 40 miles per hour (64\u00a0km/h).[68] Mobileye's SuperVision system offers hands-off/eyes-on driving on all road types at speeds up to 130 kilometres per hour (81\u00a0mph).[69] GM's hands-free Super Cruise operates on specific roads in specific conditions, stopping or returning control to the driver when ODD changes. In 2024 the company announced plans to expand road coverage from 400,000 miles to 750,000 miles.[70] Ford's BlueCruise hands-off system operates on 130,000 miles of US divided highways.[71]\n The Union of Concerned Scientists defined self-driving as \"cars or trucks in which human drivers are never required to take control to safely operate the vehicle. Also known as autonomous or 'driverless' cars, they combine sensors and software to control, navigate, and drive the vehicle.\"[72]\n The British Automated and Electric Vehicles Act 2018 law defines a vehicle as \"driving itself\" if the vehicle is \"not being controlled, and does not need to be monitored, by an individual\".[73]\n Another British government definition stated, \"Self-driving vehicles are vehicles that can safely and lawfully drive themselves\".[74]\n In British English, the word automated alone has several meanings, such as in the sentence: \"Thatcham also found that the automated lane keeping systems could only meet two out of the twelve principles required to guarantee safety, going on to say they cannot, therefore, be classed as 'automated driving', preferring 'assisted driving'\".[75] The first occurrence of the \"automated\" word refers to an Unece automated system, while the second refers to the British legal definition of an automated vehicle. British law interprets the meaning of \"automated vehicle\" based on the interpretation section related to a vehicle \"driving itself\" and an insured vehicle.[76]\n In November 2023 the British Government introduced the Automated Vehicles Bill. It proposed definitions for related terms:[77]\n A six-level classification system \u2013 ranging from fully manual to fully automated \u2013 was published in 2014 by SAE International as J3016, Taxonomy and Definitions for Terms Related to On-Road Motor Vehicle Automated Driving Systems; the details are revised occasionally.[80] This classification is based on the role of the driver, rather than the vehicle's capabilities, although these are related. After SAE updated its classification in 2016, (J3016_201609),[81] the National Highway Traffic Safety Administration (NHTSA) adopted the SAE standard.[82][83] The classification is a topic of debate, with various revisions proposed.[84][85]\n A \"driving mode\", aka driving scenario, combines an ODD with matched driving requirements (e.g., expressway merging, traffic jam).[1][86] Cars may switch levels in accord with the driving mode. \n Above Level 1, level differences are related to how responsibility for safe movement is divided/shared between ADAS and driver rather than specific driving features. \n SAE Automation Levels have been criticized[by whom?] for their technological focus. It has been argued that the structure of the levels suggests that automation increases linearly and that more automation is better, which may not be the case.[87] SAE Levels also do not account for changes that may be required to infrastructure[88] and road user behavior.[89][90]\n Mobileye CEO Amnon Shashua and CTO Shai Shalev-Shwartz proposed an alternative taxonomy for autonomous driving systems, claiming that a more consumer-friendly approach was needed. Its categories reflect the amount of driver engagement that is required.[91][92] Some vehicle makers have informally adopted some of the terminology involved, while not formally committing to it.[93][94][95][96]\n The first level, hands-on/eyes-on, implies that the driver is fully engaged in operating the vehicle, but is supervised by the system, which intervenes according to the features it supports (e.g., adaptive cruise control, automatic emergency braking). The driver is entirely responsible, with hands on the wheel, and eyes on the road.[92]\n Eyes-on/hands-off allows the driver to let go of the wheel. The system drives, the driver monitors and remains prepared to resume control as needed.[92]\n Eyes-off/hands-off means that the driver can stop monitoring the system, leaving the system in full control.  Eyes-off requires that no errors be reproducible (not triggered by exotic transitory conditions) or frequent, that speeds are contextually appropriate (e.g., 80\u00a0mph on limited-access roads), and that the system handle typical maneuvers (e.g., getting cut off by another vehicle). The automation level could vary according to the road (e.g., eyes-off on freeways, eyes-on on side streets).[92]\n The highest level does not require a human driver in the car: monitoring is done either remotely (telepresence) or not at all.[92]\n A critical requirement for the higher two levels is that the vehicle be able to conduct a Minimum Risk Maneuver and stop safely out of traffic without driver intervention.[92]\n The perception system processes visual and audio data from outside and inside the car to create a local model of the vehicle, the road, traffic, traffic controls and other observable objects, and their relative motion. The control system then takes actions to move the vehicle, considering the local model, road map, and driving regulations.[97][98][99][100]\n Several classifications have been proposed to describe ADAS technology. One proposal is to adopt these categories: navigation, path planning, perception, and car control.[101]\n Navigation involves the use of maps to define a path between origin and destination. Hybrid navigation is the use of multiple navigation systems. Some systems use basic maps, relying on perception to deal with anomalies. Such a map understands which roads lead to which others, whether a road is a freeway, a highway, are one-way, etc. Other systems require highly detailed maps, including lane maps, obstacles, traffic controls, etc. \n ACs need to be able to perceive the world around them. Supporting technologies include combinations of cameras, LiDAR, radar, audio, and ultrasound,[102] GPS, and inertial measurement.[103][104][105] Deep neural networks are used to analyse inputs from these sensors to detect and identify objects and their trajectories.[106] Some systems use Bayesian simultaneous localization and mapping (SLAM) algorithms. Another technique is detection and tracking of other moving objects (DATMO), used to handle potential obstacles.[107][108] Other systems use roadside real-time locating system (RTLS) technologies to aid localization. Tesla's \"vision only\" system uses eight cameras, without LIDAR or radar, to create its bird's-eye view of the environment.[109]\n Path planning finds a sequence of segments that a vehicle can use to move from origin to destination. Techniques used for path planning include graph-based search and variational-based optimization techniques. Graph-based techniques can make harder decisions such as how to pass another vehicle/obstacle. Variational-based optimization techniques require more stringent restrictions on the vehicle's path to prevent collisions.[110] The large scale path of the vehicle can be determined by using a voronoi diagram, an occupancy grid mapping, or a driving corridor algorithm. The latter allows the vehicle to locate and drive within open space that is bounded by lanes or barriers.[111]\n Maps are necessary for navigation. Map sophistication varies from simple graphs that show which roads connect to each other, with details such as one-way vs two-way, to those that are highly detailed, with information about lanes, traffic controls, roadworks, and more.[102] Researchers at the MITComputer Science and Artificial Intelligence Laboratory (CSAIL) developed a system called MapLite, which allows self-driving cars to drive with simple maps. The system combines the GPS position of the vehicle, a \"sparse topological map\" such as OpenStreetMap (which has only 2D road features), with sensors that observe road conditions.[112] One issue with highly-detailed maps is updating them as the world changes. Vehicles that can operate with less-detailed maps do not require frequent updates or geo-fencing.\n Sensors are necessary for the vehicle to properly respond to the driving environment. Sensor types include cameras, LiDAR, ultrasound, and radar. Control systems typically combine data from multiple sensors.[113]  Multiple sensors can provide a more complete view of the surroundings and can be used to cross-check each other to correct errors.[114] For example, radar can image a scene in, e.g., a nighttime snowstorm, that defeats cameras and LiDAR, albeit at reduced precision. After experimenting with radar and ultrasound, Tesla adopted a vision-only approach, asserting that humans drive using only vision, and that cars should be able to do the same, while citing the lower cost of cameras versus other sensor types.[115] By contrast, Waymo makes use of the higher resolution of LiDAR sensors and cites the declining cost of that technology.[116]\n Drive by wire is the use of electrical or electro-mechanical systems for performing vehicle functions such as steering or speed control that are traditionally achieved by mechanical linkages.\n Driver monitoring is used to assess the driver's attention and alertness. Techniques in use include eye monitoring, and requiring the driver to maintain torque on the steering wheel.[117] It attempts to understand driver status and identify dangerous driving behaviors.[118]\n Vehicles can potentially benefit from communicating with others to share information about traffic, road obstacles, to receive map and software updates, etc.[119][120][102]\n ISO/TC 22 specifies in-vehicle transport information and control systems,[121] while ISO/TC 204 specifies information, communication and control systems in surface transport.[122] International standards have been developed for ADAS functions, connectivity, human interaction, in-vehicle systems, management/engineering, dynamic map and positioning, privacy and security.[123]\n Rather than communicating among vehicles, they can communicate with road-based systems to receive similar information.\n Software controls the vehicle, and can provide entertainment and other services. Over-the-air updates can deliver bug fixes and additional features over the internet. Software updates are one way to accomplish recalls that in the past required a visit to a service center. In March 2021, the UNECE regulation on software update and software update management systems was published.[124]\n A safety model is software that attempts to formalize rules that ensure that ACs operate safely.[125]\n IEEE is attempting to forge a standard for safety models as \"IEEE P2846: A Formal Model for Safety Considerations in Automated Vehicle Decision Making\".[126] In 2022, a research group at National Institute of Informatics (NII, Japan) enhanced Mobileye's Reliable Safety System as \"Goal-Aware RSS\" to enable RSS rules to deal with complex scenarios via program logic.[127]\n The US has standardized the use of turquoise lights to inform other drivers that a vehicle is driving autonomously. It will be used in the 2026 Mercedes-Benz EQS and S-Class sedans with Drive Pilot, an SAE Level 3 driving system.[citation needed]\n As of 2023, the Turquoise light had not been standardized by the P.R.C or the UN-ECE.[128]\n Artificial intelligence (AI) plays a pivotal role in the development and operation of autonomous vehicles (AVs), enabling them to perceive their surroundings, make decisions, and navigate safely without human intervention. AI algorithms empower AVs to interpret sensory data from various onboard sensors, such as cameras, LiDAR, radar, and GPS, to understand their environment and improve its technological ability and overall safety over time.[129]\n The primary obstacle to ACs is the advanced software and mapping required to make them work safely across the wide variety of conditions that drivers experience.[130] In addition to handling day/night driving in good and bad weather[131] on roads of arbitrary quality, ACs must cope with other vehicles, road obstacles, poor/missing traffic controls, flawed maps, and handle endless edge cases, such as following the instructions of a police officer managing traffic at a crash site.\n Other obstacles include cost, liability,[132][133] consumer reluctance,[134] ethical dilemmas,[135][136] security,[137][138][139][140] privacy,[131] and legal/regulatory framework.[141] Further, AVs could automate the work of professional drivers, eliminating many jobs, which could slow acceptance.[142]\n Tesla calls its Level 2 ADAS \"Full Self-Driving (FSD) Beta\".[143] US Senators Richard Blumenthal and Edward Markey called on the Federal Trade Commission (FTC) to investigate this marketing in 2021.[144] In December 2021 in Japan, Mercedes-Benz was punished by the Consumer Affairs Agency for misleading product descriptions.[145]\n Mercedes-Benz was criticized for a misleading US commercial advertising E-Class models.[146] At that time, Mercedes-Benz rejected the claims and stopped its \"self-driving car\" ad campaign that had been running.[147][148] In August 2022, the California Department of Motor Vehicles (DMV) accused Tesla of deceptive marketing practices.[149]\n With the Automated Vehicles Bill (AVB) self-driving car-makers could face prison for misleading adverts in the United-Kingdom.[150]\n In the 2020s, concerns over ACs' vulnerability to cyberattacks and data theft emerged.[151]\n In 2018 and 2019 former Apple engineers were charged with stealing information related to Apple's self-driving car project.[152][153][154] In 2021 the United States Department of Justice (DOJ) accused Chinese security officials of coordinating a hacking campaign to steal information from government entities, including research related to autonomous vehicles.[155][156] China has prepared \"the Provisions on Management of Automotive Data Security (Trial) to protect its own data\".[157][158]\n Cellular Vehicle-to-Everything technologies are based on 5G wireless networks.[159] As of November 2022[update], the US Congress was considering the possibility that imported Chinese AC technology could facilitate espionage.[160]\n Testing of Chinese automated cars in the US has raised concern over which US data are collected by Chinese vehicles to be stored in Chinese country and concern with any link with the Chinese communist party.[161]\n ACs complicate the need for drivers to communicate with each other, e.g., to decide which car enters an intersection first. In an AC without a driver, traditional means such as hand signals do not work (no driver, no hands).[162]\n ACs must be able to predict the behavior of possibly moving vehicles, pedestrians, etc in real time in order to proceed safely.[99] The task becomes more challenging the further into the future the prediction extends, requiring rapid revisions to the estimate to cope with unpredicted behavior. One approach is to wholly recompute the position and trajectory of each object many times per second. Another is to cache the results of an earlier prediction for use in the next one to reduce computational complexity.[163][164]\n The ADAS has to be able to safely accept control from and return control to the driver.[165]\n Consumers will avoid ACs unless they trust them as safe.[166][167] Robotaxis operating in San Francisco received pushback over perceived safety risks.[168] Automatic elevators were invented in 1900, but did not become common until operator strikes and trust was built with advertising and features such as an emergency stop button.[169][170] However, with repeated use of autonomous driving functions, drivers' behavior and trust in autonomous vehicles gradually improved and both entered a more stable state. At the same time this also improved the performance and reliability of the vehicle in complex conditions, thereby increasing public trust.[171]\n Autonomous also present various political and economic implications. The transportation sector holds significant sway in many the political and economic landscapes. For instance, many US states generates much annual revenue from transportation fees and taxes.[172] The advent of self-driving cars could profoundly affect the economy by potentially altering state tax revenue streams. Furthermore, the transition to autonomous vehicles might disrupt employment patterns and labor markets, particularly in industries heavily reliant on driving professions.[172] Data from the U.S. Bureau of Labor Statistics indicates that in 2019, the sector employed over two million individuals as tractor-trailer truck drivers.[173] Additionally, taxi and delivery drivers represented approximately 370,400 positions, and bus drivers constituted a workforce of over 680,000.[174][175][176] Collectively, this amounts to a conceivable displacement of nearly 2.9 million jobs, surpassing the job losses experienced in the 2008 Great Recession.[177]\n The prominence of certain demographic groups within the tech industry inevitably shapes the trajectory of autonomous vehicle (AV) development, potentially perpetuating existing inequalities. There are others in society without a political agenda who believe that the advancement of technology has nothing to do with promoting inequalities in certain groups and see this as a ridiculous presumption.  [178]\n Research from Georgia Tech revealed that autonomous vehicle detection systems were generally five percent less effective at recognizing darker-skinned individuals. This accuracy gap persisted despite adjustments for environmental variables like lighting and visual obstructions.[179]\n Standards for liability have yet to be adopted to address crashes and other incidents. Liability could rest with the vehicle occupant, its owner, the vehicle manufacturer, or even the ADAS technology supplier, possibly depending on the circumstances of the crash.[180] Additionally, the infusion of ArtificiaI Intelligence technology in autonomous vehicles adds layers of complexity to ownership and ethical dynamics. Given that AI systems are inherently self-learning, a question arises of whether accountability should rest with the vehicle owner, the manufacturer, or the AI developer?[181]\n The trolley problem is a thought experiment in ethics. Adapted for ACs, it considers an AC carrying one passenger confronts a pedestrian who steps in its way. The ADAS notionally has to choose between killing the pedestrian or swerving into a wall, killing the passenger.[182] Possible frameworks include deontology (formal rules) and utilitarianism (harm reduction).[99][183][184]\n One public opinion survey reported that harm reduction was preferred, except that passengers wanted the vehicle to prefer them, while pedestrians took the opposite view. Utilitarian regulations were unpopular.[185] Additionally, cultural viewpoints exert substantial influence on shaping responses to these ethical quandaries. Another study found that cultural biases impact preferences in prioritizing the rescue of certain individuals over others in car accident scenarios.[181]\n Some ACs require an internet connection to function, opening the possibility that a hacker might gain access to private information such as destinations, routes, camera recordings, media preferences, and/or behavioral patterns, although this is true of an internet-connected device.[186][187][188]\n ACs make use of road infrastructure (e.g., traffic signs, turn lanes) and may require modifications to that infrastructure to fully achieve their safety and other goals.[189] In March 2023, the Japanese government unveiled a plan to set up a dedicated highway lane for ACs.[190] In April 2023, JR East announced their challenge to raise their self-driving level of Kesennuma Line bus rapid transit (BRT) in rural area from the current Level 2 to Level 4 at 60\u00a0km/h.[191]\n ACs can be tested via digital simulations,[192][193] in a controlled test environment,[194] and/or on public roads. Road testing typically requires some form of permit[195] or a commitment to adhere to acceptable operating principles.[196] For example, New York requires a test driver to be in the vehicle, prepared to override the ADAS as necessary.[197]\n In California, self-driving car manufacturers are required to submit annual reports describing how often their vehicles autonomously disengaged from autonomous mode.[198] This is one measure of system robustness (ideally, the system should never disengage).[199]\n In 2017, Waymo reported 63 disengagements over 352,545\u00a0mi (567,366\u00a0km) of testing, an average distance of 5,596\u00a0mi (9,006\u00a0km) between disengagements, the highest (best) among companies reporting such figures. Waymo also logged more autonomous miles than other companies. Their 2017 rate of 0.18 disengagements per 1,000\u00a0mi (1,600\u00a0km) was an improvement over the 0.2 disengagements per 1,000\u00a0mi (1,600\u00a0km) in 2016, and 0.8 in 2015. In March 2017, Uber reported an average of 0.67\u00a0mi (1.08\u00a0km) per disengagement. In the final three months of 2017, Cruise (owned by GM) averaged 5,224\u00a0mi (8,407\u00a0km) per disengagement over 62,689\u00a0mi (100,888\u00a0km).[200]\n Reporting companies use varying definitions of what qualifies as a disengagement, and such definitions can change over time.[202][199] Executives of self-driving car companies have criticized disengagements as a deceptive metric, because it does not consider varying road conditions.[203]\n In April 2021, WP.29 GRVA proposed a \"Test Method for Automated Driving (NATM)\".[204]\n In October 2021, Europe's pilot test, L3Pilot, demonstrated ADAS for cars in Hamburg, Germany, in conjunction with ITS World Congress 2021. SAE Level 3 and 4 functions were tested on ordinary roads.[205][206][207]\n In November 2022, an International Standard ISO 34502 on \"Scenario based safety evaluation framework\" was published.[208][209]\n In April 2022, collision avoidance testing was demonstrated by Nissan.[210][211] Waymo published a document about collision avoidance testing in December 2022.[212]\n In September 2022, Biprogy released Driving Intelligence Validation Platform (DIVP) as part of Japanese national project \"SIP-adus\", which is interoperable with Open Simulation Interface (OSI) of ASAM.[213][214][215]\n In November 2022, Toyota demonstrated one of its GR Yaris test cars, which had been trained using professional rally drivers.[216] Toyota used its collaboration with Microsoft in FIA World Rally Championship since the 2017 season.[217]\n In 2023 David R. Large, senior research fellow with the Human Factors Research Group at the University of Nottingham, disguised himself as a car seat in a study to test people's reactions to driverless cars. He said, \"We wanted to explore how pedestrians would interact with a driverless car and developed this unique methodology to explore their reactions.\" The study found that, in the absence of someone in the driving seat, pedestrians trust certain visual prompts more than others when deciding whether to cross the road.[218]\n As of 2023, Tesla's ADAS Autopilot/Full Self Driving (beta) was classified as Level 2 ADAS.[219]\n On 20 January 2016, the first of five known fatal crashes of a Tesla with Autopilot occurred, in China's Hubei province.[220] Initially, Tesla stated that the vehicle was so badly damaged from the impact that their recorder was not able to determine whether the car had been on Autopilot at the time. However, the car failed to take evasive action. \n Another fatal Autopilot crash occurred in May in Florida in a Tesla Model S[221][222] that crashed into a tractor-trailer. In a civil suit between the father of the driver killed and Tesla, Tesla documented that the car had been on Autopilot.[223] According to Tesla, \"neither Autopilot nor the driver noticed the white side of the tractor-trailer against a brightly lit sky, so the brake was not applied.\" Tesla claimed that this was Tesla's first known Autopilot death in over 130\u00a0million miles (210\u00a0million kilometers) with Autopilot engaged. Tesla claimed that on average one fatality occurs every 94\u00a0million miles (151\u00a0million kilometers) across all vehicle types in the US.[224][225][226] However, this number also includes motorcycle/pedestrian fatalities.[227][228] The ultimate National Transportation Safety Board (NTSB) report concluded Tesla was not at fault; the investigation revealed that for Tesla cars, the crash rate dropped by 40 percent after Autopilot was installed.[229]\n In June 2015, Google confirmed that 12 vehicles had suffered collisions as of that date. Eight involved rear-end collisions at a stop sign or traffic light, in two of which the vehicle was side-swiped by another driver, one in which another driver rolled a stop sign, and one where a driver was controlling the car manually.[230] In July 2015, three employees suffered minor injuries when their vehicle was rear-ended by a car whose driver failed to brake. This was the first collision that resulted in injuries.[231]\n According to Google Waymo's accident reports as of early 2016, their test cars had been involved in 14 collisions, of which other drivers were at fault 13 times, although in 2016 the car's software caused a crash.[232] On 14 February 2016 a Google vehicle attempted to avoid sandbags blocking its path. During the maneuver it struck a bus. Google stated, \"In this case, we clearly bear some responsibility, because if our car hadn't moved, there wouldn't have been a collision.\"[233][234] Google characterized the crash as a misunderstanding and a learning experience. No injuries were reported.[232]\n In March 2018, Elaine Herzberg died after she was hit by an AC tested by Uber's Advanced Technologies Group (ATG) in Arizona. A safety driver was in the car. Herzberg was crossing the road about 400 feet from an intersection.[235] Some experts said a human driver could have avoided the crash.[236] Arizona governor Doug Ducey suspended the company's ability to test its ACs citing an \"unquestionable failure\" of Uber to protect public safety.[237] Uber also stopped testing in California until receiving a new permit in 2020.[238][239]\n NTSB's final report determined that the immediate cause of the accident was that safety driver Rafaela Vasquez failed to monitor the road, because she was distracted by her phone, but that Uber's \"inadequate safety culture\" contributed. The report noted that the victim had \"a very high level\" of methamphetamine in her body.[240] The board called on federal regulators to carry out a review before allowing automated test vehicles to operate on public roads.[241][242]\n In September 2020, Vasquez pled guilty to endangerment and was sentenced to three years' probation.[243][39]\n On 12 August 2021, a 31-year-old Chinese man was killed after his NIO ES8 collided with a construction vehicle.[citation needed] NIO's self-driving feature was in beta and could not deal with static obstacles.[244] The vehicle's manual clearly stated that the driver must take over near construction sites. Lawyers of the deceased's family questioned NIO's private access to the vehicle, which they argued did not guarantee the integrity of the data.[245]\n In November 2021, the California Department of Motor Vehicles (DMV) notified Pony.ai that it was suspending its testing permit following a reported collision in Fremont on 28 October.[246] In May 2022, DMV revoked Pony.ai's permit for failing to monitor the driving records of its safety drivers.[247]\n In April 2022, Cruise's testing vehicle was reported to have blocked a fire engine on emergency call, and sparked questions about its ability to handle unexpected circumstances.[248][249]\n In February 2024, a driver using the Ford BlueCruise hands-free driving feature struck and killed the driver of a stationary car with no lights on in the middle lane of a freeway in Texas.[250]\n In March 2024, a drunk driver who was speeding, holding her cell phone, and using BlueCruise on a Pennsylvania freeway struck and killed two people who had been driving two cars.[251] The first car had become disabled and was on the left shoulder with part of the car in the left driving lane.[251] The second driver had parked his car behind the first car presumably to help the first driver.[251]\n The NTSB is investigating both incidents.[252]\n The NHTSA began mandating incident reports from autonomous vehicle companies in June 2021. Some reports cite incidents from as early as August 2019, with current data available through June 17, 2024.[253]\n There have been a total of 3,979 autonomous vehicle incidents (both ADS and ADAS) reported during this timeframe. 2,146 of those incidents (53.9%) involved Tesla vehicles.[254]\n In a 2011 online survey of 2,006 US and UK consumers, 49% said they would be comfortable using a \"driverless car\".[255]\n A 2012 survey of 17,400 vehicle owners found 37% who initially said they would be interested in purchasing a \"fully autonomous car\". However, that figure dropped to 20% if told the technology would cost US$3,000 more.[256]\n In a 2012 survey of about 1,000 German drivers, 22% had a positive attitude, 10% were undecided, 44% were skeptical and 24% were hostile.[257]\n A 2013 survey of 1,500 consumers across 10 countries found 57% \"stated they would be likely to ride in a car controlled entirely by technology that does not require a human driver\", with Brazil, India and China the most willing to trust automated technology.[258]\n In a 2014 US telephone survey, over three-quarters of licensed drivers said they would consider buying a self-driving car, rising to 86% if car insurance were cheaper. 31.7% said they would not continue to drive once an automated car was available.[259]\n In 2015, a survey of 5,000 people from 109 countries reported that average respondents found manual driving the most enjoyable. 22% did not want to pay more money for autonomy. Respondents were found to be most concerned about hacking/misuse, and were also concerned about legal issues and safety. Finally, respondents from more developed countries were less comfortable with their vehicle sharing data.[260] The survey reported consumer interest in purchasing an AC, stating that 37% of surveyed current owners were either \"definitely\" or \"probably\" interested.[260]\n In 2016, a survey of 1,603 people in Germany that controlled for age, gender, and education reported that men felt less anxiety and more enthusiasm, whereas women showed the opposite. The difference was pronounced between young men and women and decreased with age.[261]\n In a 2016 US  survey of 1,584 people, \"66 percent of respondents said they think autonomous cars are probably smarter than the average human driver\". People were worried about safety and hacking risk. Nevertheless, only 13% of the interviewees saw no advantages in this new kind of cars.[262]\n In a 2017 survey of 4,135 US adults found that many Americans anticipated significant impacts from various automation technologies including the widespread adoption of automated vehicles.[263]\n In 2019, results from two opinion surveys of 54 and 187 US adults respectively were published. The questionnaire was termed the autonomous vehicle acceptance model (AVAM), including additional description to help respondents better understand the implications of various automation levels. Users were less accepting of high autonomy levels and displayed significantly lower intention to use autonomous vehicles. Additionally, partial autonomy (regardless of level) was perceived as requiring uniformly higher driver engagement (usage of hands, feet and eyes) than full autonomy.[264]\n In 2022, a survey reported that only a quarter (27%) of the world's population would feel safe in self-driving cars.[265]\n In 2024, a study by Saravanos et al.[266] at New York University reported that 87% of their respondents (from a sample of 358) believed that conditionally automated cars (at Level 3) would be easy to use.\n Opinion surveys may have little salience given that few respondents had any personal experience with ACs.\n The regulation of autonomous cars concerns liability, approvals, and international conventions.\n In the 2010s, researchers openly worried that delayed regulations could delay deployment.[267] In 2020, UNECE WP.29 GRVA was issued to address regulation of Level 3 automated driving.\n Vehicles operating below Level 5 still offer many advantages.[268]\n As of 2023[update] most commercially available ADAS vehicles are SAE Level 2. A couple of companies reached higher levels, but only in restricted (geofenced) locations.[269]\n SAE Level 2 features are available as part of the ADAS systems in many vehicles. In the US, 50% of new cars provide driver assistance for both steering and speed.[270]\n Ford started offering BlueCruise service on certain vehicles in 2022; the system is named ActiveGlide in Lincoln vehicles. The system provided features such as lane centering, street sign recognition, and hands-free highway driving on more than 130,000 miles of divided highways. The 2022 1.2 version added features including hands-free lane changing, in-lane repositioning, and predictive speed assist.[271][272] In April 2023 BlueCruise was approved in the UK for use on certain motorways, starting with 2023 models of Ford's electric Mustang Mach-E SUV.[273]\n Tesla's Autopilot and its Full Self-Driving (FSD) ADAS suites are available on all Tesla cars since 2016. FSD offers highway and street driving (without geofencing), navigation/turn management, steering, and dynamic cruise control, collision avoidance, lane-keeping/switching, emergency braking, obstacle avoidance, but still requires the driver to remain ready to control the vehicle at any moment. Its driver management system combines eye tracking with monitoring pressure on the steering wheel to ensure that drives are both eyes on and hands on.[274][275]\n Tesla's FSD rewrite V12 (released in March 2024) uses a single deep learning transformer model for all aspects of perception, monitoring, and control.[276][277] It relies on its eight cameras for its vision-only perception system, without use of LiDAR, radar, or ultrasound.[277] As of April 2024, FSD has been deployed on two million Tesla cars.[278] As of January 2024, Tesla has not initiated requests for Level 3 status for its systems and has not disclosed its reason for not doing so.[275]\n General Motors is developing the \"Ultra Cruise\" ADAS system, that will be a dramatic improvement over their current \"Super Cruise\" system. Ultra Cruise will cover \"95 percent\" of driving scenarios on 2 million miles of roads in the US, according to the company. The system hardware in and around the car includes multiple cameras, short- and long-range radar, and a LiDAR sensor, and will be powered by the Qualcomm Snapdragon Ride Platform. The luxury Cadillac Celestiq electric vehicle will be one of the first vehicles to feature Ultra Cruise.[279]\n Europe is developing a new \"Driver Control Assistance Systems\" (DCAS) level 2 regulation to no longer limit the use of lane changing systems to roads with 2 lanes and a physical separation from traffic in the opposite direction.[280][281]\n As of April\u00a02024[update], two car manufacturers have sold or leased Level 3 cars: Honda in Japan, and Mercedes in Germany, Nevada and California.[53]\n Mercedes Drive Pilot has been available on the EQS and S-class sedan in Germany since 2022, and in California and Nevada since 2023.[68] A subscription costs between \u20ac5,000 and \u20ac7,000 for three years in Germany and $2,500 for one year in the United States.[282] Drive Pilot can only be used when the vehicle is traveling under 40 miles per hour (64\u00a0km/h), there is a vehicle in front, readable line markings, during the day, clear weather, and on freeways mapped by Mercedes down to the centimeter (100,000 miles in California).[282][68] As of April 2024, one Mercedes vehicle with this capability has been sold in California.[282]\n Honda continued to enhance its Level 3 technology.[283][284] As of 2023, 80 vehicles with Level 3 support had been sold.[285]\n Mercedes-Benz received authorization in early 2023 to pilot its Level 3 software in Las Vegas.[15] California also authorized Drive Pilot in 2023.[286]\n BMW commercialized its AC in 2021.[287]  In 2023 BMW stated that its Level-3 technology was nearing release. It would be the second manufacturer to deliver Level-3 technology, but the only one with a Level 3 technology which works in the dark.[288]\n In 2023, in China, IM Motors, Mercedes, and BMW obtained authorization to test vehicles with Level 3 systems on motorways.[289][290]\n In September 2021, Stellantis presented its findings from its Level 3 pilot testing on Italian highways. Stellantis's Highway Chauffeur claimed Level 3 capabilities, as tested on the Maserati Ghibli and Fiat 500X prototypes.[291]\n Polestar, a Volvo Cars' brand, announced in January 2022 its plan to offer Level 3 autonomous driving system in the Polestar 3 SUV, a Volvo XC90 successor, with technologies from Luminar Technologies, Nvidia, and Zenseact.[292]\n In January 2022, Bosch and the Volkswagen Group subsidiary CARIAD released a collaboration for autonomous driving up to Level 3. This joint development targets Level 4 capabilities.[293]\n Hyundai Motor Company is enhancing cybersecurity of connected cars to offer a Level 3 self-driving Genesis G90.[294] Kia and Hyundai Korean car makers delayed their Level 3 plans, and will not deliver Level 3 vehicles in 2023.[295]\n Waymo offers robotaxi services in parts of Arizona (Phoenix) and California (San Francisco and Los Angeles), as fully autonomous vehicles without safety drivers.[296]\n In April 2023 in Japan, a Level 4 protocol became part of the amended Road Traffic Act.[297] ZEN drive Pilot Level 4 made by AIST operates there.[298]\n In July 2020, Toyota started public demonstration rides on Lexus LS (fifth generation) based TRI-P4 with Level 4 capability.[299] In August 2021, Toyota operated a potentially Level 4 service using e-Palette around the Tokyo 2020 Olympic Village.[300]\n In September 2020, Mercedes-Benz introduced world's first commercial Level 4 Automated Valet Parking (AVP) system named Intelligent Park Pilot for its new S-Class.[301][302] In November 2022, Germany\u2019s Federal Motor Transport Authority (KBA) approved the system for use at Stuttgart Airport.[303]\n In September 2021, Cruise, General Motors, and Honda started a joint testing programme, using Cruise AV.[304] In 2023, the Origin was put on indefinite hold following Cruise's loss of its operating permit.[305]\n In January 2023, Holon announced an autonomous shuttle during the 2023 Consumer Electronics Show (CES). The company claimed the vehicle is the world's first Level 4 shuttle built to automotive standard.[306]\n  Media related to Autonomous automobiles at Wikimedia Commons \n These books are based on presentations and discussions at the Automated Vehicles Symposium organized annually by TRB and AUVSI.\n",
        "doc_number": 9
    },
    {
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_healthcare",
        "content": "\n Artificial intelligence in healthcare is the application of artificial intelligence (AI) to analyze and understand complex medical and healthcare data. In some cases, it can exceed or augment human capabilities by providing better or faster ways to diagnose, treat, or prevent disease.[1][2][3]\n As widespread use of AI in healthcare is relatively new, research is ongoing into its application in various subdisciplines of medicine and related industries. AI programs are applied to practices such as diagnostics,[4] treatment protocol development,[5] drug development,[6] personalized medicine,[7] and patient monitoring and care.[8] Because radiographs are the most common imaging tests conducted in radiology departments, the potential for AI to help with triage and interpretation of radiographs is particularly noteworthy.[9]\n Using AI also presents unprecedented ethical concerns related to issues such as data privacy, automation of jobs, and amplifying already existing biases.[10] Furthermore, new technologies such as AI are often resisted by healthcare leaders, leading to slow and erratic adoption.[11] In contrast, there are also several cases where AI has been put to use in healthcare without proper testing.[12][13][14][15] Moreover, meta-studies have found that the scientific literature on AI in healthcare often suffers from a lack of reproducibility.[16][17][18][19]\n Accurate and early diagnosis of diseases is still a challenge in healthcare. Recognising medical conditions and their symptoms is a complex problem. AI can assist clinicians with its data processing capabilities to save time and improve accuracy.[20] Through the use of machine learning, artificial intelligence can be able to substantially aid doctors in patient diagnosis through the analysis of mass electronic health records (EHRs).[21] AI can help early prediction, for example, of Alzheimer's disease and dementias, by looking through large numbers of similar cases and possible treatments.[22]\n Doctors' decision making could also be supported by AI in urgent situations, for example in the emergency department. Here AI algorithms can help prioritize more serious cases and reduce waiting time. Decision support systems augmented with AI can offer real-time suggestions and faster data interpretation to aid the decisions made by healthcare professionals.[20]\n In 2023 a study reported higher satisfaction rates with ChatGPT-generated responses compared with those from physicians for medical questions posted on Reddit\u2019s r/AskDocs.[23] Evaluators preferred ChatGPT's responses to physician responses in 78.6% of 585 evaluations, noting better quality and empathy. The authors noted that these were isolated questions taken from an online forum, not in the context of an established patient-physician relationship.[23] Moreover, responses were not graded on the accuracy of medical information, and some have argued that the experiment was not properly blinded, with the evaluators being coauthors of the study.[24][25][26]\n Recent developments in statistical physics, machine learning, and inference algorithms are also being explored for their potential in improving medical diagnostic approaches.[27]\n Electronic health records (EHR) are crucial to the digitalization and information spread of the healthcare industry. Now that around 80% of medical practices use EHR, some anticipate the use of artificial intelligence to interpret the records and provide new information to physicians.[28]\n One application uses natural language processing (NLP) to make more succinct reports that limit the variation between medical terms by matching similar medical terms.[28] For example, the term heart attack and myocardial infarction mean the same things, but physicians may use one over the over based on personal preferences.[28] NLP algorithms consolidate these differences so that larger datasets can be analyzed.[28] Another use of NLP identifies phrases that are redundant due to repetition in a physician's notes and keeps the relevant information to make it easier to read.[28] Other applications use concept processing to analyze the information entered by the current patient's doctor to present similar cases and help the physician remember to include all relevant details.[29]\n Beyond making content edits to an EHR, there are AI algorithms that evaluate an individual patient's record and predict a risk for a disease based on their previous information and family history.[30] One general algorithm is a rule-based system that makes decisions similarly to how humans use flow charts.[31] This system takes in large amounts of data and creates a set of rules that connect specific observations to concluded diagnoses.[31] Thus, the algorithm can take in a new patient's data and try to predict the likeliness that they will have a certain condition or disease.[31] Since the algorithms can evaluate a patient's information based on collective data, they can find any outstanding issues to bring to a physician's attention and save time.[30] One study conducted by the Centerstone research institute found that predictive modeling of EHR data has achieved 70\u201372% accuracy in predicting individualized treatment response.[32] These methods are helpful due to the fact that the amount of online health records doubles every five years.[30] Physicians do not have the bandwidth to process all this data manually, and AI can leverage this data to assist physicians in treating their patients.[30]\n Improvements in natural language processing led to the development of algorithms to identify drug-drug interactions in medical literature.[33][34][35][36] Drug-drug interactions pose a threat to those taking multiple medications simultaneously, and the danger increases with the number of medications being taken.[37] To address the difficulty of tracking all known or suspected drug-drug interactions, machine learning algorithms have been created to extract information on interacting drugs and their possible effects from medical literature. Efforts were consolidated in 2013 in the DDIExtraction Challenge, in which a team of researchers at Carlos III University assembled a corpus of literature on drug-drug interactions to form a standardized test for such algorithms.[38] Competitors were tested on their ability to accurately determine, from the text, which drugs were shown to interact and what the characteristics of their interactions were.[39]  Researchers continue to use this corpus to standardize the measurement of the effectiveness of their algorithms.[33][34][36]\n Other algorithms identify drug-drug interactions from patterns in user-generated content, especially electronic health records and/or adverse event reports.[34][35] Organizations such as the FDA Adverse Event Reporting System (FAERS) and the World Health Organization's VigiBase allow doctors to submit reports of possible negative reactions to medications. Deep learning algorithms have been developed to parse these reports and detect patterns that imply drug-drug interactions.[40]\n The increase of telemedicine, the treatment of patients remotely, has shown the rise of possible AI applications.[41] AI can assist in caring for patients remotely by monitoring their information through sensors.[42] A wearable device may allow for constant monitoring of a patient and the ability to notice changes that may be less distinguishable by humans. The information can be compared to other data that has already been collected using artificial intelligence algorithms that alert physicians if there are any issues to be aware of.[42]\n Another application of artificial intelligence is chat-bot therapy. Some researchers charge that the reliance on chatbots for mental healthcare does not offer the reciprocity and accountability of care that should exist in the relationship between the consumer of mental healthcare and the care provider (be it a chat-bot or psychologist), though.[43]\n Since the average age has risen due to a longer life expectancy, artificial intelligence could be useful in helping take care of older populations.[44] Tools such as environment and personal sensors can identify a person's regular activities and alert a caretaker if a behavior or a measured vital is abnormal.[44] Although the technology is useful, there are also discussions about limitations of monitoring in order to respect a person's privacy since there are technologies that are designed to map out home layouts and detect human interactions.[44]\n AI has the potential to streamline care coordination and reduce the workload. AI algorithms can automate administrative tasks, prioritize patient needs and facilitate seamless communication in a healthcare team.[45] This enables healthcare providers to focus more on direct patient care and ensures the efficient and coordinated delivery of healthcare services.\n Artificial intelligence algorithms have shown promising results in accurately diagnosing and risk stratifying patients with concern for coronary artery disease, showing potential as an initial triage tool.[46][47] Other algorithms have been used in predicting patient mortality, medication effects, and adverse events following treatment for acute coronary syndrome.[46] Wearables, smartphones, and internet-based technologies have also shown the ability to monitor patients' cardiac data points, expanding the amount of data and the various settings AI models can use and potentially enabling earlier detection of cardiac events occurring outside of the hospital.[48] Another growing area of research is the utility of AI in classifying heart sounds and diagnosing valvular disease.[49] Challenges of AI in cardiovascular medicine have included the limited data available to train machine learning models, such as limited data on social determinants of health as they pertain to cardiovascular disease.[50]\n A key limitation in early studies evaluating AI were omissions of data comparing algorithmic performance to humans. Examples of studies which assess AI performance relative to physicians includes how AI is non-inferior to humans in interpretation of cardiac echocardiograms[51] and that AI can diagnose heart attack better than human physicians in the emergency setting, reducing both low-value testing and missed diagnoses.[52]\n In cardiovascular tissue engineering and organoid studies, AI is increasingly used to analyze microscopy images, and integrate electrophysiological read outs.[53]\n Medical imaging (such as X-ray and photography) is a commonly used tool in dermatology[54] and the development of deep learning has been strongly tied to image\u00a0processing. Therefore, there is a natural fit between the dermatology and deep learning. Machine learning learning holds great potential to process these images for better diagnoses.[55] Han et al. showed keratinocytic skin cancer detection from face photographs.[56] Esteva et al. demonstrated dermatologist-level classification of skin cancer from lesion images.[57] Noyan et al. demonstrated a convolutional neural network that achieved 94% accuracy at identifying skin cells from microscopic Tzanck smear images.[58] A concern raised with this work is that it has not engaged with disparities related to skin color or differential treatment of patients with non-white skin tones.[59]\n According to some researchers, AI algorithms have been shown to be more effective than dermatologists at identifying cancer.[60] However, a 2021 review article found that a majority of papers analyzing the performance of AI algorithms designed for skin cancer classification failed to use external test sets.[61] Only four research studies were found in which the AI algorithms were tested on clinics, regions, or populations distinct from those it was trained on, and in each of those four studies, the performance of dermatologists was found to be on par with that of the algorithm. Moreover, only one study[62] was set in the context of a full clinical examination; others were based on interaction through web-apps or online questionnaires, with most based entirely on context-free images of lesions. In this study, it was found that dermatologists significantly outperformed the algorithms. Many articles claiming superior performance of AI algorithms also fail to distinguish between trainees and board-certified dermatologists in their analyses.[61]\n It has also been suggested that AI could be used to automatically evaluate the outcome of maxillo-facial surgery or cleft palate therapy in regard to facial attractiveness or age appearance.[63][64]\n AI can play a role in various facets of the field of gastroenterology. Endoscopic exams such as esophagogastroduodenoscopies (EGD) and colonoscopies rely on rapid detection of abnormal tissue. By enhancing these endoscopic procedures with AI, clinicians can more rapidly identify diseases, determine their severity, and visualize blind spots. Early trials in using AI detection systems of early stomach cancer have shown sensitivity close to expert endoscopists.[65]\n AI can assist doctors treating ulcerative colitis in detecting the microscopic activity of the disease in people and predicting when flare-ups will happen. For example, an AI-powered tool  was developed to analyse digitised bowel samples (biopsies). The tool was able to distinguish with 80% accuracy between samples that show remission of colitis and those with active disease. It also predicted the risk of a flare-up happening with the same accuracy. These rates of successfully using microscopic disease activity to predict disease flare are similar to the accuracy of pathologists.[66][67]\n Artificial intelligence utilises massive amounts of data to help with predicting illness, prevention, and diagnosis, as well as patient monitoring. In obstetrics, artificial intelligence is utilized in magnetic resonance imaging, ultrasound, and foetal cardiotocography. AI contributes in the resolution of a variety of obstetrical diagnostic issues.[68]\n AI has shown potential in both the laboratory and clinical spheres of infectious disease medicine.[69] During the COVID-19 pandemic, AI has been used for early detection, tracking virus spread and analysing virus behaviour, among other things.[70] However, there were only a few examples of AI being used directly in clinical practice during the pandemic itself.[71]\n Other applications  of AI around infectious diseases include support-vector machines identifying antimicrobial resistance, machine learning analysis of blood smears to detect malaria, and improved point-of-care testing of Lyme disease based on antigen detection. Additionally, AI has been investigated for improving diagnosis of meningitis, sepsis, and tuberculosis, as well as predicting treatment complications in hepatitis B and hepatitis C patients.[69]\n AI has been used to identify causes of knee pain that doctors miss, that disproportionately affect Black patients.[72] Underserved populations experience higher levels of pain. These disparities persist even after controlling for the objective severity of diseases like osteoarthritis, as graded by human physicians using medical images, raising the possibility that underserved patients\u2019 pain stems from factors external to the knee, such as stress. Researchers have conducted a study using a machine-learning algorithm to show that standard radiographic measures of severity overlook objective but undiagnosed features that disproportionately affect diagnosis and management of underserved populations with knee pain. They proposed that new algorithmic measure ALG-P could potentially enable expanded access to treatments for underserved patients.[73]\n The use of AI technologies has been explored for use in the diagnosis and prognosis of Alzheimer's disease (AD). For diagnostic purposes, machine learning models have been developed that rely on structural MRI inputs.[74] The input datasets for these models are drawn from databases such as the Alzheimer's Disease Neuroimaging Initiative.[75] Researchers have developed models that rely on convolutional neural networks with the aim of improving early diagnostic accuracy.[76] Generative adversarial networks are a form of deep learning that have also performed well in diagnosing AD.[77] There have also been efforts to develop machine learning models into forecasting tools that can predict the prognosis of patients with AD. Forecasting patient outcomes through generative models has been proposed by researchers as a means of synthesizing training and validation sets.[78] They suggest that generated patient forecasts could be used to provide future models larger training datasets than current open access databases.\n AI has been explored for use in cancer diagnosis, risk stratification, molecular characterization of tumors, and cancer drug discovery. A particular challenge in oncologic care that AI is being developed to address is the ability to accurately predict which treatment protocols will be best suited for each patient based on their individual genetic, molecular, and tumor-based characteristics.[79] AI has been trialed in cancer diagnostics with the reading of imaging studies and pathology slides.[80]\n In January 2020, Google DeepMind announced an algorithm capable of surpassing human experts in breast cancer detection in screening scans.[81][82] A number of researchers, including Trevor Hastie, Joelle Pineau, and Robert Tibshirani among others, published a reply claiming that DeepMind's research publication in Nature lacked key details on methodology and code, \"effectively undermin[ing] its scientific value\" and making it impossible for the scientific community to confirm the work.[83] In the MIT Technology Review, author Benjamin Haibe-Kains characterized DeepMind's work as \"an advertisement\" having little to do with science.[84]\n In July 2020, it was reported that an AI algorithm developed by the University of Pittsburgh achieves the highest accuracy to date in identifying prostate cancer, with 98% sensitivity and 97% specificity.[85][86] In 2023 a study reported the use of AI for CT-based radiomics classification at grading the aggressiveness of retroperitoneal sarcoma with 82% accuracy compared with 44% for lab analysis of biopsies.[87][88]\n Artificial intelligence-enhanced technology is being used as an aid in the screening of eye disease and prevention of blindness.[89]\u00a0In 2018, the U.S. Food and Drug Administration authorized the marketing of the first medical device to diagnose a specific type of eye disease, diabetic retinopathy using an artificial intelligence algorithm.[90] Moreover, AI technology may be used to further improve \"diagnosis rates\" because of the potential to decrease detection time.[91]\n For many diseases, pathological analysis of cells and tissues is considered to be the gold standard of disease diagnosis. Methods of digital pathology allows microscopy slides to be scanned and digitally analyzed. AI-assisted pathology tools have been developed to assist with the diagnosis of a number of diseases, including breast cancer, hepatitis B, gastric cancer, and colorectal cancer. AI has also been used to predict genetic mutations and prognosticate disease outcomes.[65] AI is well-suited for use in low-complexity pathological analysis of large-scale screening samples, such as colorectal or breast cancer screening, thus lessening the burden on pathologists and allowing for faster turnaround of sample analysis.[93] Several deep learning and artificial neural network models have shown accuracy similar to that of human pathologists,[93] and a study of deep learning assistance in diagnosing metastatic breast cancer in lymph nodes showed that the accuracy of humans with the assistance of a deep learning program was higher than either the humans alone or the AI program alone.[94] Additionally, implementation of digital pathology is predicted to save over $12 million for a university center over the course of five years,[95] though savings attributed to AI specifically have not yet been widely researched. The use of augmented and virtual reality could prove to be a stepping stone to wider implementation of AI-assisted pathology, as they can highlight areas of concern on a pathology sample and present them in real-time to a pathologist for more efficient review.[93] AI also has the potential to identify histological findings at levels beyond what the human eye can see,[93] and has shown the ability to use genotypic and phenotypic data to more accurately detect the tumor of origin for metastatic cancer.[96] One of the major current barriers to widespread implementation of AI-assisted pathology tools is the lack of prospective, randomized, multi-center controlled trials in determining the true clinical utility of AI for pathologists and patients, highlighting a current area of need in AI and healthcare research.[93]\n Primary care has become one key development area for AI technologies.[97][98] AI in primary care has been used for supporting decision making, predictive modeling, and business analytics.[99] There are only a few examples of AI decision support systems that were prospectively assessed on clinical efficacy when used in practice by physicians. But there are cases where the use of these systems yielded a positive effect on treatment choice by physicians.[100]\n In psychiatry, AI applications are still in a phase of proof-of-concept.[101] Areas where the evidence is widening quickly include predictive modelling of diagnosis and treatment outcomes,[102] chatbots, conversational agents that imitate human behaviour and which have been studied for anxiety and depression.[103]\n Challenges include the fact that many applications in the field are developed and proposed by private corporations, such as the screening for suicidal ideation implemented by Facebook in 2017.[104] Such applications outside the healthcare system raise various professional, ethical and regulatory questions.[105] Another issue is often with the validity and interpretability of the models. Small training datasets contain bias that is inherited by the models, and compromises the generalizability and stability of these models. Such models may also have the potential to be discriminatory against minority groups that are underrepresented in samples.[106]\n In 2023, US-based National Eating Disorders Association replaced its human helpline staff with a chatbot but had to take it offline after users reported receiving harmful advice from it.[107][108][109]\n AI is being studied within the field of radiology to detect and diagnose diseases through computerized tomography (CT) and magnetic resonance (MR) imaging.[110] It may be particularly useful in settings where demand for human expertise exceeds supply, or where data is too complex to be efficiently interpreted by human readers.[111] Several deep learning models have shown the capability to be roughly as accurate as healthcare professionals in identifying diseases through medical imaging, though few of the studies reporting these findings have been externally validated.[112] AI can also provide non-interpretive benefit to radiologists, such as reducing noise in images, creating high-quality images from lower doses of radiation, enhancing MR image quality,[113] and automatically assessing image quality.[114] Further research investigating the use of AI in nuclear medicine focuses on image reconstruction, anatomical landmarking, and the enablement of lower doses in imaging studies.[115] The analysis of images for supervised AI applications in radiology encompasses two primary techniques at present: (1) convolutional neural network-based analysis; and (2) utilization of radiomics.[111]\n AI is also used in breast imaging for analyzing screening mammograms and can participate in improving breast cancer detection rate[116] as well as reducing radiologist's reading workload.\n The trend of large health companies merging allows for greater health data accessibility. Greater health data lays the groundwork for the implementation of AI algorithms.\n A large part of industry focus of implementation of AI in the healthcare sector is in the clinical decision support systems. As more data is collected, machine learning algorithms adapt and allow for more robust responses and solutions.[110] Numerous companies are exploring the possibilities of the incorporation of big data in the healthcare industry. Many companies investigate the market opportunities through the realms of \"data assessment, storage, management, and analysis technologies\" which are all crucial parts of the healthcare industry.[125]\n The following are examples of large companies that have contributed to AI algorithms for use in healthcare:\n Digital consultant apps use AI to give medical consultation based on personal medical history and common medical knowledge. Users report their symptoms into the app, which uses speech recognition to compare against a database of illnesses. Babylon then offers a recommended action, taking into account the user's medical history. Entrepreneurs in healthcare have been effectively using seven business model archetypes to take AI solution[buzzword] to the marketplace. These archetypes depend on the value generated for the target user (e.g. patient focus vs. healthcare provider and payer focus) and value capturing mechanisms (e.g. providing information or connecting stakeholders).\n IFlytek launched a service robot \"Xiao Man\", which integrated artificial intelligence technology to identify the registered customer and provide personalized recommendations in medical areas. It also works in the field of medical imaging. Similar robots are also being made by companies such as UBTECH (\"Cruzr\") and Softbank Robotics (\"Pepper\").\n The Indian startup Haptik recently developed a WhatsApp chatbot which answers questions associated with the deadly coronavirus in India. Similarly, a software platform ChatBot in partnership with medtech startup Infermedica launched COVID-19 Risk Assessment ChatBot.[128]\n With the market for AI expanding constantly, large tech companies such as Apple, Google, Amazon, and Baidu all have their own AI research divisions, as well as millions of dollars allocated for acquisition of smaller AI based companies.[125] Many automobile manufacturers are beginning to use machine learning healthcare in their cars as well.[125] Companies such as BMW, GE, Tesla, Toyota, and Volvo all have new research campaigns to find ways of learning a driver's vital statistics to ensure they are awake, paying attention to the road, and not under the influence of substances.[125]\n Artificial intelligence continues to expand in its abilities to diagnose more people accurately in nations where fewer doctors are accessible to the public.\u00a0 Many new technology companies such as SpaceX and the Raspberry Pi Foundation have enabled more developing countries to have access to computers and the internet than ever before.[129] With the increasing capabilities of AI over the internet, advanced machine learning algorithms can allow patients to get accurately diagnosed when they would previously have no way of knowing if they had a life-threatening disease or not.[129]\n Using AI in developing nations that do not have the resources will diminish the need for outsourcing and can improve patient care. AI can allow for not only diagnosis of patient in areas where healthcare is scarce, but also allow for a good patient experience by resourcing files to find the best treatment for a patient.[130] The ability of AI to adjust course as it goes also allows the patient to have their treatment modified based on what works for them; a level of individualized care that is nearly non-existent in developing countries.[130]\n While research on the use of AI in healthcare aims to validate its efficacy in improving patient outcomes before its broader adoption, its use may nonetheless introduce several new types of risk to patients and healthcare providers, such as algorithmic bias, Do not resuscitate implications, and other machine morality issues. AI may also compromise the protection of patients' rights, such as the right to informed consent and the right to medical data protection.[131] These challenges of the clinical use of AI have brought about a potential need for regulations. AI studies need to be completely and transparently reported to have value to inform regulatory approval. Depending on the phase of study, international consensus-based reporting guidelines (TRIPOD+AI,[132] DECIDE-AI,[133] CONSORT-AI[134]) have been developed to provide recommendations on the key details that need to be reported.\n Currently, there are regulations pertaining to the collection of patient data. This includes policies such as the Health Insurance Portability and Accountability Act (HIPAA) and the European General Data Protection Regulation (GDPR).[135] The GDPR pertains to patients within the EU and details the consent requirements for patient data use when entities collect patient healthcare data. Similarly, HIPAA protects healthcare data from patient records in the United States.[135] In May 2016, the White House announced its plan to host a series of workshops and formation of the National Science and Technology Council (NSTC) Subcommittee on Machine Learning and Artificial Intelligence. In October 2016, the group published The National Artificial Intelligence Research and Development Strategic Plan, outlining its proposed priorities for Federally-funded AI research and development (within government and academia). The report notes a strategic R&D plan for the subfield of health information technology is in development stages.\n There is concern that large language models can overwhelm people with both accurate health information and also misinformation, leading to potential challenges in public health. This calls for the need for policy and user guidance related to health information through AI.[136]\n The joint ITU-WHO Focus Group on Artificial Intelligence for Health (FG-AI4H) has built a platform - known as the ITU-WHO AI for Health Framework - for the testing and benchmarking of AI applications in health domain. As of November 2018, eight use cases are being benchmarked, including assessing breast cancer risk from histopathological imagery, guiding anti-venom selection from snake images, and diagnosing skin lesions.\n In January 2021, the US FDA published a new Action Plan, entitled Artificial Intelligence (AI) /Machine Learning (ML)-Based Software as a Medical Device (SaMD) Action Plan.[138] This plan lays out the FDA's future plans for regulation of medical devices that would include artificial intelligence in their software. There are five main actions the FDA plans to take to increase regulation: 1. Tailored Regulatory Framework for Ai/M:-based SaMD, 2. Good Machine Learning Practice (GMLP), 3. Patient-Centered Approach Incorporating Transparency to Users, 4. Regulatory Science Methods Related to Algorithm Bias & Robustness, and 5. Real-World Performance(RWP). This plan was in direct response to stakeholders' feedback on a 2019 discussion paper also published by the FDA.\n According to the U.S. Department of Health and Human Services, the Office for Civil Rights (OCR) has issued guidance on the ethical use of AI in healthcare. The guidance outlines four core ethical principles that must be followed: respect for autonomy, beneficence, non-maleficence, and justice. Respect for autonomy requires that individuals have control over their own data and decisions. Beneficence requires that AI be used to do good, such as improving the quality of care and reducing health disparities. Non-maleficence requires that AI be used to do no harm, such as avoiding discrimination in decisions. Finally, justice requires that AI be used fairly, such as using the same standards for decisions no matter a person's race, gender, or income level. Moreover, as of March 2021, the OCR hired a Chief Artificial Intelligence Officer (OCAIO) to pursue the \"implementation of the HHS AI strategy\".[139] The OCR also has issued rules and regulations to protect the privacy of individuals\u2019 health information. These regulations require healthcare providers to follow certain privacy rules when using AI. The OCR also requires healthcare providers to keep a record of how they use AI and to ensure that their AI systems are secure. Overall, the U.S. has taken steps to protect individuals\u2019 privacy and ethical issues related to AI in healthcare[140]\n The U.S. is not the only country to develop or initiate regulations of data privacy with AI. Other countries have implemented data protection regulations, more specifically with company privacy invasions. In Denmark, the Danish Expert Group on Data Ethics has adopted recommendations on 'Data for the Benefit of the People'. These recommendations are intended to encourage the responsible use of data in the business sector, with a focus on data processing. The recommendations include a focus on equality and non-discrimination with regard to bias in AI, as well as human dignity. The importance of human dignity is stressed, as it is said to outweigh profit and must be respected in all data processes[141]\n The European Union has implemented the General Data Protection Regulation (GDPR) to protect citizens' personal data, which applies to the use of AI in healthcare. In addition, the European Commission has established guidelines to ensure the ethical development of AI, including the use of algorithms to ensure fairness and transparency.[142] With GDPR, the European Union was the first to regulate AI through data protection legislation. The Union finds privacy as a fundamental human right, it wants to prevent unconsented and secondary uses of data by private or public health facilities. By streamlining access to personal data for health research and findings, they are able to instate the right and importance of patient privacy.[142] In the United States, the Health Insurance Portability and Accountability Act (HIPAA) requires organizations to protect the privacy and security of patient information. The Centers for Medicare and Medicaid Services have also released guidelines for the development of AI-based medical applications.[143]\n In order to effectively train Machine Learning and use AI in healthcare, massive amounts of data must be gathered. Acquiring this data, however, comes at the cost of patient privacy in most cases and is not well received publicly. For example, a survey conducted in the UK estimated that 63% of the population is uncomfortable with sharing their personal data in order to improve artificial intelligence technology.[135] The scarcity of real, accessible patient data is a hindrance that deters the progress of developing and deploying more artificial intelligence in healthcare.\n Furthermore, the lack of current regulations surrounding AI in the United States has generated concerns about mismanagement of patient data, such as with corporations utilizing patient data for financial gain. For example, Roche, a Swiss healthcare company, was found to have purchased healthcare data for approximately 2 million cancer patients at an estimated total cost of $1.9 billion.[144] Naturally, this generates questions of ethical concern; Is there a monetary price that can be set for data, and should it depend on its perceived value or contributions to science? Is it fair to patients to sell their data? These concerns were addressed in a survey conducted by the Pew Research Center in 2022 that asked Americans for their opinions about the increased presence of AI in their daily lives, and the survey estimated that 37% of Americans were more concerned than excited about such increased presence, with 8% of participants specifically associating their concern with \"people misusing AI\".[145] Ultimately, the current potential of artificial intelligence in healthcare is additionally hindered by  concerns about mismanagement of data collected, especially in the United States.\n A systematic review and thematic analysis in 2023 showed that most stakeholders including health professionals, patients, and the general public doubted that care involving AI could be empathetic.[146]\n According to a 2019 study, AI can replace up to 35% of jobs in the UK within the next 10 to 20 years.[147] However, of these jobs, it was concluded that AI has not eliminated any healthcare jobs so far. Though if AI were to automate healthcare-related jobs, the jobs most susceptible to automation would be those dealing with digital information, radiology, and pathology, as opposed to those dealing with doctor-to-patient interaction.[147]\n Automation can provide benefits alongside doctors as well. Some believe that AI may avert healthcare worker burnout and cognitive overload, so that doctors who take advantage of AI in healthcare will provide greater quality healthcare than doctors and medical establishments who do not.[148]\n Recently, there have been many discussions between healthcare experts in terms of AI and elder care. In relation to elder care, AI bots have been helpful in guiding older residents living in assisted living with entertainment and company. These bots are allowing staff in the home to have more one-on-one time with each resident, but the bots are also programmed with more ability in what they are able to do; such as knowing different languages and different types of care depending on the patient's conditions. The bot is an AI machine, which means it goes through the same training as any other machine - using algorithms to parse the given data, learn from it and predict the outcome in relation to what situation is at hand[149]\n Since AI makes decisions solely on the data it receives as input, it is important that this data represents accurate patient demographics. In a hospital setting, patients do not have full knowledge of how predictive algorithms are created or calibrated. Therefore, these medical establishments can unfairly code their algorithms to discriminate against minorities and prioritize profits rather than providing optimal care.[150] A recent scoping review identified 18 equity challenges along with 15 strategies that can be implemented to help address them when AI applications are developed using many-to-many mapping.[151]\n There can also be unintended bias in these algorithms that can exacerbate social and healthcare inequities.[150] \u00a0Since AI's decisions are a direct reflection of its input data, the data it receives must have accurate representation of patient demographics. White males are overly represented in medical data sets.[152] Therefore, having minimal patient data on minorities can lead to AI making more accurate predictions for majority populations, leading to unintended worse medical outcomes for minority populations.[153] Collecting data from minority communities can also lead to medical discrimination. For instance, HIV is a prevalent virus among minority communities and HIV status can be used to discriminate against patients.[152] In addition to biases that may arise from sample selection, different clinical systems used to collect data may also impact AI functionality. For example, radiographic systems and their outcomes (e.g., resolution) vary by provider. Moreover, clinician work practices, such as the positioning of the patient for radiography, can also greatly influence the data and make comparability difficult.[154] However, these biases are able to be eliminated through careful implementation and a methodical collection of representative data.\n A final source of bias, which has been called \"label choice bias\", arises when proxy measures are used to train algorithms, that build in bias against certain groups. For example, a widely used algorithm predicted health care costs as a proxy for health care needs, and used predictions to allocate resources to help patients with complex health needs. This introduced bias because Black patients have lower costs, even when they are just as unhealthy as White patients.[155] Solutions to the \"label choice bias\" aim to match the actual target (what the algorithm is predicting) more closely to the ideal target (what researchers want the algorithm to predict), so for the prior example, instead of predicting cost, researchers would focus on the variable of healthcare needs which is rather more significant. Adjusting the target led to almost double the number of Black patients being selected for the program.\n Research in the 1960s and 1970s produced the first problem-solving program, or expert system, known as Dendral.[156][157] While it was designed for applications in organic chemistry, it provided the basis for a subsequent system MYCIN,[158] considered one of the most significant early uses of artificial intelligence in medicine.[158][159] MYCIN and other systems such as INTERNIST-1 and CASNET did not achieve routine use by practitioners, however.[160]\n The 1980s and 1990s brought the proliferation of the microcomputer and new levels of network connectivity. During this time, there was a recognition by researchers and developers that AI systems in healthcare must be designed to accommodate the absence of perfect data and build on the expertise of physicians.[161] Approaches involving fuzzy set theory,[162] Bayesian networks,[163] and artificial neural networks,[164][165] have been applied to intelligent computing systems in healthcare.\n Medical and technological advancements occurring over this half-century period that have enabled the growth of healthcare-related applications of AI to include: \n",
        "doc_number": 10
    },
    {
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_finance",
        "content": "\n Artificial intelligence (AI) has been used in applications throughout industry and academia. In a manner analogous to electricity or computers, AI serves as a general-purpose technology. AI programes emulate perception and understanding, and are designed to adapt to new information and new situations. Machine learning has been used for various scientific and commercial purposes[1] including language translation, image recognition, decision-making,[2][3] credit scoring, and e-commerce.\n Machine learning is has been used for recommendation systems in for determining which posts should show up in social media feeds.[4][5] Various types of social media analysis also make use of machine learning[6][7] and there is research into its use for (semi-)automated tagging/enhancement/correction of online misinformation and related filter bubbles.[8][9][10]\n AI has been used to customize shopping options and personalize offers.[11] Online gambling companies have used AI for targeting gamblers.[12]\n Intelligent personal assistants use AI to understand many natural language requests in other ways than rudimentary commands. Common examples are Apple's Siri, Amazon's Alexa, and a more recent AI, ChatGPT by OpenAI.[13]\n Bing Chat has used artificial intelligence as part of its search engine.[14]\n Machine learning can be used to combat spam, scams, and phishing. It can scrutinize the contents of spam and phishing attacks to attempt to identify malicious elements.[15] Some models built via machine learning algorithms have over 90% accuracy in distinguishing between spam and legitimate emails.[16] These models can be refined using new data and evolving spam tactics. Machine learning also analyzes traits such as sender behavior, email header information, and attachment types, potentially enhancing spam detection.[17]\n Speech translation technology attempts to convert one language's spoken words into another language. This potentially reduces language barriers in global commerce and cross-cultural exchange, enabling speakers of various languages to communicate with one another.[18]\n AI has been used to automatically translate spoken language and textual content in products such as Microsoft Translator, Google Translate, and DeepL Translator.[19] Additionally, research and development are in progress to decode and conduct animal communication.[20][21]\n Meaning is conveyed not only by text, but also through usage and context (see semantics and pragmatics). As a result, the two primary categorization approaches for machine translations are statistical and neural machine translations (NMTs). The old method of performing translation was to use a statistical machine translation (SMT) methodology to forecast the best probable output with specific algorithms. However, with NMT, the approach employs dynamic algorithms to achieve better translations based on context.[22]\n AI has been used in facial recognition systems. Some examples are Apple's Face ID and Android's Face Unlock, which are used to secure mobile devices.[23]\n Image labeling has been used by Google Image Labeler to detect products in photos and to allow people to search based on a photo. Image labeling has also been demonstrated to generate speech to describe images to blind people.[24] Facebook's DeepFace identifies human faces in digital images.\n Games have been a major application[relevant?] of AI's capabilities since the 1950s. In the 21st century, AIs have beaten human players in many games, including chess (Deep Blue), Jeopardy! (Watson),[25] Go (AlphaGo),[26][27][28][29][30][31][32] poker (Pluribus[33] and Cepheus),[34] E-sports (StarCraft),[35][36] and general game playing (AlphaZero[37][38][39] and MuZero).[40][41][42][43]\n Kuki AI is a set of chatbots and other apps which were designed for entertainment and as a marketing tool.[44][45] Character.ai is another example of a chatbot being used for recreation.\n AI for Good is a platform launched in 2017 by the International Telecommunication Union (ITU) agency of the United Nations (UN). The goal of the platform is to use AI to help achieve the UN's Sustainable Development Goals.[citation needed]\n The University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address problems such as homelessness. Stanford researchers use AI to analyze satellite images to identify high poverty areas.[46]\n In agriculture, AI has been proposed as a way for farmers to identify areas that need irrigation, fertilization, or pesticide treatments to increase yields, thereby improving efficiency.[47] AI has been used to attempt to classify livestock pig call emotions,[20] automate greenhouses,[48] detect diseases and pests,[49] and optimize irrigation.[50]\n Cyber security companies are adopting neural networks, machine learning, and natural language processing to improve their systems.[51]\n Applications of AI in cyber security include:\n AI elevates teaching, focusing on significant issues like the knowledge nexus and educational equality. The evolution of AI in education and technology should be used to improve human capabilities in relationships where they do not replace humans. UNESCO recognizes the future of AI in education as an instrument to reach Sustainable Development Goal 4, called \"Inclusive and Equitable Quality Education.\u201d [56]\n The World Economic Forum also stresses AI's contribution to students' overall improvement and transforming teaching into a more enjoyable process.[56]\n Personalized Learning\n AI driven tutoring systems, such as Khan Academy, Duolingo and Carnegie Learning are the forefoot of delivering personalized education.[57]\n These platforms leverage AI algorithms to analyze individual learning patterns, strengths, and weaknesses, enabling the customization of content and Algorithm to suit each student's pace and style of learning.[57]\n Administrative Efficiency\n In educational institutions, AI is increasingly used to automate routine tasks like attendance tracking, grading and marking, which allows educators to devote more time to interactive teaching and direct student engagement.[58]\n Furthermore, AI tools are employed to monitor student progress, analyze learning behaviors, and predict academic challenges, facilitating timely and proactive interventions for students who may be at risk of falling behind.[58]\n Ethical and Privacy Concerns\n Despite the benefits, the integration of AI in education raises significant ethical and privacy concerns, particularly regarding the handling of sensitive student data.[57]\n It is imperative that AI systems in education are designed and operated with a strong emphasis on transparency, security, and respect for privacy to maintain trust and uphold the integrity of educational practices.[57]\n Much of the regulation will be influenced by the AI Act, the world\u2019s first comprehensive AI law. [59]\n Financial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking began in 1987 when Security Pacific National Bank launched a fraud prevention task-force to counter the unauthorized use of debit cards.[60] Kasisto and Moneystream use AI.\n Banks use AI to organize operations for bookkeeping, investing in stocks, and managing properties. AI can adapt to changes during non-business hours.[61] AI is used to combat fraud and financial crimes by monitoring behavioral patterns for any abnormal changes or anomalies.[62][63][64]\n The use of AI in applications such as online trading and decision-making has changed major economic theories.[65] For example, AI-based buying and selling platforms estimate personalized demand and supply curves, thus enabling individualized pricing. AI systems reduce information asymmetry in the market and thus make markets more efficient.[66] The application of artificial intelligence in the financial industry can alleviate the financing constraints of non-state-owned enterprises, especially for smaller and more innovative enterprises.[67]\n Algorithmic trading involves the use of AI systems to make trading decisions at speeds orders of magnitude greater than any human is capable of, making millions of trades in a day without human intervention. Such high-frequency trading represents a fast-growing sector. Many banks, funds, and proprietary trading firms now have entire portfolios that are AI-managed. Automated trading systems are typically used by large institutional investors but include smaller firms trading with their own AI systems.[68]\n Large financial institutions use AI to assist with their investment practices. BlackRock's AI engine, Aladdin, is used both within the company and by clients to help with investment decisions. Its functions include the use of natural language processing to analyze text such as news, broker reports, and social media feeds. It then gauges the sentiment on the companies mentioned and assigns a score. Banks such as UBS and Deutsche Bank use SQREEM (Sequential Quantum Reduction and Extraction Model) to mine data to develop consumer profiles and match them with wealth management products.[69]\n Online lender Upstart uses machine learning for underwriting.[70]\n ZestFinance's Zest Automated Machine Learning (ZAML) platform is used for credit underwriting. This platform uses machine learning to analyze data including purchase transactions and how a customer fills out a form to score borrowers. The platform is particularly useful to assign credit scores to those with limited credit histories.[71]\n AI makes continuous auditing possible. Potential benefits include reducing audit risk, increasing the level of assurance, and reducing audit duration.[72][quantify]\n Continuous auditing with AI allows real-time monitoring and reporting of financial activities and provides businesses with timely insights that can lead to quick decision making.[73]\n AI software, such as LaundroGraph which uses contemporary suboptimal datasets, could be used for anti-money laundering (AML).[74][75]\n In the 1980s, AI started to become prominent in finance as expert systems were commercialized. For example, Dupont created 100 expert systems, which helped them to save almost $10 million per year.[76] One of the first systems was the Pro-trader expert system that predicted the 87-point drop in the Dow Jones Industrial Average in 1986. \"The major junctions of the system were to monitor premiums in the market, determine the optimum investment strategy, execute transactions when appropriate and modify the knowledge base through a learning mechanism.\"[77]\n One of the first expert systems to help with financial plans was PlanPowerm and Client Profiling System, created by Applied Expert Systems (APEX). It was launched in 1986. It helped create personal financial plans for people.[78]\n In the 1990s AI was applied to fraud detection. In 1993 FinCEN Artificial Intelligence System (FAIS) launched. It was able to review over 200,000 transactions per week and over two years it helped identify 400 potential cases of money laundering equal to $1 billion.[79] These expert systems were later replaced by machine learning systems.[80]\n AI can enhance entrepreneurial activity and AI is one of the most dynamic areas for start-ups, with significant venture capital flowing into AI.[81]\n AI facial recognition systems are used for mass surveillance, notably in China.[82][83] In 2019, Bengaluru, India deployed AI-managed traffic signals. This system uses cameras to monitor traffic density and adjust signal timing based on the interval needed to clear traffic.[84]\n Various countries are deploying AI military applications.[85] The main applications enhance command and control, communications, sensors, integration and interoperability.[86] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[85] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams.[86]\n AI has been used in military operations in Iraq, Syria, Israel and Ukraine.[85][87][88][89]\n AI in healthcare is often used for classification, to evaluate a CT scan or electrocardiogram or to identify high-risk patients for population health. AI is helping with the high-cost problem of dosing. One study suggested that AI could save $16\u00a0billion. In 2016, a study reported that an AI-derived formula derived the proper dose of immunosuppressant drugs to give to transplant patients.[90] Current research has indicated that non-cardiac vascular illnesses are also being treated with artificial intelligence (AI). For certain disorders, AI algorithms can aid in diagnosis, recommended treatments, outcome prediction, and patient progress tracking. As AI technology advances, it is anticipated that it will become more significant in the healthcare industry.[91]\n The early detection of diseases like cancer is made possible by AI algorithms, which diagnose diseases by analyzing complex sets of medical data. For example, the IBM Watson system might be used to comb through massive data such as medical records and clinical trials to help diagnose a problem.[92] Microsoft's AI project Hanover helps doctors choose cancer treatments from among the more than 800 medicines and vaccines.[93][94] Its goal is to memorize all the relevant papers to predict which (combinations of) drugs will be most effective for each patient. Myeloid leukemia is one target. Another study reported on an AI that was as good as doctors in identifying skin cancers.[95] Another project monitors multiple high-risk patients by asking each patient questions based on data acquired from doctor/patient interactions.[96] In one study done with transfer learning, an AI diagnosed eye conditions similar to an ophthalmologist and recommended treatment referrals.[97]\n Another study demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel judged better than a surgeon.[98]\n Artificial neural networks are used as clinical decision support systems for medical diagnosis,[99] such as in concept processing technology in EMR software.\n Other healthcare tasks thought suitable for an AI that are in development include:\n AI-enabled chatbots decrease the need for humans to perform basic call center tasks.[115]\n Machine learning in sentiment analysis can spot fatigue in order to prevent overwork.[115] Similarly, decision support systems can prevent industrial disasters and make disaster response more efficient.[116] For manual workers in material handling, predictive analytics may be used to reduce musculoskeletal injury.[117] Data collected from wearable sensors can improve workplace health surveillance, risk assessment, and research.[116][how?]\n AI can auto-code workers' compensation claims.[118][119] AI-enabled virtual reality systems can enhance safety training for hazard recognition.[116] AI can more efficiently detect accident near misses, which are important in reducing accident rates, but are often underreported.[120]\n AlphaFold 2 can determine the 3D structure of a (folded) protein in hours rather than the months required by earlier automated approaches and was used to provide the likely structures of all proteins in the human body and essentially all proteins known to science (more than 200 million).[121][122][123][124]\n Machine learning has been used for drug design.[125] It has also been used for predicting molecular properties and exploring large chemical/reaction spaces.[126] Computer-planned syntheses via computational reaction networks, described as a platform that combines \"computational synthesis with AI algorithms to predict molecular properties\",[127] have been used to explore the origins of life on Earth,[128] drug-syntheses and developing routes for recycling 200 industrial waste chemicals into important drugs and agrochemicals (chemical synthesis design).[129] There is research about which types of computer-aided chemistry would benefit from machine learning.[130] It can also be used for \"drug discovery and development, drug repurposing, improving pharmaceutical productivity, and clinical trials\".[131] It has been used for the design of proteins with prespecified functional sites.[132][133]\n It has been used with databases for the development of a 46-day process to design, synthesize and test a drug which inhibits enzymes of a particular gene, DDR1. DDR1 is involved in cancers and fibrosis which is one reason for the high-quality datasets that enabled these results.[134]\n There are various types of applications for machine learning in decoding human biology, such as helping to map gene expression patterns to functional activation patterns[135] or identifying functional DNA motifs.[136] It is widely used in genetic research.[137]\n There also is some use of machine learning in synthetic biology,[138][139] disease biology,[139] nanotechnology (e.g. nanostructured materials and bionanotechnology),[140][141] and materials science.[142][143][144]\n There are also prototype robot scientists, including robot-embodied ones like the two Robot Scientists, which show a form of \"machine learning\" not commonly associated with the term.[145][146]\n Similarly, there is research and development of biological \"wetware computers\" that can learn (e.g. for use as biosensors) and/or implantation into an organism's body (e.g. for use to control prosthetics).[147][148][149] Polymer-based artificial neurons operate directly in biological environments and define biohybrid neurons made of artificial and living components.[150][151]\n Moreover, if whole brain emulation is possible via both scanning and replicating the, at least, bio-chemical brain \u2013 as premised in the form of digital replication in The Age of Em, possibly using physical neural networks \u2013 that may have applications as or more extensive than e.g. valued human activities and may imply that society would face substantial moral choices, societal risks and ethical problems[152][153] such as whether (and how) such are built, sent through space and used compared to potentially competing e.g. potentially more synthetic and/or less human and/or non/less-sentient types of artificial/semi-artificial intelligence.[additional citation(s) needed] An alternative or additive approach to scanning are types of reverse engineering of the brain.[154][155]\n A subcategory of artificial intelligence is embodied,[156][157] some of which are mobile robotic systems that each consist of one or multiple robots that are able to learn in the physical world.\n However, biological computers, even if both highly artificial and intelligent, are typically distinguished from synthetic, often silicon-based, computers \u2013 they could however be combined or used for the design of either. Moreover, many tasks may be carried out inadequately by artificial intelligence even if its algorithms were transparent, understood, bias-free, apparently effective, and goal-aligned and its trained data sufficiently large and cleansed \u2013 such as in cases were the underlying or available metrics, values or data are inappropriate. Computer-aided is a phrase used to describe human activities that make use of computing as tool in more comprehensive activities and systems such as AI for narrow tasks or making use of such without substantially relying on its results (see also: human-in-the-loop).[citation needed] A study described the biological as a limitation of AI with \"as long as the biological system cannot be understood, formalized, and imitated, we will not be able to develop technologies that can mimic it\" and that if it was understood this does not mean there being \"a technological solution to imitate natural intelligence\".[158] Technologies that integrate biology and are often AI-based include biorobotics.\n Artificial intelligence is used in astronomy to analyze increasing amounts of available data[159][160] and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights\" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy.[161] It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance,[162] and more autonomous operation.[163][164][165][160]\n In the search for extraterrestrial intelligence (SETI), machine learning has been used in attempts to identify artificially generated electromagnetic waves in available data[166][167] \u2013 such as real-time observations[168] \u2013 and other technosignatures, e.g. via anomaly detection.[169] In ufology, the SkyCAM-5 project headed by Prof. Hakan Kayal[170] and the Galileo Project headed by Avi Loeb use machine learning to attempt to detect and classify types of UFOs.[171][172][173][174][175] The Galileo Project also seeks to detect two further types of potential extraterrestrial technological signatures with the use of AI: 'Oumuamua-like interstellar objects, and non-manmade artificial satellites.[176][177]\n Machine learning can also be used to produce datasets of spectral signatures of molecules that may be involved in the atmospheric production or consumption of particular chemicals \u2013 such as phosphine possibly detected on Venus \u2013 which could prevent miss assignments and, if accuracy is improved, be used in future detections and identifications of molecules on other planets.[178]\n In April 2024, the Scientific Advice Mechanism to the European Commission published advice[179] including a comprehensive evidence review of the opportunities and challenges posed by artificial intelligence in scientific research.\n As benefits, the evidence review[180] highlighted:\n As challenges:\n Machine learning can help to restore and attribute ancient texts.[181] It can help to index texts for example to enable better and easier searching[182] and classification of fragments.[183]\n \nArtificial intelligence can also be used to investigate genomes to uncover genetic history, such as interbreeding between archaic and modern humans by which for example the past existence of a ghost population, not Neanderthal or Denisovan, was inferred.[184]  \nIt can also be used for \"non-invasive and non-destructive access to internal structures of archaeological remains\".[185]  A deep learning system was reported to learn intuitive physics from visual data (of virtual 3D environments) based on an unpublished approach inspired by studies of visual cognition in infants.[186][187] Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems' future dynamics from video recordings of their behavior.[188][189] In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems.[188]\n AI could be used for materials optimization and discovery such as the discovery of stable materials and the prediction of their crystal structure.[190][191][192]\n In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.[193][194][195]\n Machine learning is used in diverse types of reverse engineering. For example, machine learning has been used to reverse engineer a composite material part, enabling unauthorized production of high quality parts,[196] and for quickly understanding the behavior of malware.[197][198][199] It can be used to reverse engineer artificial intelligence models.[200] It can also design components by engaging in a type of reverse engineering of not-yet existent virtual components such as inverse molecular design for particular desired functionality[201] or protein design for prespecified functional sites.[132][133] Biological network reverse engineering could model interactions in a human understandable way, e.g. bas on time series data of gene expression levels.[202]\n AI is a mainstay of law-related professions. Algorithms and machine learning do some tasks previously done by entry-level lawyers.[203] While its use is common, it is not expected to replace most work done by lawyers in the near future.[204]\n The electronic discovery industry uses machine learning to reduce manual searching.[205]\n Law enforcement has begun using facial recognition systems (FRS) to identify suspects from visual data. FRS results have proven to be more accurate when compared to eyewitness results. Furthermore, FRS has shown to have much a better ability to identify individuals when video clarity and visibility are low in comparison to human participants. [206]\n COMPAS is a commercial system used by U.S. courts to assess the likelihood of recidivism.[207]\n One concern relates to algorithmic bias, AI programs may become biased after processing data that exhibits bias.[208] ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than that of white defendants.[207]\n In 2019, the city of Hangzhou, China established a pilot program artificial intelligence-based Internet Court to adjudicate disputes related to ecommerce and internet-related intellectual property claims.[209]:\u200a124\u200a Parties appear before the court via videoconference and AI evaluates the evidence presented and applies relevant legal standards.[209]:\u200a124\u200a\n Another application of AI is in human resources. AI can screen resumes and rank candidates based on their qualifications, predict candidate success in given roles, and automate repetitive communication tasks via chatbots.[210]\n AI has simplified the recruiting /job search process for both recruiters and job seekers. According to Raj Mukherjee from Indeed, 65% of job searchers search again within 91 days after hire. An AI-powered engine streamlines the complexity of job hunting by assessing information on job skills, salaries, and user tendencies, matching job seekers to the most relevant positions. Machine intelligence calculates appropriate wages and highlights resume information for recruiters using NLP, which extracts relevant words and phrases from text. Another application is an AI resume builder that compiles a CV in 5 minutes.[211] Chatbots assist website visitors and refine workflows.\n AI underlies avatars (automated online assistants) on web pages.[212] It can reduce operation and training costs.[212] Pypestream automated customer service for its mobile application to streamline communication with customers.[213]\n A Google app analyzes language and converts speech into text. The platform can identify angry customers through their language and respond appropriately.[214] Amazon uses a chatbot for customer service that can perform tasks like checking the status of an order, cancelling orders, offering refunds and connecting the customer with a human representative.[215] Generative AI (GenAI), such as ChatGPT, is increasingly used in business to automate tasks and enhance decision-making.[216]\n In the hospitality industry, AI is used to reduce repetitive tasks, analyze trends, interact with guests, and predict customer needs.[217] AI hotel services come in the form of a chatbot,[218] application, virtual voice assistant and service robots.\n AI applications analyze media content such as movies, TV programs, advertisement videos or user-generated content. The solutions often involve computer vision.\n Typical scenarios include the analysis of images using object recognition or face recognition techniques, or the analysis of video for scene recognizing scenes, objects or faces. AI-based media analysis can facilitate media search, the creation of descriptive keywords for content, content policy monitoring (such as verifying the suitability of content for a particular TV viewing time), speech to text for archival or other purposes, and the detection of logos, products or celebrity faces for ad placement.\n Deep-fakes can be used for comedic purposes but are better known for fake news and hoaxes.\n Deepfakes can portray individuals in harmful or compromising situations, causing significant reputational damage and emotional distress, especially when the content is defamatory or violates personal ethics. While defamation and false light laws offer some recourse, their focus on false statements rather than fabricated images or videos often leaves victims with limited legal protection and a challenging burden of proof.[232]\n In January 2016,[233] the Horizon 2020 program financed the InVID Project[234][235] to help journalists and researchers detect fake documents, made available as browser plugins.[236][237]\n In June 2016, the visual computing group of the Technical University of Munich and from Stanford University developed Face2Face,[238] a program that animates photographs of faces, mimicking the facial expressions of another person. The technology has been demonstrated animating the faces of people including Barack Obama and Vladimir Putin. Other methods have been demonstrated based on deep neural networks, from which the name deep fake was taken.\n In September 2018, U.S. Senator Mark Warner proposed to penalize social media companies that allow sharing of deep-fake documents on their platforms.[239]\n In 2018, Darius Afchar and Vincent Nozick found a way to detect faked content by analyzing the mesoscopic properties of video frames.[240] DARPA gave 68 million dollars to work on deep-fake detection.[240]\n Audio deepfakes[241][242] and AI software capable of detecting deep-fakes and cloning human voices have been developed.[243][244]\n Respeecher is a program that enables one person to speak with the voice of another.\n AI algorithms have been used to detect deepfake videos.[245][246]\n Artificial intelligence is also starting to be used in video production, with tools and software being developed that utilize generative AI in order to create new video, or alter existing video. Some of the major tools that are being used in these processes currently are DALL-E, Mid-journey, and Runway.[247]  Way mark Studios utilized the tools offered by both DALL-E and Mid-journey to create a fully AI generated film called The Frost in the summer of 2023.[247] Way mark Studios is experimenting with using these AI tools to generate advertisements and commercials for companies in mere seconds.[247]  Yves Bergquist, a director of the AI & Neuroscience in Media Project at USC's Entertainment Technology Center, says post production crews in Hollywood are already using generative AI, and predicts that in the future more companies will embrace this new technology.[248]\n AI has been used to compose music of various genres.\n David Cope created an AI called Emily Howell that managed to become well known in the field of algorithmic computer music.[249] The algorithm behind Emily Howell is registered as a US patent.[250]\n In 2012, AI Iamus created the first complete classical album.[251]\n AIVA (Artificial Intelligence Virtual Artist), composes symphonic music, mainly classical music for film scores.[252] It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.[253]\n Melomics creates computer-generated music for stress and pain relief.[254]\n At Sony CSL Research Laboratory, the Flow Machines software creates pop songs by learning music styles from a huge database of songs. It can compose in multiple styles.\n The Watson Beat uses reinforcement learning and deep belief networks to compose music on a simple seed input melody and a select style. The software was open sourced[255] and musicians such as Taryn Southern[256] collaborated with the project to create music.\n South Korean singer, Hayeon's, debut song, \"Eyes on You\" was composed using AI which was supervised by real composers, including NUVO.[257]\n Narrative Science sells computer-generated news and reports. It summarizes sporting events based on statistical data from the game. It also creates financial reports and real estate analyses.[258] Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football.[259]\n Yseop, uses AI to turn structured data into natural language comments and recommendations. Yseop writes financial reports, executive summaries, personalized sales or marketing documents and more in multiple languages, including English, Spanish, French, and German.[260]\n TALESPIN made up stories similar to the fables of Aesop. The program started with a set of characters who wanted to achieve certain goals. The story narrated their attempts to satisfy these goals.[citation needed] Mark Riedl and Vadim Bulitko asserted that the essence of storytelling was experience management, or \"how to balance the need for a coherent story progression with user agency, which is often at odds\".[261]\n While AI storytelling focuses on story generation (character and plot), story communication also received attention. In 2002, researchers developed an architectural framework for narrative prose generation. They faithfully reproduced text variety and complexity on stories such as Little Red Riding Hood.[262] In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.[263]\n South Korean company Hanteo Global uses a journalism bot to write articles.[264]\n Literary authors are also exploring uses of AI. An example is David Jhave Johnston's work ReRites (2017-2019), where the poet created a daily rite of editing the poetic output of a neural network to create a series of performances and publications.\n In 2010, artificial intelligence used baseball statistics to automatically generate news articles. This was launched by The Big Ten Network using software from Narrative Science.[265]\n After being unable to cover every Minor League Baseball game with a large team, Associated Press collaborated with Automated Insights in 2016 to create game recaps that were automated by artificial intelligence.[266]\n UOL in Brazil expanded the use of AI in its writing. Rather than just generating news stories, they programmed the AI to include commonly searched words on Google.[266]\n El Pais, a Spanish news site that covers many things including sports, allows users to make comments on each news article. They use the Perspective API to moderate these comments and if the software deems a comment to contain toxic language, the commenter must modify it in order to publish it.[266]\n A local Dutch media group used AI to create automatic coverage of amateur soccer, set to cover 60,000 games in just a single season. NDC partnered with United Robots to create this algorithm and cover what would have never been possible before without an extremely large team.[266]\n Lede AI has been used in 2023 to take scores from high school football games to generate stories automatically for the local newspaper. This was met with significant criticism from readers for the very robotic diction that was published. With some descriptions of games being a \"close encounter of the athletic kind,\" readers were not pleased and let the publishing company, Gannett, know on social media. Gannett has since halted their used of Lede AI until they come up with a solution for what they call an experiment.[267]\n  Millions of its articles have been edited by bots[271] which however are usually not artificial intelligence software. Many AI platforms use Wikipedia data,[272] mainly for training machine learning applications. There is research and development of various artificial intelligence applications for Wikipedia such as for identifying outdated sentences,[273] detecting covert vandalism[274] or recommending articles and tasks to new editors.\n Machine translation (see above) has also be used for translating Wikipedia articles and could play a larger role in creating, updating, expanding, and generally improving articles in the future. A content translation tool allows editors of some Wikipedias to more easily translate articles across several select languages.[275][276]\n In video games, AI is routinely used to generate behavior in non-player characters (NPCs). In addition, AI is used for pathfinding. Some researchers consider NPC AI in games to be a \"solved problem\" for most production tasks.[who?] Games with less typical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010).[277][278] AI is also used in Alien Isolation (2014) as a way to control the actions the Alien will perform next.[279]\n Kinect, which provides a 3D body\u2013motion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from AI research.[280][which?]\n AI has been used to produce visual art. The first AI art program, called AARON, was developed by Harold Cohen in 1968[281] with the goal of being able to code the act of drawing. It started by creating simple black and white drawings, and later to painting using special brushes and dyes that were chosen by the program itself without mediation from Cohen.[282]\n AI platforms such as \"DALL-E\",[283] Stable Diffusion,[283] Imagen,[284] and Midjourney[285] have been used for generating visual images from inputs such as text or other images.[286] Some AI tools allow users to input images and output changed versions of that image, such as to display an object or product in different environments. AI image models can also attempt to replicate the specific styles of artists, and can add visual complexity to rough sketches.\n Since their design in 2014, generative adversarial networks (GANs) have been used by AI artists. GAN computer programming, generates technical images through machine learning frameworks that surpass the need for human operators.[281] Examples of GAN programs that generate art include Artbreeder and DeepDream.\n In addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. Although the main goal of the large-scale digitization of artwork in the past few decades was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.[287]\nTwo computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art.[288] While distant viewing includes the analysis of large collections, close reading involves one piece of artwork.\n AI has been in use since the early 2000s, most notably by a system designed by Pixar called \"Genesis\".[289] It was designed to learn algorithms and create 3D models for its characters and props. Notable movies that used this technology included Up and The Good Dinosaur.[290] AI has been used less ceremoniously in recent years. In 2023, it was revealed Netflix of Japan was using AI to generate background images for their upcoming show to be met with backlash online.[291]  In recent years, motion capture became an easily accessible form of AI animation. For example, Move AI is a program built to capture any human movement and reanimate it in its animation program using learning AI.[292]\n Power electronics converters are used in renewable energy, energy storage, electric vehicles and high-voltage direct current transmission. These converters are failure-prone, which can interrupt service and require costly maintenance or catastrophic consequences in mission critical applications.[citation needed] AI can guide the design process for reliable power electronics converters, by calculating exact design parameters that ensure the required lifetime.[293]\n The U.S. Department of Energy underscores AI's pivotal role in realizing national climate goals. With AI, the ambitious target of achieving net-zero greenhouse gas emissions across the economy becomes feasible. AI also helps make room for wind and solar on the grid by avoiding congestion and increasing grid reliability. [294]\n Machine learning can be used for energy consumption prediction and scheduling, e.g. to help with renewable energy intermittency management (see also: smart grid and climate change mitigation in the power grid).[295][296][297][298][125]\n Many telecommunications companies make use of heuristic search to manage their workforces. For example, BT Group deployed heuristic search[299] in an application that schedules 20,000 engineers. Machine learning is also used for speech recognition (SR), including of voice-controlled devices, and SR-related transcription, including of videos.[300][301]\n Artificial intelligence has been combined with digital spectrometry by IdeaCuria Inc.,[302][303] enable applications such as at-home water quality monitoring.\n In the 1990s, early artificial intelligence tools controlled Tamagotchis and Giga Pets, the Internet, and the first widely released robot, Furby. Aibo was a domestic robot in the form of a robotic dog with intelligent features and autonomy.\n Mattel created an assortment of AI-enabled toys that \"understand\" conversations, give intelligent responses, and learn.[304]\n Oil and gas companies have used artificial intelligence tools to automate functions, foresee equipment issues, and increase oil and gas output.[305][306]\n AI in transport is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities. The major development challenge is the complexity of transportation systems that involves independent components and parties, with potentially conflicting objectives.[307]\n AI-based fuzzy logic controllers operate gearboxes. For example, the 2006 Audi TT, VW Touareg [citation needed] and VW Caravell feature the DSP transmission. A number of \u0160koda variants (\u0160koda Fabia) include a fuzzy logic-based controller. Cars have AI-based driver-assist features such as self-parking and adaptive cruise control.\n There are also prototypes of autonomous automotive public transport vehicles such as electric mini-buses[308][309][310][311] as well as autonomous rail transport in operation.[312][313][314]\n There also are prototypes of autonomous delivery vehicles, sometimes including delivery robots.[315][316][317][318][319][320][321]\n Transportation's complexity means that in most cases training an AI in a real-world driving environment is impractical. Simulator-based testing can reduce the risks of on-road training.[322]\n AI underpins self-driving vehicles. Companies involved with AI include Tesla, Waymo, and General Motors. AI-based systems control functions such as braking, lane changing, collision prevention, navigation and mapping.[323]\n Autonomous trucks are in the testing phase. The UK government passed legislation to begin testing of autonomous truck platoons in 2018.[324] A group of autonomous trucks follow closely behind each other. German corporation Daimler is testing its Freightliner Inspiration.[325]\n Autonomous vehicles require accurate maps to be able to navigate between destinations.[326] Some autonomous vehicles do not allow human drivers (they have no steering wheels or pedals).[327]\n AI has been used to optimize traffic management, which reduces wait times, energy use, and emissions by as much as 25 percent.[328]\n Smart traffic lights have been developed at Carnegie Mellon since 2009.  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities.  It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.[329]\n The Royal Australian Air Force (RAAF) Air Operations Division (AOD) uses AI for expert systems. AIs operate as surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.[330]\n Aircraft simulators use AI for training aviators. Flight conditions can be simulated that allow pilots to make mistakes without risking themselves or expensive aircraft. Air combat can also be simulated.\n AI can also be used to operate planes analogously to their control of ground vehicles. Autonomous drones can fly independently or in swarms.[331]\n AOD uses the Interactive Fault Diagnosis and Isolation System, or IFDIS, which is a rule-based expert system using information from TF-30 documents and expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for the F-111C. The system replaced specialized workers. The system allowed regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers.\n Speech recognition allows traffic controllers to give verbal directions to drones.\n Artificial intelligence supported design of aircraft,[332] or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule-based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective.\n In 2003 a Dryden Flight Research Center project created software that could enable a damaged aircraft to continue flight until a safe landing can be achieved.[333] The software compensated for damaged components by relying on the remaining undamaged components.[334]\n The 2016 Intelligent Autopilot System combined apprenticeship learning and behavioral cloning whereby the autopilot observed low-level actions required to maneuver the airplane and high-level strategy used to apply those actions.[335]\n Neural networks are used by situational awareness systems in ships and boats.[336] There also are autonomous boats.\n Autonomous ships that monitor the ocean, AI-driven satellite data analysis, passive acoustics[337] or remote sensing and other applications of environmental monitoring make use of machine learning.[338][339][340][165]\n For example, \"Global Plastic Watch\" is an AI-based satellite monitoring-platform for analysis/tracking of plastic waste sites to help prevention of plastic pollution \u2013 primarily ocean pollution \u2013 by helping identify who and where mismanages plastic waste, dumping it into oceans.[341][342]\n Machine learning can be used to spot early-warning signs of disasters and environmental issues, possibly including natural pandemics,[343][344] earthquakes,[345][346][347] landslides,[348] heavy rainfall,[349] long-term water supply vulnerability,[350] tipping-points of ecosystem collapse,[351] cyanobacterial bloom outbreaks,[352] and droughts.[353][354][355]\n AI can be used for real-time code completion, chat, and automated test generation. These tools are typically integrated with editors and IDEs as plugins. They differ in functionality, quality, speed, and approach to privacy.[356] Code suggestions could be incorrect, and should be carefully reviewed by software developers before accepted.\n GitHub Copilot is an artificial intelligence model developed by GitHub and OpenAI that is able to autocomplete code in multiple programming languages.[357] Price for individuals: $10/mo or $100/yr, with one free month trial.\n Tabnine was created by Jacob Jackson and was originally owned by Tabnine company. In late 2019, Tabnine was acquired by Codota.[358] Tabnine tool is available as plugin to most popular IDEs. It offers multiple pricing options, including limited \"starter\" free version.[359]\n CodiumAI by CodiumAI, small startup in Tel Aviv, offers automated test creation. Currently supports Python, JS, and TS.[360]\n Ghostwriter by Replit offers code completion and chat.[361] They have multiple pricing plans, including a free one and a \"Hacker\" plan for $7/month.\n CodeWhisperer by Amazon collects individual users' content, including files open in the IDE. They claim to focus on security both during transmission and when storing.[362] Individual plan is free, professional plan is $19/user/month.\n Other tools: SourceGraph Cody, CodeCompleteFauxPilot, Tabby[356]\n AI can be used to create other AIs. For example, around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and POCO F1. NASNet's performance exceeded all previously published performance on ImageNet.[363]\n Machine learning has been used for noise-cancelling in quantum technology,[364] including quantum sensors.[365] Moreover, there is substantial research and development of using quantum computers with machine learning algorithms. For example, there is a prototype, photonic, quantum memristive device for neuromorphic (quantum-)computers (NC)/artificial neural networks and NC-using quantum materials with some variety of potential neuromorphic computing-related applications,[366][367] and quantum machine learning is a field with some variety of applications under development. AI could be used for quantum simulators which may have the application of solving physics and chemistry[368][369] problems as well as for quantum annealers for training of neural networks for AI applications.[370] There may also be some usefulness in chemistry, e.g. for drug discovery, and in materials science, e.g. for materials optimization/discovery (with possible relevance to quantum materials manufacturing[191][192]).[371][372][373][better\u00a0source\u00a0needed]\n AI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered AI. All of the following were originally developed in AI laboratories:[374]\n An optical character reader is used in the extraction of data in business documents like invoices and receipts. It can also be used in business contract documents e.g. employment agreements to extract critical data like employment terms, delivery terms, termination clauses, etc.[375]\n Artificial intelligence in architecture describes the use of artificial intelligence in automation, design and planning in the architectural process or in assisting human skills in the field of architecture. Artificial Intelligence is thought to potentially lead to and ensue major changes in architecture.[376][377][378]\n AI in architecture has created a way for architects to create things beyond human understanding. AI implementation of machine learning text-to-render technologies, like DALL-E and stable Diffusion, gives power to visualization complex.[379]\n AI allows designers to demonstrate their creativity and even invent new ideas while designing. In future, AI will not replace architects; instead, it will improve the speed of translating ideas sketching.[379]\n",
        "doc_number": 11
    },
    {
        "url": "https://en.wikipedia.org/wiki/Artificial_intelligence_in_video_games",
        "content": "\n In video games, artificial intelligence (AI) is used to generate responsive, adaptive or intelligent behaviors primarily in non-playable characters (NPCs) similar to human-like intelligence. Artificial intelligence has been an integral part of video games since their inception in 1948, first seen in the game Nim.[1] AI in video games is a distinct subfield and differs from academic AI. It serves to improve the game-player experience rather than machine learning or decision making. During the golden age of arcade video games the idea of AI opponents was largely popularized in the form of graduated difficulty levels, distinct movement patterns, and in-game events dependent on the player's input. Modern games often implement existing techniques such as pathfinding and decision trees to guide the actions of NPCs. AI is often used in mechanisms which are not immediately visible to the user, such as data mining and procedural-content generation.[2] One of the most infamous examples of this NPC technology and gradual difficulty levels can be found in the game Mike Tyson's Punch-Out!! (1987).[3]\n In general, game AI does not, as might be thought and sometimes is depicted to be the case, mean a realization of an artificial person corresponding to an NPC in the manner of the Turing test or an artificial general intelligence.\n The term game AI is used to refer to a broad set of algorithms that also include techniques from control theory, robotics, computer graphics and computer science in general, and so video game AI may often not constitute \"true AI\" in that such techniques do not necessarily facilitate computer learning or other standard criteria, only constituting \"automated computation\" or a predetermined and limited set of responses to a predetermined and limited set of inputs.[4][5][6]\n Many industries and corporate voices[who?] argue that game AI has come a long way in the sense that it has revolutionized the way humans interact with all forms of technology, although many[who?] expert researchers are skeptical of such claims, and particularly of the notion that such technologies fit the definition of \"intelligence\" standardly used in the cognitive sciences.[4][5][6][7] Industry voices[who?] make the argument that AI has become more versatile in the way we use all technological devices for more than their intended purpose because the AI allows the technology to operate in multiple ways, allegedly developing their own personalities and carrying out complex instructions of the user.[8][9]\n People[who?] in the field of AI have argued that video game AI is not true intelligence, but an advertising buzzword used to describe computer programs that use simple sorting and matching algorithms to create the illusion of intelligent behavior while bestowing software with a misleading aura of scientific or technological complexity and advancement.[4][5][6][10] Since game AI for NPCs is centered on appearance of intelligence and good gameplay within environment restrictions, its approach is very different from that of traditional AI.\n Game playing was an area of research in AI from its inception. One of the first examples of AI is the computerized game of Nim made in 1951 and published in 1952. Despite being advanced technology in the year it was made, 20 years before Pong, the game took the form of a relatively small box and was able to regularly win games even against highly skilled players of the game.[1] In 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program and Dietrich Prinz wrote one for chess.[11] These were among the first computer programs ever written. Arthur Samuel's checkers program, developed in the middle 1950s and early 1960s, eventually achieved sufficient skill to challenge a respectable amateur.[12] Work on checkers and chess would culminate in the defeat of Garry Kasparov by IBM's Deep Blue computer in 1997.[13] The first video games developed in the 1960s and early 1970s, like Spacewar!, Pong, and Gotcha (1973), were games implemented on discrete logic and strictly based on the competition of two players, without AI.\n Games that featured a single player mode with enemies started appearing in the 1970s.  The first notable ones for the arcade appeared in 1974: the Taito game Speed Race (racing video game) and the Atari games Qwak (duck hunting light gun shooter) and Pursuit (fighter aircraft dogfighting simulator).  Two text-based computer games, Star Trek (1971) and Hunt the Wumpus (1973), also had enemies.  Enemy movement was based on stored patterns. The incorporation of microprocessors would allow more computation and random elements overlaid into movement patterns.\n It was during the golden age of video arcade games that the idea of AI opponents was largely popularized, due to the success of Space Invaders (1978), which sported an increasing difficulty level, distinct movement patterns, and in-game events dependent on hash functions based on the player's input. Galaxian (1979) added more complex and varied enemy movements, including maneuvers by individual enemies who break out of formation. Pac-Man (1980) introduced AI patterns to maze games, with the added quirk of different personalities for each enemy. Karate Champ (1984) later introduced AI patterns to fighting games. First Queen (1988) was a tactical action RPG which featured characters that can be controlled by the computer's AI in following the leader.[14][15] The role-playing video game Dragon Quest IV (1990) introduced a \"Tactics\" system, where the user can adjust the AI routines of non-player characters during battle, a concept later introduced to the action role-playing game genre by Secret of Mana (1993).\n Games like Madden Football, Earl Weaver Baseball and Tony La Russa Baseball all based their AI in an attempt to duplicate on the computer the coaching or managerial style of the selected celebrity.  Madden, Weaver and La Russa all did extensive work with these game development teams to maximize the accuracy of the games.[citation needed]  Later sports titles allowed users to \"tune\" variables in the AI to produce a player-defined managerial or coaching strategy.\n The emergence of new game genres in the 1990s prompted the use of formal AI tools like finite-state machines. Real-time strategy games taxed the AI with many objects, incomplete information, pathfinding problems, real-time decisions and economic planning, among other things.[16] The first games of the genre had notorious problems. Herzog Zwei (1989), for example, had almost broken pathfinding and very basic three-state state machines for unit control, and Dune II (1992) attacked the players' base in a beeline and used numerous cheats.[17] Later games in the genre exhibited more sophisticated AI.\n Later games have used bottom-up AI methods, such as the emergent behaviour and evaluation of player actions in games like Creatures or Black & White. Fa\u00e7ade (interactive story) was released in 2005 and used interactive multiple way dialogs and AI as the main aspect of game.\n Games have provided an environment for developing artificial intelligence with potential applications beyond gameplay. Examples include Watson, a Jeopardy!-playing computer; and the RoboCup tournament, where robots are trained to compete in soccer.[18]\n Many experts[who?] complain that the \"AI\" in the term game AI overstates its worth, as game AI is not about intelligence, and shares few of the objectives of the academic field of AI. Whereas \"real AI\" addresses fields of machine learning, decision making based on arbitrary data input, and even the ultimate goal of strong AI that can reason, \"game AI\" often consists of a half-dozen rules of thumb, or heuristics, that are just enough to give a good gameplay experience.[citation needed] Historically, academic game-AI projects have been relatively separate from commercial products because the academic approaches tended to be simple and non-scalable. Commercial game AI has developed its own set of tools, which have been sufficient to give good performance in many cases.[2]\n Game developers' increasing awareness of academic AI and a growing interest in computer games by the academic community is causing the definition of what counts as AI in a game to become less idiosyncratic.  Nevertheless, significant differences between different application domains of AI mean that game AI can still be viewed as a distinct subfield of AI.  In particular, the ability to legitimately solve some AI problems in games by cheating creates an important distinction. For example, inferring the position of an unseen object from past observations can be a difficult problem when AI is applied to robotics, but in a computer game a NPC can simply look up the position in the game's scene graph.  Such cheating can lead to unrealistic behavior and so is not always desirable.  But its possibility serves to distinguish game AI and leads to new problems to solve, such as when and how to cheat.[citation needed]\n The major limitation to strong AI is the inherent depth of thinking and the extreme complexity of the decision-making process. This means that although it would be then theoretically possible to make \"smart\" AI the problem would take considerable processing power.[citation needed]\n Game AI/heuristic algorithms are used in a wide variety of quite disparate fields inside a game.  The most obvious is in the control of any NPCs in the game, although \"scripting\" (decision tree) is currently the most common means of control.[19] These handwritten decision trees often result in \"artificial stupidity\" such as repetitive behavior, loss of immersion, or abnormal behavior in situations the developers did not plan for.[20]\n Pathfinding, another common use for AI, is widely seen in real-time strategy games. Pathfinding is the method for determining how to get a NPC from one point on a map to another, taking into consideration the terrain, obstacles and possibly \"fog of war\".[21][22] Commercial videogames often use fast and simple \"grid-based pathfinding\", wherein the terrain is mapped onto a rigid grid of uniform squares and a pathfinding algorithm such as A* or IDA* is applied to the grid.[23][24][25] Instead of just a rigid grid, some games use irregular polygons and assemble a navigation mesh out of the areas of the map that NPCs can walk to.[23][26] As a third method, it is sometimes convenient for developers to manually select \"waypoints\" that NPCs should use to navigate; the cost is that such waypoints can create unnatural-looking movement. In addition, waypoints tend to perform worse than navigation meshes in complex environments.[27][28] Beyond static pathfinding, navigation is a sub-field of Game AI focusing on giving NPCs the capability to navigate in a dynamic environment, finding a path to a target while avoiding collisions with other entities (other NPC, players...) or collaborating with them (group navigation).[citation needed] Navigation in dynamic strategy games with large numbers of units, such as Age of Empires (1997) or Civilization V (2010), often performs poorly; units often get in the way of other units.[28]\n Rather than improve the Game AI to properly solve a difficult problem in the virtual environment, it is often more cost-effective to just modify the scenario to be more tractable. If pathfinding gets bogged down over a specific obstacle, a developer may just end up moving or deleting the obstacle.[29] In Half-Life (1998), the pathfinding algorithm sometimes failed to find a reasonable way for all the NPCs to evade a thrown grenade; rather than allow the NPCs to attempt to bumble out of the way and risk appearing stupid, the developers instead scripted the NPCs to crouch down and cover in place in that situation.[30]\n Many contemporary video games fall under the category of action, first-person shooter, or adventure. In most of these types of games, there is some level of combat that takes place. The AI's ability to be efficient in combat is important in these genres. A common goal today is to make the AI more human or at least appear so.\n One of the more positive and efficient features found in modern-day video game AI is the ability to hunt. AI originally reacted in a very black and white manner. If the player were in a specific area then the AI would react in either a complete offensive manner or be entirely defensive. In recent years, the idea of \"hunting\" has been introduced; in this 'hunting' state the AI will look for realistic markers, such as sounds made by the character or footprints they may have left behind.[31] These developments ultimately allow for a more complex form of play. With this feature, the player can actually consider how to approach or avoid an enemy. This is a feature that is particularly prevalent in the stealth genre.\n Another development in recent game AI has been the development of \"survival instinct\". In-game computers can recognize different objects in an environment and determine whether it is beneficial or detrimental to its survival. Like a user, the AI can look for cover in a firefight before taking actions that would leave it otherwise vulnerable, such as reloading a weapon or throwing a grenade. There can be set markers that tell it when to react in a certain way. For example, if the AI is given a command to check its health throughout a game then further commands can be set so that it reacts a specific way at a certain percentage of health. If the health is below a certain threshold then the AI can be set to run away from the player and avoid it until another function is triggered. Another example could be if the AI notices it is out of bullets, it will find a cover object and hide behind it until it has reloaded. Actions like these make the AI seem more human. However, there is still a need for improvement in this area.\n Another side-effect of combat AI occurs when two AI-controlled characters encounter each other; first popularized in the id Software game Doom, so-called 'monster infighting' can break out in certain situations.  Specifically, AI agents that are programmed to respond to hostile attacks will sometimes attack each other if their cohort's attacks land too close to them.[citation needed] In the case of Doom, published gameplay manuals even suggest taking advantage of monster infighting in order to survive certain levels and difficulty settings.\n Procedural content generation (PCG) is an AI technique to autonomously create ingame content through algorithms with minimal input from designers.[32] PCG is typically used to dynamically generate game features such as levels, NPC dialogue, and sounds. Developers input specific parameters to guide the algorithms into making content for them. PCG offers numerous advantages from both a developmental and player experience standpoint. Game studios are able to spend less money on artists and save time on production.[33] Players are given a fresh, highly replayable experience as the game generates new content each time they play. PCG allows game content to adapt in real time to the player's actions.[34]\n Generative algorithms (a rudimentary form of AI) have been used for level creation for decades. The iconic 1980 dungeon crawler computer game Rogue is a foundational example. Players are tasked with descending through the increasingly difficult levels of a dungeon to retrieve the Amulet of Yendor. The dungeon levels are algorithmically generated at the start of each game. The save file is deleted every time the player dies.[35] The algorithmic dungeon generation creates unique gameplay that would not otherwise be there as the goal of retrieving the amulet is the same each time.\n Opinions on total level generation as seen in games like Rogue can vary. Some developers can be skeptical of the quality of generated content and desire to create a world with a more \"human\" feel so they will use PCG more sparingly.[32] Consequently, they will only use PCG to generate specific components of an otherwise handcrafted level. A notable example of this is Ubisoft's 2017 tactical shooter Tom Clancy's Ghost Recon Wildlands. Developers used a pathfinding algorithm trained with a data set of real maps to create road networks that would weave through handcrafted villages within the game world.[34] This is an intelligent use of PCG as the AI would have a large amount of real world data to work with and roads are straightforward to create. However, the AI would likely miss nuances and subtleties if it was tasked with creating a village where people live.\n As AI has become more advanced, developer goals are shifting to create massive repositories of levels from data sets. In 2023, researchers from New York University and the University of the Witwatersrand trained a large language model to generate levels in the style of the 1981 puzzle game Sokoban. They found that the model excelled at generating levels with specifically requested characteristics such as difficulty level or layout.[36] However, current models such as the one used in the study require large datasets of levels to be effective. They concluded that, while promising, the high data cost of large language models currently outweighs the benefits for this application.[36] Continued advancements in the field will likely lead to more mainstream use in the future.\n The musical score of a video game is an important expression of the emotional tone of a scene to the player. Sound effects such as the noise of a weapon hitting an enemy help indicate the effect of the player's actions. Generating these in real time creates an engaging experience for the player because the game is more responsive to their input.[32] An example is the 2013 adventure game Proteus where an algorithm dynamically adapts the music based on the angle the player is viewing the ingame landscape from.[35]\n Recent breakthroughs in AI have resulted in the creation of advanced tools that are capable of creating music and sound based on evolving factors with minimal developer input. One such example is the MetaComposure music generator. MetaComposure is an evolutionary algorithm designed to generate original music compositions during real time gameplay to match the current mood of the environment.[37] The algorithm is able to assess the current mood of the game state through \"mood tagging\". Research indicates that there is a significant positive statistical correlation regarding player rated game engagement and the dynamically generated musical compositions when they accurately match their current emotions.[38]\n Game AI often amounts to pathfinding and finite-state machines. Pathfinding gets the AI from point A to point B, usually in the most direct way possible. State machines permit transitioning between different behaviors. The Monte Carlo tree search method[39] provides a more engaging game experience by creating additional obstacles for the player to overcome. The MCTS consists of a tree diagram in which the AI essentially plays tic-tac-toe. Depending on the outcome, it selects a pathway yielding the next obstacle for the player. In complex video games, these trees may have more branches, provided that the player can come up with several strategies to surpass the obstacle.\n Academic AI may play a role within game AI, outside the traditional concern of controlling NPC behavior. Georgios N. Yannakakis highlighted four potential application areas:[2]\n Rather than procedural generation, some researchers have used generative adversarial networks (GANs) to create new content. In 2018 researchers at Cornwall University trained a GAN on a thousand human-created levels for Doom; following training, the neural net prototype was able to design new playable levels on its own. Similarly, researchers at the University of California prototyped a GAN to generate levels for Super Mario.[40] In 2020 Nvidia displayed a GAN-created clone of Pac-Man; the GAN learned how to recreate the game by watching 50,000 (mostly bot-generated) playthroughs.[41]\n Non-player characters are entities within video games that are not controlled by players, but instead are managed by AI systems. NPCs contribute to the immersion, storytelling, and the mechanics of a game. They often serve as companions, quest-givers, merchants and much more. Their realism has advanced significantly in the past few years, thanks to improvements in AI technologies.\n NPCs are essential in both narrative-driven as well as open-world games. They help convey the lore and context of the game, making them pivotal to world-building and narrative progression. For instance, an NPC can provide critical information, offer quests, or simply populate the world to add a sense of realism to the game. Additionally, their role as quest-givers or merchants makes them integral to the gameplay loop, giving players access to resources, missions, or services that enable further progression.\n Additionally, NPCs can be designed to serve functional roles in games, such as a merchant or to provide a service to the player. These characters are central to facilitating game mechanics by acting as intermediaries between the player and in-game systems. Academics [who?] say the interactions between players and NPCs are often designed to be straightforward but contextually relevant, ensuring that the player receives necessary feedback or resources for gameplay continuity.\n Recent advancements[as of?] in artificial intelligence have significantly enhanced the complexity and realism of NPCs. Before these advancements, AI operated on pre-programmed behaviors, making them predictable and repeatable. With AI developing NPCs have become more adaptive and able to dynamically respond to players. Experts[who?] think the integration of deep learning and reinforcement learning techniques has enabled NPCs to adjust their behavior in response to player actions, creating a more interactive and personalized gameplay experience.\n One such development is the use of adaptive behavior models. These allow NPCs to analyze and learn from players decisions in real time. This behavior allows for a much more engaging experience. For example as said by experts in the field, [who?] NPCs in modern video games can now react to player actions with increased sophistication, such as adjusting their tactics in combat or changing their dialogue based on past interactions. By using deep learning algorithms these systems emulate human-like decisions-making, thus making NPCs feel more like real people rather than static game elements.\n Another advancements in NPC AI is the use of natural language processing, which allows NPCs to engage in more realistic conversations with players. Before this NPC dialogue was limited to a fixed set of responses. It is said [by whom?] that NLP has improved the fluidity of NPC conversations, allowing them to respond more contextually to player inputs. This development has increased the depth and immersion of player-NPC interactions, as players can now engage in more complex dialogues that affect the storyline and gameplay outcomes.\n Additionally, deep learning models have allowed NPCs to become more capable of predicting players behaviors. Deep learning allows NPCs to process large amounts of data and adapt to player strategies, making interactions with them less predictable and more varied. This creates a more immersive experience, as NPCs are now able to \"learn\" from player behavior, which provides a greater sense of realism within the game.\n Despite all of these advancements in NPC AI, there are still significant challenges that developers face in designing NPCs. They need to balance realism, functionally, and players expectations. The key challenge is to make sure that NPCs enhance the players experience, rather than disturb the gameplay. Overly realistic NPCs that behave unpredictably can frustrate players by hindering progression or breaking immersion. Conversely, NPCs that are too predictable or simplistic may fail to engage players, reducing the overall effectiveness of the game's narrative and mechanics.\n Another factor that needs to be accounted for is the computation cost of implementing advanced AI for NPCs. The use of these Advanced AI techniques requires large amount of processing power, which can limit its usage. Balancing the performance of AI-driven NPCs with the game's overall technical limitations is crucial for ensuring smooth gameplay. Experts [who?] mentioned how developers must allocate resources efficiently to avoid overburdening the game\u2019s systems, particularly in large, open-world games where numerous NPCs must interact with the player simultaneously. \n Finally, creating NPCs that can respond dynamically to a wide range of player behaviors remains a difficult task. NPCs must be able to handle both scripted interactions and unscripted scenarios where players may behave in unexpected ways. Designing NPCs capable of adapting to such variability requires complex AI models that can account for numerous possible interactions, which can be resource-intensive and time-consuming for developers.\n Gamers always ask if the AI cheats (presumably so they can complain if they lose) In the context of artificial intelligence in video games, cheating refers to the programmer giving agents actions and access to information that would be unavailable to the player in the same situation.[43] Believing that the Atari 8-bit could not compete against a human player, Chris Crawford did not fix a bug in Eastern Front (1941) that benefited the computer-controlled Russian side.[44] Computer Gaming World in 1994 reported that \"It is a well-known fact that many AIs 'cheat' (or, at least, 'fudge') in order to be able to keep up with human players\".[45]\n For example, if the agents want to know if the player is nearby they can either be given complex, human-like sensors (seeing, hearing, etc.), or they can cheat by simply asking the game engine for the player's position. Common variations include giving AIs higher speeds in racing games to catch up to the player or spawning them in advantageous positions in first-person shooters. The use of cheating in AI shows the limitations of the \"intelligence\" achievable artificially; generally speaking, in games where strategic creativity is important, humans could easily beat the AI after a minimum of trial and error if it were not for this advantage. Cheating is often implemented for performance reasons where in many cases it may be considered acceptable as long as the effect is not obvious to the player. While cheating refers only to privileges given specifically to the AI\u2014it does not include the inhuman swiftness and precision natural to a computer\u2014a player might call the computer's inherent advantages \"cheating\" if they result in the agent acting unlike a human player.[43] Sid Meier stated that he omitted multiplayer alliances in Civilization because he found that the computer was almost as good as humans in using them, which caused players to think that the computer was cheating.[46] Developers say that most game AIs are honest but they dislike players erroneously complaining about \"cheating\" AI. In addition, humans use tactics against computers that they would not against other people.[44]\n In the 1996 game Creatures, the user \"hatches\" small furry animals and teaches them how to behave. These \"Norns\" can talk, feed themselves, and protect themselves against vicious creatures. It was the first popular application of machine learning in an interactive simulation. Neural networks are used by the creatures to learn what to do. The game is regarded as a breakthrough in artificial life research, which aims to model the behavior of creatures interacting with their environment.[47]\n In the 2001 first-person shooter Halo: Combat Evolved the player assumes the role of the Master Chief, battling various aliens on foot or in vehicles. Enemies use cover very wisely, and employ suppressing fire and grenades. The squad situation affects the individuals, so certain enemies flee when their leader dies. Attention is paid to the little details, with enemies notably throwing back grenades or team-members responding to being bothered. The underlying \"behavior tree\" technology has become very popular in the games industry since Halo 2.[47]\n The 2005 psychological horror first-person shooter F.E.A.R. has player characters engage a battalion of cloned super-soldiers, robots and paranormal creatures. The AI uses a planner to generate context-sensitive behaviors, the first time in a mainstream game. This technology is still used as a reference for many studios. The Replicas are capable of utilizing the game environment to their advantage, such as overturning tables and shelves to create cover, opening doors, crashing through windows, or even noticing (and alerting the rest of their comrades to) the player's flashlight. In addition, the AI is also capable of performing flanking maneuvers, using suppressing fire, throwing grenades to flush the player out of cover, and even playing dead. Most of these actions, in particular the flanking, is the result of emergent behavior.[48][49]\n The survival horror series S.T.A.L.K.E.R. (2007\u2013) confronts the player with man-made experiments, military soldiers, and mercenaries known as Stalkers. The various encountered enemies (if the difficulty level is set to its highest) use combat tactics and behaviors such as healing wounded allies, giving orders, out-flanking the player and using weapons with pinpoint accuracy.[citation needed]\n The 2010 real-time strategy game StarCraft II: Wings of Liberty gives the player control of one of three factions in a 1v1, 2v2, or 3v3 battle arena. The player must defeat their opponents by destroying all their units and bases. This is accomplished by creating units that are effective at countering opponents' units. Players can play against multiple different levels of AI difficulty ranging from very easy to Cheater 3 (insane). The AI is able to cheat at the difficulty Cheater 1 (vision), where it can see units and bases when a player in the same situation could not. Cheater 2 gives the AI extra resources, while Cheater 3 gives an extensive advantage over its opponent.[50]\n Red Dead Redemption 2, released by Rockstar Games in 2018, exemplifies the advanced use of AI in modern video games. The game incorporates a highly detailed AI system that governs the behavior of NPCs and the dynamic game world. NPCs in the game display complex and varied behaviors based on a wide range of factors including their environment, player interactions, and time of day. This level of AI integration creates a rich, immersive experience where characters react to players in a realistic manner, contributing to the game's reputation as one of the most advanced open-world games ever created.[51]\n The 2024 browser-based sandbox game Infinite Craft uses generative AI software, including LLaMA. When two elements are being combined, a new element is generated by the AI.[52]\n The 2024 browser-based game Oasis uses generative AI to simulate the video game Minecraft. Oasis is trained on millions of hours of footage from Minecraft, and predicts how the next frame of gameplay looks using this dataset. Oasis does not have object permanence because it does not store any data.[53]\n Generative artificial intelligence, AI system that can response to prompts and produce text, images, and audio and video clips, arose in 2023 with systems like ChatGPT and Stable Diffusion. In video games, these systems could create the potential for game assets to be created indefinitely, bypassing typical limitations on human creations. However, there are similar concerns in other fields particularly the potential for loss of jobs normally dedicated to the creation of these assets.[54]\n In January 2024, SAG-AFTRA, a United States union representing actors, signed a contract with Replica Studios that would allow Replica to capture the voicework of union actors for creating AI voice systems based on their voices for use in video games, with the contract assuring pay and rights protections. While the contract was agreed upon by a SAG-AFTRA committee, many members expressed criticism of the move, having not been told of it until it was completed and that the deal did not do enough to protect the actors.[55]\n Recent advancements in AI for video games have led to more complex and adaptive behaviors in non-playable characters (NPCs). For instance, AI systems now utilize sophisticated techniques such as decision trees and state machines to enhance NPC interactions and realism, as discussed in \"Artificial Intelligence in Games\".[56] Recent advancements in AI for video games have also focused on improving dynamic and adaptive behaviors in NPCs. For example, recent research has explored the use of complex neural networks to enable NPCs to learn and adapt their behavior based on player actions, enhancing the overall gaming experience. This approach is detailed in the IEEE paper on \"AI Techniques for Interactive Game Systems\".[57]\n",
        "doc_number": 12
    },
    {
        "url": "https://en.wikipedia.org/wiki/Computer_vision",
        "content": "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions.[1][2][3][4] \"Understanding\" in this context signifies the transformation of visual images (the input to the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.\n The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. Image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, 3D point clouds from LiDaR sensors, or medical scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems.\n Subdisciplines of computer vision include scene reconstruction, object detection, event detection, activity recognition, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration.\n Computer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.[5][6][7] \"Computer vision is concerned with the automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\"[8] As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner.[9] As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems. Machine vision refers to a systems engineering discipline, especially in the context of factory automation. In more recent times, the terms computer vision and machine vision have converged to a greater degree.[10]:\u200a13\u200a\n In the late 1960s, computer vision began at universities that were pioneering artificial intelligence. It was meant to mimic the human visual system as a stepping stone to endowing robots with intelligent behavior.[11] In 1966, it was believed that this could be achieved through an undergraduate summer project,[12] by attaching a camera to a computer and having it \"describe what it saw\".[13][14]\n What distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.[11]\n The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields.[15]\nBy the 1990s, some of the previous research topics became more active than others. Research in projective 3-D reconstructions led to better understanding of camera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry. This led to methods for sparse 3-D reconstructions of scenes from multiple images. Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques. At the same time, variations of graph cut were used to solve image segmentation. This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (see Eigenface). Toward the end of the 1990s, a significant change came about with the increased interaction between the fields of computer graphics and computer vision. This included image-based rendering, image morphing, view interpolation, panoramic image stitching and early light-field rendering.[11]\n Recent work has seen the resurgence of feature-based methods used in conjunction with machine learning techniques and complex optimization frameworks.[16][17] \nThe advancement of Deep Learning techniques has brought further life to the field of computer vision. The accuracy of deep learning algorithms on several benchmark computer vision data sets for tasks ranging from classification,[18] segmentation and optical flow has surpassed prior methods. [citation needed][19]\n Solid-state physics is another field that is closely related to computer vision. Most computer vision systems rely on image sensors, which detect electromagnetic radiation, which is typically in the form of either visible, infrared or ultraviolet light. The sensors are designed using quantum physics. The process by which light interacts with surfaces is explained using physics. Physics explains the behavior of optics which are a core part of most imaging systems. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process.[11] Also, various measurement problems in physics can be addressed using computer vision, for example, motion in fluids.\n Neurobiology has greatly influenced the development of computer vision algorithms. Over the last century, there has been an extensive study of eyes, neurons, and brain structures devoted to the processing of visual stimuli in both humans and various animals. This has led to a coarse yet convoluted description of how natural vision systems operate in order to solve certain vision-related tasks. These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in neurobiology.  The Neocognitron, a neural network developed in the 1970s by Kunihiko Fukushima, is an early example of computer vision taking direct inspiration from neurobiology, specifically the primary visual cortex.\n Some strands of computer vision research are closely related to the study of biological vision\u2014indeed, just as many strands of AI research are closely tied with research into human intelligence and the use of stored knowledge to interpret, integrate, and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, develops and describes the algorithms implemented in software and hardware behind artificial vision systems. An interdisciplinary exchange between biological and computer vision has proven fruitful for both fields.[21]\n Yet another field related to computer vision is signal processing. Many methods for processing one-variable signals, typically temporal signals, can be extended in a natural way to the processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images, there are many methods developed within computer vision that have no counterpart in the processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision.\n Robot navigation sometimes deals with autonomous path planning or deliberation for robotic systems to navigate through an environment.[22] A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot\n Besides the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view. For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion eCommerce, inventory management, patent search, furniture, and the beauty industry.[23]\n The fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences, and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented. In image processing, the input is an image and the output is an image as well, whereas in computer vision, an image or a video is taken as an input and the output could be an enhanced image, an understanding of the content of an image or even behavior of a computer system based on such understanding.\n Computer graphics produces image data from 3D models, and computer vision often produces 3D models from image data.[24] There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality.\n The following characterizations appear relevant but should not be taken as universally accepted:\n Photogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision.\n Applications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. In many computer-vision applications, computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for:\n One of the most prominent application fields is medical computer vision, or medical image processing, characterized by the extraction of information from image data to diagnose a patient. An example of this is the detection of tumours, arteriosclerosis or other malign changes, and a variety of dental pathologies; measurements of organ dimensions, blood flow, etc. are another example. It also supports medical research by providing new information: e.g., about the structure of the brain or the quality of medical treatments. Applications of computer vision in the medical area also include enhancement of images interpreted by humans\u2014ultrasonic images or X-ray images, for example\u2014to reduce the influence of noise.\n A second application area in computer vision is in industry, sometimes called machine vision, where information is extracted for the purpose of supporting a production process. One example is quality control where details or final products are being automatically inspected in order to find defects. One of the most prevalent fields for such inspection is the Wafer industry in which every single Wafer is being measured and inspected for inaccuracies or defects to prevent a computer chip from coming to market in an unusable manner. Another example is a measurement of the position and orientation of details to be picked up by a robot arm. Machine vision is also heavily used in the agricultural processes to remove undesirable foodstuff from bulk material, a process called optical sorting.[32]\n Military applications are probably one of the largest areas of computer vision[citation needed]. The obvious examples are the detection of enemy soldiers or vehicles and missile guidance. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as \"battlefield awareness\", imply that various sensors, including image sensors, provide a rich set of information about a combat scene that can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability.\n One of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars, or trucks), aerial vehicles, and unmanned aerial vehicles (UAV). The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer-vision-based systems support a driver or a pilot in various situations. Fully autonomous vehicles typically use computer vision for navigation, e.g., for knowing where they are or mapping their environment (SLAM), for detecting obstacles. It can also be used for detecting certain task-specific events, e.g., a UAV looking for forest fires. Examples of supporting systems are obstacle warning systems in cars, cameras and LiDAR sensors in vehicles, and systems for autonomous landing of aircraft. Several car manufacturers have demonstrated systems for autonomous driving of cars. There are ample examples of military autonomous vehicles ranging from advanced missiles to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision, e.g., NASA's Curiosity and CNSA's Yutu-2 rover.\n Materials such as rubber and silicon are being used to create sensors that allow for applications such as detecting microundulations and calibrating robotic hands. Rubber can be used in order to create a mold that can be placed over a finger, inside of this mold would be multiple strain gauges. The finger mold and sensors could then be placed on top of a small sheet of rubber containing an array of rubber pins. A user can then wear the finger mold and trace a surface. A computer can then read the data from the strain gauges and measure if one or more of the pins are being pushed upward. If a pin is being pushed upward then the computer can recognize this as an imperfection in the surface. This sort of technology is useful in order to receive accurate data on imperfections on a very large surface.[33] Another variation of this finger mold sensor are sensors that contain a camera suspended in silicon. The silicon forms a dome around the outside of the camera and embedded in the silicon are point markers that are equally spaced. These cameras can then be placed on devices such as robotic hands in order to allow the computer to receive highly accurate tactile data.[34]\n Other application areas include:\n Each of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below.\n Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions.[1][2][3][4] Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.[39]\n The classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity. Different varieties of recognition problem are described in the literature.[40]\n Currently, the best algorithms for such tasks are based on convolutional neural networks. An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and 1000 object classes used in the competition.[41] Performance of convolutional neural networks on the ImageNet tests is now close to that of humans.[41] The best algorithms still struggle with objects that are small or thin, such as a small ant on the stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease.[citation needed]\n Several specialized tasks based on recognition exist, such as:\n Several tasks relate to motion estimation, where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene or even of the camera that produces the images. Examples of such tasks are:\n Given one or (typically) more images of a scene, or a video, scene reconstruction aims at computing a 3D model of the scene. In the simplest case, the model can be a set of 3D points. More sophisticated methods produce a complete 3D surface model. The advent of 3D imaging not requiring motion or scanning, and related processing algorithms is enabling rapid advances in this field. Grid-based 3D sensing can be used to acquire 3D images from multiple angles. Algorithms are now available to stitch multiple 3D images together into point clouds and 3D models.[24]\n Image restoration comes into the picture when the original image is degraded or damaged due to some external factors like lens wrong positioning, transmission interference, low lighting or motion blurs, etc., which is referred to as noise. When the images are degraded or damaged, the information to be extracted from them also gets damaged. Therefore, we need to recover or restore the image as it was intended to be. The aim of image restoration is the removal of noise (sensor noise, motion blur, etc.) from images. The simplest possible approach for noise removal is various types of filters, such as low-pass filters or median filters. More sophisticated methods assume a model of how the local image structures look to distinguish them from noise. By first analyzing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches.\n An example in this field is inpainting.\n The organization of a computer vision system is highly application-dependent. Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on whether its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions that are found in many computer vision systems.\n Image-understanding systems (IUS) include three levels of abstraction as follows: low level includes image primitives such as edges, texture elements, or regions; intermediate level includes boundaries, surfaces and volumes; and high level includes objects, scenes, or events. Many of these requirements are entirely topics for further research.\n The representational requirements in the designing of IUS for these levels are: representation of prototypical concepts, concept organization, spatial knowledge, temporal knowledge, scaling, and description by comparison and differentiation.\n While inference refers to the process of deriving new, not explicitly represented facts from currently known facts, control refers to the process that selects which of the many inference, search, and matching techniques should be applied at a particular stage of processing. Inference and control requirements for IUS are: search and hypothesis activation, matching and hypothesis testing, generation and use of expectations, change and focus of attention, certainty and strength of belief, inference and goal satisfaction.[48]\n There are many kinds of computer vision systems; however, all of them contain these basic elements: a power source, at least one image acquisition device (camera, ccd, etc.), a processor, and control and communication cables or some kind of wireless interconnection mechanism. In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories, such as camera supports, cables, and connectors.\n Most computer vision systems use visible-light cameras passively viewing a scene at frame rates of at most 60 frames per second (usually far slower).\n A few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such as structured-light 3D scanners, thermographic cameras, hyperspectral imagers, radar imaging, lidar scanners, magnetic resonance images, side-scan sonar, synthetic aperture sonar, etc. Such hardware captures \"images\" that are then processed often using the same computer vision algorithms used to process visible-light images.\n While traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realized.[49]\n Egocentric vision systems are composed of a wearable camera that automatically take pictures from a first-person perspective.\n As of 2016, vision processing units are emerging as a new class of processors to complement CPUs and graphics processing units (GPUs) in this role.[50]\n",
        "doc_number": 13
    },
    {
        "url": "https://en.wikipedia.org/wiki/Reinforcement_learning",
        "content": "Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent should take actions in a dynamic environment in order to maximize a reward signal. Reinforcement learning is one of the three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\n Q-learning at its simplest stores data in tables. This approach becomes infeasible as the number of states/actions increases (e.g., if the state space or action space were continuous), as the probability of the agent visiting a particular state and performing a particular action diminishes. \n Reinforcement learning differs from supervised learning in not needing labelled input-output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead, the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) with the goal of maximizing the cumulative reward (the feedback of which might be incomplete or delayed).[1] The search for this balance is known as the exploration-exploitation dilemma.\n \nThe environment is typically stated in the form of a Markov decision process (MDP), as many reinforcement learning algorithms use dynamic programming techniques.[2] The main difference between classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process, and they target large MDPs where exact methods become infeasible.[3]  Due to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics. In the operations research and control literature, RL is called approximate dynamic programming, or neuro-dynamic programming. The problems of interest in RL have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation (particularly in the absence of a mathematical model of the environment).\n Basic reinforcement learning is modeled as a Markov decision process:\n The purpose of reinforcement learning is for the agent to learn an optimal (or near-optimal) policy that maximizes the reward function or other user-provided reinforcement signal that accumulates from immediate rewards. This is similar to processes that appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals learn to adopt behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.[4][5]\n A basic reinforcement learning agent interacts with its environment in discrete time steps. At each time step t, the agent receives the current state \n\n\n\n\nS\n\nt\n\n\n\n\n{\\displaystyle S_{t}}\n\n and reward \n\n\n\n\nR\n\nt\n\n\n\n\n{\\displaystyle R_{t}}\n\n. It then chooses an action \n\n\n\n\nA\n\nt\n\n\n\n\n{\\displaystyle A_{t}}\n\n from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state \n\n\n\n\nS\n\nt\n+\n1\n\n\n\n\n{\\displaystyle S_{t+1}}\n\n and the reward \n\n\n\n\nR\n\nt\n+\n1\n\n\n\n\n{\\displaystyle R_{t+1}}\n\n associated with the transition \n\n\n\n(\n\nS\n\nt\n\n\n,\n\nA\n\nt\n\n\n,\n\nS\n\nt\n+\n1\n\n\n)\n\n\n{\\displaystyle (S_{t},A_{t},S_{t+1})}\n\n is determined. The goal of a reinforcement learning agent is to learn a policy: \n \n\n\n\n\u03c0\n:\n\n\nS\n\n\n\u00d7\n\n\nA\n\n\n\u2192\n[\n0\n,\n1\n]\n\n\n{\\displaystyle \\pi :{\\mathcal {S}}\\times {\\mathcal {A}}\\rightarrow [0,1]}\n\n, \n\n\n\n\u03c0\n(\ns\n,\na\n)\n=\nPr\n(\n\nA\n\nt\n\n\n=\na\n\u2223\n\nS\n\nt\n\n\n=\ns\n)\n\n\n{\\displaystyle \\pi (s,a)=\\Pr(A_{t}=a\\mid S_{t}=s)}\n\n\n that maximizes the expected cumulative reward.\n Formulating the problem as a Markov decision process assumes the agent directly observes the current environmental state; in this case, the problem is said to have full observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have partial observability, and formally the problem must be formulated as a partially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.\n When the agent's performance is compared to that of an agent that acts optimally, the difference in performance yields the notion of regret. In order to act near optimally, the agent must reason about long-term consequences of its actions (i.e., maximize future rewards), although the immediate reward associated with this might be negative.\n Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including energy storage,[6] robot control,[7] photovoltaic generators,[8] backgammon, checkers,[9] Go (AlphaGo), and autonomous driving systems.[10]\n Two elements make reinforcement learning powerful: the use of samples to optimize performance, and the use of function approximation to deal with large environments. Thanks to these two key components, RL can be used in large environments in the following situations:\n The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.\n The exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and for finite state space Markov decision processes in Burnetas and Katehakis (1997).[12]\n Reinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.\n One such method is \n\n\n\n\u03b5\n\n\n{\\displaystyle \\varepsilon }\n\n-greedy, where \n\n\n\n0\n<\n\u03b5\n<\n1\n\n\n{\\displaystyle 0<\\varepsilon <1}\n\n is a parameter controlling the amount of exploration vs. exploitation.  With probability \n\n\n\n1\n\u2212\n\u03b5\n\n\n{\\displaystyle 1-\\varepsilon }\n\n, exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability \n\n\n\n\u03b5\n\n\n{\\displaystyle \\varepsilon }\n\n, exploration is chosen, and the action is chosen uniformly at random. \n\n\n\n\u03b5\n\n\n{\\displaystyle \\varepsilon }\n\n is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.[13]\n Even if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.\n The agent's action selection is modeled as a map called policy:\n The policy map gives the probability of taking action \n\n\n\na\n\n\n{\\displaystyle a}\n\n when in state \n\n\n\ns\n\n\n{\\displaystyle s}\n\n.[14]:\u200a61\u200a There are also deterministic policies.\n The state-value function \n\n\n\n\nV\n\n\u03c0\n\n\n(\ns\n)\n\n\n{\\displaystyle V_{\\pi }(s)}\n\n is defined as, expected discounted return starting with state \n\n\n\ns\n\n\n{\\displaystyle s}\n\n, i.e. \n\n\n\n\nS\n\n0\n\n\n=\ns\n\n\n{\\displaystyle S_{0}=s}\n\n, and successively following policy \n\n\n\n\u03c0\n\n\n{\\displaystyle \\pi }\n\n. Hence, roughly speaking, the value function estimates \"how good\" it is to be in a given state.[14]:\u200a60\u200a\n where the random variable \n\n\n\nG\n\n\n{\\displaystyle G}\n\n denotes the discounted return, and is defined as the sum of future discounted rewards:\n where \n\n\n\n\nR\n\nt\n+\n1\n\n\n\n\n{\\displaystyle R_{t+1}}\n\n is the reward for transitioning from state \n\n\n\n\nS\n\nt\n\n\n\n\n{\\displaystyle S_{t}}\n\n to \n\n\n\n\nS\n\nt\n+\n1\n\n\n\n\n{\\displaystyle S_{t+1}}\n\n, \n\n\n\n0\n\u2264\n\u03b3\n<\n1\n\n\n{\\displaystyle 0\\leq \\gamma <1}\n\n is the discount rate. \n\n\n\n\u03b3\n\n\n{\\displaystyle \\gamma }\n\n is less than 1, so rewards in the distant future are weighted less than rewards in the immediate future.\n The algorithm must find a policy with maximum expected discounted return. From the theory of Markov decision processes it is known that, without loss of generality, the search can be restricted to the set of so-called stationary policies. A policy is stationary if the action-distribution returned by it depends only on the last state visited (from the observation agent's history). The search can be further restricted to deterministic stationary policies. A deterministic stationary policy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.\n The brute force approach entails two steps:\n One problem with this is that the number of policies can be large, or even infinite. Another is that the variance of the returns may be large, which requires many samples to accurately estimate the discounted return of each policy.\n These problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this are value function estimation and direct policy search.\n Value function approaches attempt to find a policy that maximizes the discounted return by maintaining a set of estimates of expected discounted returns \n\n\n\n\n\nE\n\n\n\u2061\n[\nG\n]\n\n\n{\\displaystyle \\operatorname {\\mathbb {E} } [G]}\n\n for some policy (usually either the \"current\" [on-policy] or the optimal [off-policy] one).\n These methods rely on the theory of Markov decision processes, where optimality is defined in a sense stronger than the one above: A policy is optimal if it achieves the best-expected discounted return from any initial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found among stationary policies.\n To define optimality in a formal manner, define the state-value of a policy \n\n\n\n\u03c0\n\n\n{\\displaystyle \\pi }\n\n by\n where \n\n\n\nG\n\n\n{\\displaystyle G}\n\n stands for the discounted return associated with following \n\n\n\n\u03c0\n\n\n{\\displaystyle \\pi }\n\n from the initial state \n\n\n\ns\n\n\n{\\displaystyle s}\n\n. Defining \n\n\n\n\nV\n\n\u2217\n\n\n(\ns\n)\n\n\n{\\displaystyle V^{*}(s)}\n\n as the maximum possible state-value of \n\n\n\n\nV\n\n\u03c0\n\n\n(\ns\n)\n\n\n{\\displaystyle V^{\\pi }(s)}\n\n, where \n\n\n\n\u03c0\n\n\n{\\displaystyle \\pi }\n\n is allowed to change,\n A policy that achieves these optimal state-values in each state is called optimal. Clearly, a policy that is optimal in this sense is also optimal in the sense that it maximizes the expected discounted return, since \n\n\n\n\nV\n\n\u2217\n\n\n(\ns\n)\n=\n\nmax\n\n\u03c0\n\n\n\nE\n\n[\nG\n\u2223\ns\n,\n\u03c0\n]\n\n\n{\\displaystyle V^{*}(s)=\\max _{\\pi }\\mathbb {E} [G\\mid s,\\pi ]}\n\n, where \n\n\n\ns\n\n\n{\\displaystyle s}\n\n is a state randomly sampled from the distribution \n\n\n\n\u03bc\n\n\n{\\displaystyle \\mu }\n\n of initial states (so \n\n\n\n\u03bc\n(\ns\n)\n=\nPr\n(\n\nS\n\n0\n\n\n=\ns\n)\n\n\n{\\displaystyle \\mu (s)=\\Pr(S_{0}=s)}\n\n).\n Although state-values suffice to define optimality, it is useful to define action-values. Given a state \n\n\n\ns\n\n\n{\\displaystyle s}\n\n, an action \n\n\n\na\n\n\n{\\displaystyle a}\n\n and a policy \n\n\n\n\u03c0\n\n\n{\\displaystyle \\pi }\n\n, the action-value of the pair \n\n\n\n(\ns\n,\na\n)\n\n\n{\\displaystyle (s,a)}\n\n under \n\n\n\n\u03c0\n\n\n{\\displaystyle \\pi }\n\n is defined by\n where \n\n\n\nG\n\n\n{\\displaystyle G}\n\n now stands for the random discounted return associated with first taking action \n\n\n\na\n\n\n{\\displaystyle a}\n\n in state \n\n\n\ns\n\n\n{\\displaystyle s}\n\n and following \n\n\n\n\u03c0\n\n\n{\\displaystyle \\pi }\n\n, thereafter.\n The theory of Markov decision processes states that if \n\n\n\n\n\u03c0\n\n\u2217\n\n\n\n\n{\\displaystyle \\pi ^{*}}\n\n is an optimal policy, we act optimally (take the optimal action) by choosing the action from \n\n\n\n\nQ\n\n\n\u03c0\n\n\u2217\n\n\n\n\n(\ns\n,\n\u22c5\n)\n\n\n{\\displaystyle Q^{\\pi ^{*}}(s,\\cdot )}\n\n with the highest action-value at each state, \n\n\n\ns\n\n\n{\\displaystyle s}\n\n. The action-value function of such an optimal policy (\n\n\n\n\nQ\n\n\n\u03c0\n\n\u2217\n\n\n\n\n\n\n{\\displaystyle Q^{\\pi ^{*}}}\n\n) is called the optimal action-value function and is commonly denoted by \n\n\n\n\nQ\n\n\u2217\n\n\n\n\n{\\displaystyle Q^{*}}\n\n. In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally.\n Assuming full knowledge of the Markov decision process, the two basic approaches to compute the optimal action-value function are value iteration and policy iteration. Both algorithms compute a sequence of functions \n\n\n\n\nQ\n\nk\n\n\n\n\n{\\displaystyle Q_{k}}\n\n (\n\n\n\nk\n=\n0\n,\n1\n,\n2\n,\n\u2026\n\n\n{\\displaystyle k=0,1,2,\\ldots }\n\n) that converge to \n\n\n\n\nQ\n\n\u2217\n\n\n\n\n{\\displaystyle Q^{*}}\n\n. Computing these functions involves computing expectations over the whole state-space, which is impractical for all but the smallest (finite) Markov decision processes. In reinforcement learning methods, expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state-action spaces.\n Monte Carlo methods[15] are used to solve reinforcement learning problems by averaging sample returns. Unlike methods that require full knowledge of the environment\u2019s dynamics, Monte Carlo methods rely solely on actual or simulated experience\u2014sequences of states, actions, and rewards obtained from interaction with an environment. This makes them applicable in situations where the complete dynamics are unknown. Learning from actual experience does not require prior knowledge of the environment and can still lead to optimal behavior. When using simulated experience, only a model capable of generating sample transitions is required, rather than a full specification of transition probabilities, which is necessary for dynamic programming methods.\n Monte Carlo methods apply to episodic tasks, where experience is divided into episodes that eventually terminate. Policy and value function updates occur only after the completion of an episode, making these methods incremental on an episode-by-episode basis, though not on a step-by-step (online) basis. The term \"Monte Carlo\" generally refers to any method involving random sampling; however, in this context, it specifically refers to methods that compute averages from complete returns, rather than partial returns.\n These methods function similarly to the bandit algorithms, in which returns are averaged for each state-action pair. The key difference is that actions taken in one state affect the returns of subsequent states within the same episode, making the problem non-stationary. To address this non-stationarity, Monte Carlo methods use the framework of general policy iteration (GPI). While dynamic programming computes value functions using full knowledge of the Markov decision process (MDP), Monte Carlo methods learn these functions through sample returns. The value functions and policies interact similarly to dynamic programming to achieve optimality, first addressing the prediction problem and then extending to policy improvement and control, all based on sampled experience.[14]\n The first problem is corrected by allowing the procedure to change the policy (at some or all states) before the values settle. This too may be problematic as it might prevent convergence. Most current algorithms do this, giving rise to the class of generalized policy iteration algorithms. Many actor-critic methods belong to this category.\n The second issue can be corrected by allowing trajectories to contribute to any state-action pair in them. This may also help to some extent with the third problem, although a better solution when returns have high variance is Sutton's temporal difference (TD) methods that are based on the recursive Bellman equation.[16][17] The computation in TD methods can be incremental (when after each transition the memory is changed and the transition is thrown away), or batch (when the transitions are batched and the estimates are computed once based on the batch). Batch methods, such as the least-squares temporal difference method,[18] may use the information in the samples better, while incremental methods are the only choice when batch methods are infeasible due to their high computational or memory complexity. Some methods try to combine the two approaches. Methods based on temporal differences also overcome the fourth issue.\n Another problem specific to TD comes from their reliance on the recursive Bellman equation. Most TD methods have a so-called \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n parameter \n\n\n\n(\n0\n\u2264\n\u03bb\n\u2264\n1\n)\n\n\n{\\displaystyle (0\\leq \\lambda \\leq 1)}\n\n that can continuously interpolate between Monte Carlo methods that do not rely on the Bellman equations and the basic TD methods that rely entirely on the Bellman equations. This can be effective in palliating this issue.\n In order to address the fifth issue, function approximation methods are used. Linear function approximation starts with a mapping \n\n\n\n\u03d5\n\n\n{\\displaystyle \\phi }\n\n that assigns a finite-dimensional vector to each state-action pair. Then, the action values of a state-action pair \n\n\n\n(\ns\n,\na\n)\n\n\n{\\displaystyle (s,a)}\n\n are obtained by linearly combining the components of \n\n\n\n\u03d5\n(\ns\n,\na\n)\n\n\n{\\displaystyle \\phi (s,a)}\n\n with some weights \n\n\n\n\u03b8\n\n\n{\\displaystyle \\theta }\n\n:\n The algorithms then adjust the weights, instead of adjusting the values associated with the individual state-action pairs. Methods based on ideas from nonparametric statistics (which can be seen to construct their own features) have been explored.\n Value iteration can also be used as a starting point, giving rise to the Q-learning algorithm and its many variants.[19] Including Deep Q-learning methods when a neural network is used to represent Q, with various applications in stochastic search problems.[20]\n The problem with using action-values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy, though this problem is mitigated to some extent by temporal difference methods. Using the so-called compatible function approximation method compromises generality and efficiency.\n An alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case of stochastic optimization. The two approaches available are gradient-based and gradient-free methods.\n Gradient-based methods (policy gradient methods) start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector \n\n\n\n\u03b8\n\n\n{\\displaystyle \\theta }\n\n, let \n\n\n\n\n\u03c0\n\n\u03b8\n\n\n\n\n{\\displaystyle \\pi _{\\theta }}\n\n denote the policy associated to \n\n\n\n\u03b8\n\n\n{\\displaystyle \\theta }\n\n. Defining the performance function by \n\n\n\n\u03c1\n(\n\u03b8\n)\n=\n\n\u03c1\n\n\n\u03c0\n\n\u03b8\n\n\n\n\n\n\n{\\displaystyle \\rho (\\theta )=\\rho ^{\\pi _{\\theta }}}\n\n under mild conditions this function will be differentiable as a function of the parameter vector \n\n\n\n\u03b8\n\n\n{\\displaystyle \\theta }\n\n. If the gradient of \n\n\n\n\u03c1\n\n\n{\\displaystyle \\rho }\n\n was known, one could use gradient ascent. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams' REINFORCE method[21] (which is known as the likelihood ratio method in the simulation-based optimization literature).[22]\n A large class of methods avoids relying on gradient information. These include simulated annealing, cross-entropy search or methods of evolutionary computation. Many gradient-free methods can achieve (in theory and in the limit) a global optimum.\n Policy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years, actor\u2013critic methods have been proposed and performed well on various problems.[23]\n Policy search methods have been used in the robotics context.[24] Many policy search methods may get stuck in local optima (as they are based on local search).\n Finally, all of the above methods can be combined with algorithms that first learn a model of the Markov Decision Process, the probability of each next state given an action taken from an existing state. For instance, the Dyna algorithm[25] learns a model from experience, and uses that to provide more modelled transitions for a value function, in addition to the real transitions.  Such methods can sometimes be extended to use of non-parametric models, such as when the transitions are simply stored and 'replayed'[26] to the learning algorithm.\n Model-based methods can be more computationally intensive than model-free approaches, and their utility can be limited by the extent to which the Markov Decision Process can be learnt.[27]\n There are other ways to use models than to update a value function.[28] For instance, in model predictive control the model is used to update the behavior directly.\n Both the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.\n Efficient exploration of Markov decision processes is given in  Burnetas and Katehakis (1997).[12] Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.\n For incremental algorithms, asymptotic convergence issues have been settled[clarification needed]. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).\n Research topics include:\n Associative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification tasks. In associative reinforcement learning tasks, the learning system interacts in a closed loop with its environment.[46]\n This approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space.[47] The work on learning ATARI games by Google DeepMind increased attention to deep reinforcement learning or end-to-end reinforcement learning.[48]\n Adversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies. In this research area some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations.[49][50][51] While some methods have been proposed to overcome these susceptibilities, in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies.[52]\n By introducing fuzzy inference in reinforcement learning,[53] approximating the state-action value function with fuzzy rules in continuous space becomes possible. The IF - THEN form of fuzzy rules make this approach suitable for expressing the results in a form close to natural language. Extending FRL with Fuzzy Rule Interpolation [54] allows the use of reduced size sparse fuzzy rule-bases to emphasize cardinal rules (most important state-action values).\n In inverse reinforcement learning (IRL), no reward function is given. Instead, the reward function is inferred given an observed behavior from an expert. The idea is to mimic observed behavior, which is often optimal or close to optimal.[55] One popular IRL paradigm is named maximum entropy inverse reinforcement learning (MaxEnt IRL). [56] MaxEnt IRL estimates the parameters of a linear model of the reward function by maximizing the entropy of the probability distribution of observed trajectories subject to constraints related to matching expected feature counts. Recently it has been shown that MaxEnt IRL is a particular case of a more general framework named random utility inverse reinforcement learning (RU-IRL). [57] RU-IRL is based on random utility theory and Markov decision processes. While prior IRL approaches assume that the apparent random behavior of an observed agent is due to it following a random policy, RU-IRL assumes that the observed agent follows a deterministic policy but randomness in observed behavior is due to the fact that an observer only has partial access to the features the observed agent uses in decision making. The utility function is modeled as a random variable to account for the ignorance of the observer regarding the features the observed agent actually considers in its utility function.\n Safe reinforcement learning (SRL) can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes.[58] An alternative approach is risk-averse reinforcement learning, where instead of the expected return, a risk-measure of the return is optimized, such as the Conditional Value at Risk (CVaR).[59] In addition to mitigating risk, the CVaR objective increases robustness to model uncertainties.[60][61] However, CVaR optimization in risk-averse RL requires special care, to prevent gradient bias[62] and blindness to success.[63]\n Self-reinforcement learning (or self learning), is a learning paradigm which does not use the concept of immediate reward Ra(s,s') after transition from s to s' with action a. It does not use an external reinforcement, it only uses the agent internal self-reinforcement. The internal self-reinforcement is provided by mechanism of feelings and emotions. In the learning process emotions are backpropagated by a mechanism of secondary reinforcement. The learning equation does not include the immediate reward, it only includes the state evaluation. \n The self-reinforcement algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning routine:\n1. in situation s perform action a\n2. receive a consequence situation s'\n3. compute state evaluation v(s') of how good is to be in the consequence situation s'\n4. update crossbar memory w'(a,s) = w(a,s) + v(s')\n Initial conditions of the memory are received as input from the genetic environment. It is a system with only one input (situation), and only one output (action, or behavior). \n Self reinforcement (self learning) was introduced in 1982 along with a neural network capable of self-reinforcement learning, named Crossbar Adaptive Array (CAA).[64][65] The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence states. The system is driven by the interaction between cognition and emotion. [66]\n Efficient comparison of RL algorithms is essential for research, deployment and monitoring of RL systems. To compare different algorithms on a given environment, an agent can be trained for each algorithm. Since the performance is sensitive to implementation details, all algorithms should be implemented as closely as possible to each other.[67] After the training is finished, the agents can be run on a sample of test episodes, and their scores (returns) can be compared. Since episodes are typically assumed to be i.i.d, standard statistical tools can be used for hypothesis testing, such as T-test and permutation test.[68] This requires to accumulate all the rewards within an episode into a single number - the episodic return. However, this causes a loss of information, as different time-steps are averaged together, possibly with different levels of noise. Whenever the noise level varies across the episode, the statistical power can be improved significantly, by weighting the rewards according to their estimated noise.[69]\n",
        "doc_number": 14
    },
    {
        "url": "https://en.wikipedia.org/wiki/Supervised_learning",
        "content": "In machine learning, supervised learning (SL) is a paradigm where a model is trained using input objects (e.g. a vector of predictor variables) and desired output values (also known as a supervisory signal), which are often human-made labels. The training process builds a function that maps new data to expected output values.[1] An optimal scenario will allow for the algorithm to accurately determine output values for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias). This statistical quality of an algorithm is measured via a generalization error.\n To solve a given problem of supervised learning, the following steps must be performed:\n A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem).\n There are four major issues to consider in supervised learning:\n A first issue is the tradeoff between bias and variance.[2] Imagine that we have available several different, but equally good, training data sets. A learning algorithm is biased for a particular input \n\n\n\nx\n\n\n{\\displaystyle x}\n\n if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for \n\n\n\nx\n\n\n{\\displaystyle x}\n\n. A learning algorithm has high variance for a particular input \n\n\n\nx\n\n\n{\\displaystyle x}\n\n if it predicts different output values when trained on different training sets. The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm.[3] Generally, there is a tradeoff between bias and variance. A learning algorithm with low bias must be \"flexible\" so that it can fit the data well. But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance. A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).\n The second issue is of the amount of training data available relative to the complexity of the \"true\" function (classifier or regression function). If the true function is simple, then an \"inflexible\" learning algorithm with high bias and low variance will be able to learn it from a small amount of data. But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn with a large amount of training data paired with a \"flexible\" learning algorithm with low bias and high variance.\n A third issue is the dimensionality of the input space. If the input feature vectors have large dimensions, learning the function can be difficult even if the true function only depends on a small number of those features. This is because the many \"extra\" dimensions can confuse the learning algorithm and cause it to have high variance. Hence, input data of large dimensions typically requires tuning the classifier to have low variance and high bias. In practice, if the engineer can manually remove irrelevant features from the input data, it will likely improve the accuracy of the learned function. In addition, there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones. This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.\n A fourth issue is the degree of noise in the desired output values (the supervisory target variables). If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples. Attempting to fit the data too carefully leads to overfitting. You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled \"corrupts\" your training data - this phenomenon has been called deterministic noise. When either type of noise is present, it is better to go with a higher bias, lower variance estimator.\n In practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm. There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance.[4][5]\n Other factors to consider when choosing and applying a learning algorithm include the following:\n When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see  cross-validation). Tuning the performance of a learning algorithm can be very time-consuming. Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.\n The most widely used learning algorithms are: \n Given a set of \n\n\n\nN\n\n\n{\\displaystyle N}\n\n training examples of the form \n\n\n\n{\n(\n\nx\n\n1\n\n\n,\n\ny\n\n1\n\n\n)\n,\n.\n.\n.\n,\n(\n\nx\n\nN\n\n\n,\n\n\ny\n\nN\n\n\n)\n}\n\n\n{\\displaystyle \\{(x_{1},y_{1}),...,(x_{N},\\;y_{N})\\}}\n\n such that \n\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystyle x_{i}}\n\n is the feature vector of the \n\n\n\ni\n\n\n{\\displaystyle i}\n\n-th example and \n\n\n\n\ny\n\ni\n\n\n\n\n{\\displaystyle y_{i}}\n\n is its label (i.e., class), a learning algorithm seeks a function \n\n\n\ng\n:\nX\n\u2192\nY\n\n\n{\\displaystyle g:X\\to Y}\n\n, where \n\n\n\nX\n\n\n{\\displaystyle X}\n\n is the input space and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n is the output space. The function \n\n\n\ng\n\n\n{\\displaystyle g}\n\n is an element of some space of possible functions \n\n\n\nG\n\n\n{\\displaystyle G}\n\n, usually called the hypothesis space. It is sometimes convenient to represent \n\n\n\ng\n\n\n{\\displaystyle g}\n\n using a scoring function \n\n\n\nf\n:\nX\n\u00d7\nY\n\u2192\n\nR\n\n\n\n{\\displaystyle f:X\\times Y\\to \\mathbb {R} }\n\n such that \n\n\n\ng\n\n\n{\\displaystyle g}\n\n is defined as returning the \n\n\n\ny\n\n\n{\\displaystyle y}\n\n value that gives the highest score: \n\n\n\ng\n(\nx\n)\n=\n\n\n\narg\n\u2061\nmax\n\ny\n\n\n\nf\n(\nx\n,\ny\n)\n\n\n{\\displaystyle g(x)={\\underset {y}{\\arg \\max }}\\;f(x,y)}\n\n. Let \n\n\n\nF\n\n\n{\\displaystyle F}\n\n denote the space of scoring functions.\n Although \n\n\n\nG\n\n\n{\\displaystyle G}\n\n and \n\n\n\nF\n\n\n{\\displaystyle F}\n\n can be any space of functions, many learning algorithms are probabilistic models where \n\n\n\ng\n\n\n{\\displaystyle g}\n\n takes the form of a conditional probability model \n\n\n\ng\n(\nx\n)\n=\n\n\n\narg\n\u2061\nmax\n\ny\n\n\n\nP\n(\ny\n\n|\n\nx\n)\n\n\n{\\displaystyle g(x)={\\underset {y}{\\arg \\max }}\\;P(y|x)}\n\n, or \n\n\n\nf\n\n\n{\\displaystyle f}\n\n takes the form of a joint probability model \n\n\n\nf\n(\nx\n,\ny\n)\n=\nP\n(\nx\n,\ny\n)\n\n\n{\\displaystyle f(x,y)=P(x,y)}\n\n. For example, naive Bayes and linear discriminant analysis are joint probability models, whereas logistic regression is a conditional probability model.\n There are two basic approaches to choosing \n\n\n\nf\n\n\n{\\displaystyle f}\n\n or \n\n\n\ng\n\n\n{\\displaystyle g}\n\n: empirical risk minimization and structural risk minimization.[6] Empirical risk minimization seeks the function that best fits the training data. Structural risk minimization includes a penalty function that controls the bias/variance tradeoff.\n In both cases, it is assumed that the training set consists of a sample of independent and identically distributed pairs, \n\n\n\n(\n\nx\n\ni\n\n\n,\n\n\ny\n\ni\n\n\n)\n\n\n{\\displaystyle (x_{i},\\;y_{i})}\n\n. In order to measure how well a function fits the training data, a loss function \n\n\n\nL\n:\nY\n\u00d7\nY\n\u2192\n\n\nR\n\n\n\u2265\n0\n\n\n\n\n{\\displaystyle L:Y\\times Y\\to \\mathbb {R} ^{\\geq 0}}\n\n is defined. For training example \n\n\n\n(\n\nx\n\ni\n\n\n,\n\n\ny\n\ni\n\n\n)\n\n\n{\\displaystyle (x_{i},\\;y_{i})}\n\n, the loss of predicting the value \n\n\n\n\n\n\ny\n^\n\n\n\n\n\n{\\displaystyle {\\hat {y}}}\n\n is \n\n\n\nL\n(\n\ny\n\ni\n\n\n,\n\n\n\ny\n^\n\n\n\n)\n\n\n{\\displaystyle L(y_{i},{\\hat {y}})}\n\n.\n The risk \n\n\n\nR\n(\ng\n)\n\n\n{\\displaystyle R(g)}\n\n of function \n\n\n\ng\n\n\n{\\displaystyle g}\n\n is defined as the expected loss of \n\n\n\ng\n\n\n{\\displaystyle g}\n\n. This can be estimated from the training data as\n In empirical risk minimization, the supervised learning algorithm seeks the function \n\n\n\ng\n\n\n{\\displaystyle g}\n\n that minimizes \n\n\n\nR\n(\ng\n)\n\n\n{\\displaystyle R(g)}\n\n. Hence, a supervised learning algorithm can be constructed by applying an optimization algorithm to find \n\n\n\ng\n\n\n{\\displaystyle g}\n\n.\n When \n\n\n\ng\n\n\n{\\displaystyle g}\n\n is a conditional probability distribution \n\n\n\nP\n(\ny\n\n|\n\nx\n)\n\n\n{\\displaystyle P(y|x)}\n\n and the loss function is the negative log likelihood: \n\n\n\nL\n(\ny\n,\n\n\n\ny\n^\n\n\n\n)\n=\n\u2212\nlog\n\u2061\nP\n(\ny\n\n|\n\nx\n)\n\n\n{\\displaystyle L(y,{\\hat {y}})=-\\log P(y|x)}\n\n, then empirical risk minimization is equivalent to maximum likelihood estimation.\n When \n\n\n\nG\n\n\n{\\displaystyle G}\n\n contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization. The learning algorithm is able to memorize the training examples without generalizing well (overfitting).\n Structural risk minimization seeks to prevent overfitting by incorporating a regularization penalty into the optimization. The regularization penalty can be viewed as implementing a form of Occam's razor that prefers simpler functions over more complex ones.\n A wide variety of penalties have been employed that correspond to different definitions of complexity. For example, consider the case where the function \n\n\n\ng\n\n\n{\\displaystyle g}\n\n is a linear function of the form\n A popular regularization penalty is \n\n\n\n\n\u2211\n\nj\n\n\n\n\u03b2\n\nj\n\n\n2\n\n\n\n\n{\\displaystyle \\sum _{j}\\beta _{j}^{2}}\n\n, which is the squared Euclidean norm of the weights, also known as the \n\n\n\n\nL\n\n2\n\n\n\n\n{\\displaystyle L_{2}}\n\n norm. Other norms include the \n\n\n\n\nL\n\n1\n\n\n\n\n{\\displaystyle L_{1}}\n\n norm, \n\n\n\n\n\u2211\n\nj\n\n\n\n|\n\n\n\u03b2\n\nj\n\n\n\n|\n\n\n\n{\\displaystyle \\sum _{j}|\\beta _{j}|}\n\n, and the \n\n\n\n\nL\n\n0\n\n\n\n\n{\\displaystyle L_{0}}\n\n \"norm\", which is the number of non-zero \n\n\n\n\n\u03b2\n\nj\n\n\n\n\n{\\displaystyle \\beta _{j}}\n\ns. The penalty will be denoted by \n\n\n\nC\n(\ng\n)\n\n\n{\\displaystyle C(g)}\n\n.\n The supervised learning optimization problem is to find the function \n\n\n\ng\n\n\n{\\displaystyle g}\n\n that minimizes\n The parameter \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n controls the bias-variance tradeoff. When \n\n\n\n\u03bb\n=\n0\n\n\n{\\displaystyle \\lambda =0}\n\n, this gives empirical risk minimization with low bias and high variance. When \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n is large, the learning algorithm will have high bias and low variance. The value of \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n can be chosen empirically via  cross-validation.\n The complexity penalty has a Bayesian interpretation as the negative log prior probability of \n\n\n\ng\n\n\n{\\displaystyle g}\n\n, \n\n\n\n\u2212\nlog\n\u2061\nP\n(\ng\n)\n\n\n{\\displaystyle -\\log P(g)}\n\n, in which case \n\n\n\nJ\n(\ng\n)\n\n\n{\\displaystyle J(g)}\n\n is the posterior probability of \n\n\n\ng\n\n\n{\\displaystyle g}\n\n.\n The training methods described above are discriminative training methods, because they seek to find a function \n\n\n\ng\n\n\n{\\displaystyle g}\n\n that discriminates well between the different output values (see discriminative model). For the special case where \n\n\n\nf\n(\nx\n,\ny\n)\n=\nP\n(\nx\n,\ny\n)\n\n\n{\\displaystyle f(x,y)=P(x,y)}\n\n is a joint probability distribution and the loss function is the negative log likelihood \n\n\n\n\u2212\n\n\u2211\n\ni\n\n\nlog\n\u2061\nP\n(\n\nx\n\ni\n\n\n,\n\ny\n\ni\n\n\n)\n,\n\n\n{\\displaystyle -\\sum _{i}\\log P(x_{i},y_{i}),}\n\n a risk minimization algorithm is said to perform generative training, because \n\n\n\nf\n\n\n{\\displaystyle f}\n\n can be regarded as a generative model that explains how the data were generated. Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms. In some cases, the solution can be computed in closed form as in naive Bayes and linear discriminant analysis.\n There are several ways in which the standard supervised learning problem can be generalized:\n",
        "doc_number": 15
    },
    {
        "url": "https://en.wikipedia.org/wiki/Unsupervised_learning",
        "content": "Unsupervised learning is a framework in machine learning where, in contrast to supervised learning, algorithms learn patterns exclusively from unlabeled data.[1] Other frameworks in the spectrum of supervisions include weak- or semi-supervision, where a small portion of the data is tagged, and self-supervision. Some researchers consider self-supervised learning a form of unsupervised learning.[2]\n Conceptually, unsupervised learning divides into the aspects of data, training, algorithm, and downstream applications. Typically, the dataset is harvested cheaply \"in the wild\", such as massive text corpus obtained by web crawling, with only minor filtering (such as Common Crawl). This compares favorably to supervised learning, where the dataset (such as the ImageNet1000) is typically constructed manually, which is much more expensive.\n There were algorithms designed specifically for unsupervised learning, such as clustering algorithms like k-means, dimensionality reduction techniques like principal component analysis (PCA), Boltzmann machine learning, and autoencoders. After the rise of deep learning, most large-scale unsupervised learning have been done by training general-purpose neural network architectures by gradient descent, adapted to performing unsupervised learning by designing an appropriate training procedure.\n Sometimes a trained model can be used as-is, but more often they are modified for downstream applications. For example, the generative pretraining method trains a model to generate a textual dataset, before finetuning it for other applications, such as text classification.[3][4] As another example, autoencoders are trained to good features, which can then be used as a module for other models, such as in a latent diffusion model.\n Tasks are often categorized as discriminative (recognition) or generative (imagination).  Often but not always, discriminative tasks use supervised methods and generative tasks use unsupervised (see Venn diagram); however, the separation is very hazy.  For example, object recognition favors supervised learning but unsupervised learning can also cluster objects into groups.  Furthermore, as progress marches onward some tasks employ both methods, and some tasks swing from one to another.  For example, image recognition started off as heavily supervised, but became hybrid by employing unsupervised pre-training, and then moved towards supervision again with the advent of dropout, ReLU, and adaptive learning rates.\n A typical generative task is as follows. At each step, a datapoint is sampled from the dataset, and part of the data is removed, and the model must infer the removed part. This is particularly clear for the denoising autoencoders and BERT.\n During the learning phase, an unsupervised network tries to mimic the data it's given and uses the error in its mimicked output to correct itself (i.e. correct its weights and biases). Sometimes the error is expressed as a low probability that the erroneous output occurs, or it might be expressed as an unstable high energy state in the network.\n In contrast to supervised methods' dominant use of backpropagation, unsupervised learning also employs other methods  including: Hopfield learning rule, Boltzmann learning rule, Contrastive Divergence, Wake Sleep, Variational Inference, Maximum Likelihood, Maximum A Posteriori, Gibbs Sampling, and backpropagating reconstruction errors or hidden state reparameterizations. See the table below for more details.\n An energy function is a macroscopic measure of a network's activation state.  In Boltzmann machines, it plays the role of the Cost function.  This analogy with physics is inspired by Ludwig Boltzmann's analysis of a gas' macroscopic energy from the microscopic probabilities of particle motion \n\n\n\np\n\u221d\n\ne\n\n\u2212\nE\n\n/\n\nk\nT\n\n\n\n\n{\\displaystyle p\\propto e^{-E/kT}}\n\n, where k is the Boltzmann constant and T is temperature. In the RBM network the relation is \n\n\n\np\n=\n\ne\n\n\u2212\nE\n\n\n\n/\n\nZ\n\n\n{\\displaystyle p=e^{-E}/Z}\n\n,[5] where \n\n\n\np\n\n\n{\\displaystyle p}\n\n and \n\n\n\nE\n\n\n{\\displaystyle E}\n\n vary over every possible activation pattern and \n\n\n\n\n\nZ\n=\n\n\u2211\n\n\n\nAll Patterns\n\n\n\n\n\ne\n\n\u2212\nE\n(\n\npattern\n\n)\n\n\n\n\n\n\n{\\displaystyle \\textstyle {Z=\\sum _{\\scriptscriptstyle {\\text{All Patterns}}}e^{-E({\\text{pattern}})}}}\n\n. To be more precise, \n\n\n\np\n(\na\n)\n=\n\ne\n\n\u2212\nE\n(\na\n)\n\n\n\n/\n\nZ\n\n\n{\\displaystyle p(a)=e^{-E(a)}/Z}\n\n, where \n\n\n\na\n\n\n{\\displaystyle a}\n\n is an activation pattern of all neurons (visible and hidden). Hence, some early neural networks bear the name Boltzmann Machine.  Paul Smolensky calls \n\n\n\n\u2212\nE\n\n\n\n{\\displaystyle -E\\,}\n\n the Harmony. A network seeks low energy which is high Harmony.\n This table shows connection diagrams of various unsupervised networks, the details of which will be given in the section Comparison of Networks.  Circles are neurons and edges between them are connection weights.  As network design changes, features are added on to enable new capabilities or removed to make learning faster.  For instance, neurons change between deterministic (Hopfield) and stochastic (Boltzmann) to allow robust output, weights are removed within a layer (RBM) to hasten learning, or connections are allowed to become asymmetric (Helmholtz).\n Of the networks bearing people's names, only Hopfield worked directly with neural networks.  Boltzmann and Helmholtz came before artificial neural networks, but their work in physics and physiology inspired the analytical methods that were used.\n Here, we highlight some characteristics of select networks.  The details of each are given in the comparison table below. \n The classical example of unsupervised learning in the study of neural networks is Donald Hebb's principle, that is, neurons that fire together wire together.[8] In Hebbian learning, the connection is reinforced irrespective of an error, but is exclusively a function of the coincidence between action potentials between the two neurons.[9] A similar version that modifies synaptic weights takes into account the time between the action potentials (spike-timing-dependent plasticity or STDP). Hebbian Learning has been hypothesized to underlie a range of cognitive functions, such as pattern recognition and experiential learning.\n Among neural network models, the self-organizing map (SOM) and adaptive resonance theory (ART) are commonly used in unsupervised learning algorithms. The SOM is a topographic organization in which nearby locations in the map represent inputs with similar properties. The ART model allows the number of clusters to vary with problem size and lets the user control the degree of similarity between members of the same clusters by means of a user-defined constant called the vigilance parameter. ART networks are used for many pattern recognition tasks, such as automatic target recognition and seismic signal processing.[10]\n Two of the main methods used in unsupervised learning are principal component and cluster analysis. Cluster analysis is used in unsupervised learning to group, or segment, datasets with shared attributes in order to extrapolate algorithmic relationships.[11] Cluster analysis is a branch of machine learning that groups the data that has not been labelled, classified or categorized. Instead of responding to feedback, cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. This approach helps detect anomalous data points that do not fit into either group.\n A central application of unsupervised learning is in the field of density estimation in statistics,[12] though unsupervised learning encompasses many other domains involving summarizing and explaining data features. It can be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution  conditioned on the label  of input data; unsupervised learning intends to infer an a priori probability distribution .\n Some of the most common algorithms used in unsupervised learning include: (1) Clustering, (2) Anomaly detection, (3) Approaches for learning latent variable models. Each approach uses several methods as follows:\n One of the statistical approaches for unsupervised learning is the method of moments. In the method of moments, the unknown parameters (of interest) in the model are related to the moments of one or more random variables, and thus, these unknown parameters can be estimated given the moments. The moments are usually estimated from samples empirically. The basic moments are first and second order moments. For a random vector, the first order moment is the mean vector, and the second order moment is the covariance matrix (when the mean is zero). Higher order moments are usually represented using tensors which are the generalization of matrices to higher orders as multi-dimensional arrays.\n In particular, the method of moments is shown to be effective in learning the parameters of latent variable models. Latent variable models are statistical models where in addition to the observed variables, a set of latent variables also exists which is not observed. A highly practical example of latent variable models in machine learning is the topic modeling which is a statistical model for generating the words (observed variables) in the document based on the topic (latent variable) of the document. In the topic modeling, the words in the document are generated according to different statistical parameters when the topic of the document is changed. It is shown that method of moments (tensor decomposition techniques) consistently recover the parameters of a large class of latent variable models under some assumptions.[15]\n The Expectation\u2013maximization algorithm (EM) is also one of the most practical methods for learning latent variable models. However, it can get stuck in local optima, and it is not guaranteed that the algorithm will converge to the true unknown parameters of the model. In contrast, for the method of moments, the global convergence is guaranteed under some conditions.\n",
        "doc_number": 16
    },
    {
        "url": "https://en.wikipedia.org/wiki/Convolutional_neural_network",
        "content": "A convolutional neural network (CNN) is a regularized type of feed-forward neural network that learns features by itself via filter (or kernel) optimization. This type of deep learning network has been applied to process and make predictions from many different types of data including text, images and audio.[1] Convolution-based networks are the de-facto standard in deep learning-based approaches to computer vision and image processing, and have only recently been replaced -- in some cases -- by newer deep learning architectures such as the transformer. Vanishing gradients and exploding gradients, seen during backpropagation in earlier neural networks, are prevented by using regularized weights over fewer connections.[2][3] For example, for each neuron in the fully-connected layer, 10,000 weights would be required for processing an image sized 100 \u00d7 100 pixels. However, applying cascaded convolution (or cross-correlation) kernels,[4][5] only 25 neurons are required to process 5x5-sized tiles.[6][7] Higher-layer features are extracted from wider context windows, compared to lower-layer features.\n Some applications of CNNs include: \n CNNs are also known as shift invariant or space invariant artificial neural networks, based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps.[13][14] Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input.[15]\n Feed-forward neural networks are usually fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The \"full connectivity\" of these networks makes them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) Robust datasets also increase the probability that CNNs will learn the generalized principles that characterize a given dataset rather than the biases of a poorly-populated set.[16]\n Convolutional networks were inspired by biological processes[17][18][19][20] in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\n CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage.[to whom?]\n A convolutional neural network consists of an input layer, hidden layers and an output layer. In a convolutional neural network, the hidden layers include one or more layers that perform convolutions. Typically this includes a layer that performs a dot product of the convolution kernel with the layer's input matrix. This product is usually the Frobenius inner product, and its activation function is commonly ReLU. As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers, fully connected layers, and normalization layers.\nHere it should be noted how close a convolutional neural network is to a matched filter.[21]\n In a CNN, the input is a tensor with shape:\n (number of inputs) \u00d7 (input height) \u00d7 (input width) \u00d7 (input channels)\n After passing through a convolutional layer, the image becomes abstracted to a feature map, also called an activation map, with shape:\n (number of inputs) \u00d7 (feature map height) \u00d7 (feature map width) \u00d7 (feature map channels).\n Convolutional layers convolve the input and pass its result to the next layer. This is similar to the response of a neuron in the visual cortex to a specific stimulus.[22] Each convolutional neuron processes data only for its receptive field. \n Although fully connected feedforward neural networks can be used to learn features and classify data, this architecture is generally impractical for larger inputs (e.g., high-resolution images), which would require massive numbers of neurons because each pixel is a relevant input feature. A fully connected layer for an image of size 100 \u00d7 100 has 10,000 weights for each neuron in the second layer. Convolution reduces the number of free parameters, allowing the network to be deeper.[6] For example, using a 5 \u00d7 5 tiling region, each with the same shared weights, requires only 25 neurons. Using regularized weights over fewer parameters avoids the vanishing gradients and exploding gradients problems seen during backpropagation in earlier neural networks.[2][3]\n To speed processing, standard convolutional layers can be replaced by depthwise separable convolutional layers,[23] which are based on a depthwise convolution followed by a pointwise convolution. The depthwise convolution is a spatial convolution applied independently over each channel of the input tensor, while the pointwise convolution is a standard convolution restricted to the use of \n\n\n\n1\n\u00d7\n1\n\n\n{\\displaystyle 1\\times 1}\n\n kernels.\n Convolutional networks may include local and/or global pooling layers along with traditional convolutional layers. Pooling layers reduce the dimensions of data by combining the outputs of neuron clusters at one layer into a single neuron in the next layer. Local pooling combines small clusters, tiling sizes such as 2 \u00d7 2 are commonly used. Global pooling acts on all the neurons of the feature map.[24][25] There are two common types of pooling in popular use: max and average. Max pooling uses the maximum value of each local cluster of neurons in the feature map,[26][27] while average pooling takes the average value.\n Fully connected layers connect every neuron in one layer to every neuron in another layer. It is the same as a traditional multilayer perceptron neural network (MLP). The flattened matrix goes through a fully connected layer to classify the images.\n In neural networks, each neuron receives input from some number of locations in the previous layer. In a convolutional layer, each neuron receives input from only a restricted area of the previous layer called the neuron's receptive field. Typically the area is a square (e.g. 5 by 5 neurons). Whereas, in a fully connected layer, the receptive field is the entire previous layer. Thus, in each convolutional layer, each neuron takes input from a larger area in the input than previous layers. This is due to applying the convolution over and over, which takes the value of a pixel into account, as well as its surrounding pixels. When using dilated layers, the number of pixels in the receptive field remains constant, but the field is more sparsely populated as its dimensions grow when combining the effect of several layers.\n To manipulate the receptive field size as desired, there are some alternatives to the standard convolutional layer. For example, atrous or dilated convolution[28][29] expands the receptive field size without increasing the number of parameters by interleaving visible and blind regions. Moreover, a single dilated convolutional layer can comprise filters with multiple dilation ratios,[30] thus having a variable receptive field size.\n Each neuron in a neural network computes an output value by applying a specific function to the input values received from the receptive field in the previous layer. The function that is applied to the input values is determined by a vector of weights and a bias (typically real numbers). Learning consists of iteratively adjusting these biases and weights.\n The vectors of weights and biases are called filters and represent particular features of the input (e.g., a particular shape). A distinguishing feature of CNNs is that many neurons can share the same filter. This reduces the memory footprint because a single bias and a single vector of weights are used across all receptive fields that share that filter, as opposed to each receptive field having its own bias and vector weighting.[31]\n \n A deconvolutional neural network is essentially the reverse of a CNN. It consists of deconvolutional layers and unpooling layers.[32]\n A deconvolutional layer is the transpose of a convolutional layer. Specifically, a convolutional layer can be written as a multiplication with a matrix, and a deconvolutional layer is multiplication with the transpose of that matrix.[33]\n An unpooling layer expands the layer. The max-unpooling layer is the simplest, as it simply copies each entry multiple times. For example, a 2-by-2 max-unpooling layer is \n\n\n\n[\nx\n]\n\u21a6\n\n\n[\n\n\n\nx\n\n\nx\n\n\n\n\nx\n\n\nx\n\n\n\n]\n\n\n\n\n{\\displaystyle [x]\\mapsto {\\begin{bmatrix}x&x\\\\x&x\\end{bmatrix}}}\n\n.\n Deconvolution layers are used in image generators. By default, it creates periodic checkerboard artifact, which can be fixed by upscale-then-convolve.[34]\n CNN are often compared to the way the brain achieves vision processing in living organisms.[35]\n Work by Hubel and Wiesel in the 1950s and 1960s showed that cat visual cortices contain neurons that individually respond to small regions of the visual field. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its receptive field.[36] Neighboring cells have similar and overlapping receptive fields. Receptive field size and location varies systematically across the cortex to form a complete map of visual space.[citation needed] The cortex in each hemisphere represents the contralateral visual field.[citation needed]\n Their 1968 paper identified two basic visual cell types in the brain:[18]\n Hubel and Wiesel also proposed a cascading model of these two types of cells for use in pattern recognition tasks.[37][36]\n Inspired by Hubel and Wiesel's work, in 1969, Kunihiko Fukushima published a deep CNN that uses ReLU activation function.[38] Unlike most modern networks, this network used hand-designed kernels. It was not used in his neocognitron, since all the weights were nonnegative; lateral inhibition was used instead. The rectifier has become the most popular activation function for CNNs and deep neural networks in general.[39]\n The \"neocognitron\" was introduced by Kunihiko Fukushima in 1979.[40][19][17] The kernels were trained by unsupervised learning. It was inspired by the above-mentioned work of Hubel and Wiesel. The neocognitron introduced the two basic types of layers:\n In a variant of the neocognitron called the cresceptron, instead of using Fukushima's spatial averaging with inhibition and saturation, J. Weng et al. in 1993 introduced a method called max-pooling where a downsampling unit computes the maximum of the activations of the units in its patch.[41] Max-pooling is often used in modern CNNs.[42]\n Several supervised and unsupervised learning algorithms have been proposed over the decades to train the weights of a neocognitron.[17] Today, however, the CNN architecture is usually trained through backpropagation.\n The term \"convolution\" first appears in neural networks in a paper by Toshiteru Homma, Les Atlas, and Robert Marks II at the first Conference on Neural Information Processing Systems in 1987. Their paper replaced multiplication with convolution in time, inherently providing shift invariance, motivated by and connecting more directly to the signal-processing concept of a filter, and demonstrated it on a speech recognition task.[7] They also pointed out that as a data-trainable system, convolution is essentially equivalent to correlation since reversal of the weights does not affect the final learned function (\"For convenience, we denote * as correlation instead of convolution. Note that convolving a(t) with b(t) is equivalent to correlating a(-t) with b(t).\").[7] Modern CNN implementations typically do correlation and call it convolution, for convenience, as they did here.\n The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel et al. for phoneme recognition and was one of the first convolutional networks, as it achieved shift-invariance.[43] A TDNN is a 1-D convolutional neural net where the convolution is performed along the time axis of the data. It is the first CNN utilizing weight sharing in combination with a training by gradient descent, using backpropagation.[44] Thus, while also using a pyramidal structure as in the neocognitron, it performed a global optimization of the weights instead of a local one.[43]\n TDNNs are convolutional networks that share weights along the temporal dimension.[45] They allow speech signals to be processed time-invariantly. In 1990 Hampshire and Waibel introduced a variant that performs a two-dimensional convolution.[46] Since these TDNNs operated on spectrograms, the resulting phoneme recognition system was invariant to both time and frequency shifts, as with images processed by a neocognitron.\n TDNNs improved the performance of far-distance speech recognition.[47]\n Denker et al. (1989) designed a 2-D CNN system to recognize hand-written ZIP Code numbers.[48] However, the lack of an efficient training method to determine the kernel coefficients of the involved convolutions meant that all the coefficients had to be laboriously hand-designed.[49]\n Following the advances in the training of 1-D CNNs by Waibel et al. (1987), Yann LeCun et al. (1989)[49] used back-propagation to learn the convolution kernel coefficients directly from images of hand-written numbers. Learning was thus fully automatic, performed better than manual coefficient design, and was suited to a broader range of image recognition problems and image types. \nWei Zhang et al. (1988)[13][14] used back-propagation to train the convolution kernels of a CNN for alphabets recognition. The model was called shift-invariant pattern recognition neural network before the name CNN was coined later in the early 1990s. Wei Zhang et al. also applied the same CNN without the last fully connected layer for medical image object segmentation (1991)[50] and breast cancer detection in mammograms (1994).[51]\n This approach became a foundation of modern computer vision.\n In 1990 Yamaguchi et al. introduced the concept of max pooling, a fixed filtering operation that calculates and propagates the maximum value of a given region. They did so by combining TDNNs with max pooling to realize a speaker-independent isolated word recognition system.[26] In their system they used several TDNNs per word, one for each syllable. The results of each TDNN over the input signal were combined using max pooling and the outputs of the pooling layers were then passed on to networks performing the actual word classification.\n LeNet-5, a pioneering 7-level convolutional network by LeCun et al. in 1995,[52] classifies hand-written numbers on checks (British English: cheques) digitized in 32x32 pixel images. The ability to process higher-resolution images requires larger and more layers of convolutional neural networks, so this technique is constrained by the availability of computing resources.\n It was superior than other commercial courtesy amount reading systems (as of 1995). The system was integrated in NCR's check reading systems, and fielded in several American banks since June 1996, reading millions of checks per day.[53]\n A shift-invariant neural network was proposed by Wei Zhang et al. for image character recognition in 1988.[13][14] It is a modified Neocognitron by keeping only the convolutional interconnections between the image feature layers and the last fully connected layer. The model was trained with back-propagation. The training algorithm was further improved in 1991[54] to improve its generalization ability. The model architecture was modified by removing the last fully connected layer and applied for medical image segmentation (1991)[50] and automatic detection of breast cancer in mammograms (1994).[51]\n A different convolution-based design was proposed in 1988[55] for application to decomposition of one-dimensional electromyography convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.[56][57]\n Topological deep learning was first introduced in 2017.\n[58]\nIt integrates topological data analysis and convolutional neural networks for intricately complex data. \nTopological deep learning has become a new frontier in deep learning.\n[59]\n Although CNNs were invented in the 1980s, their breakthrough in the 2000s required fast implementations on graphics processing units (GPUs).\n In 2004, it was shown by K. S. Oh and K. Jung that standard neural networks can be greatly accelerated on GPUs. Their implementation was 20 times faster than an equivalent implementation on CPU.[60] In 2005, another paper also emphasised the value of GPGPU for machine learning.[61]\n The first GPU-implementation of a CNN was described in 2006 by K. Chellapilla et al. Their implementation was 4 times faster than an equivalent implementation on CPU.[62] In the same period, GPUs were also used for unsupervised training of deep belief networks.[63][64][65][66]\n In 2010, Dan Ciresan et al. at IDSIA trained deep feedforward networks on GPUs.[67] In 2011, they extended this to CNNs, accelerating by 60 compared to training CPU.[24] In 2011, the network win an image recognition contest where they achieved superhuman performance for the first time.[68] Then they won more competitions and achieved state of the art on several benchmarks.[69][42][27]\n Subsequently, AlexNet, a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012.[70] It was an early catalytic event for the AI boom.\n Compared to the training of CNNs using GPUs, not much attention was given to CPU. (Viebke et al 2019) parallelizes CNN by thread- and SIMD-level parallelism that is available on the Intel Xeon Phi.[71][72]\n In the past, traditional multilayer perceptron (MLP) models were used for image recognition.[example needed] However, the full connectivity between nodes caused the curse of dimensionality, and was computationally intractable with higher-resolution images. A 1000\u00d71000-pixel image with RGB color channels has 3 million weights per fully-connected neuron, which is too high to feasibly process efficiently at scale.\n For example, in CIFAR-10, images are only of size 32\u00d732\u00d73 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in the first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200\u00d7200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.\n Also, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in data with a grid-topology (such as images), both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns.\n Convolutional neural networks are variants of multilayer perceptrons, designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:\n Together, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.\n \nA CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below. The convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or kernels), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the dot product between the filter entries and the input, producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.[75][nb 1]\n Stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input. Each entry in an activation map use the same set of parameters that define the filter.\n Self-supervised learning has been adapted for use in convolutional layers by using sparse patches with a high-mask ratio and a global response normalization layer.[citation needed]\n When dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a sparse local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume.\n The extent of this connectivity is a hyperparameter called the receptive field of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learned (British English: learnt) filters produce the strongest response to a spatially local input pattern.\n Three hyperparameters control the size of the output volume of the convolutional layer: the depth, stride, and padding size:\n The spatial size of the output volume is a function of the input volume size \n\n\n\nW\n\n\n{\\displaystyle W}\n\n, the kernel field size \n\n\n\nK\n\n\n{\\displaystyle K}\n\n of the convolutional layer neurons, the stride \n\n\n\nS\n\n\n{\\displaystyle S}\n\n, and the amount of zero padding \n\n\n\nP\n\n\n{\\displaystyle P}\n\n on the border. The number of neurons that \"fit\" in a given volume is then:\n If this number is not an integer, then the strides are incorrect and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general, setting zero padding to be \n\n\n\nP\n=\n(\nK\n\u2212\n1\n)\n\n/\n\n2\n\n\n{\\textstyle P=(K-1)/2}\n\n when the stride is \n\n\n\nS\n=\n1\n\n\n{\\displaystyle S=1}\n\n ensures that the input volume and output volume will have the same size spatially. However, it is not always completely necessary to use all of the neurons of the previous layer. For example, a neural network designer may decide to use just a portion of padding.\n A parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on the assumption that if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. Denoting a single 2-dimensional slice of depth as a depth slice, the neurons in each depth slice are constrained to use the same weights and bias.\n Since all neurons in a single depth slice share the same parameters, the forward pass in each depth slice of the convolutional layer can be computed as a convolution of the neuron's weights with the input volume.[nb 2] Therefore, it is common to refer to the sets of weights as a filter (or a kernel), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.[15]\n Sometimes, the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure; for which we expect completely different features to be learned on different spatial locations. One practical example is when the inputs are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a \"locally connected layer\".\n Another important concept of CNNs is pooling, which is used as a form of non-linear down-sampling. Pooling provides downsampling because it reduces the spatial dimensions (height and width) of the input feature maps while retaining the most important information. There are several non-linear functions to implement pooling, where max pooling and average pooling are the most common. Pooling aggregates information from small regions of the input creating partitions of the input feature map, typically using a fixed-size window (like 2x2) and applying a stride (often 2) to move the window across the input.[77] Note that without using a stride greater than 1, pooling would not perform downsampling, as it would simply move the pooling window across the input one step at a time, without reducing the size of the feature map. In other words, the stride is what actually causes the downsampling by determining how much the pooling window moves over the input.\n Intuitively, the exact location of a feature is less important than its rough location relative to other features. This is the idea behind the use of pooling in convolutional neural networks. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters, memory footprint and amount of computation in the network, and hence to also control overfitting. This is known as down-sampling. It is common to periodically insert a pooling layer between successive convolutional layers (each one typically followed by an activation function, such as a ReLU layer) in a CNN architecture.[75]:\u200a460\u2013461\u200a While pooling layers contribute to local translation invariance, they do not provide global translation invariance in a CNN, unless a form of global pooling is used.[15][74] The pooling layer commonly operates independently on every depth, or slice, of the input and resizes it spatially. A very common form of max pooling is a layer with filters of size 2\u00d72, applied with a stride of 2, which subsamples every depth slice in the input by 2 along both width and height, discarding 75% of the activations:\n\n\n\n\nf\n\nX\n,\nY\n\n\n(\nS\n)\n=\n\nmax\n\na\n,\nb\n=\n0\n\n\n1\n\n\n\nS\n\n2\nX\n+\na\n,\n2\nY\n+\nb\n\n\n.\n\n\n{\\displaystyle f_{X,Y}(S)=\\max _{a,b=0}^{1}S_{2X+a,2Y+b}.}\n\n\nIn this case, every max operation is over 4 numbers. The depth dimension remains unchanged (this is true for other forms of pooling as well).\n In addition to max pooling, pooling units can use other functions, such as average pooling or \u21132-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which generally performs better in practice.[78]\n Due to the effects of fast spatial reduction of the size of the representation,[which?] there is a recent trend towards using smaller filters[79] or discarding pooling layers altogether.[80]\n A channel max pooling (CMP) operation layer conducts the MP operation along the channel side among the corresponding positions of the consecutive feature maps for the purpose of redundant information elimination. The CMP makes the significant features gather together within fewer channels, which is important for fine-grained image classification that needs more discriminating features. Meanwhile, another advantage of the CMP operation is to make the channel number of feature maps smaller before it connects to the first fully connected (FC) layer. Similar to the MP operation, we denote the input feature maps and output feature maps of a CMP layer as F \u2208 R(C\u00d7M\u00d7N) and C \u2208 R(c\u00d7M\u00d7N), respectively, where C and c are the channel numbers of the input and output feature maps, M and N are the widths and the height of the feature maps, respectively. Note that the CMP operation only changes the channel number of the feature maps. The width and the height of the feature maps are not changed, which is different from the MP operation.[81]\n See [82][83] for reviews for pooling methods.\n ReLU is the abbreviation of rectified linear unit. It was proposed by Alston Householder in 1941,[84] and used in CNN by Kunihiko Fukushima in 1969.[38] ReLU applies the non-saturating activation function \n\n\n\nf\n(\nx\n)\n=\nmax\n(\n0\n,\nx\n)\n\n\n{\\textstyle f(x)=\\max(0,x)}\n\n.[70] It effectively removes negative values from an activation map by setting them to zero.[85] It introduces nonlinearity to the decision function and in the overall network without affecting the receptive fields of the convolution layers.\nIn 2011, Xavier Glorot, Antoine Bordes and Yoshua Bengio found that ReLU enables better training of deeper networks,[86] compared to widely used activation functions prior to 2011.\n Other functions can also be used to increase nonlinearity, for example the saturating hyperbolic tangent \n\n\n\nf\n(\nx\n)\n=\ntanh\n\u2061\n(\nx\n)\n\n\n{\\displaystyle f(x)=\\tanh(x)}\n\n, \n\n\n\nf\n(\nx\n)\n=\n\n|\n\ntanh\n\u2061\n(\nx\n)\n\n|\n\n\n\n{\\displaystyle f(x)=|\\tanh(x)|}\n\n, and the sigmoid function \n\n\n\n\u03c3\n(\nx\n)\n=\n(\n1\n+\n\ne\n\n\u2212\nx\n\n\n\n)\n\n\u2212\n1\n\n\n\n\n{\\textstyle \\sigma (x)=(1+e^{-x})^{-1}}\n\n. ReLU is often preferred to other functions because it trains the neural network several times faster without a significant penalty to generalization accuracy.[87]\n After several convolutional and max pooling layers, the final classification is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular (non-convolutional) artificial neural networks. Their activations can thus be computed as an affine transformation, with matrix multiplication followed by a bias offset (vector addition of a learned or fixed bias term).\n The \"loss layer\", or \"loss function\", specifies how training penalizes the deviation between the predicted output of the network, and the true data labels (during supervised learning). Various loss functions can be used, depending on the specific task.\n The Softmax loss function is used for predicting a single class of K mutually exclusive classes.[nb 3] Sigmoid cross-entropy loss is used for predicting K independent probability values in \n\n\n\n[\n0\n,\n1\n]\n\n\n{\\displaystyle [0,1]}\n\n. Euclidean loss is used for regressing to real-valued labels \n\n\n\n(\n\u2212\n\u221e\n,\n\u221e\n)\n\n\n{\\displaystyle (-\\infty ,\\infty )}\n\n.\n Hyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron (MLP).\n The kernel is the number of pixels processed together. It is typically expressed as the kernel's dimensions, e.g., 2x2, or 3x3.\n Padding is the addition of (typically) 0-valued pixels on the borders of an image. This is done so that the border pixels are not undervalued (lost) from the output because they would ordinarily participate in only a single receptive field instance. The padding applied is typically one less than the corresponding kernel dimension. For example, a convolutional layer using 3x3 kernels would receive a 2-pixel pad, that is 1 pixel on each side of the image.[citation needed]\n The stride is the number of pixels that the analysis window moves on each iteration. A stride of 2 means that each kernel is offset by 2 pixels from its predecessor.\n Since feature map size decreases with depth, layers near the input layer tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the product of feature values va with pixel position is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.\n The number of feature maps directly controls the capacity and depends on the number of available examples and task complexity.\n Common filter sizes found in the literature vary greatly, and are usually chosen based on the data set. Typical filter sizes range from 1x1 to 7x7. As two famous examples, AlexNet used 3x3, 5x5, and 11x11. Inceptionv3 used 1x1, 3x3, and 5x5.\n The challenge is to find the right level of granularity so as to create abstractions at the proper scale, given a particular data set, and without overfitting.\n Max pooling is typically used, often with a 2x2 dimension. This implies that the input is drastically downsampled, reducing processing cost.\n Greater pooling reduces the dimension of the signal, and may result in unacceptable information loss. Often, non-overlapping pooling windows perform best.[78]\n Dilation involves ignoring pixels within a kernel. This reduces processing memory potentially without significant signal loss. A dilation of 2 on a 3x3 kernel expands the kernel to 5x5, while still processing 9 (evenly spaced) pixels. Specifically, the processed pixels after the dilation are the cells (1,1), (1,3), (1,5), (3,1), (3,3), (3,5), (5,1), (5,3), (5,5), where (i,j) denotes the cell of the i-th row and j-th column in the expanded 5x5 kernel. Accordingly, dilation of 4 expands the kernel to 7x7.[citation needed]\n It is commonly assumed that CNNs are invariant to shifts of the input. Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations of the input.[74] However, layers with a stride greater than one ignore the Nyquist-Shannon sampling theorem and might lead to aliasing of the input signal[74] While, in principle, CNNs are capable of implementing anti-aliasing filters, it has been observed that this does not happen in practice [88] and yield models that are not equivariant to translations.\nFurthermore, if a CNN makes use of fully connected layers, translation equivariance does not imply translation invariance, as the fully connected layers are not invariant to shifts of the input.[89][15] One solution for complete translation invariance is avoiding any down-sampling throughout the network and applying global average pooling at the last layer.[74] Additionally, several other partial solutions have been proposed, such as anti-aliasing before downsampling operations,[90] spatial transformer networks,[91] data augmentation, subsampling combined with pooling,[15] and capsule neural networks.[92]\n The accuracy of the final model is based on a sub-part of the dataset set apart at the start, often called a test-set. Other times methods such as k-fold cross-validation are applied. Other strategies include using conformal prediction.[93][94]\n Regularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization.\n Because a fully connected layer occupies most of the parameters, it is prone to overfitting. One method to reduce overfitting is dropout, introduced in 2014.[95] At each training stage, individual nodes are either \"dropped out\" of the net (ignored) with probability \n\n\n\n1\n\u2212\np\n\n\n{\\displaystyle 1-p}\n\n or kept with probability \n\n\n\np\n\n\n{\\displaystyle p}\n\n, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.\n In the training stages, \n\n\n\np\n\n\n{\\displaystyle p}\n\n is usually 0.5; for input nodes, it is typically much higher because information is directly lost when input nodes are ignored.\n At testing time after training has finished, we would ideally like to find a sample average of all possible \n\n\n\n\n2\n\nn\n\n\n\n\n{\\displaystyle 2^{n}}\n\n dropped-out networks; unfortunately this is unfeasible for large values of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n. However, we can find an approximation by using the full network with each node's output weighted by a factor of \n\n\n\np\n\n\n{\\displaystyle p}\n\n, so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates \n\n\n\n\n2\n\nn\n\n\n\n\n{\\displaystyle 2^{n}}\n\n neural nets, and as such allows for model combination, at test time only a single network needs to be tested.\n By avoiding training all nodes on all training data, dropout decreases overfitting. The method also significantly improves training speed. This makes the model combination practical, even for deep neural networks. The technique seems to reduce node interactions, leading them to learn more robust features[clarification needed] that better generalize to new data.\n DropConnect is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability \n\n\n\n1\n\u2212\np\n\n\n{\\displaystyle 1-p}\n\n. Each unit thus receives input from a random subset of units in the previous layer.[96]\n DropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.\n A major drawback to Dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.\n Even before Dropout, in 2013 a technique called stochastic pooling,[97] the conventional deterministic pooling operations were replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a multinomial distribution, given by the activities within the pooling region. This approach is free of hyperparameters and can be combined with other regularization approaches, such as dropout and data augmentation.\n An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images,[98] which delivers excellent performance on the MNIST data set.[98] Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.\n Because the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Because there is often not enough available data to train, especially considering that some part should be spared for later testing, two approaches are to either generate new data from scratch (if possible) or perturb existing data to create new ones. The latter one is used since mid-1990s.[52] For example, input images can be cropped, rotated, or rescaled to create new examples with the same labels as the original training set.[99]\n One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted.\n Another simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a \"zero norm\".\n A simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (L1 norm) or squared magnitude (L2 norm) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant('alpha' hyperparameter), thus increasing the penalty for large weight vectors.\n L2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the useful property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot.\n L1 regularization is also common. It makes the weight vectors sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs. L1 with L2 regularization can be combined; this is called elastic net regularization.\n Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector \n\n\n\n\n\n\nw\n\u2192\n\n\n\n\n\n{\\displaystyle {\\vec {w}}}\n\n of every neuron to satisfy \n\n\n\n\u2016\n\n\n\nw\n\u2192\n\n\n\n\n\u2016\n\n2\n\n\n<\nc\n\n\n{\\displaystyle \\|{\\vec {w}}\\|_{2}<c}\n\n. Typical values of \n\n\n\nc\n\n\n{\\displaystyle c}\n\n are order of 3\u20134. Some papers report improvements[100] when using this form of regularization.\n Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.[101]\n An earlier common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.[102]\n Thus, one way to represent something is to embed the coordinate frame within it. This allows large features to be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). This approach ensures that the higher-level entity (e.g. face) is present when the lower-level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (\"pose vectors\") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes.[103]\n CNNs are often used in image recognition systems. In 2012, an error rate of 0.23% on the MNIST database was reported.[27] Another paper on using CNN for image classification reported that the learning process was \"surprisingly fast\"; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database.[24] Subsequently, a similar CNN called AlexNet[104] won the ImageNet Large Scale Visual Recognition Challenge 2012.\n When applied to facial recognition, CNNs achieved a large decrease in error rate.[105] Another paper reported a 97.6% recognition rate on \"5,600 still images of more than 10 subjects\".[20] CNNs were used to assess video quality in an objective way after manual training; the resulting system had a very low root mean square error.[106]\n The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014,[107] a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet[108] (the foundation of DeepDream) increased the mean average precision of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. That performance of convolutional neural networks on the ImageNet tests was close to that of humans.[109] The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.[citation needed]\n In 2015, a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded, with competitive performance. The network was trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.[110]\n Compared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space.[111][112] Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream.[113][114][115] Long short-term memory (LSTM) recurrent units are typically incorporated after the CNN to account for inter-frame or inter-clip dependencies.[116][117] Unsupervised learning schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted Boltzmann Machines[118] and Independent Subspace Analysis.[119] Its application can be seen in text-to-video model.[citation needed]\n CNNs have also been explored for natural language processing. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing,[120] search query retrieval,[121] sentence modeling,[122] classification,[123] prediction[124] and other traditional NLP tasks.[125]\nCompared to traditional language processing methods such as recurrent neural networks, CNNs can represent different contextual realities of language that do not rely on a series-sequence assumption, while RNNs are better suitable when classical time series modeling is required.[126][127][128][129]\n A CNN with 1-D convolutions was used on time series in the frequency domain (spectral residual) by an unsupervised model to detect anomalies in the time domain.[130]\n CNNs have been used in drug discovery. Predicting the interaction between molecules and biological proteins can identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based drug design.[131] The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures,[132] AtomNet discovers chemical features, such as aromaticity, sp3 carbons, and hydrogen bonding. Subsequently, AtomNet was used to predict novel candidate biomolecules for multiple disease targets, most notably treatments for the Ebola virus[133] and multiple sclerosis.[134]\n In 2016-2019, topologyNet [58] and mathematical deep learning \n[135]\n[136] achieved first place in multiple categories of the D3R Grand Challenges, a worldwide annual competition series focused on computer-aided drug design. \n CNNs have been used in the game of checkers. From 1999 to 2001, Fogel and Chellapilla published papers showing how a convolutional neural network could learn to play checker using co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and the difference in number of pieces between the two sides. Ultimately, the program (Blondie24) was tested on 165 games against players and ranked in the highest 0.4%.[137][138] It also earned a win against the program Chinook at its \"expert\" level of play.[139]\n CNNs have been used in computer Go. In December 2014, Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform GNU Go and win some games against Monte Carlo tree search Fuego 1.1 in a fraction of the time it took Fuego to play.[140] Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a 6 dan human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program GNU Go in 97% of games, and matched the performance of the Monte Carlo tree search program Fuego simulating ten thousand playouts (about a million positions) per move.[141]\n A couple of CNNs for choosing moves to try (\"policy network\") and evaluating positions (\"value network\") driving MCTS were used by AlphaGo, the first to beat the best human player at the time.[142]\n Recurrent neural networks are generally considered the best neural network architectures for time series forecasting (and sequence modeling in general), but recent studies show that convolutional networks can perform comparably or even better.[143][12] Dilated convolutions[144] might enable one-dimensional convolutional neural networks to effectively learn time series dependences.[145] Convolutions can be implemented more efficiently than RNN-based solutions, and they do not suffer from vanishing (or exploding) gradients.[146] Convolutional networks can provide an improved forecasting performance when there are multiple similar time series to learn from.[147] CNNs can also be applied to further tasks in time series analysis (e.g., time series classification[148] or quantile forecasting[149]).\n As archaeological findings such as clay tablets with cuneiform writing are increasingly acquired using 3D scanners, benchmark datasets are becoming available, including HeiCuBeDa[150] providing almost 2000 normalized 2-D and 3-D datasets prepared with the GigaMesh Software Framework.[151] So curvature-based measures are used in conjunction with geometric neural networks (GNNs), e.g. for period classification of those clay tablets being among the oldest documents of human history.[152][153]\n For many applications, training data is not very available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights, this is known as transfer learning. Furthermore, this technique allows convolutional network architectures to successfully be applied to problems with tiny training sets.[154]\n End-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars.[155] With recent advances in visual salience, spatial attention, and temporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.[156][157]\n A deep Q-network (DQN) is a type of deep learning model that combines a deep neural network with Q-learning, a form of reinforcement learning. Unlike earlier reinforcement learning agents, DQNs that utilize CNNs can learn directly from high-dimensional sensory inputs via reinforcement learning.[158]\n Preliminary results were presented in 2014, with an accompanying paper in February 2015.[159] The research described an application to Atari 2600 gaming. Other deep reinforcement learning models preceded it.[160]\n Convolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like deep belief networks. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR[161] have been obtained using CDBNs.[162] The feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid[163] by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated, e.g., for semantic segmentation, image reconstruction, and object localization tasks.\n",
        "doc_number": 17
    },
    {
        "url": "https://en.wikipedia.org/wiki/Recurrent_neural_network",
        "content": "Recurrent neural networks (RNNs) are a class of artificial neural network commonly used for sequential data processing. Unlike feedforward neural networks, which process data in a single pass, RNNs process data across multiple time steps, making them well-adapted for modelling and processing text, speech, and time series.[1]\n The building block of RNNs is the recurrent unit. This unit maintains a hidden state, essentially a form of memory, which is updated at each time step based on the current input and the previous hidden state. This feedback loop allows the network to learn from past inputs, and incorporate that knowledge into its current processing.\n Early RNNs suffered from the vanishing gradient problem, limiting their ability to learn long-range dependencies. This was solved by the long short-term memory (LSTM) variant in 1997, thus making it the standard architecture for RNN.\n RNNs have been applied to tasks such as unsegmented, connected handwriting recognition,[2] speech recognition,[3][4] natural language processing, and neural machine translation.[5][6]\n \nOne origin of RNN was neuroscience. The word \"recurrent\" is used to describe loop-like structures in anatomy. In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex formed by parallel fiber, Purkinje cells, and granule cells.[7][8] In 1933, Lorente de N\u00f3 discovered \"recurrent, reciprocal connections\" by Golgi's method, and proposed that excitatory loops explain certain aspects of the vestibulo-ocular reflex.[9][10] During 1940s, multiple people proposed the existence of feedback in the brain, which was a contrast to the previous understanding of the neural system as a purely feedforward structure. Hebb considered \"reverberating circuit\" as an explanation for short-term memory.[11] The McCulloch and Pitts paper (1943), which proposed the McCulloch-Pitts neuron model, considered networks that contains cycles. The current activity of such networks can be affected by activity indefinitely far in the past.[12] They were both interested in closed loops as possible explanations for e.g. epilepsy and causalgia.[13][14] Recurrent inhibition was proposed in 1946 as a negative feedback mechanism in motor control. Neural feedback loops were a common topic of discussion at the Macy conferences.[15] See [16] for an extensive review of recurrent neural network models in neuroscience. Frank Rosenblatt in 1960 published \"close-loop cross-coupled perceptrons\", which are 3-layered perceptron networks whose middle layer contains recurrent connections that change by a Hebbian learning rule.[18]:\u200a73\u201375\u200a Later, in Principles of Neurodynamics (1961), he described \"closed-loop cross-coupled\" and \"back-coupled\" perceptron networks, and made theoretical and experimental studies for Hebbian learning in these networks,[17]:\u200aChapter 19, 21\u200a and noted that a fully cross-coupled perceptron network is equivalent to an infinitely deep feedforward network.[17]:\u200aSection 19.11\u200a\n Similar networks were published by Kaoru Nakano in 1971[19][20],Shun'ichi Amari in 1972,[21] and William A. Little\u00a0[de] in 1974,[22] who was acknowledged by Hopfield in his 1982 paper.\n Another origin of RNN was statistical mechanics. The Ising model was developed by Wilhelm Lenz[23] and Ernst Ising[24] in the 1920s[25] as a simple statistical mechanical model of magnets at equilibrium. Glauber in 1963 studied the Ising model evolving in time, as a process towards equilibrium (Glauber dynamics), adding in the component of time.[26]\n The Sherrington\u2013Kirkpatrick model of spin glass, published in 1975,[27] is the Hopfield network with random initialization. Sherrington and Kirkpatrick found that it is highly likely for the energy function of the SK model to have many local minima. In the 1982 paper, Hopfield applied this recently developed theory to study the Hopfield network with binary activation functions.[28] In a 1984 paper he extended this to continuous activation functions.[29] It became a standard model for the study of neural networks through statistical mechanics.[30][31]\n Modern RNN networks are mainly based on two architectures: LSTM and BRNN.[32]\n At the resurgence of neural networks in the 1980s, recurrent networks were studied again. They were sometimes called \"iterated nets\".[33] Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology. In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.[34]\n Long short-term memory (LSTM) networks were invented by Hochreiter and Schmidhuber in 1995 and set accuracy records in multiple applications domains.[35][36] It became the default choice for RNN architecture.\n Bidirectional recurrent neural networks (BRNN) uses two RNN that processes the same input in opposite directions.[37] These two are often combined, giving the bidirectional LSTM architecture.\n Around 2006, bidirectional LSTM started to revolutionize speech recognition, outperforming traditional models in certain speech applications.[38][39] They also improved large-vocabulary speech recognition[3][4] and text-to-speech synthesis[40] and was used in Google voice search, and dictation on Android devices.[41] They broke records for improved machine translation,[42] language modeling[43] and Multilingual Language Processing.[44] Also, LSTM combined with convolutional neural networks (CNNs) improved automatic image captioning.[45]\n The idea of encoder-decoder sequence transduction had been developed in the early 2010s. The papers most commonly cited as the originators that produced seq2seq are two papers from 2014.[46][47] A seq2seq architecture employs two RNN, typically LSTM, an \"encoder\" and a \"decoder\", for sequence transduction, such as machine translation. They became state of the art in machine translation, and was instrumental in the development of attention mechanism and Transformer.\n An RNN-based model can be factored into two parts: configuration and architecture. Multiple RNN can be combined in a data flow, and the data flow itself is the configuration. Each RNN itself may have any architecture, including LSTM, GRU, etc.\n RNNs come in many variants. Abstractly speaking, an RNN is a function \n\n\n\n\nf\n\n\u03b8\n\n\n\n\n{\\displaystyle f_{\\theta }}\n\n of type \n\n\n\n(\n\nx\n\nt\n\n\n,\n\nh\n\nt\n\n\n)\n\u21a6\n(\n\ny\n\nt\n\n\n,\n\nh\n\nt\n+\n1\n\n\n)\n\n\n{\\displaystyle (x_{t},h_{t})\\mapsto (y_{t},h_{t+1})}\n\n, where\n In words, it is a neural network that maps an input \n\n\n\n\nx\n\nt\n\n\n\n\n{\\displaystyle x_{t}}\n\n into an output \n\n\n\n\ny\n\nt\n\n\n\n\n{\\displaystyle y_{t}}\n\n, with the hidden vector \n\n\n\n\nh\n\nt\n\n\n\n\n{\\displaystyle h_{t}}\n\n playing the role of \"memory\", a partial record of all previous input-output pairs. At each step, it transforms input to an output, and modifies its \"memory\" to help it to better perform future processing.\n The illustration to the right may be misleading to many because practical neural network topologies are frequently organized in \"layers\" and the drawing gives that appearance. However, what appears to be layers are, in fact, different steps in time, \"unfolded\" to produce the appearance of layers.\n A stacked RNN, or deep RNN, is composed of multiple RNNs stacked one above the other. Abstractly, it is structured as follows\n Each layer operates as a stand-alone RNN, and each layer's output sequence is used as the input sequence to the layer above. There is no conceptual limit to the depth of stacked RNN.\n A bidirectional RNN (biRNN) is composed of two RNNs, one processing the input sequence in one direction, and another in the opposite direction. Abstractly, it is structured as follows:\n The two output sequences are then concatenated to give the total output: \n\n\n\n(\n(\n\ny\n\n0\n\n\n,\n\ny\n\n0\n\n\u2032\n\n)\n,\n(\n\ny\n\n1\n\n\n,\n\ny\n\n1\n\n\u2032\n\n)\n,\n\u2026\n,\n(\n\ny\n\nN\n\n\n,\n\ny\n\nN\n\n\u2032\n\n)\n)\n\n\n{\\displaystyle ((y_{0},y_{0}'),(y_{1},y_{1}'),\\dots ,(y_{N},y_{N}'))}\n\n.\n Bidirectional RNN allows the model to process a token both in the context of what came before it and what came after it. By stacking multiple bidirectional RNNs together, the model can process a token increasingly contextually. The ELMo model (2018)[48] is a stacked bidirectional LSTM which takes character-level as inputs and produces word-level embeddings.\n \nTwo RNNs can be run front-to-back in an encoder-decoder configuration. The encoder RNN processes an input sequence into a sequence of hidden vectors, and the decoder RNN processes the sequence of hidden vectors to an output sequence, with an optional attention mechanism. This was used to construct state of the art neural machine translators during the 2014\u20132017 period. This was an instrumental step towards the development of Transformers.[49]\n An RNN may process data with more than one dimension. PixelRNN processes two-dimensional data, with many possible directions.[50] For example, the row-by-row direction processes an \n\n\n\nn\n\u00d7\nn\n\n\n{\\displaystyle n\\times n}\n\n grid of vectors \n\n\n\n\nx\n\ni\n,\nj\n\n\n\n\n{\\displaystyle x_{i,j}}\n\n in the following order: \n\n\n\n\nx\n\n1\n,\n1\n\n\n,\n\nx\n\n1\n,\n2\n\n\n,\n\u2026\n,\n\nx\n\n1\n,\nn\n\n\n,\n\nx\n\n2\n,\n1\n\n\n,\n\nx\n\n2\n,\n2\n\n\n,\n\u2026\n,\n\nx\n\n2\n,\nn\n\n\n,\n\u2026\n,\n\nx\n\nn\n,\nn\n\n\n\n\n{\\displaystyle x_{1,1},x_{1,2},\\dots ,x_{1,n},x_{2,1},x_{2,2},\\dots ,x_{2,n},\\dots ,x_{n,n}}\n\nThe diagonal BiLSTM uses two LSTMs to process the same grid. One processes it from the top-left corner to the bottom-right, such that it processes \n\n\n\n\nx\n\ni\n,\nj\n\n\n\n\n{\\displaystyle x_{i,j}}\n\n depending on its hidden state and cell state on the top and the left side: \n\n\n\n\nh\n\ni\n\u2212\n1\n,\nj\n\n\n,\n\nc\n\ni\n\u2212\n1\n,\nj\n\n\n\n\n{\\displaystyle h_{i-1,j},c_{i-1,j}}\n\n and \n\n\n\n\nh\n\ni\n,\nj\n\u2212\n1\n\n\n,\n\nc\n\ni\n,\nj\n\u2212\n1\n\n\n\n\n{\\displaystyle h_{i,j-1},c_{i,j-1}}\n\n. The other processes it from the top-right corner to the bottom-left.\n Fully recurrent neural networks (FRNN) connect the outputs of all neurons to the inputs of all neurons. In other words, it is a fully connected network. This is the most general neural network topology, because all other topologies can be represented by setting some connection weights to zero to simulate the lack of connections between those neurons.\n The Hopfield network is an RNN in which all connections across layers are equally sized. It requires stationary inputs and is thus not a general RNN, as it does not process sequences of patterns. However, it guarantees that it will converge. If the connections are trained using Hebbian learning, then the Hopfield network can perform as robust content-addressable memory, resistant to connection alteration.\n An Elman network is a three-layer network (arranged horizontally as x, y, and z in the illustration) with the addition of a set of context units (u in the illustration). The middle (hidden) layer is connected to these context units fixed with a weight of one.[51] At each time step, the input is fed forward and a learning rule is applied. The fixed back-connections save a copy of the previous values of the hidden units in the context units (since they propagate over the connections before the learning rule is applied). Thus the network can maintain a sort of state, allowing it to perform tasks such as sequence-prediction that are beyond the power of a standard multilayer perceptron.\n Jordan networks are similar to Elman networks. The context units are fed from the output layer instead of the hidden layer. The context units in a Jordan network are also called the state layer. They have a recurrent connection to themselves.[51]\n Elman and Jordan networks are also known as \"Simple recurrent networks\" (SRN).\n Variables and functions\n Long short-term memory (LSTM) is the most widely used RNN architecture. It was designed to solve the vanishing gradient problem. LSTM is normally augmented by recurrent gates called \"forget gates\".[54] LSTM prevents backpropagated errors from vanishing or exploding.[55] Instead, errors can flow backward through unlimited numbers of virtual layers unfolded in space. That is, LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier. Problem-specific LSTM-like topologies can be evolved.[56] LSTM works even given long delays between significant events and can handle signals that mix low and high-frequency components.\n Many applications use stacks of LSTMs,[57] for which it is called \"deep LSTM\". LSTM can learn to recognize context-sensitive languages unlike previous models based on hidden Markov models (HMM) and similar concepts.[58]\n Gated recurrent unit (GRU), introduced in 2014, was designed as a simplification of LSTM. They are used in the full form and several further simplified variants.[59][60] They have fewer parameters than LSTM, as they lack an output gate.[61]\n Their performance on polyphonic music modeling and speech signal modeling was found to be similar to that of long short-term memory.[62] There does not appear to be particular performance difference between LSTM and GRU.[62][63]\n Introduced by Bart Kosko,[64] a bidirectional associative memory (BAM) network is a variant of a Hopfield network that stores associative data as a vector. The bidirectionality comes from passing information through a matrix and its transpose. Typically, bipolar encoding is preferred to binary encoding of the associative pairs. Recently, stochastic BAM models using Markov stepping were optimized for increased network stability and relevance to real-world applications.[65]\n A BAM network has two layers, either of which can be driven as an input to recall an association and produce an output on the other layer.[66]\n Echo state networks (ESN) have a sparsely connected random hidden layer. The weights of output neurons are the only part of the network that can change (be trained). ESNs are good at reproducing certain time series.[67] A variant for spiking neurons is known as a liquid state machine.[68]\n A recursive neural network[69] is created by applying the same set of weights recursively over a differentiable graph-like structure by traversing the structure in topological order. Such networks are typically also trained by the reverse mode of automatic differentiation.[70][71] They can process distributed representations of structure, such as logical terms. A special case of recursive neural networks is the RNN whose structure corresponds to a linear chain. Recursive neural networks have been applied to natural language processing.[72] The Recursive Neural Tensor Network uses a tensor-based composition function for all nodes in the tree.[73]\n Neural Turing machines (NTMs) are a method of extending recurrent neural networks by coupling them to external memory resources with which they interact. The combined system is analogous to a Turing machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent.[74]\n Differentiable neural computers (DNCs) are an extension of Neural Turing machines, allowing for the usage of fuzzy amounts of each memory address and a record of chronology.[75]\n Neural network pushdown automata (NNPDA) are similar to NTMs, but tapes are replaced by analog stacks that are differentiable and trained. In this way, they are similar in complexity to recognizers of context free grammars (CFGs).[76]\n Recurrent neural networks are Turing complete and can run arbitrary programs to process arbitrary sequences of inputs.[77]\n An RNN can be trained into a conditionally generative model of sequences, aka autoregression.\n Concretely, let us consider the problem of machine translation, that is, given a sequence \n\n\n\n(\n\nx\n\n1\n\n\n,\n\nx\n\n2\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n)\n\n\n{\\displaystyle (x_{1},x_{2},\\dots ,x_{n})}\n\n of English words, the model is to produce a sequence \n\n\n\n(\n\ny\n\n1\n\n\n,\n\u2026\n,\n\ny\n\nm\n\n\n)\n\n\n{\\displaystyle (y_{1},\\dots ,y_{m})}\n\n of French words. It is to be solved by a seq2seq model.\n Now, during training, the encoder half of the model would first ingest \n\n\n\n(\n\nx\n\n1\n\n\n,\n\nx\n\n2\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n)\n\n\n{\\displaystyle (x_{1},x_{2},\\dots ,x_{n})}\n\n, then the decoder half would start generating a sequence \n\n\n\n(\n\n\n\n\ny\n^\n\n\n\n\n1\n\n\n,\n\n\n\n\ny\n^\n\n\n\n\n2\n\n\n,\n\u2026\n,\n\n\n\n\ny\n^\n\n\n\n\nl\n\n\n)\n\n\n{\\displaystyle ({\\hat {y}}_{1},{\\hat {y}}_{2},\\dots ,{\\hat {y}}_{l})}\n\n. The problem is that if the model makes a mistake early on, say at \n\n\n\n\n\n\n\ny\n^\n\n\n\n\n2\n\n\n\n\n{\\displaystyle {\\hat {y}}_{2}}\n\n, then subsequent tokens are likely to also be mistakes. This makes it inefficient for the model to obtain a learning signal, since the model would mostly learn to shift \n\n\n\n\n\n\n\ny\n^\n\n\n\n\n2\n\n\n\n\n{\\displaystyle {\\hat {y}}_{2}}\n\n towards \n\n\n\n\ny\n\n2\n\n\n\n\n{\\displaystyle y_{2}}\n\n, but not the others.\n Teacher forcing makes it so that the decoder uses the correct output sequence for generating the next entry in the sequence. So for example, it would see \n\n\n\n(\n\ny\n\n1\n\n\n,\n\u2026\n,\n\ny\n\nk\n\n\n)\n\n\n{\\displaystyle (y_{1},\\dots ,y_{k})}\n\n in order to generate \n\n\n\n\n\n\n\ny\n^\n\n\n\n\nk\n+\n1\n\n\n\n\n{\\displaystyle {\\hat {y}}_{k+1}}\n\n.\n Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. In neural networks, it can be used to minimize the error term by changing each weight in proportion to the derivative of the error with respect to that weight, provided the non-linear activation functions are differentiable.\n The standard method for training RNN by gradient descent is the \"backpropagation through time\" (BPTT) algorithm, which is a special case of the general algorithm of backpropagation. A more computationally expensive online variant is called \"Real-Time Recurrent Learning\" or RTRL,[78][79] which is an instance of automatic differentiation in the forward accumulation mode with stacked tangent vectors. Unlike BPTT, this algorithm is local in time but not local in space.\n In this context, local in space means that a unit's weight vector can be updated using only information stored in the connected units and the unit itself such that update complexity of a single unit is linear in the dimensionality of the weight vector. Local in time means that the updates take place continually (on-line) and depend only on the most recent time step rather than on multiple time steps within a given time horizon as in BPTT. Biological neural networks appear to be local with respect to both time and space.[80][81]\n For recursively computing the partial derivatives, RTRL has a time-complexity of O(number of hidden x number of weights) per time step for computing the Jacobian matrices, while BPTT only takes O(number of weights) per time step, at the cost of storing all forward activations within the given time horizon.[82] An online hybrid between BPTT and RTRL with intermediate complexity exists,[83][84] along with variants for continuous time.[85]\n A major problem with gradient descent for standard RNN architectures is that error gradients vanish exponentially quickly with the size of the time lag between important events.[55][86] LSTM combined with a BPTT/RTRL hybrid learning method attempts to overcome these problems.[36] This problem is also solved in the independently recurrent neural network (IndRNN)[87] by reducing the context of a neuron to its own past state and the cross-neuron information can then be explored in the following layers. Memories of different ranges including long-term memory can be learned without the gradient vanishing and exploding problem.\n The on-line algorithm called causal recursive backpropagation (CRBP), implements and combines BPTT and RTRL paradigms for locally recurrent networks.[88] It works with the most general locally recurrent networks. The CRBP algorithm can minimize the global error term. This fact improves the stability of the algorithm, providing a unifying view of gradient calculation techniques for recurrent networks with local feedback.\n One approach to gradient information computation in RNNs with arbitrary architectures is based on signal-flow graphs diagrammatic derivation.[89] It uses the BPTT batch algorithm, based on Lee's theorem for network sensitivity calculations.[90] It was proposed by Wan and Beaufays, while its fast online version was proposed by Campolucci, Uncini and Piazza.[90]\n The connectionist temporal classification (CTC)[91] is a specialized loss function for training RNNs for sequence modeling problems where the timing is variable.[92]\n Training the weights in a neural network can be modeled as a non-linear global optimization problem. A target function can be formed to evaluate the fitness or error of a particular weight vector as follows: First, the weights in the network are set according to the weight vector. Next, the network is evaluated against the training sequence. Typically, the sum-squared difference between the predictions and the target values specified in the training sequence is used to represent the error of the current weight vector. Arbitrary global optimization techniques may then be used to minimize this target function.\n The most common global optimization method for training RNNs is genetic algorithms, especially in unstructured networks.[93][94][95]\n Initially, the genetic algorithm is encoded with the neural network weights in a predefined manner where one gene in the chromosome represents one weight link. The whole network is represented as a single chromosome. The fitness function is evaluated as follows:\n Many chromosomes make up the population; therefore, many different neural networks are evolved until a stopping criterion is satisfied. A common stopping scheme is: \n The fitness function evaluates the stopping criterion as it receives the mean-squared error reciprocal from each network during training. Therefore, the goal of the genetic algorithm is to maximize the fitness function, reducing the mean-squared error.\n Other global (and/or evolutionary) optimization techniques may be used to seek a good set of weights, such as simulated annealing or particle swarm optimization.\n The independently recurrent neural network (IndRNN)[87] addresses the gradient vanishing and exploding problems in the traditional fully connected RNN. Each neuron in one layer only receives its own past state as context information (instead of full connectivity to all other neurons in this layer) and thus neurons are independent of each other's history. The gradient backpropagation can be regulated to avoid gradient vanishing and exploding in order to keep long or short-term memory. The cross-neuron information is explored in the next layers. IndRNN can be robustly trained with non-saturated nonlinear functions such as ReLU. Deep networks can be trained using skip connections.\n The neural history compressor is an unsupervised stack of RNNs.[96] At the input level, it learns to predict its next input from the previous inputs. Only unpredictable inputs of some RNN in the hierarchy become inputs to the next higher level RNN, which therefore recomputes its internal state only rarely. Each higher level RNN thus studies a compressed representation of the information in the RNN below. This is done such that the input sequence can be precisely reconstructed from the representation at the highest level.\n The system effectively minimizes the description length or the negative logarithm of the probability of the data.[97] Given a lot of learnable predictability in the incoming data sequence, the highest level RNN can use supervised learning to easily classify even deep sequences with long intervals between important events.\n It is possible to distill the RNN hierarchy into two RNNs: the \"conscious\" chunker (higher level) and the \"subconscious\" automatizer (lower level).[96] Once the chunker has learned to predict and compress inputs that are unpredictable by the automatizer, then the automatizer can be forced in the next learning phase to predict or imitate through additional units the hidden units of the more slowly changing chunker. This makes it easy for the automatizer to learn appropriate, rarely changing memories across long intervals. In turn, this helps the automatizer to make many of its once unpredictable inputs predictable, such that the chunker can focus on the remaining unpredictable events.[96]\n A generative model partially overcame the vanishing gradient problem[55] of automatic differentiation or backpropagation in neural networks in 1992. In 1993, such a system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.[34]\n Second-order RNNs use higher order weights \n\n\n\nw\n\n\n\n\ni\nj\nk\n\n\n\n\n{\\displaystyle w{}_{ijk}}\n\n instead of the standard \n\n\n\nw\n\n\n\n\ni\nj\n\n\n\n\n{\\displaystyle w{}_{ij}}\n\n weights, and states can be a product. This allows a direct mapping to a finite-state machine both in training, stability, and representation.[98][99] Long short-term memory is an example of this but has no such formal mappings or proof of stability.\n Hierarchical recurrent neural networks (HRNN) connect their neurons in various ways to decompose hierarchical behavior into useful subprograms.[96][100] Such hierarchical structures of cognition are present in theories of memory presented by philosopher Henri Bergson, whose philosophical views have inspired hierarchical models.[101]\n Hierarchical recurrent neural networks are useful in forecasting, helping to predict disaggregated inflation components of the consumer price index (CPI). The HRNN model leverages information from higher levels in the CPI hierarchy to enhance lower-level predictions. Evaluation of a substantial dataset from the US CPI-U index demonstrates the superior performance of the HRNN model compared to various established inflation prediction methods.[102]\n Generally, a recurrent multilayer perceptron network (RMLP network) consists of cascaded subnetworks, each containing multiple layers of nodes. Each subnetwork is feed-forward except for the last layer, which can have feedback connections. Each of these subnets is connected only by feed-forward connections.[103]\n A multiple timescales recurrent neural network (MTRNN) is a neural-based computational model that can simulate the functional hierarchy of the brain through self-organization depending on the spatial connection between neurons and on distinct types of neuron activities, each with distinct time properties.[104][105] With such varied neuronal activities, continuous sequences of any set of behaviors are segmented into reusable primitives, which in turn are flexibly integrated into diverse sequential behaviors. The biological approval of such a type of hierarchy was discussed in the memory-prediction theory of brain function by Hawkins in his book On Intelligence.[citation needed] Such a hierarchy also agrees with theories of memory posited by philosopher Henri Bergson, which have been incorporated into an MTRNN model.[101][106]\n Greg Snider of HP Labs describes a system of cortical computing with memristive nanodevices.[107] The memristors (memory resistors) are implemented by thin film materials in which the resistance is electrically tuned via the transport of ions or oxygen vacancies within the film. DARPA's SyNAPSE project has funded IBM Research and HP Labs, in collaboration with the Boston University Department of Cognitive and Neural Systems (CNS), to develop neuromorphic architectures that may be based on memristive systems.\nMemristive networks are a particular type of physical neural network that have very similar properties to (Little-)Hopfield networks, as they have continuous dynamics, a limited memory capacity and natural relaxation via the minimization of a function which is asymptotic to the Ising model. In this sense, the dynamics of a memristive circuit have the advantage compared to a Resistor-Capacitor network to have a more interesting non-linear behavior. From this point of view, engineering analog memristive networks account for a peculiar type of neuromorphic engineering in which the device behavior depends on the circuit wiring or topology.\nThe evolution of these networks can be studied analytically using variations of the Caravelli\u2013Traversa\u2013Di Ventra equation.[108]\n A continuous-time recurrent neural network (CTRNN) uses a system of ordinary differential equations to model the effects on a neuron of the incoming inputs. They are typically analyzed by dynamical systems theory. Many RNN models in neuroscience are continuous-time.[16]\n For a neuron \n\n\n\ni\n\n\n{\\displaystyle i}\n\n in the network with activation \n\n\n\n\ny\n\ni\n\n\n\n\n{\\displaystyle y_{i}}\n\n, the rate of change of activation is given by:\n Where:\n CTRNNs have been applied to evolutionary robotics where they have been used to address vision,[109] co-operation,[110] and minimal cognitive behaviour.[111]\n Note that, by the Shannon sampling theorem, discrete-time recurrent neural networks can be viewed as continuous-time recurrent neural networks where the differential equations have transformed into equivalent difference equations.[112] This transformation can be thought of as occurring after the post-synaptic node activation functions \n\n\n\n\ny\n\ni\n\n\n(\nt\n)\n\n\n{\\displaystyle y_{i}(t)}\n\n have been low-pass filtered but prior to sampling.\n They are in fact recursive neural networks with a particular structure: that of a linear chain. Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step.\n From a time-series perspective, RNNs can appear as nonlinear versions of finite impulse response and infinite impulse response filters and also as a nonlinear autoregressive exogenous model (NARX).[113] RNN has infinite impulse response whereas convolutional neural networks have finite impulse response. Both classes of networks exhibit temporal dynamic behavior.[114] A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that cannot be unrolled.\n The effect of memory-based learning for the recognition of sequences can also be implemented by a more biological-based model which uses the silencing mechanism exhibited in neurons with a relatively high frequency spiking activity.[115]\n Additional stored states and the storage under direct control by the network can be added to both infinite-impulse and finite-impulse networks. Another network or graph can also replace the storage if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated states or gated memory and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedback Neural Network (FNN).\n Modern libraries provide runtime-optimized implementations of the above functionality or allow to speed up the slow loop by just-in-time compilation.\n Applications of recurrent neural networks include:\n",
        "doc_number": 18
    },
    {
        "url": "https://en.wikipedia.org/wiki/Generative_adversarial_network",
        "content": "\n A generative adversarial network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative artificial intelligence. The concept was initially developed by Ian Goodfellow and his colleagues in June 2014.[1] In a GAN, two neural networks contest with each other in the form of a zero-sum game, where one agent's gain is another agent's loss.\n Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proved useful for semi-supervised learning,[2] fully supervised learning,[3] and reinforcement learning.[4]\n The core idea of a GAN is based on the \"indirect\" training through the discriminator, another neural network that can tell how \"realistic\" the input seems, which itself is also being updated dynamically.[5] This means that the generator is not trained to minimize the distance to a specific image, but rather to fool the discriminator. This enables the model to learn in an unsupervised manner.\n GANs are similar to mimicry in evolutionary biology, with an evolutionary arms race between both networks.\n \nThe original GAN is defined as the following game:[1] Each probability space \n\n\n\n(\n\u03a9\n,\n\n\u03bc\n\nref\n\n\n)\n\n\n{\\displaystyle (\\Omega ,\\mu _{\\text{ref}})}\n\n defines a GAN game.\n There are 2 players: generator and discriminator.\n The generator's strategy set is \n\n\n\n\n\nP\n\n\n(\n\u03a9\n)\n\n\n{\\displaystyle {\\mathcal {P}}(\\Omega )}\n\n, the set of all probability measures \n\n\n\n\n\u03bc\n\nG\n\n\n\n\n{\\displaystyle \\mu _{G}}\n\n on \n\n\n\n\u03a9\n\n\n{\\displaystyle \\Omega }\n\n.\n The discriminator's strategy set is the set of Markov kernels \n\n\n\n\n\u03bc\n\nD\n\n\n:\n\u03a9\n\u2192\n\n\nP\n\n\n[\n0\n,\n1\n]\n\n\n{\\displaystyle \\mu _{D}:\\Omega \\to {\\mathcal {P}}[0,1]}\n\n, where \n\n\n\n\n\nP\n\n\n[\n0\n,\n1\n]\n\n\n{\\displaystyle {\\mathcal {P}}[0,1]}\n\n is the set of probability measures on \n\n\n\n[\n0\n,\n1\n]\n\n\n{\\displaystyle [0,1]}\n\n.\n The GAN game is a zero-sum game, with objective function\n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\n\u03bc\n\nD\n\n\n)\n:=\n\nE\n\nx\n\u223c\n\n\u03bc\n\nref\n\n\n,\ny\n\u223c\n\n\u03bc\n\nD\n\n\n(\nx\n)\n\n\n\u2061\n[\nln\n\u2061\ny\n]\n+\n\nE\n\nx\n\u223c\n\n\u03bc\n\nG\n\n\n,\ny\n\u223c\n\n\u03bc\n\nD\n\n\n(\nx\n)\n\n\n\u2061\n[\nln\n\u2061\n(\n1\n\u2212\ny\n)\n]\n.\n\n\n{\\displaystyle L(\\mu _{G},\\mu _{D}):=\\operatorname {E} _{x\\sim \\mu _{\\text{ref}},y\\sim \\mu _{D}(x)}[\\ln y]+\\operatorname {E} _{x\\sim \\mu _{G},y\\sim \\mu _{D}(x)}[\\ln(1-y)].}\n\n\nThe generator aims to minimize the objective, and the discriminator aims to maximize the objective.\n The generator's task is to approach \n\n\n\n\n\u03bc\n\nG\n\n\n\u2248\n\n\u03bc\n\nref\n\n\n\n\n{\\displaystyle \\mu _{G}\\approx \\mu _{\\text{ref}}}\n\n, that is, to match its own output distribution as closely as possible to the reference distribution. The discriminator's task is to output a value close to 1 when the input appears to be from the reference distribution, and to output a value close to 0 when the input looks like it came from the generator distribution.\n The generative network generates candidates while the discriminative network evaluates them.[1] The contest operates in terms of data distributions. Typically, the generative network learns to map from a latent space to a data distribution of interest, while the discriminative network distinguishes candidates produced by the generator from the true data distribution. The generative network's training objective is to increase the error rate of the discriminative network (i.e., \"fool\" the discriminator network by producing novel candidates that the discriminator thinks are not synthesized (are part of the true data distribution)).[1][6]\n A known dataset serves as the initial training data for the discriminator. Training involves presenting it with samples from the training dataset until it achieves acceptable accuracy. The generator is trained based on whether it succeeds in fooling the discriminator. Typically, the generator is seeded with randomized input that is sampled from a predefined latent space (e.g. a multivariate normal distribution). Thereafter, candidates synthesized by the generator are evaluated by the discriminator. Independent backpropagation procedures are applied to both networks so that the generator produces better samples, while the discriminator becomes more skilled at flagging synthetic samples.[7] When used for image generation, the generator is typically a deconvolutional neural network, and the discriminator is a convolutional neural network.\n GANs are implicit generative models,[8] which means that they do not explicitly model the likelihood function nor provide a means for finding the latent variable corresponding to a given sample, unlike alternatives such as flow-based generative model.\n Compared to fully visible belief networks such as WaveNet and PixelRNN and autoregressive models in general, GANs can generate one complete sample in one pass, rather than multiple passes through the network.\n Compared to Boltzmann machines and linear ICA, there is no restriction on the type of function used by the network.\n Since neural networks are universal approximators, GANs are asymptotically consistent. Variational autoencoders might be universal approximators, but it is not proven as of 2017.[9]\n This section provides some of the mathematical theory behind these methods.\n \nIn modern probability theory based on measure theory, a probability space also needs to be equipped with a \u03c3-algebra. As a result, a more rigorous definition of the GAN game would make the following changes: Each probability space \n\n\n\n(\n\u03a9\n,\n\n\nB\n\n\n,\n\n\u03bc\n\nref\n\n\n)\n\n\n{\\displaystyle (\\Omega ,{\\mathcal {B}},\\mu _{\\text{ref}})}\n\n defines a GAN game.\n The generator's strategy set is \n\n\n\n\n\nP\n\n\n(\n\u03a9\n,\n\n\nB\n\n\n)\n\n\n{\\displaystyle {\\mathcal {P}}(\\Omega ,{\\mathcal {B}})}\n\n, the set of all probability measures \n\n\n\n\n\u03bc\n\nG\n\n\n\n\n{\\displaystyle \\mu _{G}}\n\n on the measure-space \n\n\n\n(\n\u03a9\n,\n\n\nB\n\n\n)\n\n\n{\\displaystyle (\\Omega ,{\\mathcal {B}})}\n\n.\n \nThe discriminator's strategy set is the set of Markov kernels \n\n\n\n\n\u03bc\n\nD\n\n\n:\n(\n\u03a9\n,\n\n\nB\n\n\n)\n\u2192\n\n\nP\n\n\n(\n[\n0\n,\n1\n]\n,\n\n\nB\n\n\n(\n[\n0\n,\n1\n]\n)\n)\n\n\n{\\displaystyle \\mu _{D}:(\\Omega ,{\\mathcal {B}})\\to {\\mathcal {P}}([0,1],{\\mathcal {B}}([0,1]))}\n\n, where \n\n\n\n\n\nB\n\n\n(\n[\n0\n,\n1\n]\n)\n\n\n{\\displaystyle {\\mathcal {B}}([0,1])}\n\n is the Borel \u03c3-algebra on \n\n\n\n[\n0\n,\n1\n]\n\n\n{\\displaystyle [0,1]}\n\n. Since issues of measurability never arise in practice, these will not concern us further.\n In the most generic version of the GAN game described above, the strategy set for the discriminator contains all Markov kernels \n\n\n\n\n\u03bc\n\nD\n\n\n:\n\u03a9\n\u2192\n\n\nP\n\n\n[\n0\n,\n1\n]\n\n\n{\\displaystyle \\mu _{D}:\\Omega \\to {\\mathcal {P}}[0,1]}\n\n, and the strategy set for the generator contains arbitrary probability distributions \n\n\n\n\n\u03bc\n\nG\n\n\n\n\n{\\displaystyle \\mu _{G}}\n\n on \n\n\n\n\u03a9\n\n\n{\\displaystyle \\Omega }\n\n.\n However, as shown below, the optimal discriminator strategy against any \n\n\n\n\n\u03bc\n\nG\n\n\n\n\n{\\displaystyle \\mu _{G}}\n\n is deterministic, so there is no loss of generality in restricting the discriminator's strategies to deterministic functions \n\n\n\nD\n:\n\u03a9\n\u2192\n[\n0\n,\n1\n]\n\n\n{\\displaystyle D:\\Omega \\to [0,1]}\n\n. In most applications, \n\n\n\nD\n\n\n{\\displaystyle D}\n\n is a deep neural network function.\n As for the generator, while \n\n\n\n\n\u03bc\n\nG\n\n\n\n\n{\\displaystyle \\mu _{G}}\n\n could theoretically be any computable probability distribution, in practice, it is usually implemented as a pushforward: \n\n\n\n\n\u03bc\n\nG\n\n\n=\n\n\u03bc\n\nZ\n\n\n\u2218\n\nG\n\n\u2212\n1\n\n\n\n\n{\\displaystyle \\mu _{G}=\\mu _{Z}\\circ G^{-1}}\n\n. That is, start with a random variable \n\n\n\nz\n\u223c\n\n\u03bc\n\nZ\n\n\n\n\n{\\displaystyle z\\sim \\mu _{Z}}\n\n, where \n\n\n\n\n\u03bc\n\nZ\n\n\n\n\n{\\displaystyle \\mu _{Z}}\n\n is a probability distribution that is easy to compute (such as the uniform distribution, or the Gaussian distribution), then define a function \n\n\n\nG\n:\n\n\u03a9\n\nZ\n\n\n\u2192\n\u03a9\n\n\n{\\displaystyle G:\\Omega _{Z}\\to \\Omega }\n\n. Then the distribution \n\n\n\n\n\u03bc\n\nG\n\n\n\n\n{\\displaystyle \\mu _{G}}\n\n is the distribution of \n\n\n\nG\n(\nz\n)\n\n\n{\\displaystyle G(z)}\n\n.\n Consequently, the generator's strategy is usually defined as just \n\n\n\nG\n\n\n{\\displaystyle G}\n\n, leaving \n\n\n\nz\n\u223c\n\n\u03bc\n\nZ\n\n\n\n\n{\\displaystyle z\\sim \\mu _{Z}}\n\n implicit. In this formalism, the GAN game objective is\n\n\n\nL\n(\nG\n,\nD\n)\n:=\n\nE\n\nx\n\u223c\n\n\u03bc\n\nref\n\n\n\n\n\u2061\n[\nln\n\u2061\nD\n(\nx\n)\n]\n+\n\nE\n\nz\n\u223c\n\n\u03bc\n\nZ\n\n\n\n\n\u2061\n[\nln\n\u2061\n(\n1\n\u2212\nD\n(\nG\n(\nz\n)\n)\n)\n]\n.\n\n\n{\\displaystyle L(G,D):=\\operatorname {E} _{x\\sim \\mu _{\\text{ref}}}[\\ln D(x)]+\\operatorname {E} _{z\\sim \\mu _{Z}}[\\ln(1-D(G(z)))].}\n\n\n The GAN architecture has two main components. One is casting optimization into a game, of form \n\n\n\n\nmin\n\nG\n\n\n\nmax\n\nD\n\n\nL\n(\nG\n,\nD\n)\n\n\n{\\displaystyle \\min _{G}\\max _{D}L(G,D)}\n\n, which is different from the usual kind of optimization, of form \n\n\n\n\nmin\n\n\u03b8\n\n\nL\n(\n\u03b8\n)\n\n\n{\\displaystyle \\min _{\\theta }L(\\theta )}\n\n. The other is the decomposition of \n\n\n\n\n\u03bc\n\nG\n\n\n\n\n{\\displaystyle \\mu _{G}}\n\n into \n\n\n\n\n\u03bc\n\nZ\n\n\n\u2218\n\nG\n\n\u2212\n1\n\n\n\n\n{\\displaystyle \\mu _{Z}\\circ G^{-1}}\n\n, which can be understood as a reparametrization trick.\n To see its significance, one must compare GAN with previous methods for learning generative models, which were plagued with \"intractable probabilistic computations that arise in maximum likelihood estimation and related strategies\".[1]\n At the same time, Kingma and Welling[10] and Rezende et al.[11] developed the same idea of reparametrization into a general stochastic backpropagation method. Among its first applications was the variational autoencoder.\n In the original paper, as well as most subsequent papers, it is usually assumed that the generator moves first, and the discriminator moves second, thus giving the following minimax game:\n\n\n\n\nmin\n\n\n\u03bc\n\nG\n\n\n\n\n\nmax\n\n\n\u03bc\n\nD\n\n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\n\u03bc\n\nD\n\n\n)\n:=\n\nE\n\nx\n\u223c\n\n\u03bc\n\nref\n\n\n,\ny\n\u223c\n\n\u03bc\n\nD\n\n\n(\nx\n)\n\n\n\u2061\n[\nln\n\u2061\ny\n]\n+\n\nE\n\nx\n\u223c\n\n\u03bc\n\nG\n\n\n,\ny\n\u223c\n\n\u03bc\n\nD\n\n\n(\nx\n)\n\n\n\u2061\n[\nln\n\u2061\n(\n1\n\u2212\ny\n)\n]\n.\n\n\n{\\displaystyle \\min _{\\mu _{G}}\\max _{\\mu _{D}}L(\\mu _{G},\\mu _{D}):=\\operatorname {E} _{x\\sim \\mu _{\\text{ref}},y\\sim \\mu _{D}(x)}[\\ln y]+\\operatorname {E} _{x\\sim \\mu _{G},y\\sim \\mu _{D}(x)}[\\ln(1-y)].}\n\n\n If both the generator's and the discriminator's strategy sets are spanned by a finite number of strategies, then by the minimax theorem,\n\n\n\n\nmin\n\n\n\u03bc\n\nG\n\n\n\n\n\nmax\n\n\n\u03bc\n\nD\n\n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\n\u03bc\n\nD\n\n\n)\n=\n\nmax\n\n\n\u03bc\n\nD\n\n\n\n\n\nmin\n\n\n\u03bc\n\nG\n\n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\n\u03bc\n\nD\n\n\n)\n\n\n{\\displaystyle \\min _{\\mu _{G}}\\max _{\\mu _{D}}L(\\mu _{G},\\mu _{D})=\\max _{\\mu _{D}}\\min _{\\mu _{G}}L(\\mu _{G},\\mu _{D})}\n\nthat is, the move order does not matter.\n However, since the strategy sets are both not finitely spanned, the minimax theorem does not apply, and the idea of an \"equilibrium\" becomes delicate. To wit, there are the following different concepts of equilibrium:\n For general games, these equilibria do not have to agree, or even to exist. For the original GAN game, these equilibria all exist, and are all equal. However, for more general GAN games, these do not necessarily exist, or agree.[12]\n \nThe original GAN paper proved the following two theorems:[1] Theorem\u00a0(the optimal discriminator computes the Jensen\u2013Shannon divergence)\u00a0\u2014\u00a0For any fixed generator strategy \n\n\n\n\n\u03bc\n\nG\n\n\n\n\n{\\displaystyle \\mu _{G}}\n\n, let the optimal reply be \n\n\n\n\nD\n\n\u2217\n\n\n=\narg\n\u2061\n\nmax\n\nD\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\nD\n)\n\n\n{\\displaystyle D^{*}=\\arg \\max _{D}L(\\mu _{G},D)}\n\n, then\n \n\n\n\n\n\n\n\n\nD\n\n\u2217\n\n\n(\nx\n)\n\n\n\n=\n\n\n\nd\n\n\u03bc\n\nref\n\n\n\n\nd\n(\n\n\u03bc\n\nref\n\n\n+\n\n\u03bc\n\nG\n\n\n)\n\n\n\n\n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\nD\n\n\u2217\n\n\n)\n\n\n\n=\n2\n\nD\n\nJ\nS\n\n\n(\n\n\u03bc\n\nref\n\n\n;\n\n\u03bc\n\nG\n\n\n)\n\u2212\n2\nln\n\u2061\n2\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}D^{*}(x)&={\\frac {d\\mu _{\\text{ref}}}{d(\\mu _{\\text{ref}}+\\mu _{G})}}\\\\[6pt]L(\\mu _{G},D^{*})&=2D_{JS}(\\mu _{\\text{ref}};\\mu _{G})-2\\ln 2\\end{aligned}}}\n\n\n where the derivative is the Radon\u2013Nikodym derivative, and \n\n\n\n\nD\n\nJ\nS\n\n\n\n\n{\\displaystyle D_{JS}}\n\n is the Jensen\u2013Shannon divergence.\n By Jensen's inequality,\n \n\n\n\n\nE\n\nx\n\u223c\n\n\u03bc\n\nref\n\n\n,\ny\n\u223c\n\n\u03bc\n\nD\n\n\n(\nx\n)\n\n\n\u2061\n[\nln\n\u2061\ny\n]\n\u2264\n\nE\n\nx\n\u223c\n\n\u03bc\n\nref\n\n\n\n\n\u2061\n[\nln\n\u2061\n\nE\n\ny\n\u223c\n\n\u03bc\n\nD\n\n\n(\nx\n)\n\n\n\u2061\n[\ny\n]\n]\n\n\n{\\displaystyle \\operatorname {E} _{x\\sim \\mu _{\\text{ref}},y\\sim \\mu _{D}(x)}[\\ln y]\\leq \\operatorname {E} _{x\\sim \\mu _{\\text{ref}}}[\\ln \\operatorname {E} _{y\\sim \\mu _{D}(x)}[y]]}\n\n\nand similarly for the other term. Therefore, the optimal reply can be deterministic, i.e. \n\n\n\n\n\u03bc\n\nD\n\n\n(\nx\n)\n=\n\n\u03b4\n\nD\n(\nx\n)\n\n\n\n\n{\\displaystyle \\mu _{D}(x)=\\delta _{D(x)}}\n\n for some function \n\n\n\nD\n:\n\u03a9\n\u2192\n[\n0\n,\n1\n]\n\n\n{\\displaystyle D:\\Omega \\to [0,1]}\n\n, in which case\n \n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\n\u03bc\n\nD\n\n\n)\n:=\n\nE\n\nx\n\u223c\n\n\u03bc\n\nref\n\n\n\n\n\u2061\n[\nln\n\u2061\nD\n(\nx\n)\n]\n+\n\nE\n\nx\n\u223c\n\n\u03bc\n\nG\n\n\n\n\n\u2061\n[\nln\n\u2061\n(\n1\n\u2212\nD\n(\nx\n)\n)\n]\n.\n\n\n{\\displaystyle L(\\mu _{G},\\mu _{D}):=\\operatorname {E} _{x\\sim \\mu _{\\text{ref}}}[\\ln D(x)]+\\operatorname {E} _{x\\sim \\mu _{G}}[\\ln(1-D(x))].}\n\n\n To define suitable density functions, we define a base measure \n\n\n\n\u03bc\n:=\n\n\u03bc\n\nref\n\n\n+\n\n\u03bc\n\nG\n\n\n\n\n{\\displaystyle \\mu :=\\mu _{\\text{ref}}+\\mu _{G}}\n\n, which allows us to take the Radon\u2013Nikodym derivatives\n \n\n\n\n\n\u03c1\n\nref\n\n\n=\n\n\n\nd\n\n\u03bc\n\nref\n\n\n\n\nd\n\u03bc\n\n\n\n\n\n\u03c1\n\nG\n\n\n=\n\n\n\nd\n\n\u03bc\n\nG\n\n\n\n\nd\n\u03bc\n\n\n\n\n\n{\\displaystyle \\rho _{\\text{ref}}={\\frac {d\\mu _{\\text{ref}}}{d\\mu }}\\quad \\rho _{G}={\\frac {d\\mu _{G}}{d\\mu }}}\n\n\nwith \n\n\n\n\n\u03c1\n\nref\n\n\n+\n\n\u03c1\n\nG\n\n\n=\n1\n\n\n{\\displaystyle \\rho _{\\text{ref}}+\\rho _{G}=1}\n\n.\n We then have\n \n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\n\u03bc\n\nD\n\n\n)\n:=\n\u222b\n\u03bc\n(\nd\nx\n)\n\n[\n\n\n\u03c1\n\nref\n\n\n(\nx\n)\nln\n\u2061\n(\nD\n(\nx\n)\n)\n+\n\n\u03c1\n\nG\n\n\n(\nx\n)\nln\n\u2061\n(\n1\n\u2212\nD\n(\nx\n)\n)\n\n]\n\n.\n\n\n{\\displaystyle L(\\mu _{G},\\mu _{D}):=\\int \\mu (dx)\\left[\\rho _{\\text{ref}}(x)\\ln(D(x))+\\rho _{G}(x)\\ln(1-D(x))\\right].}\n\n\n The integrand is just the negative cross-entropy between two Bernoulli random variables with parameters \n\n\n\n\n\u03c1\n\nref\n\n\n(\nx\n)\n\n\n{\\displaystyle \\rho _{\\text{ref}}(x)}\n\n and \n\n\n\nD\n(\nx\n)\n\n\n{\\displaystyle D(x)}\n\n. We can write this as \n\n\n\n\u2212\nH\n(\n\n\u03c1\n\nref\n\n\n(\nx\n)\n)\n\u2212\n\nD\n\nK\nL\n\n\n(\n\n\u03c1\n\nref\n\n\n(\nx\n)\n\u2225\nD\n(\nx\n)\n)\n\n\n{\\displaystyle -H(\\rho _{\\text{ref}}(x))-D_{KL}(\\rho _{\\text{ref}}(x)\\parallel D(x))}\n\n, where \n\n\n\nH\n\n\n{\\displaystyle H}\n\n is the binary entropy function, so\n \n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\n\u03bc\n\nD\n\n\n)\n=\n\u2212\n\u222b\n\u03bc\n(\nd\nx\n)\n(\nH\n(\n\n\u03c1\n\nref\n\n\n(\nx\n)\n)\n+\n\nD\n\nK\nL\n\n\n(\n\n\u03c1\n\nref\n\n\n(\nx\n)\n\u2225\nD\n(\nx\n)\n)\n)\n.\n\n\n{\\displaystyle L(\\mu _{G},\\mu _{D})=-\\int \\mu (dx)(H(\\rho _{\\text{ref}}(x))+D_{KL}(\\rho _{\\text{ref}}(x)\\parallel D(x))).}\n\n\n This means that the optimal strategy for the discriminator is \n\n\n\nD\n(\nx\n)\n=\n\n\u03c1\n\nref\n\n\n(\nx\n)\n\n\n{\\displaystyle D(x)=\\rho _{\\text{ref}}(x)}\n\n, with   \n\n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\n\u03bc\n\nD\n\n\n\u2217\n\n\n)\n=\n\u2212\n\u222b\n\u03bc\n(\nd\nx\n)\nH\n(\n\n\u03c1\n\nref\n\n\n(\nx\n)\n)\n=\n\nD\n\nJ\nS\n\n\n(\n\n\u03bc\n\nref\n\n\n\u2225\n\n\u03bc\n\nG\n\n\n)\n\u2212\n2\nln\n\u2061\n2\n\n\n{\\displaystyle L(\\mu _{G},\\mu _{D}^{*})=-\\int \\mu (dx)H(\\rho _{\\text{ref}}(x))=D_{JS}(\\mu _{\\text{ref}}\\parallel \\mu _{G})-2\\ln 2}\n\n\n after routine calculation.\n Interpretation: For any fixed generator strategy \n\n\n\n\n\u03bc\n\nG\n\n\n\n\n{\\displaystyle \\mu _{G}}\n\n, the optimal discriminator keeps track of the likelihood ratio between the reference distribution and the generator distribution:\n\n\n\n\n\n\nD\n(\nx\n)\n\n\n1\n\u2212\nD\n(\nx\n)\n\n\n\n=\n\n\n\nd\n\n\u03bc\n\nref\n\n\n\n\nd\n\n\u03bc\n\nG\n\n\n\n\n\n(\nx\n)\n=\n\n\n\n\n\u03bc\n\nref\n\n\n(\nd\nx\n)\n\n\n\n\u03bc\n\nG\n\n\n(\nd\nx\n)\n\n\n\n;\n\nD\n(\nx\n)\n=\n\u03c3\n(\nln\n\u2061\n\n\u03bc\n\nref\n\n\n(\nd\nx\n)\n\u2212\nln\n\u2061\n\n\u03bc\n\nG\n\n\n(\nd\nx\n)\n)\n\n\n{\\displaystyle {\\frac {D(x)}{1-D(x)}}={\\frac {d\\mu _{\\text{ref}}}{d\\mu _{G}}}(x)={\\frac {\\mu _{\\text{ref}}(dx)}{\\mu _{G}(dx)}};\\quad D(x)=\\sigma (\\ln \\mu _{\\text{ref}}(dx)-\\ln \\mu _{G}(dx))}\n\nwhere \n\n\n\n\u03c3\n\n\n{\\displaystyle \\sigma }\n\n is the logistic function.\nIn particular, if the prior probability for an image \n\n\n\nx\n\n\n{\\displaystyle x}\n\n to come from the reference distribution is equal to \n\n\n\n\n\n1\n2\n\n\n\n\n{\\displaystyle {\\frac {1}{2}}}\n\n, then \n\n\n\nD\n(\nx\n)\n\n\n{\\displaystyle D(x)}\n\n is just the posterior probability that \n\n\n\nx\n\n\n{\\displaystyle x}\n\n came from the reference distribution:\n\n\n\nD\n(\nx\n)\n=\nPr\n(\nx\n\n\u00a0came from reference distribution\n\n\u2223\nx\n)\n.\n\n\n{\\displaystyle D(x)=\\Pr(x{\\text{ came from reference distribution}}\\mid x).}\n\n\n Theorem\u00a0(the unique equilibrium point)\u00a0\u2014\u00a0For any GAN game, there exists a pair \n\n\n\n(\n\n\n\n\n\u03bc\n^\n\n\n\n\nD\n\n\n,\n\n\n\n\n\u03bc\n^\n\n\n\n\nG\n\n\n)\n\n\n{\\displaystyle ({\\hat {\\mu }}_{D},{\\hat {\\mu }}_{G})}\n\n that is both a sequential equilibrium and a Nash equilibrium:\n \n\n\n\n\n\n\n\n\nL\n(\n\n\n\n\n\u03bc\n^\n\n\n\n\nG\n\n\n,\n\n\n\n\n\u03bc\n^\n\n\n\n\nD\n\n\n)\n=\n\nmin\n\n\n\u03bc\n\nG\n\n\n\n\n\nmax\n\n\n\u03bc\n\nD\n\n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\n\u03bc\n\nD\n\n\n)\n=\n\n\n\nmax\n\n\n\u03bc\n\nD\n\n\n\n\n\nmin\n\n\n\u03bc\n\nG\n\n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\n\u03bc\n\nD\n\n\n)\n=\n\u2212\n2\nln\n\u2061\n2\n\n\n\n\n\n\n\n\n\n\u03bc\n^\n\n\n\n\nD\n\n\n\u2208\narg\n\u2061\n\nmax\n\n\n\u03bc\n\nD\n\n\n\n\n\nmin\n\n\n\u03bc\n\nG\n\n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\n\u03bc\n\nD\n\n\n)\n,\n\n\n\n\n\n\n\n\u03bc\n^\n\n\n\n\nG\n\n\n\u2208\narg\n\u2061\n\nmin\n\n\n\u03bc\n\nG\n\n\n\n\n\nmax\n\n\n\u03bc\n\nD\n\n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\n\u03bc\n\nD\n\n\n)\n\n\n\n\n\n\n\n\n\n\u03bc\n^\n\n\n\n\nD\n\n\n\u2208\narg\n\u2061\n\nmax\n\n\n\u03bc\n\nD\n\n\n\n\nL\n(\n\n\n\n\n\u03bc\n^\n\n\n\n\nG\n\n\n,\n\n\u03bc\n\nD\n\n\n)\n,\n\n\n\n\n\n\n\n\u03bc\n^\n\n\n\n\nG\n\n\n\u2208\narg\n\u2061\n\nmin\n\n\n\u03bc\n\nG\n\n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\n\n\n\n\u03bc\n^\n\n\n\n\nD\n\n\n)\n\n\n\n\n\n\u2200\nx\n\u2208\n\u03a9\n,\n\n\n\n\n\u03bc\n^\n\n\n\n\nD\n\n\n(\nx\n)\n=\n\n\u03b4\n\n\n1\n2\n\n\n\n,\n\n\n\n\n\n\n\n\u03bc\n^\n\n\n\n\nG\n\n\n=\n\n\u03bc\n\nref\n\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}&L({\\hat {\\mu }}_{G},{\\hat {\\mu }}_{D})=\\min _{\\mu _{G}}\\max _{\\mu _{D}}L(\\mu _{G},\\mu _{D})=&\\max _{\\mu _{D}}\\min _{\\mu _{G}}L(\\mu _{G},\\mu _{D})=-2\\ln 2\\\\[6pt]&{\\hat {\\mu }}_{D}\\in \\arg \\max _{\\mu _{D}}\\min _{\\mu _{G}}L(\\mu _{G},\\mu _{D}),&\\quad {\\hat {\\mu }}_{G}\\in \\arg \\min _{\\mu _{G}}\\max _{\\mu _{D}}L(\\mu _{G},\\mu _{D})\\\\[6pt]&{\\hat {\\mu }}_{D}\\in \\arg \\max _{\\mu _{D}}L({\\hat {\\mu }}_{G},\\mu _{D}),&\\quad {\\hat {\\mu }}_{G}\\in \\arg \\min _{\\mu _{G}}L(\\mu _{G},{\\hat {\\mu }}_{D})\\\\[6pt]&\\forall x\\in \\Omega ,{\\hat {\\mu }}_{D}(x)=\\delta _{\\frac {1}{2}},&\\quad {\\hat {\\mu }}_{G}=\\mu _{\\text{ref}}\\end{aligned}}}\n\n\n That is, the generator perfectly mimics the reference, and the discriminator outputs \n\n\n\n\n\n1\n2\n\n\n\n\n{\\displaystyle {\\frac {1}{2}}}\n\n deterministically on all inputs.\n From the previous proposition,\n \n\n\n\narg\n\u2061\n\nmin\n\n\n\u03bc\n\nG\n\n\n\n\n\nmax\n\n\n\u03bc\n\nD\n\n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\n\u03bc\n\nD\n\n\n)\n=\n\n\u03bc\n\nref\n\n\n;\n\n\nmin\n\n\n\u03bc\n\nG\n\n\n\n\n\nmax\n\n\n\u03bc\n\nD\n\n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\n\u03bc\n\nD\n\n\n)\n=\n\u2212\n2\nln\n\u2061\n2.\n\n\n{\\displaystyle \\arg \\min _{\\mu _{G}}\\max _{\\mu _{D}}L(\\mu _{G},\\mu _{D})=\\mu _{\\text{ref}};\\quad \\min _{\\mu _{G}}\\max _{\\mu _{D}}L(\\mu _{G},\\mu _{D})=-2\\ln 2.}\n\n\n For any fixed discriminator strategy \n\n\n\n\n\u03bc\n\nD\n\n\n\n\n{\\displaystyle \\mu _{D}}\n\n, any \n\n\n\n\n\u03bc\n\nG\n\n\n\n\n{\\displaystyle \\mu _{G}}\n\n concentrated on the set\n \n\n\n\n{\nx\n\u2223\n\nE\n\ny\n\u223c\n\n\u03bc\n\nD\n\n\n(\nx\n)\n\n\n\u2061\n[\nln\n\u2061\n(\n1\n\u2212\ny\n)\n]\n=\n\ninf\n\nx\n\n\n\nE\n\ny\n\u223c\n\n\u03bc\n\nD\n\n\n(\nx\n)\n\n\n\u2061\n[\nln\n\u2061\n(\n1\n\u2212\ny\n)\n]\n}\n\n\n{\\displaystyle \\{x\\mid \\operatorname {E} _{y\\sim \\mu _{D}(x)}[\\ln(1-y)]=\\inf _{x}\\operatorname {E} _{y\\sim \\mu _{D}(x)}[\\ln(1-y)]\\}}\n\n\nis an optimal strategy for the generator. Thus,\n \n\n\n\narg\n\u2061\n\nmax\n\n\n\u03bc\n\nD\n\n\n\n\n\nmin\n\n\n\u03bc\n\nG\n\n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\n\u03bc\n\nD\n\n\n)\n=\narg\n\u2061\n\nmax\n\n\n\u03bc\n\nD\n\n\n\n\n\nE\n\nx\n\u223c\n\n\u03bc\n\nref\n\n\n,\ny\n\u223c\n\n\u03bc\n\nD\n\n\n(\nx\n)\n\n\n\u2061\n[\nln\n\u2061\ny\n]\n+\n\ninf\n\nx\n\n\n\nE\n\ny\n\u223c\n\n\u03bc\n\nD\n\n\n(\nx\n)\n\n\n\u2061\n[\nln\n\u2061\n(\n1\n\u2212\ny\n)\n]\n.\n\n\n{\\displaystyle \\arg \\max _{\\mu _{D}}\\min _{\\mu _{G}}L(\\mu _{G},\\mu _{D})=\\arg \\max _{\\mu _{D}}\\operatorname {E} _{x\\sim \\mu _{\\text{ref}},y\\sim \\mu _{D}(x)}[\\ln y]+\\inf _{x}\\operatorname {E} _{y\\sim \\mu _{D}(x)}[\\ln(1-y)].}\n\n\n By Jensen's inequality, the discriminator can only improve by adopting the deterministic strategy of always playing \n\n\n\nD\n(\nx\n)\n=\n\nE\n\ny\n\u223c\n\n\u03bc\n\nD\n\n\n(\nx\n)\n\n\n\u2061\n[\ny\n]\n\n\n{\\displaystyle D(x)=\\operatorname {E} _{y\\sim \\mu _{D}(x)}[y]}\n\n. Therefore,\n \n\n\n\narg\n\u2061\n\nmax\n\n\n\u03bc\n\nD\n\n\n\n\n\nmin\n\n\n\u03bc\n\nG\n\n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\n\u03bc\n\nD\n\n\n)\n=\narg\n\u2061\n\nmax\n\nD\n\n\n\nE\n\nx\n\u223c\n\n\u03bc\n\nref\n\n\n\n\n\u2061\n[\nln\n\u2061\nD\n(\nx\n)\n]\n+\n\ninf\n\nx\n\n\nln\n\u2061\n(\n1\n\u2212\nD\n(\nx\n)\n)\n\n\n{\\displaystyle \\arg \\max _{\\mu _{D}}\\min _{\\mu _{G}}L(\\mu _{G},\\mu _{D})=\\arg \\max _{D}\\operatorname {E} _{x\\sim \\mu _{\\text{ref}}}[\\ln D(x)]+\\inf _{x}\\ln(1-D(x))}\n\n\n By Jensen's inequality,\n \n\n\n\n\n\n\n\n\nln\n\u2061\n\nE\n\nx\n\u223c\n\n\u03bc\n\nref\n\n\n\n\n\u2061\n[\nD\n(\nx\n)\n]\n+\n\ninf\n\nx\n\n\nln\n\u2061\n(\n1\n\u2212\nD\n(\nx\n)\n)\n\n\n\n\n=\n\n\n\n\nln\n\u2061\n\nE\n\nx\n\u223c\n\n\u03bc\n\nref\n\n\n\n\n\u2061\n[\nD\n(\nx\n)\n]\n+\nln\n\u2061\n(\n1\n\u2212\n\nsup\n\nx\n\n\nD\n(\nx\n)\n)\n\n\n\n\n=\n\n\n\n\nln\n\u2061\n[\n\nE\n\nx\n\u223c\n\n\u03bc\n\nref\n\n\n\n\n\u2061\n[\nD\n(\nx\n)\n]\n(\n1\n\u2212\n\nsup\n\nx\n\n\nD\n(\nx\n)\n)\n]\n\u2264\nln\n\u2061\n[\n\nsup\n\nx\n\n\nD\n(\nx\n)\n)\n(\n1\n\u2212\n\nsup\n\nx\n\n\nD\n(\nx\n)\n)\n]\n\u2264\nln\n\u2061\n\n\n1\n4\n\n\n,\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}&\\ln \\operatorname {E} _{x\\sim \\mu _{\\text{ref}}}[D(x)]+\\inf _{x}\\ln(1-D(x))\\\\[6pt]={}&\\ln \\operatorname {E} _{x\\sim \\mu _{\\text{ref}}}[D(x)]+\\ln(1-\\sup _{x}D(x))\\\\[6pt]={}&\\ln[\\operatorname {E} _{x\\sim \\mu _{\\text{ref}}}[D(x)](1-\\sup _{x}D(x))]\\leq \\ln[\\sup _{x}D(x))(1-\\sup _{x}D(x))]\\leq \\ln {\\frac {1}{4}},\\end{aligned}}}\n\n\n with equality if \n\n\n\nD\n(\nx\n)\n=\n\n\n1\n2\n\n\n\n\n{\\displaystyle D(x)={\\frac {1}{2}}}\n\n, so\n \n\n\n\n\u2200\nx\n\u2208\n\u03a9\n,\n\n\n\n\n\u03bc\n^\n\n\n\n\nD\n\n\n(\nx\n)\n=\n\n\u03b4\n\n\n1\n2\n\n\n\n;\n\n\nmax\n\n\n\u03bc\n\nD\n\n\n\n\n\nmin\n\n\n\u03bc\n\nG\n\n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\n\u03bc\n\nD\n\n\n)\n=\n\u2212\n2\nln\n\u2061\n2.\n\n\n{\\displaystyle \\forall x\\in \\Omega ,{\\hat {\\mu }}_{D}(x)=\\delta _{\\frac {1}{2}};\\quad \\max _{\\mu _{D}}\\min _{\\mu _{G}}L(\\mu _{G},\\mu _{D})=-2\\ln 2.}\n\n\n Finally, to check that this is a Nash equilibrium, note that when \n\n\n\n\n\u03bc\n\nG\n\n\n=\n\n\u03bc\n\nref\n\n\n\n\n{\\displaystyle \\mu _{G}=\\mu _{\\text{ref}}}\n\n, we have\n \n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\n\n\u03bc\n\nD\n\n\n)\n:=\n\nE\n\nx\n\u223c\n\n\u03bc\n\nref\n\n\n,\ny\n\u223c\n\n\u03bc\n\nD\n\n\n(\nx\n)\n\n\n\u2061\n[\nln\n\u2061\n(\ny\n(\n1\n\u2212\ny\n)\n)\n]\n\n\n{\\displaystyle L(\\mu _{G},\\mu _{D}):=\\operatorname {E} _{x\\sim \\mu _{\\text{ref}},y\\sim \\mu _{D}(x)}[\\ln(y(1-y))]}\n\n\nwhich is always maximized by \n\n\n\ny\n=\n\n\n1\n2\n\n\n\n\n{\\displaystyle y={\\frac {1}{2}}}\n\n.\n When \n\n\n\n\u2200\nx\n\u2208\n\u03a9\n,\n\n\u03bc\n\nD\n\n\n(\nx\n)\n=\n\n\u03b4\n\n\n1\n2\n\n\n\n\n\n{\\displaystyle \\forall x\\in \\Omega ,\\mu _{D}(x)=\\delta _{\\frac {1}{2}}}\n\n, any strategy is optimal for the generator.\n While the GAN game has a unique global equilibrium point when both the generator and discriminator have access to their entire strategy sets, the equilibrium is no longer guaranteed when they have a restricted strategy set.[12]\n In practice, the generator has access only to measures of form \n\n\n\n\n\u03bc\n\nZ\n\n\n\u2218\n\nG\n\n\u03b8\n\n\n\u2212\n1\n\n\n\n\n{\\displaystyle \\mu _{Z}\\circ G_{\\theta }^{-1}}\n\n, where \n\n\n\n\nG\n\n\u03b8\n\n\n\n\n{\\displaystyle G_{\\theta }}\n\n is a function computed by a neural network with parameters \n\n\n\n\u03b8\n\n\n{\\displaystyle \\theta }\n\n, and \n\n\n\n\n\u03bc\n\nZ\n\n\n\n\n{\\displaystyle \\mu _{Z}}\n\n is an easily sampled distribution, such as the uniform or normal distribution. Similarly, the discriminator has access only to functions of form \n\n\n\n\nD\n\n\u03b6\n\n\n\n\n{\\displaystyle D_{\\zeta }}\n\n, a function computed by a neural network with parameters \n\n\n\n\u03b6\n\n\n{\\displaystyle \\zeta }\n\n. These restricted strategy sets take up a vanishingly small proportion of their entire strategy sets.[13]\n Further, even if an equilibrium still exists, it can only be found by searching in the high-dimensional space of all possible neural network functions. The standard strategy of using gradient descent to find the equilibrium often does not work for GAN, and often the game \"collapses\" into one of several failure modes. To improve the convergence stability, some training strategies start with an easier task, such as generating low-resolution images[14] or simple images (one object with uniform background),[15] and gradually increase the difficulty of the task during training. This essentially translates to applying a curriculum learning scheme.[16]\n GANs often suffer from mode collapse where they fail to generalize properly, missing entire modes from the input data. For example, a GAN trained on the MNIST dataset containing many samples of each digit might only generate pictures of digit 0. This was termed \"the Helvetica scenario\".[1]\n One way this can happen is if the generator learns too fast compared to the discriminator. If the discriminator \n\n\n\nD\n\n\n{\\displaystyle D}\n\n is held constant, then the optimal generator would only output elements of \n\n\n\narg\n\u2061\n\nmax\n\nx\n\n\nD\n(\nx\n)\n\n\n{\\displaystyle \\arg \\max _{x}D(x)}\n\n.[17] So for example, if during GAN training for generating MNIST dataset, for a few epochs, the discriminator somehow prefers the digit 0 slightly more than other digits, the generator may seize the opportunity to generate only digit 0, then be unable to escape the local minimum after the discriminator improves.\n Some researchers perceive the root problem to be a weak discriminative network that fails to notice the pattern of omission, while others assign blame to a bad choice of objective function. Many solutions have been proposed, but it is still an open problem.[18][19]\n Even the state-of-the-art architecture, BigGAN (2019), could not avoid mode collapse. The authors resorted to \"allowing collapse to occur at the later stages of training, by which time a model is sufficiently trained to achieve good results\".[20]\n The two time-scale update rule (TTUR) is proposed to make GAN convergence more stable by making the learning rate of the generator lower than that of the discriminator. The authors argued that the generator should move slower than the discriminator, so that it does not \"drive the discriminator steadily into new regions without capturing its gathered information\".\n They proved that a general class of games that included the GAN game, when trained under TTUR, \"converges under mild assumptions to a stationary local Nash equilibrium\".[21]\n They also proposed using the Adam stochastic optimization[22] to avoid mode collapse, as well as the Fr\u00e9chet inception distance for evaluating GAN performances.\n Conversely, if the discriminator learns too fast compared to the generator, then the discriminator could almost perfectly distinguish \n\n\n\n\n\u03bc\n\n\nG\n\n\u03b8\n\n\n\n\n,\n\n\u03bc\n\nref\n\n\n\n\n{\\displaystyle \\mu _{G_{\\theta }},\\mu _{\\text{ref}}}\n\n. In such case, the generator \n\n\n\n\nG\n\n\u03b8\n\n\n\n\n{\\displaystyle G_{\\theta }}\n\n could be stuck with a very high loss no matter which direction it changes its \n\n\n\n\u03b8\n\n\n{\\displaystyle \\theta }\n\n, meaning that the gradient \n\n\n\n\n\u2207\n\n\u03b8\n\n\nL\n(\n\nG\n\n\u03b8\n\n\n,\n\nD\n\n\u03b6\n\n\n)\n\n\n{\\displaystyle \\nabla _{\\theta }L(G_{\\theta },D_{\\zeta })}\n\n would be close to zero. In such case, the generator cannot learn, a case of the vanishing gradient problem.[13]\n Intuitively speaking, the discriminator is too good, and since the generator cannot take any small step (only small steps are considered in gradient descent) to improve its payoff, it does not even try.\n One important method for solving this problem is the Wasserstein GAN.\n GANs are usually evaluated by Inception score (IS), which measures how varied the generator's outputs are (as classified by an image classifier, usually Inception-v3), or Fr\u00e9chet inception distance (FID), which measures how similar the generator's outputs are to a reference set (as classified by a learned image featurizer, such as Inception-v3 without its final layer). Many papers that propose new GAN architectures for image generation report how their architectures break the state of the art on FID or IS.\n Another evaluation method is the Learned Perceptual Image Patch Similarity (LPIPS), which starts with a learned image featurizer \n\n\n\n\nf\n\n\u03b8\n\n\n:\n\nImage\n\n\u2192\n\n\nR\n\n\nn\n\n\n\n\n{\\displaystyle f_{\\theta }:{\\text{Image}}\\to \\mathbb {R} ^{n}}\n\n, and finetunes it by supervised learning on a set of \n\n\n\n(\nx\n,\n\nx\n\u2032\n\n,\n\np\ne\nr\nc\ne\np\nt\nu\na\nl\n\u00a0\nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\n\n\u2061\n(\nx\n,\n\nx\n\u2032\n\n)\n)\n\n\n{\\displaystyle (x,x',\\operatorname {perceptual~difference} (x,x'))}\n\n, where \n\n\n\nx\n\n\n{\\displaystyle x}\n\n is an image, \n\n\n\n\nx\n\u2032\n\n\n\n{\\displaystyle x'}\n\n is a perturbed version of it, and \n\n\n\n\np\ne\nr\nc\ne\np\nt\nu\na\nl\n\u00a0\nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\n\n\u2061\n(\nx\n,\n\nx\n\u2032\n\n)\n\n\n{\\displaystyle \\operatorname {perceptual~difference} (x,x')}\n\n is how much they differ, as reported by human subjects. The model is finetuned so that it can approximate \n\n\n\n\u2016\n\nf\n\n\u03b8\n\n\n(\nx\n)\n\u2212\n\nf\n\n\u03b8\n\n\n(\n\nx\n\u2032\n\n)\n\u2016\n\u2248\n\np\ne\nr\nc\ne\np\nt\nu\na\nl\n\u00a0\nd\ni\nf\nf\ne\nr\ne\nn\nc\ne\n\n\u2061\n(\nx\n,\n\nx\n\u2032\n\n)\n\n\n{\\displaystyle \\|f_{\\theta }(x)-f_{\\theta }(x')\\|\\approx \\operatorname {perceptual~difference} (x,x')}\n\n. This finetuned model is then used to define \n\n\n\nLPIPS\n\u2061\n(\nx\n,\n\nx\n\u2032\n\n)\n:=\n\u2016\n\nf\n\n\u03b8\n\n\n(\nx\n)\n\u2212\n\nf\n\n\u03b8\n\n\n(\n\nx\n\u2032\n\n)\n\u2016\n\n\n{\\displaystyle \\operatorname {LPIPS} (x,x'):=\\|f_{\\theta }(x)-f_{\\theta }(x')\\|}\n\n.[23]\n Other evaluation methods are reviewed in.[24]\n There is a veritable zoo of GAN variants.[25] Some of the most prominent are as follows:\n Conditional GANs are similar to standard GANs except they allow the model to conditionally generate samples based on additional information. For example, if we want to generate a cat face given a dog picture, we could use a conditional GAN.\n The generator in a GAN game generates \n\n\n\n\n\u03bc\n\nG\n\n\n\n\n{\\displaystyle \\mu _{G}}\n\n, a probability distribution on the probability space \n\n\n\n\u03a9\n\n\n{\\displaystyle \\Omega }\n\n. This leads to the idea of a conditional GAN, where instead of generating one probability distribution on \n\n\n\n\u03a9\n\n\n{\\displaystyle \\Omega }\n\n, the generator generates a different probability distribution \n\n\n\n\n\u03bc\n\nG\n\n\n(\nc\n)\n\n\n{\\displaystyle \\mu _{G}(c)}\n\n on \n\n\n\n\u03a9\n\n\n{\\displaystyle \\Omega }\n\n, for each given class label \n\n\n\nc\n\n\n{\\displaystyle c}\n\n.\n For example, for generating images that look like ImageNet, the generator should be able to generate a picture of cat when given the class label \"cat\".\n In the original paper,[1] the authors noted that GAN can be trivially extended to conditional GAN by providing the labels to both the generator and the discriminator.\n Concretely, the conditional GAN game is just the GAN game with class labels provided:\n\n\n\nL\n(\n\n\u03bc\n\nG\n\n\n,\nD\n)\n:=\n\nE\n\nc\n\u223c\n\n\u03bc\n\nC\n\n\n,\nx\n\u223c\n\n\u03bc\n\nref\n\n\n(\nc\n)\n\n\n\u2061\n[\nln\n\u2061\nD\n(\nx\n,\nc\n)\n]\n+\n\nE\n\nc\n\u223c\n\n\u03bc\n\nC\n\n\n,\nx\n\u223c\n\n\u03bc\n\nG\n\n\n(\nc\n)\n\n\n\u2061\n[\nln\n\u2061\n(\n1\n\u2212\nD\n(\nx\n,\nc\n)\n)\n]\n\n\n{\\displaystyle L(\\mu _{G},D):=\\operatorname {E} _{c\\sim \\mu _{C},x\\sim \\mu _{\\text{ref}}(c)}[\\ln D(x,c)]+\\operatorname {E} _{c\\sim \\mu _{C},x\\sim \\mu _{G}(c)}[\\ln(1-D(x,c))]}\n\nwhere \n\n\n\n\n\u03bc\n\nC\n\n\n\n\n{\\displaystyle \\mu _{C}}\n\n is a probability distribution over classes, \n\n\n\n\n\u03bc\n\nref\n\n\n(\nc\n)\n\n\n{\\displaystyle \\mu _{\\text{ref}}(c)}\n\n is the probability distribution of real images of class \n\n\n\nc\n\n\n{\\displaystyle c}\n\n, and \n\n\n\n\n\u03bc\n\nG\n\n\n(\nc\n)\n\n\n{\\displaystyle \\mu _{G}(c)}\n\n the probability distribution of images generated by the generator when given class label \n\n\n\nc\n\n\n{\\displaystyle c}\n\n.\n In 2017, a conditional GAN learned to generate 1000 image classes of ImageNet.[26]\n The GAN game is a general framework and can be run with any reasonable parametrization of the generator \n\n\n\nG\n\n\n{\\displaystyle G}\n\n and discriminator \n\n\n\nD\n\n\n{\\displaystyle D}\n\n. In the original paper, the authors demonstrated it using multilayer perceptron networks and convolutional neural networks. Many alternative architectures have been tried.\n Deep convolutional GAN (DCGAN):[27] For both generator and discriminator, uses only deep networks consisting entirely of convolution-deconvolution layers, that is, fully convolutional networks.[28]\n Self-attention GAN (SAGAN):[29] Starts with the DCGAN, then adds residually-connected standard self-attention modules to the generator and discriminator.\n Variational autoencoder GAN (VAEGAN):[30] Uses a variational autoencoder (VAE) for the generator.\n Transformer GAN (TransGAN):[31] Uses the pure transformer architecture for both the generator and discriminator, entirely devoid of convolution-deconvolution layers.\n Flow-GAN:[32] Uses flow-based generative model for the generator, allowing efficient computation of the likelihood function.\n Many GAN variants are merely obtained by changing the loss functions for the generator and discriminator.\n Original GAN:\n We recast the original GAN objective into a form more convenient for comparison:\n\n\n\n\n\n{\n\n\n\n\nmin\n\nD\n\n\n\nL\n\nD\n\n\n(\nD\n,\n\n\u03bc\n\nG\n\n\n)\n=\n\u2212\n\nE\n\nx\n\u223c\n\n\u03bc\n\nG\n\n\n\n\n\u2061\n[\nln\n\u2061\nD\n(\nx\n)\n]\n\u2212\n\nE\n\nx\n\u223c\n\n\u03bc\n\nref\n\n\n\n\n\u2061\n[\nln\n\u2061\n(\n1\n\u2212\nD\n(\nx\n)\n)\n]\n\n\n\n\n\nmin\n\nG\n\n\n\nL\n\nG\n\n\n(\nD\n,\n\n\u03bc\n\nG\n\n\n)\n=\n\u2212\n\nE\n\nx\n\u223c\n\n\u03bc\n\nG\n\n\n\n\n\u2061\n[\nln\n\u2061\n(\n1\n\u2212\nD\n(\nx\n)\n)\n]\n\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{cases}\\min _{D}L_{D}(D,\\mu _{G})=-\\operatorname {E} _{x\\sim \\mu _{G}}[\\ln D(x)]-\\operatorname {E} _{x\\sim \\mu _{\\text{ref}}}[\\ln(1-D(x))]\\\\\\min _{G}L_{G}(D,\\mu _{G})=-\\operatorname {E} _{x\\sim \\mu _{G}}[\\ln(1-D(x))]\\end{cases}}}\n\n\n Original GAN, non-saturating loss:\n This objective for generator was recommended in the original paper for faster convergence.[1]\n\n\n\n\nL\n\nG\n\n\n=\n\nE\n\nx\n\u223c\n\n\u03bc\n\nG\n\n\n\n\n\u2061\n[\nln\n\u2061\nD\n(\nx\n)\n]\n\n\n{\\displaystyle L_{G}=\\operatorname {E} _{x\\sim \\mu _{G}}[\\ln D(x)]}\n\nThe effect of using this objective is analyzed in Section 2.2.2 of Arjovsky et al.[33]\n Original GAN, maximum likelihood:\n \n\n\n\n\nL\n\nG\n\n\n=\n\nE\n\nx\n\u223c\n\n\u03bc\n\nG\n\n\n\n\n\u2061\n[\n(\n\nexp\n\n\u2218\n\n\u03c3\n\n\u2212\n1\n\n\n\u2218\nD\n)\n(\nx\n)\n]\n\n\n{\\displaystyle L_{G}=\\operatorname {E} _{x\\sim \\mu _{G}}[({\\exp }\\circ \\sigma ^{-1}\\circ D)(x)]}\n\nwhere \n\n\n\n\u03c3\n\n\n{\\displaystyle \\sigma }\n\n is the logistic function. When the discriminator is optimal, the generator gradient is the same as in maximum likelihood estimation, even though GAN cannot perform maximum likelihood estimation itself.[34][35]\n Hinge loss GAN:[36]\n\n\n\n\nL\n\nD\n\n\n=\n\u2212\n\nE\n\nx\n\u223c\n\np\n\nref\n\n\n\n\n\u2061\n\n[\n\nmin\n\n(\n\n0\n,\n\u2212\n1\n+\nD\n(\nx\n)\n\n)\n\n\n]\n\n\u2212\n\nE\n\nx\n\u223c\n\n\u03bc\n\nG\n\n\n\n\n\u2061\n\n[\n\nmin\n\n(\n\n0\n,\n\u2212\n1\n\u2212\nD\n\n(\nx\n)\n\n\n)\n\n\n]\n\n\n\n{\\displaystyle L_{D}=-\\operatorname {E} _{x\\sim p_{\\text{ref}}}\\left[\\min \\left(0,-1+D(x)\\right)\\right]-\\operatorname {E} _{x\\sim \\mu _{G}}\\left[\\min \\left(0,-1-D\\left(x\\right)\\right)\\right]}\n\n\n\n\n\n\nL\n\nG\n\n\n=\n\u2212\n\nE\n\nx\n\u223c\n\n\u03bc\n\nG\n\n\n\n\n\u2061\n[\nD\n(\nx\n)\n]\n\n\n{\\displaystyle L_{G}=-\\operatorname {E} _{x\\sim \\mu _{G}}[D(x)]}\n\nLeast squares GAN:[37]\n\n\n\n\nL\n\nD\n\n\n=\n\nE\n\nx\n\u223c\n\n\u03bc\n\nref\n\n\n\n\n\u2061\n[\n(\nD\n(\nx\n)\n\u2212\nb\n\n)\n\n2\n\n\n]\n+\n\nE\n\nx\n\u223c\n\n\u03bc\n\nG\n\n\n\n\n\u2061\n[\n(\nD\n(\nx\n)\n\u2212\na\n\n)\n\n2\n\n\n]\n\n\n{\\displaystyle L_{D}=\\operatorname {E} _{x\\sim \\mu _{\\text{ref}}}[(D(x)-b)^{2}]+\\operatorname {E} _{x\\sim \\mu _{G}}[(D(x)-a)^{2}]}\n\n\n\n\n\n\nL\n\nG\n\n\n=\n\nE\n\nx\n\u223c\n\n\u03bc\n\nG\n\n\n\n\n\u2061\n[\n(\nD\n(\nx\n)\n\u2212\nc\n\n)\n\n2\n\n\n]\n\n\n{\\displaystyle L_{G}=\\operatorname {E} _{x\\sim \\mu _{G}}[(D(x)-c)^{2}]}\n\nwhere \n\n\n\na\n,\nb\n,\nc\n\n\n{\\displaystyle a,b,c}\n\n are parameters to be chosen. The authors recommended \n\n\n\na\n=\n\u2212\n1\n,\nb\n=\n1\n,\nc\n=\n0\n\n\n{\\displaystyle a=-1,b=1,c=0}\n\n.\n The Wasserstein GAN modifies the GAN game at two points:\n One of its purposes is to solve the problem of mode collapse (see above).[13] The authors claim \"In no experiment did we see evidence of mode collapse for the WGAN algorithm\".\n An adversarial autoencoder (AAE)[38] is more autoencoder than GAN. The idea is to start with a plain autoencoder, but train a discriminator to discriminate the latent vectors from a reference distribution (often the normal distribution).\n In conditional GAN, the generator receives both a noise vector \n\n\n\nz\n\n\n{\\displaystyle z}\n\n and a label \n\n\n\nc\n\n\n{\\displaystyle c}\n\n, and produces an image \n\n\n\nG\n(\nz\n,\nc\n)\n\n\n{\\displaystyle G(z,c)}\n\n. The discriminator receives image-label pairs \n\n\n\n(\nx\n,\nc\n)\n\n\n{\\displaystyle (x,c)}\n\n, and computes \n\n\n\nD\n(\nx\n,\nc\n)\n\n\n{\\displaystyle D(x,c)}\n\n.\n When the training dataset is unlabeled, conditional GAN does not work directly.\n The idea of InfoGAN is to decree that every latent vector in the latent space can be decomposed as \n\n\n\n(\nz\n,\nc\n)\n\n\n{\\displaystyle (z,c)}\n\n: an incompressible noise part \n\n\n\nz\n\n\n{\\displaystyle z}\n\n, and an informative label part \n\n\n\nc\n\n\n{\\displaystyle c}\n\n, and encourage the generator to comply with the decree, by encouraging it to maximize \n\n\n\nI\n(\nc\n,\nG\n(\nz\n,\nc\n)\n)\n\n\n{\\displaystyle I(c,G(z,c))}\n\n, the mutual information between \n\n\n\nc\n\n\n{\\displaystyle c}\n\n and \n\n\n\nG\n(\nz\n,\nc\n)\n\n\n{\\displaystyle G(z,c)}\n\n, while making no demands on the mutual information \n\n\n\nz\n\n\n{\\displaystyle z}\n\n between \n\n\n\nG\n(\nz\n,\nc\n)\n\n\n{\\displaystyle G(z,c)}\n\n.\n Unfortunately, \n\n\n\nI\n(\nc\n,\nG\n(\nz\n,\nc\n)\n)\n\n\n{\\displaystyle I(c,G(z,c))}\n\n is intractable in general, The key idea of InfoGAN is Variational Mutual Information Maximization:[39] indirectly maximize it by maximizing a lower bound\n\n\n\n\n\n\nI\n^\n\n\n\n(\nG\n,\nQ\n)\n=\n\n\nE\n\n\nz\n\u223c\n\n\u03bc\n\nZ\n\n\n,\nc\n\u223c\n\n\u03bc\n\nC\n\n\n\n\n[\nln\n\u2061\nQ\n(\nc\n\u2223\nG\n(\nz\n,\nc\n)\n)\n]\n;\n\nI\n(\nc\n,\nG\n(\nz\n,\nc\n)\n)\n\u2265\n\nsup\n\nQ\n\n\n\n\n\nI\n^\n\n\n\n(\nG\n,\nQ\n)\n\n\n{\\displaystyle {\\hat {I}}(G,Q)=\\mathbb {E} _{z\\sim \\mu _{Z},c\\sim \\mu _{C}}[\\ln Q(c\\mid G(z,c))];\\quad I(c,G(z,c))\\geq \\sup _{Q}{\\hat {I}}(G,Q)}\n\nwhere \n\n\n\nQ\n\n\n{\\displaystyle Q}\n\n ranges over all Markov kernels of type \n\n\n\nQ\n:\n\n\u03a9\n\nY\n\n\n\u2192\n\n\nP\n\n\n(\n\n\u03a9\n\nC\n\n\n)\n\n\n{\\displaystyle Q:\\Omega _{Y}\\to {\\mathcal {P}}(\\Omega _{C})}\n\n.\n \nThe InfoGAN game is defined as follows:[40] Three probability spaces define an InfoGAN game:\n There are 3 players in 2 teams: generator, Q, and discriminator. The generator and Q are on one team, and the discriminator on the other team.\n The objective function is\n\n\n\nL\n(\nG\n,\nQ\n,\nD\n)\n=\n\nL\n\nG\nA\nN\n\n\n(\nG\n,\nD\n)\n\u2212\n\u03bb\n\n\n\nI\n^\n\n\n\n(\nG\n,\nQ\n)\n\n\n{\\displaystyle L(G,Q,D)=L_{GAN}(G,D)-\\lambda {\\hat {I}}(G,Q)}\n\nwhere \n\n\n\n\nL\n\nG\nA\nN\n\n\n(\nG\n,\nD\n)\n=\n\nE\n\nx\n\u223c\n\n\u03bc\n\nref\n\n\n,\n\n\n\u2061\n[\nln\n\u2061\nD\n(\nx\n)\n]\n+\n\nE\n\nz\n\u223c\n\n\u03bc\n\nZ\n\n\n\n\n\u2061\n[\nln\n\u2061\n(\n1\n\u2212\nD\n(\nG\n(\nz\n,\nc\n)\n)\n)\n]\n\n\n{\\displaystyle L_{GAN}(G,D)=\\operatorname {E} _{x\\sim \\mu _{\\text{ref}},}[\\ln D(x)]+\\operatorname {E} _{z\\sim \\mu _{Z}}[\\ln(1-D(G(z,c)))]}\n\n is the original GAN game objective, and \n\n\n\n\n\n\nI\n^\n\n\n\n(\nG\n,\nQ\n)\n=\n\n\nE\n\n\nz\n\u223c\n\n\u03bc\n\nZ\n\n\n,\nc\n\u223c\n\n\u03bc\n\nC\n\n\n\n\n[\nln\n\u2061\nQ\n(\nc\n\u2223\nG\n(\nz\n,\nc\n)\n)\n]\n\n\n{\\displaystyle {\\hat {I}}(G,Q)=\\mathbb {E} _{z\\sim \\mu _{Z},c\\sim \\mu _{C}}[\\ln Q(c\\mid G(z,c))]}\n\n\n \nGenerator-Q team aims to minimize the objective, and discriminator aims to maximize it:\n\n\n\n\nmin\n\nG\n,\nQ\n\n\n\nmax\n\nD\n\n\nL\n(\nG\n,\nQ\n,\nD\n)\n\n\n{\\displaystyle \\min _{G,Q}\\max _{D}L(G,Q,D)}\n\n The standard GAN generator is a function of type \n\n\n\nG\n:\n\n\u03a9\n\nZ\n\n\n\u2192\n\n\u03a9\n\nX\n\n\n\n\n{\\displaystyle G:\\Omega _{Z}\\to \\Omega _{X}}\n\n, that is, it is a mapping from a latent space \n\n\n\n\n\u03a9\n\nZ\n\n\n\n\n{\\displaystyle \\Omega _{Z}}\n\n to the image space \n\n\n\n\n\u03a9\n\nX\n\n\n\n\n{\\displaystyle \\Omega _{X}}\n\n. This can be understood as a \"decoding\" process, whereby every latent vector \n\n\n\nz\n\u2208\n\n\u03a9\n\nZ\n\n\n\n\n{\\displaystyle z\\in \\Omega _{Z}}\n\n is a code for an image \n\n\n\nx\n\u2208\n\n\u03a9\n\nX\n\n\n\n\n{\\displaystyle x\\in \\Omega _{X}}\n\n, and the generator performs the decoding. This naturally leads to the idea of training another network that performs \"encoding\", creating an autoencoder out of the encoder-generator pair.\n Already in the original paper,[1] the authors noted that \"Learned approximate inference can be performed by training an auxiliary network to predict \n\n\n\nz\n\n\n{\\displaystyle z}\n\n given \n\n\n\nx\n\n\n{\\displaystyle x}\n\n\". The bidirectional GAN architecture performs exactly this.[41]\n \nThe BiGAN is defined as follows:  Two probability spaces define a BiGAN game:\n There are 3 players in 2 teams: generator, encoder, and discriminator. The generator and encoder are on one team, and the discriminator on the other team.\n The generator's strategies are functions \n\n\n\nG\n:\n\n\u03a9\n\nZ\n\n\n\u2192\n\n\u03a9\n\nX\n\n\n\n\n{\\displaystyle G:\\Omega _{Z}\\to \\Omega _{X}}\n\n, and the encoder's strategies are functions \n\n\n\nE\n:\n\n\u03a9\n\nX\n\n\n\u2192\n\n\u03a9\n\nZ\n\n\n\n\n{\\displaystyle E:\\Omega _{X}\\to \\Omega _{Z}}\n\n. The discriminator's strategies are functions \n\n\n\nD\n:\n\n\u03a9\n\nX\n\n\n\u2192\n[\n0\n,\n1\n]\n\n\n{\\displaystyle D:\\Omega _{X}\\to [0,1]}\n\n.\n The objective function is\n\n\n\nL\n(\nG\n,\nE\n,\nD\n)\n=\n\n\nE\n\n\nx\n\u223c\n\n\u03bc\n\nX\n\n\n\n\n[\nln\n\u2061\nD\n(\nx\n,\nE\n(\nx\n)\n)\n]\n+\n\n\nE\n\n\nz\n\u223c\n\n\u03bc\n\nZ\n\n\n\n\n[\nln\n\u2061\n(\n1\n\u2212\nD\n(\nG\n(\nz\n)\n,\nz\n)\n)\n]\n\n\n{\\displaystyle L(G,E,D)=\\mathbb {E} _{x\\sim \\mu _{X}}[\\ln D(x,E(x))]+\\mathbb {E} _{z\\sim \\mu _{Z}}[\\ln(1-D(G(z),z))]}\n\n\n \nGenerator-encoder team aims to minimize the objective, and discriminator aims to maximize it:\n\n\n\n\nmin\n\nG\n,\nE\n\n\n\nmax\n\nD\n\n\nL\n(\nG\n,\nE\n,\nD\n)\n\n\n{\\displaystyle \\min _{G,E}\\max _{D}L(G,E,D)}\n\n  In the paper, they gave a more abstract definition of the objective as:\n\n\n\nL\n(\nG\n,\nE\n,\nD\n)\n=\n\n\nE\n\n\n(\nx\n,\nz\n)\n\u223c\n\n\u03bc\n\nE\n,\nX\n\n\n\n\n[\nln\n\u2061\nD\n(\nx\n,\nz\n)\n]\n+\n\n\nE\n\n\n(\nx\n,\nz\n)\n\u223c\n\n\u03bc\n\nG\n,\nZ\n\n\n\n\n[\nln\n\u2061\n(\n1\n\u2212\nD\n(\nx\n,\nz\n)\n)\n]\n\n\n{\\displaystyle L(G,E,D)=\\mathbb {E} _{(x,z)\\sim \\mu _{E,X}}[\\ln D(x,z)]+\\mathbb {E} _{(x,z)\\sim \\mu _{G,Z}}[\\ln(1-D(x,z))]}\n\nwhere \n\n\n\n\n\u03bc\n\nE\n,\nX\n\n\n(\nd\nx\n,\nd\nz\n)\n=\n\n\u03bc\n\nX\n\n\n(\nd\nx\n)\n\u22c5\n\n\u03b4\n\nE\n(\nx\n)\n\n\n(\nd\nz\n)\n\n\n{\\displaystyle \\mu _{E,X}(dx,dz)=\\mu _{X}(dx)\\cdot \\delta _{E(x)}(dz)}\n\n is the probability distribution on \n\n\n\n\n\u03a9\n\nX\n\n\n\u00d7\n\n\u03a9\n\nZ\n\n\n\n\n{\\displaystyle \\Omega _{X}\\times \\Omega _{Z}}\n\n obtained by pushing \n\n\n\n\n\u03bc\n\nX\n\n\n\n\n{\\displaystyle \\mu _{X}}\n\n forward via \n\n\n\nx\n\u21a6\n(\nx\n,\nE\n(\nx\n)\n)\n\n\n{\\displaystyle x\\mapsto (x,E(x))}\n\n, and \n\n\n\n\n\u03bc\n\nG\n,\nZ\n\n\n(\nd\nx\n,\nd\nz\n)\n=\n\n\u03b4\n\nG\n(\nz\n)\n\n\n(\nd\nx\n)\n\u22c5\n\n\u03bc\n\nZ\n\n\n(\nd\nz\n)\n\n\n{\\displaystyle \\mu _{G,Z}(dx,dz)=\\delta _{G(z)}(dx)\\cdot \\mu _{Z}(dz)}\n\n is the probability distribution on \n\n\n\n\n\u03a9\n\nX\n\n\n\u00d7\n\n\u03a9\n\nZ\n\n\n\n\n{\\displaystyle \\Omega _{X}\\times \\Omega _{Z}}\n\n obtained by pushing \n\n\n\n\n\u03bc\n\nZ\n\n\n\n\n{\\displaystyle \\mu _{Z}}\n\n forward via \n\n\n\nz\n\u21a6\n(\nG\n(\nx\n)\n,\nz\n)\n\n\n{\\displaystyle z\\mapsto (G(x),z)}\n\n.\n Applications of bidirectional models include semi-supervised learning,[42] interpretable machine learning,[43] and neural machine translation.[44]\n CycleGAN is an architecture for performing translations between two domains, such as between photos of horses and photos of zebras, or photos of night cities and photos of day cities.\n \nThe CycleGAN game is defined as follows:[45] There are two probability spaces \n\n\n\n(\n\n\u03a9\n\nX\n\n\n,\n\n\u03bc\n\nX\n\n\n)\n,\n(\n\n\u03a9\n\nY\n\n\n,\n\n\u03bc\n\nY\n\n\n)\n\n\n{\\displaystyle (\\Omega _{X},\\mu _{X}),(\\Omega _{Y},\\mu _{Y})}\n\n, corresponding to the two domains needed for translations fore-and-back.\n There are 4 players in 2 teams: generators \n\n\n\n\nG\n\nX\n\n\n:\n\n\u03a9\n\nX\n\n\n\u2192\n\n\u03a9\n\nY\n\n\n,\n\nG\n\nY\n\n\n:\n\n\u03a9\n\nY\n\n\n\u2192\n\n\u03a9\n\nX\n\n\n\n\n{\\displaystyle G_{X}:\\Omega _{X}\\to \\Omega _{Y},G_{Y}:\\Omega _{Y}\\to \\Omega _{X}}\n\n, and discriminators \n\n\n\n\nD\n\nX\n\n\n:\n\n\u03a9\n\nX\n\n\n\u2192\n[\n0\n,\n1\n]\n,\n\nD\n\nY\n\n\n:\n\n\u03a9\n\nY\n\n\n\u2192\n[\n0\n,\n1\n]\n\n\n{\\displaystyle D_{X}:\\Omega _{X}\\to [0,1],D_{Y}:\\Omega _{Y}\\to [0,1]}\n\n.\n The objective function is\n\n\n\nL\n(\n\nG\n\nX\n\n\n,\n\nG\n\nY\n\n\n,\n\nD\n\nX\n\n\n,\n\nD\n\nY\n\n\n)\n=\n\nL\n\nG\nA\nN\n\n\n(\n\nG\n\nX\n\n\n,\n\nD\n\nX\n\n\n)\n+\n\nL\n\nG\nA\nN\n\n\n(\n\nG\n\nY\n\n\n,\n\nD\n\nY\n\n\n)\n+\n\u03bb\n\nL\n\nc\ny\nc\nl\ne\n\n\n(\n\nG\n\nX\n\n\n,\n\nG\n\nY\n\n\n)\n\n\n{\\displaystyle L(G_{X},G_{Y},D_{X},D_{Y})=L_{GAN}(G_{X},D_{X})+L_{GAN}(G_{Y},D_{Y})+\\lambda L_{cycle}(G_{X},G_{Y})}\n\n\n \nwhere \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n is a positive adjustable parameter, \n\n\n\n\nL\n\nG\nA\nN\n\n\n\n\n{\\displaystyle L_{GAN}}\n\n is the GAN game objective, and \n\n\n\n\nL\n\nc\ny\nc\nl\ne\n\n\n\n\n{\\displaystyle L_{cycle}}\n\n is the cycle consistency loss:\n\n\n\n\nL\n\nc\ny\nc\nl\ne\n\n\n(\n\nG\n\nX\n\n\n,\n\nG\n\nY\n\n\n)\n=\n\nE\n\nx\n\u223c\n\n\u03bc\n\nX\n\n\n\n\n\u2016\n\nG\n\nX\n\n\n(\n\nG\n\nY\n\n\n(\nx\n)\n)\n\u2212\nx\n\u2016\n+\n\nE\n\ny\n\u223c\n\n\u03bc\n\nY\n\n\n\n\n\u2016\n\nG\n\nY\n\n\n(\n\nG\n\nX\n\n\n(\ny\n)\n)\n\u2212\ny\n\u2016\n\n\n{\\displaystyle L_{cycle}(G_{X},G_{Y})=E_{x\\sim \\mu _{X}}\\|G_{X}(G_{Y}(x))-x\\|+E_{y\\sim \\mu _{Y}}\\|G_{Y}(G_{X}(y))-y\\|}\n\nThe generators aim to minimize the objective, and the discriminators aim to maximize it:\n\n\n\n\nmin\n\n\nG\n\nX\n\n\n,\n\nG\n\nY\n\n\n\n\n\nmax\n\n\nD\n\nX\n\n\n,\n\nD\n\nY\n\n\n\n\nL\n(\n\nG\n\nX\n\n\n,\n\nG\n\nY\n\n\n,\n\nD\n\nX\n\n\n,\n\nD\n\nY\n\n\n)\n\n\n{\\displaystyle \\min _{G_{X},G_{Y}}\\max _{D_{X},D_{Y}}L(G_{X},G_{Y},D_{X},D_{Y})}\n\n  Unlike previous work like pix2pix,[46] which requires paired training data, cycleGAN requires no paired data. For example, to train a pix2pix model to turn a summer scenery photo to winter scenery photo and back, the dataset must contain pairs of the same place in summer and winter, shot at the same angle; cycleGAN would only need a set of summer scenery photos, and an unrelated set of winter scenery photos.\n The BigGAN is essentially a self-attention GAN trained on a large scale (up to 80 million parameters) to generate large images of ImageNet (up to 512 x 512 resolution), with numerous engineering tricks to make it converge.[20][47]\n When there is insufficient training data, the reference distribution \n\n\n\n\n\u03bc\n\nref\n\n\n\n\n{\\displaystyle \\mu _{\\text{ref}}}\n\n cannot be well-approximated by the empirical distribution given by the training dataset. In such cases, data augmentation can be applied, to allow training GAN on smaller datasets. Na\u00efve data augmentation, however, brings its problems.\n Consider the original GAN game, slightly reformulated as follows:\n\n\n\n\n\n{\n\n\n\n\nmin\n\nD\n\n\n\nL\n\nD\n\n\n(\nD\n,\n\n\u03bc\n\nG\n\n\n)\n=\n\u2212\n\nE\n\nx\n\u223c\n\n\u03bc\n\nref\n\n\n\n\n\u2061\n[\nln\n\u2061\nD\n(\nx\n)\n]\n\u2212\n\nE\n\nx\n\u223c\n\n\u03bc\n\nG\n\n\n\n\n\u2061\n[\nln\n\u2061\n(\n1\n\u2212\nD\n(\nx\n)\n)\n]\n\n\n\n\n\nmin\n\nG\n\n\n\nL\n\nG\n\n\n(\nD\n,\n\n\u03bc\n\nG\n\n\n)\n=\n\u2212\n\nE\n\nx\n\u223c\n\n\u03bc\n\nG\n\n\n\n\n\u2061\n[\nln\n\u2061\n(\n1\n\u2212\nD\n(\nx\n)\n)\n]\n\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{cases}\\min _{D}L_{D}(D,\\mu _{G})=-\\operatorname {E} _{x\\sim \\mu _{\\text{ref}}}[\\ln D(x)]-\\operatorname {E} _{x\\sim \\mu _{G}}[\\ln(1-D(x))]\\\\\\min _{G}L_{G}(D,\\mu _{G})=-\\operatorname {E} _{x\\sim \\mu _{G}}[\\ln(1-D(x))]\\end{cases}}}\n\nNow we use data augmentation by randomly sampling semantic-preserving transforms \n\n\n\nT\n:\n\u03a9\n\u2192\n\u03a9\n\n\n{\\displaystyle T:\\Omega \\to \\Omega }\n\n and applying them to the dataset, to obtain the reformulated GAN game:\n\n\n\n\n\n{\n\n\n\n\nmin\n\nD\n\n\n\nL\n\nD\n\n\n(\nD\n,\n\n\u03bc\n\nG\n\n\n)\n=\n\u2212\n\nE\n\nx\n\u223c\n\n\u03bc\n\nref\n\n\n,\nT\n\u223c\n\n\u03bc\n\ntrans\n\n\n\n\n\u2061\n[\nln\n\u2061\nD\n(\nT\n(\nx\n)\n)\n]\n\u2212\n\nE\n\nx\n\u223c\n\n\u03bc\n\nG\n\n\n\n\n\u2061\n[\nln\n\u2061\n(\n1\n\u2212\nD\n(\nx\n)\n)\n]\n\n\n\n\n\nmin\n\nG\n\n\n\nL\n\nG\n\n\n(\nD\n,\n\n\u03bc\n\nG\n\n\n)\n=\n\u2212\n\nE\n\nx\n\u223c\n\n\u03bc\n\nG\n\n\n\n\n\u2061\n[\nln\n\u2061\n(\n1\n\u2212\nD\n(\nx\n)\n)\n]\n\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{cases}\\min _{D}L_{D}(D,\\mu _{G})=-\\operatorname {E} _{x\\sim \\mu _{\\text{ref}},T\\sim \\mu _{\\text{trans}}}[\\ln D(T(x))]-\\operatorname {E} _{x\\sim \\mu _{G}}[\\ln(1-D(x))]\\\\\\min _{G}L_{G}(D,\\mu _{G})=-\\operatorname {E} _{x\\sim \\mu _{G}}[\\ln(1-D(x))]\\end{cases}}}\n\nThis is equivalent to a GAN game with a different distribution \n\n\n\n\n\u03bc\n\nref\n\n\u2032\n\n\n\n{\\displaystyle \\mu _{\\text{ref}}'}\n\n, sampled by \n\n\n\nT\n(\nx\n)\n\n\n{\\displaystyle T(x)}\n\n, with \n\n\n\nx\n\u223c\n\n\u03bc\n\nref\n\n\n,\nT\n\u223c\n\n\u03bc\n\ntrans\n\n\n\n\n{\\displaystyle x\\sim \\mu _{\\text{ref}},T\\sim \\mu _{\\text{trans}}}\n\n. For example, if \n\n\n\n\n\u03bc\n\nref\n\n\n\n\n{\\displaystyle \\mu _{\\text{ref}}}\n\n is the distribution of images in ImageNet, and \n\n\n\n\n\u03bc\n\ntrans\n\n\n\n\n{\\displaystyle \\mu _{\\text{trans}}}\n\n samples identity-transform with probability 0.5, and horizontal-reflection with probability 0.5, then \n\n\n\n\n\u03bc\n\nref\n\n\u2032\n\n\n\n{\\displaystyle \\mu _{\\text{ref}}'}\n\n is the distribution of images in ImageNet and horizontally-reflected ImageNet, combined.\n The result of such training would be a generator that mimics \n\n\n\n\n\u03bc\n\nref\n\n\u2032\n\n\n\n{\\displaystyle \\mu _{\\text{ref}}'}\n\n. For example, it would generate images that look like they are randomly cropped, if the data augmentation uses random cropping.\n The solution is to apply data augmentation to both generated and real images:\n\n\n\n\n\n{\n\n\n\n\nmin\n\nD\n\n\n\nL\n\nD\n\n\n(\nD\n,\n\n\u03bc\n\nG\n\n\n)\n=\n\u2212\n\nE\n\nx\n\u223c\n\n\u03bc\n\nref\n\n\n,\nT\n\u223c\n\n\u03bc\n\ntrans\n\n\n\n\n\u2061\n[\nln\n\u2061\nD\n(\nT\n(\nx\n)\n)\n]\n\u2212\n\nE\n\nx\n\u223c\n\n\u03bc\n\nG\n\n\n,\nT\n\u223c\n\n\u03bc\n\ntrans\n\n\n\n\n\u2061\n[\nln\n\u2061\n(\n1\n\u2212\nD\n(\nT\n(\nx\n)\n)\n)\n]\n\n\n\n\n\nmin\n\nG\n\n\n\nL\n\nG\n\n\n(\nD\n,\n\n\u03bc\n\nG\n\n\n)\n=\n\u2212\n\nE\n\nx\n\u223c\n\n\u03bc\n\nG\n\n\n,\nT\n\u223c\n\n\u03bc\n\ntrans\n\n\n\n\n\u2061\n[\nln\n\u2061\n(\n1\n\u2212\nD\n(\nT\n(\nx\n)\n)\n)\n]\n\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{cases}\\min _{D}L_{D}(D,\\mu _{G})=-\\operatorname {E} _{x\\sim \\mu _{\\text{ref}},T\\sim \\mu _{\\text{trans}}}[\\ln D(T(x))]-\\operatorname {E} _{x\\sim \\mu _{G},T\\sim \\mu _{\\text{trans}}}[\\ln(1-D(T(x)))]\\\\\\min _{G}L_{G}(D,\\mu _{G})=-\\operatorname {E} _{x\\sim \\mu _{G},T\\sim \\mu _{\\text{trans}}}[\\ln(1-D(T(x)))]\\end{cases}}}\n\nThe authors demonstrated high-quality generation using just 100-picture-large datasets.[48]\n The StyleGAN-2-ADA paper points out a further point on data augmentation: it must be invertible.[49] Continue with the example of generating ImageNet pictures. If the data augmentation is \"randomly rotate the picture by 0, 90, 180, 270 degrees with equal probability\", then there is no way for the generator to know which is the true orientation: Consider two generators \n\n\n\nG\n,\n\nG\n\u2032\n\n\n\n{\\displaystyle G,G'}\n\n, such that for any latent \n\n\n\nz\n\n\n{\\displaystyle z}\n\n, the generated image \n\n\n\nG\n(\nz\n)\n\n\n{\\displaystyle G(z)}\n\n is a 90-degree rotation of \n\n\n\n\nG\n\u2032\n\n(\nz\n)\n\n\n{\\displaystyle G'(z)}\n\n. They would have exactly the same expected loss, and so neither is preferred over the other.\n The solution is to only use invertible data augmentation: instead of \"randomly rotate the picture by 0, 90, 180, 270 degrees with equal probability\", use \"randomly rotate the picture by 90, 180, 270 degrees with 0.1 probability, and keep the picture as it is with 0.7 probability\". This way, the generator is still rewarded  to keep images oriented the same way as un-augmented ImageNet pictures.\n Abstractly, the effect of randomly sampling transformations \n\n\n\nT\n:\n\u03a9\n\u2192\n\u03a9\n\n\n{\\displaystyle T:\\Omega \\to \\Omega }\n\n from the distribution \n\n\n\n\n\u03bc\n\ntrans\n\n\n\n\n{\\displaystyle \\mu _{\\text{trans}}}\n\n is to define a Markov kernel \n\n\n\n\nK\n\ntrans\n\n\n:\n\u03a9\n\u2192\n\n\nP\n\n\n(\n\u03a9\n)\n\n\n{\\displaystyle K_{\\text{trans}}:\\Omega \\to {\\mathcal {P}}(\\Omega )}\n\n. Then, the data-augmented GAN game pushes the generator to find some \n\n\n\n\n\n\n\n\u03bc\n^\n\n\n\n\nG\n\n\n\u2208\n\n\nP\n\n\n(\n\u03a9\n)\n\n\n{\\displaystyle {\\hat {\\mu }}_{G}\\in {\\mathcal {P}}(\\Omega )}\n\n, such that \n\n\n\n\nK\n\ntrans\n\n\n\u2217\n\n\u03bc\n\nref\n\n\n=\n\nK\n\ntrans\n\n\n\u2217\n\n\n\n\n\u03bc\n^\n\n\n\n\nG\n\n\n\n\n{\\displaystyle K_{\\text{trans}}*\\mu _{\\text{ref}}=K_{\\text{trans}}*{\\hat {\\mu }}_{G}}\n\nwhere \n\n\n\n\u2217\n\n\n{\\displaystyle *}\n\n is the Markov kernel convolution.\nA data-augmentation method is defined to be invertible if its Markov kernel \n\n\n\n\nK\n\ntrans\n\n\n\n\n{\\displaystyle K_{\\text{trans}}}\n\n satisfies\n\n\n\n\nK\n\ntrans\n\n\n\u2217\n\u03bc\n=\n\nK\n\ntrans\n\n\n\u2217\n\n\u03bc\n\u2032\n\n\n\u27f9\n\n\u03bc\n=\n\n\u03bc\n\u2032\n\n\n\u2200\n\u03bc\n,\n\n\u03bc\n\u2032\n\n\u2208\n\n\nP\n\n\n(\n\u03a9\n)\n\n\n{\\displaystyle K_{\\text{trans}}*\\mu =K_{\\text{trans}}*\\mu '\\implies \\mu =\\mu '\\quad \\forall \\mu ,\\mu '\\in {\\mathcal {P}}(\\Omega )}\n\nImmediately by definition, we see that composing multiple invertible data-augmentation methods results in yet another invertible method. Also by definition, if the data-augmentation method is invertible, then using it in a GAN game does not change the optimal strategy \n\n\n\n\n\n\n\n\u03bc\n^\n\n\n\n\nG\n\n\n\n\n{\\displaystyle {\\hat {\\mu }}_{G}}\n\n for the generator, which is still \n\n\n\n\n\u03bc\n\nref\n\n\n\n\n{\\displaystyle \\mu _{\\text{ref}}}\n\n.\n There are two prototypical examples of invertible Markov kernels:\n Discrete case: Invertible stochastic matrices, when \n\n\n\n\u03a9\n\n\n{\\displaystyle \\Omega }\n\n is finite.\n For example, if \n\n\n\n\u03a9\n=\n{\n\u2191\n,\n\u2193\n,\n\u2190\n,\n\u2192\n}\n\n\n{\\displaystyle \\Omega =\\{\\uparrow ,\\downarrow ,\\leftarrow ,\\rightarrow \\}}\n\n is the set of four images of an arrow, pointing in 4 directions, and the data augmentation is \"randomly rotate the picture by 90, 180, 270 degrees with probability \n\n\n\np\n\n\n{\\displaystyle p}\n\n, and keep the picture as it is with probability \n\n\n\n(\n1\n\u2212\n3\np\n)\n\n\n{\\displaystyle (1-3p)}\n\n\", then the Markov kernel \n\n\n\n\nK\n\ntrans\n\n\n\n\n{\\displaystyle K_{\\text{trans}}}\n\n can be represented as a stochastic matrix:\n\n\n\n[\n\nK\n\ntrans\n\n\n]\n=\n\n\n[\n\n\n\n(\n1\n\u2212\n3\np\n)\n\n\np\n\n\np\n\n\np\n\n\n\n\np\n\n\n(\n1\n\u2212\n3\np\n)\n\n\np\n\n\np\n\n\n\n\np\n\n\np\n\n\n(\n1\n\u2212\n3\np\n)\n\n\np\n\n\n\n\np\n\n\np\n\n\np\n\n\n(\n1\n\u2212\n3\np\n)\n\n\n\n]\n\n\n\n\n{\\displaystyle [K_{\\text{trans}}]={\\begin{bmatrix}(1-3p)&p&p&p\\\\p&(1-3p)&p&p\\\\p&p&(1-3p)&p\\\\p&p&p&(1-3p)\\end{bmatrix}}}\n\n and \n\n\n\n\nK\n\ntrans\n\n\n\n\n{\\displaystyle K_{\\text{trans}}}\n\n is an invertible kernel iff \n\n\n\n[\n\nK\n\ntrans\n\n\n]\n\n\n{\\displaystyle [K_{\\text{trans}}]}\n\n is an invertible matrix, that is, \n\n\n\np\n\u2260\n1\n\n/\n\n4\n\n\n{\\displaystyle p\\neq 1/4}\n\n.\n Continuous case: The gaussian kernel, when \n\n\n\n\u03a9\n=\n\n\nR\n\n\nn\n\n\n\n\n{\\displaystyle \\Omega =\\mathbb {R} ^{n}}\n\n for some \n\n\n\nn\n\u2265\n1\n\n\n{\\displaystyle n\\geq 1}\n\n.\n For example, if \n\n\n\n\u03a9\n=\n\n\nR\n\n\n\n256\n\n2\n\n\n\n\n\n\n{\\displaystyle \\Omega =\\mathbb {R} ^{256^{2}}}\n\n is the space of 256x256 images, and the data-augmentation method is \"generate a gaussian noise \n\n\n\nz\n\u223c\n\n\nN\n\n\n(\n0\n,\n\nI\n\n\n256\n\n2\n\n\n\n\n)\n\n\n{\\displaystyle z\\sim {\\mathcal {N}}(0,I_{256^{2}})}\n\n, then add \n\n\n\n\u03f5\nz\n\n\n{\\displaystyle \\epsilon z}\n\n to the image\", then \n\n\n\n\nK\n\ntrans\n\n\n\n\n{\\displaystyle K_{\\text{trans}}}\n\n is just convolution by the density function of \n\n\n\n\n\nN\n\n\n(\n0\n,\n\n\u03f5\n\n2\n\n\n\nI\n\n\n256\n\n2\n\n\n\n\n)\n\n\n{\\displaystyle {\\mathcal {N}}(0,\\epsilon ^{2}I_{256^{2}})}\n\n. This is invertible, because convolution by a gaussian is just convolution by the heat kernel, so given any \n\n\n\n\u03bc\n\u2208\n\n\nP\n\n\n(\n\n\nR\n\n\nn\n\n\n)\n\n\n{\\displaystyle \\mu \\in {\\mathcal {P}}(\\mathbb {R} ^{n})}\n\n, the convolved distribution \n\n\n\n\nK\n\ntrans\n\n\n\u2217\n\u03bc\n\n\n{\\displaystyle K_{\\text{trans}}*\\mu }\n\n can be obtained by heating up \n\n\n\n\n\nR\n\n\nn\n\n\n\n\n{\\displaystyle \\mathbb {R} ^{n}}\n\n precisely according to \n\n\n\n\u03bc\n\n\n{\\displaystyle \\mu }\n\n, then wait for time \n\n\n\n\n\u03f5\n\n2\n\n\n\n/\n\n4\n\n\n{\\displaystyle \\epsilon ^{2}/4}\n\n. With that, we can recover \n\n\n\n\u03bc\n\n\n{\\displaystyle \\mu }\n\n by running the heat equation backwards in time for \n\n\n\n\n\u03f5\n\n2\n\n\n\n/\n\n4\n\n\n{\\displaystyle \\epsilon ^{2}/4}\n\n.\n More examples of invertible data augmentations are found in the paper.[49]\n SinGAN pushes data augmentation to the limit, by using only a single image as training data and performing data augmentation on it. The GAN architecture is adapted to this training method by using a multi-scale pipeline.\n The generator \n\n\n\nG\n\n\n{\\displaystyle G}\n\n is decomposed into a pyramid of generators \n\n\n\nG\n=\n\nG\n\n1\n\n\n\u2218\n\nG\n\n2\n\n\n\u2218\n\u22ef\n\u2218\n\nG\n\nN\n\n\n\n\n{\\displaystyle G=G_{1}\\circ G_{2}\\circ \\cdots \\circ G_{N}}\n\n, with the lowest one generating the image \n\n\n\n\nG\n\nN\n\n\n(\n\nz\n\nN\n\n\n)\n\n\n{\\displaystyle G_{N}(z_{N})}\n\n at the lowest resolution, then the generated image is scaled up to \n\n\n\nr\n(\n\nG\n\nN\n\n\n(\n\nz\n\nN\n\n\n)\n)\n\n\n{\\displaystyle r(G_{N}(z_{N}))}\n\n, and fed to the next level to generate an image \n\n\n\n\nG\n\nN\n\u2212\n1\n\n\n(\n\nz\n\nN\n\u2212\n1\n\n\n+\nr\n(\n\nG\n\nN\n\n\n(\n\nz\n\nN\n\n\n)\n)\n)\n\n\n{\\displaystyle G_{N-1}(z_{N-1}+r(G_{N}(z_{N})))}\n\n at a higher resolution, and so on. The discriminator is decomposed into a pyramid as well.[50]\n The StyleGAN family is a series of architectures published by Nvidia's research division.\n Progressive GAN[14] is a method for training GAN for large-scale image generation stably, by growing a GAN generator from small to large scale in a pyramidal fashion. Like SinGAN, it decomposes the generator as\n\n\n\nG\n=\n\nG\n\n1\n\n\n\u2218\n\nG\n\n2\n\n\n\u2218\n\u22ef\n\u2218\n\nG\n\nN\n\n\n\n\n{\\displaystyle G=G_{1}\\circ G_{2}\\circ \\cdots \\circ G_{N}}\n\n, and the discriminator as \n\n\n\nD\n=\n\nD\n\n1\n\n\n\u2218\n\nD\n\n2\n\n\n\u2218\n\u22ef\n\u2218\n\nD\n\nN\n\n\n\n\n{\\displaystyle D=D_{1}\\circ D_{2}\\circ \\cdots \\circ D_{N}}\n\n.\n During training, at first only \n\n\n\n\nG\n\nN\n\n\n,\n\nD\n\nN\n\n\n\n\n{\\displaystyle G_{N},D_{N}}\n\n are used in a GAN game to generate 4x4 images. Then \n\n\n\n\nG\n\nN\n\u2212\n1\n\n\n,\n\nD\n\nN\n\u2212\n1\n\n\n\n\n{\\displaystyle G_{N-1},D_{N-1}}\n\n are added to reach the second stage of GAN game, to generate 8x8 images, and so on, until we reach a GAN game to generate 1024x1024 images.\n To avoid shock between stages of the GAN game, each new layer is \"blended in\" (Figure 2 of the paper[14]). For example, this is how the second stage GAN game starts:\n StyleGAN-1 is designed as a combination of Progressive GAN with neural style transfer.[51]\n The key architectural choice of StyleGAN-1 is a progressive growth mechanism, similar to Progressive GAN. Each generated image starts as a constant \n\n\n\n4\n\u00d7\n4\n\u00d7\n512\n\n\n{\\displaystyle 4\\times 4\\times 512}\n\n array, and repeatedly passed through style blocks. Each style block applies a \"style latent vector\" via affine transform (\"adaptive instance normalization\"), similar to how neural style transfer uses Gramian matrix. It then adds noise, and normalize (subtract the mean, then divide by the variance).\n At training time, usually only one style latent vector is used per image generated, but sometimes two (\"mixing regularization\") in order to encourage each style block to independently perform its stylization without expecting help from other style blocks (since they might receive an entirely different style latent vector).\n After training, multiple style latent vectors can be fed into each style block. Those fed to the lower layers control the large-scale styles, and those fed to the higher layers control the fine-detail styles.\n Style-mixing between two images \n\n\n\nx\n,\n\nx\n\u2032\n\n\n\n{\\displaystyle x,x'}\n\n can be performed as well. First, run a gradient descent to find \n\n\n\nz\n,\n\nz\n\u2032\n\n\n\n{\\displaystyle z,z'}\n\n such that \n\n\n\nG\n(\nz\n)\n\u2248\nx\n,\nG\n(\n\nz\n\u2032\n\n)\n\u2248\n\nx\n\u2032\n\n\n\n{\\displaystyle G(z)\\approx x,G(z')\\approx x'}\n\n. This is called \"projecting an image back to style latent space\". Then, \n\n\n\nz\n\n\n{\\displaystyle z}\n\n can be fed to the lower style blocks, and \n\n\n\n\nz\n\u2032\n\n\n\n{\\displaystyle z'}\n\n to the higher style blocks, to generate a composite image that has the large-scale style of \n\n\n\nx\n\n\n{\\displaystyle x}\n\n, and the fine-detail style of \n\n\n\n\nx\n\u2032\n\n\n\n{\\displaystyle x'}\n\n. Multiple images can also be composed this way.\n StyleGAN-2 improves upon StyleGAN-1, by using the style latent vector to transform the convolution layer's weights instead, thus solving the \"blob\" problem.[52]\n This was updated by the StyleGAN-2-ADA (\"ADA\" stands for \"adaptive\"),[49] which uses invertible data augmentation as described above. It also tunes the amount of data augmentation applied by starting at zero, and gradually increasing it until an \"overfitting heuristic\" reaches a target level, thus the name \"adaptive\".\n StyleGAN-3[53] improves upon StyleGAN-2 by solving the \"texture sticking\" problem, which can be seen in the official videos.[54] They analyzed the problem by the Nyquist\u2013Shannon sampling theorem, and argued that the layers in the generator learned to exploit the high-frequency signal in the pixels they operate upon.\n To solve this, they proposed imposing strict lowpass filters between each generator's layers, so that the generator is forced to operate on the pixels in a way faithful to the continuous signals they represent, rather than operate on them as merely discrete signals. They further imposed rotational and translational invariance by using more signal filters. The resulting StyleGAN-3 is able to solve the texture sticking problem, as well as generating images that rotate and translate smoothly.\n Other than for generative and discriminative modelling of data, GANs have been used for other things.\n GANs have been used for transfer learning to enforce the alignment of the latent feature space, such as in deep reinforcement learning.[55] This works by feeding the embeddings of the source and target task to the discriminator which tries to guess the context. The resulting loss is then (inversely) backpropagated through the encoder.\n GAN-generated molecules were validated experimentally in mice.[72][73]\n One of the major concerns in medical imaging is preserving patient privacy. Due to these reasons, researchers often face difficulties in obtaining medical images for their research purposes. GAN has been used for generating synthetic medical images, such as MRI and PET images to address this challenge. [74]\n GAN can be used to detect glaucomatous images helping the early diagnosis which is essential to avoid partial or total loss of vision.[75]\n GANs have been used to create forensic facial reconstructions of deceased historical figures.[76]\n Concerns have been raised about the potential use of GAN-based human image synthesis for sinister purposes, e.g., to produce fake, possibly incriminating, photographs and videos.[77]\nGANs can be used to generate unique, realistic profile photos of people who do not exist, in order to automate creation of fake social media profiles.[78]\n In 2019 the state of California considered[79] and passed on October 3, 2019, the bill AB-602, which bans the use of human image synthesis technologies to make fake pornography without the consent of the people depicted, and bill AB-730, which prohibits distribution of manipulated videos of a political candidate within 60 days of an election. Both bills were authored by Assembly member Marc Berman and signed by Governor Gavin Newsom. The laws went into effect in 2020.[80]\n DARPA's Media Forensics program studies ways to counteract fake media, including fake media produced using GANs.[81]\n GANs can be used to generate art; The Verge wrote in March 2019 that \"The images created by GANs have become the defining look of contemporary AI art.\"[82] GANs can also be used to\n Some have worked with using GAN for artistic creativity, as \"creative adversarial network\".[88][89] A GAN, trained on a set of 15,000 portraits from WikiArt from the 14th to the 19th century, created the 2018 painting Edmond de Belamy, which sold for US$432,500.[90]\n GANs were used by the video game modding community to up-scale low-resolution 2D textures in old video games by recreating them in 4k or higher resolutions via image training, and then down-sampling them to fit the game's native resolution (resembling supersampling anti-aliasing).[91]\n In 2020, Artbreeder was used to create the main antagonist in the sequel to the psychological web horror series Ben Drowned. The author would later go on to praise GAN applications for their ability to help generate assets for independent artists who are short on budget and manpower.[92][93]\n In May 2020, Nvidia researchers taught an AI system (termed \"GameGAN\") to recreate the game of Pac-Man simply by watching it being played.[94][95]\n In August 2019, a large dataset consisting of 12,197 MIDI songs each with paired lyrics and melody alignment was created for neural melody generation from lyrics using conditional GAN-LSTM (refer to sources at GitHub AI Melody Generation from Lyrics).[96]\n GANs have been used to \n In 1991, Juergen Schmidhuber published \"artificial curiosity\", neural networks in a zero-sum game.[108] The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. GANs can be regarded as a case where the environmental reaction is 1 or 0 depending on whether the first network's output is in a given set.[109]\n Other people had similar ideas but did not develop them similarly. An idea involving adversarial networks was published in a 2010 blog post by Olli Niemitalo.[110] This idea was never implemented and did not involve stochasticity in the generator and thus was not a generative model. It is now known as a conditional GAN or cGAN.[111] An idea similar to GANs was used to model animal behavior by Li, Gauci and Gross in 2013.[112]\n Another inspiration for GANs was noise-contrastive estimation,[113] which uses the same loss function as GANs and which Goodfellow studied during his PhD in 2010\u20132014.\n Adversarial machine learning has other uses besides generative modeling and can be applied to models other than neural networks. In control theory, adversarial learning based on neural networks was used in 2006 to train robust controllers in a game theoretic sense, by alternating the iterations between a minimizer policy, the controller, and a maximizer policy, the disturbance.[114][115]\n In 2017, a GAN was used for image enhancement focusing on realistic textures rather than pixel-accuracy, producing a higher image quality at high magnification.[116] In 2017, the first faces were generated.[117] These were exhibited in February 2018 at the Grand Palais.[118][119] Faces generated by StyleGAN[120] in 2019 drew comparisons with Deepfakes.[121][122][123]\n",
        "doc_number": 19
    },
    {
        "url": "https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)",
        "content": "A transformer is a deep learning architecture that was developed by researchers at Google and is based on the multi-head attention mechanism, which was proposed in the 2017 paper \"Attention Is All You Need\".[1] Text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.[1] At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.\n Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM).[2] Later variations have been widely adopted for training large language models (LLM) on large (language) datasets, such as the Wikipedia corpus and Common Crawl.[3]\n \nTransformers were first developed as an improvement over previous architectures for machine translation,[4][5] but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning,[6][7] audio,[8] multimodal learning, robotics,[9] and even playing chess.[10] It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs)[11] and BERT[12] (bidirectional encoder representations from transformers). For many years, sequence modelling and generation was done by using plain recurrent neural networks (RNNs). A well-cited early example was the Elman network (1990). In theory, the information from one token can propagate arbitrarily far down the sequence, but in practice the vanishing-gradient problem leaves the model's state at the end of a long sentence without precise, extractable information about preceding tokens.\n A key breakthrough was LSTM (1995),[note 1] a RNN which used various innovations to overcome the vanishing gradient problem, allowing efficient learning of long-sequence modelling. One key innovation was the use of an attention mechanism which used neurons that multiply the outputs of other neurons, so-called multiplicative units.[13] Neural networks using multiplicative units were later called sigma-pi networks[14] or higher-order networks.[15] LSTM became the standard architecture for long sequence modelling until the 2017 publication of Transformers.\nHowever, LSTM still used sequential processing, like most other RNNs.[note 2] Specifically, RNNs operate one token at a time from first to last; they cannot operate in parallel over all tokens in a sequence. \n Modern Transformers overcome this problem, but unlike RNNs, they require computation time that is quadratic in the size of the context window.  The linearly scaling fast weight controller (1992) learns  to compute a  weight matrix for further processing depending on the input.[16] One of its two networks has  \"fast weights\" or \"dynamic links\" (1981).[17][18][19] A slow neural network learns by gradient descent to generate keys and values for computing the weight changes of the fast neural network which computes answers to queries.[16] This was later shown to be equivalent to the unnormalized linear Transformer.[20][21]\n The idea of encoder-decoder sequence transduction had been developed in the early 2010s (see previous papers[22][23]). The papers most commonly cited as the originators that produced seq2seq are two concurrently published papers from 2014.[22][23]\n A 380M-parameter model for machine translation uses two long short-term memories (LSTM).[23] Its architecture consists of two parts. The encoder is an LSTM that takes in a sequence of tokens and turns it into a vector. The decoder is another LSTM that converts the vector into a sequence of tokens. Similarly, another 130M-parameter model used gated recurrent units (GRU) instead of LSTM.[22] Later research showed that GRUs are neither better nor worse than LSTMs for seq2seq.[24][25]\n These early seq2seq models had no attention mechanism, and the state vector is accessible only after the last word of the source text was processed. Although in theory such a vector retains the information about the whole original sentence, in practice the information is poorly preserved. This is because the input is processed sequentially by one recurrent network into a fixed-size output vector, which is then processed by another recurrent network into an output. If the input is long, then the output vector would not be able to contain all relevant information, degrading the output. As evidence, reversing the input sentence improved seq2seq translation.[26]\n The RNNsearch model introduced an attention mechanism to seq2seq for machine translation to solve the bottleneck problem (of the fixed-size output vector), allowing the model to process long-distance dependencies more easily. The name is because it \"emulates searching through a source sentence during decoding a translation\".[4]\n The relative performances were compared between global (that of RNNsearch) and local (sliding window) attention model architectures for machine translation, finding that mixed attention had higher quality than global attention, while local attention reduced translation time.[27]\n In 2016, Google Translate was revamped to Google Neural Machine Translation, which replaced the previous model based on statistical machine translation. The new model was a seq2seq model where the encoder and the decoder were both 8 layers of bidirectional LSTM.[28] It took nine months to develop, and it outperformed the statistical approach, which took ten years to develop.[29]\n Seq2seq models with attention (including self-attention) still suffered from the same issue with recurrent networks, which is that they are hard to parallelize, which prevented them from being accelerated on GPUs. In 2016, decomposable attention applied a self-attention mechanism to feedforward networks, which are easy to parallelize, and achieved SOTA result in textual entailment with an order of magnitude less parameters than LSTMs.[30] One of its authors, Jakob Uszkoreit, suspected that attention without recurrence is sufficient for language translation, thus the title \"attention is all you need\".[31] That hypothesis was against conventional wisdom at the time, and even his father Hans Uszkoreit, a well-known computational linguist, was skeptical.[31] In the same year, self-attention (called intra-attention or intra-sentence attention) was proposed for LSTMs.[32]\n In 2017, the original (100M-sized) encoder-decoder transformer model was proposed in the \"Attention is all you need\" paper. At the time, the focus of the research was on improving seq2seq for machine translation, by removing its recurrence to process all tokens in parallel, but preserving its dot-product attention mechanism to keep its text processing performance.[1] This led to the introduction of a multi-head attention model that was easier to parallelize due to the use of independent heads and the lack of recurrence. Its parallelizability was an important factor to its widespread use in large neural networks.[33]\n\n Already in spring 2017, even before the \"Attention is all you need\" preprint was published, one of the co-authors applied the \"decoder-only\" variation of the architecture to generate fictitious Wikipedia articles.[34] Transformer architecture is now used in many generative models that contribute to the ongoing AI boom.\n In language modelling, ELMo (2018) was a bi-directional LSTM that produces contextualized word embeddings, improving upon the line of research from bag of words and word2vec. It was followed by BERT (2018), an encoder-only Transformer model.[35] In 2019 October, Google started using BERT to process search queries.[36] In 2020, Google Translate replaced the previous RNN-encoder\u2013RNN-decoder model by a Transformer-encoder\u2013RNN-decoder model.[37]\n Starting in 2018, the OpenAI GPT series of decoder-only Transformers became state of the art in natural language generation. In 2022, a chatbot based on GPT-3, ChatGPT, became unexpectedly popular,[38] triggering a boom around large language models.[39][40]\n Since 2020, Transformers have been applied in modalities beyond text, including the vision transformer,[41] speech recognition,[42] robotics,[6] and multimodal.[43] The vision transformer, in turn, stimulated new developments in convolutional neural networks.[44] Image and video generators like DALL-E (2021), Stable Diffusion 3 (2024),[45] and Sora (2024), are based on the Transformer architecture.\n The plain transformer architecture had difficulty converging. In the original paper[1] the authors recommended using learning rate warmup. That is, the learning rate should linearly scale up from 0 to maximal value for the first part of the training (usually recommended to be 2% of the total number of training steps), before decaying again.\n A 2020 paper found that using layer normalization before (instead of after) multiheaded attention and feedforward layers stabilizes training, not requiring learning rate warmup.[46]\n Transformers typically are first pretrained by self-supervised learning on a large generic dataset, followed by supervised fine-tuning on a small task-specific dataset. The pretrain dataset is typically an unlabeled large corpus, such as The Pile. Tasks for pretraining and fine-tuning commonly include:\n The T5 transformer report[47] documents a large number of natural language pretraining tasks. Some examples are:\n Note that while each of these tasks is trivial or obvious for human native speakers of the language (or languages), they have typically proved challenging for previous generations of machine learning architecture.\n In general, there are 3 classes of language modelling tasks: \"masked\",[49] \"autoregressive\",[50] and \"prefixLM\".[51] These classes are independent of a specific modeling architecture such as Transformer, but they are often discussed in the context of Transformer.\n In a masked task,[49] one or more of the tokens is masked out, and the model would produce a probability distribution predicting what the masked-out tokens are based on the context. The loss function for the task is typically sum of log-perplexities for the masked-out tokens: \n\n\n\n\nLoss\n\n=\n\u2212\n\n\u2211\n\nt\n\u2208\n\nmasked tokens\n\n\n\nln\n\u2061\n(\n\nprobability of\u00a0\n\nt\n\n\u00a0conditional on its context\n\n)\n\n\n{\\displaystyle {\\text{Loss}}=-\\sum _{t\\in {\\text{masked tokens}}}\\ln({\\text{probability of }}t{\\text{ conditional on its context}})}\n\nand the model is trained to minimize this loss function. The BERT series of models are trained for masked token prediction and another task.\n In an autoregressive task,[50] the entire sequence is masked at first, and the model produces a probability distribution for the first token. Then the first token is revealed and the model predicts the second token, and so on. The loss function for the task is still typically the same. The GPT series of models are trained by autoregressive tasks.\n In a prefixLM task,[51] the sequence is divided into two parts. The first part is presented as context, and the model predicts the first token of the second part. Then that would be revealed, and the model predicts the second token, and so on. The loss function for the task is still typically the same. The T5 series of models are trained by prefixLM tasks.\n Note that \"masked\" as in \"masked language modelling\" is not \"masked\" as in \"masked attention\", and \"prefixLM\" (prefix language modeling) is not \"prefixLM\" (prefix language model).\n All transformers have the same primary components:\n The following description follows exactly the Transformer as described in the original paper. There are variants, described in the following section.\n By convention, we write all vectors as row vectors. This, for example, means that pushing a vector through a linear layer means multiplying it by a weight matrix on the right, as \n\n\n\nx\nW\n\n\n{\\displaystyle xW}\n\n.\n As the Transformer architecture natively processes numerical data, not text, there must be a translation between text and tokens. A token is an integer that represents a character, or a short segment of characters. On the input side, the input text is parsed into a token sequence. Similarly, on the output side, the output tokens are parsed back to text. The module doing the conversion between texts and token sequences  is a tokenizer.\n The set of all tokens is the vocabulary of the tokenizer, and its size is the vocabulary size \n\n\n\n\nn\n\nvocabulary\n\n\n\n\n{\\displaystyle n_{\\text{vocabulary}}}\n\n. When faced with tokens outside the vocabulary, typically a special token is used, written as \"[UNK]\" for \"unknown\".\n Some commonly used tokenizers are byte pair encoding, WordPiece, and SentencePiece.\n Each token is converted into an embedding vector via a lookup table. Equivalently stated, it multiplies a one-hot representation of the token by an embedding matrix \n\n\n\nM\n\n\n{\\displaystyle M}\n\n. For example, if the input token is \n\n\n\n3\n\n\n{\\displaystyle 3}\n\n, then the one-hot representation is \n\n\n\n[\n0\n,\n0\n,\n0\n,\n1\n,\n0\n,\n0\n,\n\u2026\n]\n\n\n{\\displaystyle [0,0,0,1,0,0,\\dots ]}\n\n, and its embedding vector is\n\n\n\n\nE\nm\nb\ne\nd\n\n(\n3\n)\n=\n[\n0\n,\n0\n,\n0\n,\n1\n,\n0\n,\n0\n,\n\u2026\n]\nM\n\n\n{\\displaystyle \\mathrm {Embed} (3)=[0,0,0,1,0,0,\\dots ]M}\n\nThe token embedding vectors are added to their respective positional encoding vectors (see below), producing the sequence of input vectors. \n The number of dimensions in an embedding vector is called hidden size or embedding size and written as \n\n\n\n\nd\n\nemb\n\n\n\n\n{\\displaystyle d_{\\text{emb}}}\n\n.[35] This size is written as \n\n\n\n\nd\n\nmodel\n\n\n\n\n{\\displaystyle d_{\\text{model}}}\n\n in the original Transformer paper.[1]\n An un-embedding layer is almost the reverse of an embedding layer. Whereas an embedding layer converts a token into a vector, an un-embedding layer converts a vector into a probability distribution over tokens.\n The un-embedding layer is a linear-softmax layer:\n\n\n\n\nU\nn\nE\nm\nb\ne\nd\n\n(\nx\n)\n=\n\ns\no\nf\nt\nm\na\nx\n\n(\nx\nW\n+\nb\n)\n\n\n{\\displaystyle \\mathrm {UnEmbed} (x)=\\mathrm {softmax} (xW+b)}\n\nThe matrix has shape \n\n\n\n(\n\nd\n\nemb\n\n\n,\n\nn\n\nvocabulary\n\n\n)\n\n\n{\\displaystyle (d_{\\text{emb}},n_{\\text{vocabulary}})}\n\n. The embedding matrix \n\n\n\nM\n\n\n{\\displaystyle M}\n\n and the un-embedding matrix \n\n\n\nW\n\n\n{\\displaystyle W}\n\n are sometimes required to be transposes of each other, a practice called weight tying.[52]\n A positional encoding is a fixed-size vector representation of the relative positions of tokens within a sequence: it provides the transformer model with information about where the words are in the input sequence. This shall induce a bias towards the order of the input sequence, so that, for example, the input sequence \"man bites dog\" is processed differently from \"dog bites man\".\n The positional encoding is defined as a function of type \n\n\n\nf\n:\n\nR\n\n\u2192\n\n\nR\n\n\nd\n\n\n;\nd\n\u2208\n\nZ\n\n,\nd\n>\n0\n\n\n{\\displaystyle f:\\mathbb {R} \\to \\mathbb {R} ^{d};d\\in \\mathbb {Z} ,d>0}\n\n, where \n\n\n\nd\n\n\n{\\displaystyle d}\n\n is a positive even integer. The full positional encoding defined in the original paper[1] is:\n\n\n\n(\nf\n(\nt\n\n)\n\n2\nk\n\n\n,\nf\n(\nt\n\n)\n\n2\nk\n+\n1\n\n\n)\n=\n(\nsin\n\u2061\n(\n\u03b8\n)\n,\ncos\n\u2061\n(\n\u03b8\n)\n)\n\n\u2200\nk\n\u2208\n{\n0\n,\n1\n,\n\u2026\n,\nd\n\n/\n\n2\n\u2212\n1\n}\n\n\n{\\displaystyle (f(t)_{2k},f(t)_{2k+1})=(\\sin(\\theta ),\\cos(\\theta ))\\quad \\forall k\\in \\{0,1,\\ldots ,d/2-1\\}}\n\nwhere \n\n\n\n\u03b8\n=\n\n\nt\n\nr\n\nk\n\n\n\n\n,\nr\n=\n\nN\n\n2\n\n/\n\nd\n\n\n\n\n{\\displaystyle \\theta ={\\frac {t}{r^{k}}},r=N^{2/d}}\n\n.\n Here, \n\n\n\nN\n\n\n{\\displaystyle N}\n\n is a free parameter that should be significantly larger than the biggest \n\n\n\nk\n\n\n{\\displaystyle k}\n\n that would be input into the positional encoding function. The original paper uses \n\n\n\nN\n=\n10000\n\n\n{\\displaystyle N=10000}\n\n.\n The function is in a simpler form when written as a complex function of type \n\n\n\nf\n:\n\nR\n\n\u2192\n\n\nC\n\n\nd\n\n/\n\n2\n\n\n\n\n{\\displaystyle f:\\mathbb {R} \\to \\mathbb {C} ^{d/2}}\n\n\n\n\n\nf\n(\nt\n)\n=\n\n\n(\n\ne\n\ni\nt\n\n/\n\n\nr\n\nk\n\n\n\n\n)\n\n\nk\n=\n0\n,\n1\n,\n\u2026\n,\n\n\nd\n2\n\n\n\u2212\n1\n\n\n\n\n{\\displaystyle f(t)=\\left(e^{it/r^{k}}\\right)_{k=0,1,\\ldots ,{\\frac {d}{2}}-1}}\n\nwhere \n\n\n\nr\n=\n\nN\n\n2\n\n/\n\nd\n\n\n\n\n{\\displaystyle r=N^{2/d}}\n\n.\n The main reason for using this positional encoding function is that using it, shifts are linear transformations:\n\n\n\nf\n(\nt\n+\n\u0394\nt\n)\n=\n\nd\ni\na\ng\n\n(\nf\n(\n\u0394\nt\n)\n)\nf\n(\nt\n)\n\n\n{\\displaystyle f(t+\\Delta t)=\\mathrm {diag} (f(\\Delta t))f(t)}\n\nwhere \n\n\n\n\u0394\nt\n\u2208\n\nR\n\n\n\n{\\displaystyle \\Delta t\\in \\mathbb {R} }\n\n is the distance one wishes to shift. This allows the transformer to take any encoded position, and find the encoding of the position n-steps-ahead or n-steps-behind, by a matrix multiplication.\n By taking a linear sum, any convolution can also be implemented as linear transformations:\n\n\n\n\n\u2211\n\nj\n\n\n\nc\n\nj\n\n\nf\n(\nt\n+\n\u0394\n\nt\n\nj\n\n\n)\n=\n\n(\n\n\n\u2211\n\nj\n\n\n\nc\n\nj\n\n\n\n\nd\ni\na\ng\n\n(\nf\n(\n\u0394\n\nt\n\nj\n\n\n)\n)\n\n)\n\nf\n(\nt\n)\n\n\n{\\displaystyle \\sum _{j}c_{j}f(t+\\Delta t_{j})=\\left(\\sum _{j}c_{j}\\,\\mathrm {diag} (f(\\Delta t_{j}))\\right)f(t)}\n\nfor any constants \n\n\n\n\nc\n\nj\n\n\n\n\n{\\displaystyle c_{j}}\n\n. This allows the transformer to take any encoded position and find a linear sum of the encoded locations of its neighbors. This sum of encoded positions, when fed into the attention mechanism, would create attention weights on its neighbors, much like what happens in a convolutional neural network language model. In the author's words, \"we hypothesized it would allow the model to easily learn to attend by relative position.\"\n In typical implementations, all operations are done over the real numbers, not the complex numbers, but since complex multiplication can be implemented as real 2-by-2 matrix multiplication, this is a mere notational difference.\n Like earlier seq2seq models, the original transformer model used an encoder-decoder architecture. The encoder consists of encoding layers that process all the input tokens together one layer after another, while the decoder consists of decoding layers that iteratively process the encoder's output and the decoder's output tokens so far.\n The purpose of each encoder layer is to create contextualized representations of the tokens, where each representation corresponds to a token that \"mixes\" information from other input tokens via self-attention mechanism. Each decoder layer contains two attention sublayers: (1) cross-attention for incorporating the output of encoder (contextualized input token representations), and (2) self-attention for \"mixing\" information among the input tokens to the decoder (i.e. the tokens generated so far during inference time).[53][54]\n Both the encoder and decoder layers have a feed-forward neural network for additional processing of their outputs and contain residual connections and layer normalization steps.[54] These feed-forward layers contain most of the parameters in a Transformer model.\n  The feedforward network (FFN) modules in a Transformer are 2-layered multilayer perceptrons:\n\n\n\n\nF\nF\nN\n\n(\nx\n)\n=\n\u03d5\n(\nx\n\nW\n\n(\n1\n)\n\n\n+\n\nb\n\n(\n1\n)\n\n\n)\n\nW\n\n(\n2\n)\n\n\n+\n\nb\n\n(\n2\n)\n\n\n\n\n{\\displaystyle \\mathrm {FFN} (x)=\\phi (xW^{(1)}+b^{(1)})W^{(2)}+b^{(2)}}\n\nwhere \n\n\n\n\u03d5\n\n\n{\\displaystyle \\phi }\n\n is its activation function. The original Transformer used ReLU activation.\n The number of neurons in the middle layer is called intermediate size (GPT),[55] filter size (BERT),[35] or feedforward size (BERT).[35] It is typically larger than the embedding size. For example, in both GPT-2 series and BERT series, the intermediate size of a model is 4 times its embedding size: \n\n\n\n\nd\n\nffn\n\n\n=\n4\n\nd\n\nemb\n\n\n\n\n{\\displaystyle d_{\\text{ffn}}=4d_{\\text{emb}}}\n\n.\n The attention mechanism used in the Transformer architecture are scaled dot-product attention units. For each unit, the transformer model learns three weight matrices: the query weights \n\n\n\n\nW\n\nQ\n\n\n\n\n{\\displaystyle W^{Q}}\n\n, the key weights \n\n\n\n\nW\n\nK\n\n\n\n\n{\\displaystyle W^{K}}\n\n, and the value weights \n\n\n\n\nW\n\nV\n\n\n\n\n{\\displaystyle W^{V}}\n\n.\n The module takes three sequences, a query sequence, a key sequence, and a value sequence. The query sequence is a sequence of length \n\n\n\n\n\u2113\n\nseq, query\n\n\n\n\n{\\displaystyle \\ell _{\\text{seq, query}}}\n\n, and each entry is a vector of dimension \n\n\n\n\nd\n\nemb, query\n\n\n\n\n{\\displaystyle d_{\\text{emb, query}}}\n\n. Similarly for the key and value sequences.\n For each vector \n\n\n\n\nx\n\ni\n,\n\nquery\n\n\n\n\n\n{\\displaystyle x_{i,{\\text{query}}}}\n\n in the query sequence, it is multiplied by a matrix \n\n\n\n\nW\n\nQ\n\n\n\n\n{\\displaystyle W^{Q}}\n\n to produce a query vector \n\n\n\n\nq\n\ni\n\n\n=\n\nx\n\ni\n,\n\nquery\n\n\n\n\nW\n\nQ\n\n\n\n\n{\\displaystyle q_{i}=x_{i,{\\text{query}}}W^{Q}}\n\n. The matrix of all query vectors is the query matrix:\n\n\n\nQ\n=\n\nX\n\nquery\n\n\n\nW\n\nQ\n\n\n\n\n{\\displaystyle Q=X_{\\text{query}}W^{Q}}\n\nSimilarly, we construct the key matrix \n\n\n\nK\n=\n\nX\n\nkey\n\n\n\nW\n\nK\n\n\n\n\n{\\displaystyle K=X_{\\text{key}}W^{K}}\n\n and the value matrix \n\n\n\nV\n=\n\nX\n\nvalue\n\n\n\nW\n\nV\n\n\n\n\n{\\displaystyle V=X_{\\text{value}}W^{V}}\n\n.\n It is usually the case that all \n\n\n\n\nW\n\nQ\n\n\n,\n\nW\n\nK\n\n\n,\n\nW\n\nV\n\n\n\n\n{\\displaystyle W^{Q},W^{K},W^{V}}\n\n are square matrices, meaning \n\n\n\n\nd\n\nemb, query\n\n\n=\n\nd\n\nquery\n\n\n\n\n{\\displaystyle d_{\\text{emb, query}}=d_{\\text{query}}}\n\n, etc.\n Attention weights are calculated using the query and key vectors: the attention weight \n\n\n\n\na\n\ni\nj\n\n\n\n\n{\\displaystyle a_{ij}}\n\n from token \n\n\n\ni\n\n\n{\\displaystyle i}\n\n to token \n\n\n\nj\n\n\n{\\displaystyle j}\n\n is the dot product between \n\n\n\n\nq\n\ni\n\n\n\n\n{\\displaystyle q_{i}}\n\n and \n\n\n\n\nk\n\nj\n\n\n\n\n{\\displaystyle k_{j}}\n\n. The attention weights are divided by the square root of the dimension of the key vectors, \n\n\n\n\n\n\nd\n\nk\n\n\n\n\n\n\n{\\displaystyle {\\sqrt {d_{k}}}}\n\n, which stabilizes gradients during training, and passed through a softmax which normalizes the weights. The fact that \n\n\n\n\nW\n\nQ\n\n\n\n\n{\\displaystyle W^{Q}}\n\n and \n\n\n\n\nW\n\nK\n\n\n\n\n{\\displaystyle W^{K}}\n\n are different matrices allows attention to be non-symmetric: if token \n\n\n\ni\n\n\n{\\displaystyle i}\n\n attends to token \n\n\n\nj\n\n\n{\\displaystyle j}\n\n (i.e. \n\n\n\n\nq\n\ni\n\n\n\u22c5\n\nk\n\nj\n\n\n\n\n{\\displaystyle q_{i}\\cdot k_{j}}\n\n is large), this does not necessarily mean that token \n\n\n\nj\n\n\n{\\displaystyle j}\n\n will attend to token \n\n\n\ni\n\n\n{\\displaystyle i}\n\n (i.e. \n\n\n\n\nq\n\nj\n\n\n\u22c5\n\nk\n\ni\n\n\n\n\n{\\displaystyle q_{j}\\cdot k_{i}}\n\n could be small). The output of the attention unit for token \n\n\n\ni\n\n\n{\\displaystyle i}\n\n is the weighted sum of the value vectors of all tokens, weighted by \n\n\n\n\na\n\ni\nj\n\n\n\n\n{\\displaystyle a_{ij}}\n\n, the attention from token \n\n\n\ni\n\n\n{\\displaystyle i}\n\n to each token.\n The attention calculation for all tokens can be expressed as one large matrix calculation using the softmax function, which is useful for training due to computational matrix operation optimizations that quickly compute matrix operations. The matrices \n\n\n\nQ\n\n\n{\\displaystyle Q}\n\n, \n\n\n\nK\n\n\n{\\displaystyle K}\n\n and \n\n\n\nV\n\n\n{\\displaystyle V}\n\n are defined as the matrices where the \n\n\n\ni\n\n\n{\\displaystyle i}\n\nth rows are vectors \n\n\n\n\nq\n\ni\n\n\n\n\n{\\displaystyle q_{i}}\n\n, \n\n\n\n\nk\n\ni\n\n\n\n\n{\\displaystyle k_{i}}\n\n, and \n\n\n\n\nv\n\ni\n\n\n\n\n{\\displaystyle v_{i}}\n\n respectively. Then we can represent the attention as\n\n\n\n\n\n\n\n\nAttention\n\n(\nQ\n,\nK\n,\nV\n)\n=\n\nsoftmax\n\n\n(\n\n\n\nQ\n\nK\n\n\nT\n\n\n\n\n\n\nd\n\nk\n\n\n\n\n\n)\n\nV\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}\n\n\n where the softmax is applied over each of the rows of the matrix.\n The number of dimensions in a query vector is query size \n\n\n\n\nd\n\nquery\n\n\n\n\n{\\displaystyle d_{\\text{query}}}\n\n and similarly for the key size \n\n\n\n\nd\n\nkey\n\n\n\n\n{\\displaystyle d_{\\text{key}}}\n\n and value size \n\n\n\n\nd\n\nvalue\n\n\n\n\n{\\displaystyle d_{\\text{value}}}\n\n. The output dimension of an attention head is its head dimension \n\n\n\n\nd\n\nhead\n\n\n\n\n{\\displaystyle d_{\\text{head}}}\n\n. The attention mechanism requires the following three equalities to hold:\n\n\n\n\n\u2113\n\nseq, key\n\n\n=\n\n\u2113\n\nseq, value\n\n\n,\n\n\nd\n\nquery\n\n\n=\n\nd\n\nkey\n\n\n,\n\n\nd\n\nvalue\n\n\n=\n\nd\n\nhead\n\n\n\n\n{\\displaystyle \\ell _{\\text{seq, key}}=\\ell _{\\text{seq, value}},\\;d_{\\text{query}}=d_{\\text{key}},\\;d_{\\text{value}}=d_{\\text{head}}}\n\nbut is otherwise unconstrained.\n If the attention head is used in a self-attention fashion, then \n\n\n\n\nX\n\nquery\n\n\n=\n\nX\n\nkey\n\n\n=\n\nX\n\nvalue\n\n\n\n\n{\\displaystyle X_{\\text{query}}=X_{\\text{key}}=X_{\\text{value}}}\n\n. If the attention head is used in a cross-attention fashion, then usually \n\n\n\n\nX\n\nquery\n\n\n\u2260\n\nX\n\nkey\n\n\n=\n\nX\n\nvalue\n\n\n\n\n{\\displaystyle X_{\\text{query}}\\neq X_{\\text{key}}=X_{\\text{value}}}\n\n. It is theoretically possible for all three to be different, but that is rarely the case in practice.\n One set of \n\n\n\n\n(\n\n\nW\n\nQ\n\n\n,\n\nW\n\nK\n\n\n,\n\nW\n\nV\n\n\n\n)\n\n\n\n{\\displaystyle \\left(W^{Q},W^{K},W^{V}\\right)}\n\n matrices is called an attention head, and each layer in a transformer model has multiple attention heads. While each attention head attends to the tokens that are relevant to each token, multiple attention heads allow the model to do this for different definitions of \"relevance\". Specifically, the query and key projection matrices, \n\n\n\n\nW\n\nQ\n\n\n\n\n{\\displaystyle W^{Q}}\n\n and \n\n\n\n\nW\n\nK\n\n\n\n\n{\\displaystyle W^{K}}\n\n , which are involved in the attention score computation, defines the \"relevance\". Meanwhile, the value projection matrix \n\n\n\n\nW\n\nV\n\n\n\n\n{\\displaystyle W^{V}}\n\n, in combination with the part of the output projection matrix \n\n\n\n\nW\n\nO\n\n\n\n\n{\\displaystyle W^{O}}\n\n, determines how the attended tokens influence what information is passed to subsequent layers and ultimately the output logits. In addition, the scope of attention, or the range of token relationships captured by each attention head, can expand as tokens pass through successive layers. This allows the model to capture more complex and long-range dependencies in deeper layers. Many transformer attention heads encode relevance relations that are meaningful to humans. For example, some attention heads can attend mostly to the next word, while others mainly attend from verbs to their direct objects.[56] The computations for each attention head can be performed in parallel, which allows for fast processing. The outputs for the attention layer are concatenated to pass into the feed-forward neural network layers.\n Concretely, let the multiple attention heads be indexed by \n\n\n\ni\n\n\n{\\displaystyle i}\n\n, then we have\n\n\n\n\nMultiheadedAttention\n\n(\nQ\n,\nK\n,\nV\n)\n=\n\n\nConcat\n\n\ni\n\u2208\n[\n\nn\n\nheads\n\n\n]\n\n\n(\n\nAttention\n\n(\nX\n\nW\n\ni\n\n\nQ\n\n\n,\nX\n\nW\n\ni\n\n\nK\n\n\n,\nX\n\nW\n\ni\n\n\nV\n\n\n)\n)\n\nW\n\nO\n\n\n\n\n{\\displaystyle {\\text{MultiheadedAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}({\\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V}))W^{O}}\n\n where the matrix \n\n\n\nX\n\n\n{\\displaystyle X}\n\n is the concatenation of word embeddings, and the matrices \n\n\n\n\nW\n\ni\n\n\nQ\n\n\n,\n\nW\n\ni\n\n\nK\n\n\n,\n\nW\n\ni\n\n\nV\n\n\n\n\n{\\displaystyle W_{i}^{Q},W_{i}^{K},W_{i}^{V}}\n\n are \"projection matrices\" owned by individual attention head \n\n\n\ni\n\n\n{\\displaystyle i}\n\n, and \n\n\n\n\nW\n\nO\n\n\n\n\n{\\displaystyle W^{O}}\n\n is a final projection matrix owned by the whole multi-headed attention head.\n It is theoretically possible for each attention head to have a different head dimension \n\n\n\n\nd\n\nhead\n\n\n\n\n{\\displaystyle d_{\\text{head}}}\n\n, but that is rarely the case in practice.\n As an example, in the smallest GPT-2 model, there are only self-attention mechanisms. It has the following dimensions:\n\n\n\n\nd\n\nemb\n\n\n=\n768\n,\n\nn\n\nhead\n\n\n=\n12\n,\n\nd\n\nhead\n\n\n=\n64\n\n\n{\\displaystyle d_{\\text{emb}}=768,n_{\\text{head}}=12,d_{\\text{head}}=64}\n\nSince \n\n\n\n12\n\u00d7\n64\n=\n768\n\n\n{\\displaystyle 12\\times 64=768}\n\n, its output projection matrix \n\n\n\n\nW\n\nO\n\n\n\u2208\n\n\nR\n\n\n(\n12\n\u00d7\n64\n)\n\u00d7\n768\n\n\n\n\n{\\displaystyle W^{O}\\in \\mathbb {R} ^{(12\\times 64)\\times 768}}\n\n is a square matrix.\n The Transformer architecture is constructed to calculate output tokens iteratively. Assuming \n\n\n\nt\n=\n0\n\n\n{\\displaystyle t=0}\n\n refers to the calculation of the first output token \n\n\n\ni\n=\n0\n\n\n{\\displaystyle i=0}\n\n, for step \n\n\n\nt\n>\n0\n\n\n{\\displaystyle t>0}\n\n, the output token \n\n\n\ni\n=\n0\n\n\n{\\displaystyle i=0}\n\n shall remain constant. This ensures properties of the model similar to autoregressive models.[1] Therefore, at every time step \n\n\n\nt\n\n\n{\\displaystyle t}\n\n, the calculation for all outputs \n\n\n\ni\n\n\n{\\displaystyle i}\n\n should not have access to tokens at position \n\n\n\nj\n\n\n{\\displaystyle j}\n\n for \n\n\n\nj\n>=\ni\n\n\n{\\displaystyle j>=i}\n\n (as it naturally is the case for time step \n\n\n\nt\n=\ni\n\n\n{\\displaystyle t=i}\n\n, when tokens \n\n\n\nj\n>\nt\n\n\n{\\displaystyle j>t}\n\n are not yet calculated). This behavior may be accomplished before the softmax stage by adding a mask matrix \n\n\n\nM\n\n\n{\\displaystyle M}\n\n that is \n\n\n\n\u2212\n\u221e\n\n\n{\\displaystyle -\\infty }\n\n at entries where the attention link must be cut, and \n\n\n\n0\n\n\n{\\displaystyle 0}\n\n at other places:\n\n\n\n\n\n\n\n\nMaskedAttention\n\n(\nQ\n,\nK\n,\nV\n)\n=\n\nsoftmax\n\n\n(\n\nM\n+\n\n\n\nQ\n\nK\n\n\nT\n\n\n\n\n\n\nd\n\nk\n\n\n\n\n\n\n)\n\nV\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}{\\text{MaskedAttention}}(Q,K,V)={\\text{softmax}}\\left(M+{\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\end{aligned}}}\n\n The following matrix is commonly used in decoder self-attention modules, called \"causal masking\":\n\n\n\n\nM\n\ncausal\n\n\n=\n\n\n[\n\n\n\n0\n\n\n\u2212\n\u221e\n\n\n\u2212\n\u221e\n\n\n\u2026\n\n\n\u2212\n\u221e\n\n\n\n\n0\n\n\n0\n\n\n\u2212\n\u221e\n\n\n\u2026\n\n\n\u2212\n\u221e\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n\u2026\n\n\n\u2212\n\u221e\n\n\n\n\n\u22ee\n\n\n\u22ee\n\n\n\u22ee\n\n\n\u22f1\n\n\n\u22ee\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n\u2026\n\n\n0\n\n\n\n]\n\n\n\n\n{\\displaystyle M_{\\text{causal}}={\\begin{bmatrix}0&-\\infty &-\\infty &\\dots &-\\infty \\\\0&0&-\\infty &\\dots &-\\infty \\\\0&0&0&\\dots &-\\infty \\\\\\vdots &\\vdots &\\vdots &\\ddots &\\vdots \\\\0&0&0&\\dots &0\\end{bmatrix}}}\n\n\n In words, it means that each token can pay attention to itself, and every token before it, but not any after it. A non-masked attention module can be thought of as a masked attention module where the mask has all entries zero. As an example of an uncommon use of mask matrix, the XLNet considers all masks of the form \n\n\n\nP\n\nM\n\ncausal\n\n\n\nP\n\n\u2212\n1\n\n\n\n\n{\\displaystyle PM_{\\text{causal}}P^{-1}}\n\n, where \n\n\n\nP\n\n\n{\\displaystyle P}\n\n is a random permutation matrix.[57]\n An encoder consists of an embedding layer, followed by multiple encoder layers.\n Each encoder layer consists of two major components: a self-attention mechanism and a feed-forward layer. It takes an input as a sequence of input vectors, applies the self-attention mechanism, to produce an intermediate sequence of vectors, then applies the feed-forward layer for each vector individually. Schematically, we have:\n\n\n\n\n\n\n\n\ngiven input vectors\u00a0\n\n\n\n\nh\n\n0\n\n\n,\n\nh\n\n1\n\n\n,\n\u2026\n\n\n\n\n\ncombine them into a matrix\u00a0\n\nH\n\n\n\n=\n\n\n[\n\n\n\n\nh\n\n0\n\n\n\n\n\n\n\nh\n\n1\n\n\n\n\n\n\n\u22ee\n\n\n\n]\n\n\n\n\n\n\n\nEncoderLayer\n\n(\nH\n)\n\n\n\n=\n\n\n[\n\n\n\n\nFFN\n\n(\n\nMultiheadedAttention\n\n(\nH\n,\nH\n,\nH\n\n)\n\n0\n\n\n)\n\n\n\n\n\nFFN\n\n(\n\nMultiheadedAttention\n\n(\nH\n,\nH\n,\nH\n\n)\n\n1\n\n\n)\n\n\n\n\n\u22ee\n\n\n\n]\n\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}{\\text{given input vectors }}&h_{0},h_{1},\\dots \\\\{\\text{combine them into a matrix }}H&={\\begin{bmatrix}h_{0}\\\\h_{1}\\\\\\vdots \\end{bmatrix}}\\\\{\\text{EncoderLayer}}(H)&={\\begin{bmatrix}{\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H)_{0})\\\\{\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H)_{1})\\\\\\vdots \\end{bmatrix}}\\\\\\end{aligned}}}\n\n\n where \n\n\n\n\nFFN\n\n\n\n{\\displaystyle {\\text{FFN}}}\n\n stands for \"feed-forward network\". We can more succinctly write it as\n\n\n\n\nEncoderLayer\n\n(\nH\n)\n=\n\nFFN\n\n(\n\nMultiheadedAttention\n\n(\nH\n,\nH\n,\nH\n)\n)\n\n\n{\\displaystyle {\\text{EncoderLayer}}(H)={\\text{FFN}}({\\text{MultiheadedAttention}}(H,H,H))}\n\nwith the implicit convention that the \n\n\n\n\nFFN\n\n\n\n{\\displaystyle {\\text{FFN}}}\n\n is applied to each row of the matrix individually.\n The encoder layers are stacked. The first encoder layer takes the sequence of input vectors from the embedding layer, producing a sequence of vectors. This sequence of vectors is processed by the second encoder, and so on. The output from the final encoder layer is then used by the decoder.\n As the encoder processes the entire input all at once, every token can attend to every other token (all-to-all attention), so there is no need for causal masking.\n A decoder consists of an embedding layer, followed by multiple decoder layers, followed by an un-embedding layer.\n Each decoder consists of three major components: a causally masked self-attention mechanism, a cross-attention mechanism, and a feed-forward neural network. The decoder functions in a similar fashion to the encoder, but an additional attention mechanism is inserted which instead draws relevant information from the encodings generated by the encoders. This mechanism can also be called the encoder-decoder attention.[1][54]\n Like the first encoder, the first decoder takes positional information and embeddings of the output sequence as its input, rather than encodings. The transformer must not use the current or future output to predict an output, so the output sequence must be partially masked to prevent this reverse information flow.[1] This allows for autoregressive text generation. For decoding, all-to-all attention is inappropriate, because a token cannot attend to tokens not yet generated. Thus, the self-attention module in the decoder is causally masked.\n In contrast, the cross-attention mechanism attends to the output vectors of the encoder, which is computed before the decoder starts decoding. Consequently, there is no need for masking in the cross-attention mechanism.\n Schematically, we have:\n\n\n\n\n\n\n\n\nH\n\u2032\n\n\n\n\n=\n\nMaskedMultiheadedAttention\n\n(\nH\n,\nH\n,\nH\n)\n\n\n\n\n\nDecoderLayer\n\n(\nH\n)\n\n\n\n=\n\nFFN\n\n(\n\nMultiheadedAttention\n\n(\n\nH\n\u2032\n\n,\n\nH\n\nE\n\n\n,\n\nH\n\nE\n\n\n)\n)\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}H'&={\\text{MaskedMultiheadedAttention}}(H,H,H)\\\\{\\text{DecoderLayer}}(H)&={\\text{FFN}}({\\text{MultiheadedAttention}}(H',H^{E},H^{E}))\\end{aligned}}}\n\nwhere \n\n\n\n\nH\n\nE\n\n\n\n\n{\\displaystyle H^{E}}\n\n is the matrix with rows being the output vectors from the encoder.\n The last decoder is followed by a final un-embedding layer. to produce the output probabilities over the vocabulary. Then, one of the tokens is sampled according to the probability, and the decoder can be run again to produce the next token, etc, autoregressively generating output text.\n Many large language models, since they do not need to predict a whole new sequence from an input sequence, only use the encoder or decoder of the original transformer architecture. Early GPT models are decoder-only models trained to predict the next token in a sequence.[58] BERT, another language model, only makes use of an encoder, and is trained to predict a randomly masked token in a sequence.[35]\n Each encoder layer contains 2 sublayers: the self-attention and the feedforward network. Each decoder layer contains 3 sublayers: the causally masked self-attention, the cross-attention, and the feedforward network.\n The final points of detail are the residual connections and layer normalization (LayerNorm, or LN), which while conceptually unnecessary, are necessary for numerical stability and convergence. Similarly to how the feedforward network modules are applied individually to each vector, the LayerNorm is also applied individually to each vector. \n There are two common conventions in use: the post-LN and the pre-LN convention. In the post-LN convention, the output of each sublayer is \n\n\n\n\nL\na\ny\ne\nr\nN\no\nr\nm\n\n(\nx\n+\n\nS\nu\nb\nl\na\ny\ne\nr\n\n(\nx\n)\n)\n\n\n{\\displaystyle \\mathrm {LayerNorm} (x+\\mathrm {Sublayer} (x))}\n\nwhere \n\n\n\n\nS\nu\nb\nl\na\ny\ne\nr\n\n(\nx\n)\n\n\n{\\displaystyle \\mathrm {Sublayer} (x)}\n\n is the function implemented by the sublayer itself.\n In the pre-LN convention, the output of each sublayer is\n\n\n\nx\n+\n\nS\nu\nb\nl\na\ny\ne\nr\n\n(\n\nL\na\ny\ne\nr\nN\no\nr\nm\n\n(\nx\n)\n)\n\n\n{\\displaystyle x+\\mathrm {Sublayer} (\\mathrm {LayerNorm} (x))}\n\nThe original 2017 Transformer used the post-LN convention. It was difficult to train and required careful hyperparameter tuning and a \"warm-up\" in learning rate, where it starts small and gradually increases. The pre-LN convention, proposed several times in 2018,[59] was found to be easier to train, requiring no warm-up, leading to faster convergence.[46]\n The following is the pseudocode for a standard pre-LN encoder-decoder Transformer, adapted from[60]\n The Transformer architecture, being modular, allows variations. Several common variations are described here.[61]\n An \"encoder-only\" Transformer applies the encoder to map an input text into a sequence of vectors that represent the input text. This is usually used for text embedding and representation learning for downstream applications. BERT is encoder-only. They are less often used currently, as they were found to be not significantly better than training an encoder-decoder Transformer, then taking just the encoder.[51]\n A \"decoder-only\" Transformer is not literally decoder-only, since without an encoder, the cross-attention mechanism has nothing to attend to. Thus, the decoder layers in a decoder-only Transformer is composed of just two sublayers: the causally masked self-attention, and the feedforward network. This is usually used for text generation and instruction following. The models in the GPT series and Chinchilla series are decoder-only.\n An \"encoder-decoder\" Transformer is generally the same as the original Transformer, with 2 sublayers per encoder layer and 3 sublayers per decoder layer, etc. They might have minor architectural improvements, such as alternative activation functions, changing the location of normalization, etc. This is also usually used for text generation and instruction following. The models in the T5 series are encoder-decoder.[61]\n A \"prefixLM\" (prefix language model) is a decoder-only architecture, but with prefix masking, which is different from causal masking. Specifically, it has mask of the form[61]:\u200aFigure 3\u200a\n\n\n\n\nM\n\nprefixLM\n\n\n=\n\n\n[\n\n\n\n\n0\n\n\n\n\u2212\n\u221e\n\n\n\n\n\n0\n\n\n\n\nM\n\ncausal\n\n\n\n\n\n]\n\n\n\n\n{\\displaystyle M_{\\text{prefixLM}}={\\begin{bmatrix}\\mathbf {0} &-\\infty \\\\\\mathbf {0} &M_{\\text{causal}}\\end{bmatrix}}}\n\nwhere the first columns correspond to the \"prefix\", and the subsequent columns correspond to the autoregressively generated text based on the prefix. They resemble encoder-decoder models, but has less \"sparsity\". Such models are rarely used, though they are cited as theoretical possibilities and benchmarked comparisons.[51]\n There are also mixed seq2seq models. For example, in 2020, Google Translate replaced the previous RNN-encoder\u2013RNN-decoder model by a Transformer-encoder\u2013RNN-decoder model, on the argument that an RNN-decoder runs much faster than Transformer-decoder when run autoregressively.[62]\n The original transformer uses ReLU activation function. Other activation functions were developed. The Llama series and PaLM used SwiGLU;[63] both GPT-1 and BERT[35] used GELU.[64]\n Alternative activation functions are often used in combination with Gated Linear Units in the feedforward module.[65]\n The normalization used in the Transformer can be different from LayerNorm. One example is RMSNorm[66] which is used in the Llama series. Other examples include CapsuleNorm[67] ScaleNorm,[68] or FixNorm.[68]\n Transformers may use other positional encoding methods than sinusoidal.[69]\n The original Transformer paper reported using a learned positional encoding,[70] but finding it not superior to the sinusoidal one.[1] Later, [71] found that causal masking itself provides enough signal to a Transformer decoder that it can learn to implicitly perform absolute positional encoding without the positional encoding module.\n RoPE (rotary positional embedding),[72] is best explained by considering a list of 2-dimensional vectors \n\n\n\n[\n(\n\nx\n\n1\n\n\n(\n1\n)\n\n\n,\n\nx\n\n1\n\n\n(\n2\n)\n\n\n)\n,\n(\n\nx\n\n2\n\n\n(\n1\n)\n\n\n,\n\nx\n\n2\n\n\n(\n2\n)\n\n\n)\n,\n(\n\nx\n\n3\n\n\n(\n1\n)\n\n\n,\n\nx\n\n3\n\n\n(\n2\n)\n\n\n)\n,\n.\n.\n.\n]\n\n\n{\\displaystyle [(x_{1}^{(1)},x_{1}^{(2)}),(x_{2}^{(1)},x_{2}^{(2)}),(x_{3}^{(1)},x_{3}^{(2)}),...]}\n\n. Now pick some angle \n\n\n\n\u03b8\n\n\n{\\displaystyle \\theta }\n\n. Then RoPE encoding is\n\n\n\n\nRoPE\n\n\n\n(\n\n\n\nx\n\nm\n\n\n(\n1\n)\n\n\n,\n\nx\n\nm\n\n\n(\n2\n)\n\n\n,\nm\n\n\n)\n\n\n=\n\n\n(\n\n\n\ncos\n\u2061\nm\n\u03b8\n\n\n\u2212\nsin\n\u2061\nm\n\u03b8\n\n\n\n\nsin\n\u2061\nm\n\u03b8\n\n\ncos\n\u2061\nm\n\u03b8\n\n\n\n)\n\n\n\n\n(\n\n\n\n\nx\n\nm\n\n\n(\n1\n)\n\n\n\n\n\n\n\nx\n\nm\n\n\n(\n2\n)\n\n\n\n\n\n)\n\n\n=\n\n\n(\n\n\n\n\nx\n\nm\n\n\n(\n1\n)\n\n\ncos\n\u2061\nm\n\u03b8\n\u2212\n\nx\n\nm\n\n\n(\n2\n)\n\n\nsin\n\u2061\nm\n\u03b8\n\n\n\n\n\nx\n\nm\n\n\n(\n2\n)\n\n\ncos\n\u2061\nm\n\u03b8\n+\n\nx\n\nm\n\n\n(\n1\n)\n\n\nsin\n\u2061\nm\n\u03b8\n\n\n\n)\n\n\n\n\n{\\displaystyle {\\text{RoPE}}{\\big (}x_{m}^{(1)},x_{m}^{(2)},m{\\big )}={\\begin{pmatrix}\\cos m\\theta &-\\sin m\\theta \\\\\\sin m\\theta &\\cos m\\theta \\end{pmatrix}}{\\begin{pmatrix}x_{m}^{(1)}\\\\x_{m}^{(2)}\\\\\\end{pmatrix}}={\\begin{pmatrix}x_{m}^{(1)}\\cos m\\theta -x_{m}^{(2)}\\sin m\\theta \\\\x_{m}^{(2)}\\cos m\\theta +x_{m}^{(1)}\\sin m\\theta \\\\\\end{pmatrix}}}\n\nEquivalently, if we write the 2-dimensional vectors as complex numbers \n\n\n\n\nz\n\nm\n\n\n:=\n\nx\n\nm\n\n\n(\n1\n)\n\n\n+\ni\n\nx\n\nm\n\n\n(\n2\n)\n\n\n\n\n{\\displaystyle z_{m}:=x_{m}^{(1)}+ix_{m}^{(2)}}\n\n, then RoPE encoding is just multiplication by an angle:\n\n\n\n\nRoPE\n\n\n\n(\n\n\n\nz\n\nm\n\n\n,\nm\n\n\n)\n\n\n=\n\ne\n\ni\nm\n\u03b8\n\n\n\nz\n\nm\n\n\n\n\n{\\displaystyle {\\text{RoPE}}{\\big (}z_{m},m{\\big )}=e^{im\\theta }z_{m}}\n\nFor a list of \n\n\n\n2\nn\n\n\n{\\displaystyle 2n}\n\n-dimensional vectors, a RoPE encoder is defined by a sequence of angles \n\n\n\n\n\u03b8\n\n(\n1\n)\n\n\n,\n.\n.\n.\n,\n\n\u03b8\n\n(\nn\n)\n\n\n\n\n{\\displaystyle \\theta ^{(1)},...,\\theta ^{(n)}}\n\n. Then the RoPE encoding is applied to each pair of coordinates.\n The benefit of RoPE is that the dot-product between two vectors depends on their relative location only:\n\n\n\n\nRoPE\n\n\n\n(\n\n\nx\n,\nm\n\n\n\n)\n\n\n\nT\n\n\n\nRoPE\n\n\n\n(\n\n\ny\n,\nn\n\n\n)\n\n\n=\n\nRoPE\n\n\n\n(\n\n\nx\n,\nm\n+\nk\n\n\n\n)\n\n\n\nT\n\n\n\nRoPE\n\n\n\n(\n\n\ny\n,\nn\n+\nk\n\n\n)\n\n\n\n\n{\\displaystyle {\\text{RoPE}}{\\big (}x,m{\\big )}^{T}{\\text{RoPE}}{\\big (}y,n{\\big )}={\\text{RoPE}}{\\big (}x,m+k{\\big )}^{T}{\\text{RoPE}}{\\big (}y,n+k{\\big )}}\n\n\nfor any integer \n\n\n\nk\n\n\n{\\displaystyle k}\n\n.\n ALiBi (Attention with Linear Biases)[73] is not a replacement for the positional encoder on the original transformer. Instead, it is an additional positional encoder that is directly plugged into the attention mechanism. Specifically, the ALiBi attention mechanism is\n\n\n\n\n\n\n\n\nAttention\n\n(\nQ\n,\nK\n,\nV\n)\n=\n\nsoftmax\n\n\n(\n\n\n\n\nQ\n\nK\n\n\nT\n\n\n\n\n\n\nd\n\nk\n\n\n\n\n\n+\ns\nB\n\n)\n\nV\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+sB\\right)V\\end{aligned}}}\n\nHere, \n\n\n\ns\n\n\n{\\displaystyle s}\n\n is a real number (\"scalar\"), and \n\n\n\nB\n\n\n{\\displaystyle B}\n\n is the linear bias matrix defined by\n\n\n\nB\n=\n\n\n(\n\n\n\n0\n\n\n1\n\n\n2\n\n\n3\n\n\n\u22ef\n\n\n\n\n\u2212\n1\n\n\n0\n\n\n1\n\n\n2\n\n\n\u22ef\n\n\n\n\n\u2212\n2\n\n\n\u2212\n1\n\n\n0\n\n\n1\n\n\n\u22ef\n\n\n\n\n\u2212\n3\n\n\n\u2212\n2\n\n\n\u2212\n1\n\n\n0\n\n\n\u22ef\n\n\n\n\n\u22ee\n\n\n\u22ee\n\n\n\u22ee\n\n\n\u22ee\n\n\n\u22f1\n\n\n\n)\n\n\n\n\n{\\displaystyle B={\\begin{pmatrix}0&1&2&3&\\cdots \\\\-1&0&1&2&\\cdots \\\\-2&-1&0&1&\\cdots \\\\-3&-2&-1&0&\\cdots \\\\\\vdots &\\vdots &\\vdots &\\vdots &\\ddots \\\\\\end{pmatrix}}}\n\nin other words, \n\n\n\n\nB\n\ni\n,\nj\n\n\n=\nj\n\u2212\ni\n\n\n{\\displaystyle B_{i,j}=j-i}\n\n. The idea being that the linear bias matrix is a softened mask. Just as \n\n\n\n0\n\n\n{\\displaystyle 0}\n\n represent full attention paid, and \n\n\n\n\u2212\n\u221e\n\n\n{\\displaystyle -\\infty }\n\n represents no attention paid, the linear bias matrix increases attention paid in one direction and decreases attention paid in the other direction.\n ALiBi allows pretraining on short context windows, then finetuning on longer context windows. Since it is directly plugged into the attention mechanism, it can be combined with any positional encoder that is plugged into the \"bottom\" of the entire network (which is where the sinusoidal encoder on the original transformer, as well as RoPE and many others, are located).\n Relative Position Encodings[74] is similar to ALiBi, but more generic:\n\n\n\n\n\n\n\n\nAttention\n\n(\nQ\n,\nK\n,\nV\n)\n=\n\nsoftmax\n\n\n(\n\n\n\n\nQ\n\nK\n\n\nT\n\n\n\n\n\n\nd\n\nk\n\n\n\n\n\n+\nB\n\n)\n\nV\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}{\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}+B\\right)V\\end{aligned}}}\n\nwhere \n\n\n\nB\n\n\n{\\displaystyle B}\n\n is a Toeplitz matrix, that is, \n\n\n\n\nB\n\ni\n,\nj\n\n\n=\n\nB\n\n\ni\n\u2032\n\n,\n\nj\n\u2032\n\n\n\n\n\n{\\displaystyle B_{i,j}=B_{i',j'}}\n\n whenever \n\n\n\ni\n\u2212\nj\n=\n\ni\n\u2032\n\n\u2212\n\nj\n\u2032\n\n\n\n{\\displaystyle i-j=i'-j'}\n\n. This is contrasted with the original sinusoidal positional encoding, which is an \"absolute positional encoding\".[75]\n The transformer model has been implemented in standard deep learning frameworks such as TensorFlow and PyTorch. Transformers is a library produced by Hugging Face that supplies transformer-based architectures and pretrained models.[11]\n FlashAttention[76] is an algorithm that implements the transformer attention mechanism efficiently on a GPU. It performs matrix multiplications in blocks, such that each block fits within the cache of a GPU, and by careful management of the blocks it minimizes data copying between GPU caches (as data movement is slow).\n An improved version, FlashAttention-2,[77][78][79] was developed to cater to the rising demand for language models capable of handling longer context lengths. It offers enhancements in work partitioning and parallelism, enabling it to achieve up to 230 TFLOPs/s on A100 GPUs (FP16/BF16), a 2x speed increase over the original FlashAttention.\n Key advancements in FlashAttention-2 include the reduction of non-matmul FLOPs, improved parallelism over the sequence length dimension, better work partitioning between GPU warps, and added support for head dimensions up to 256 and multi-query attention (MQA) and grouped-query attention (GQA).[80]\n Benchmarks revealed FlashAttention-2 to be up to 2x faster than FlashAttention and up to 9x faster than a standard attention implementation in PyTorch. Future developments include optimization for new hardware like H100 GPUs and new data types like FP8.\n Multi-Query Attention changes the multiheaded attention mechanism.[81] Whereas normally,\n \n\n\n\n\nMultiheadedAttention\n\n(\nQ\n,\nK\n,\nV\n)\n=\n\n\nConcat\n\n\ni\n\u2208\n[\n\nn\n\nheads\n\n\n]\n\n\n\n(\n\n\nAttention\n\n(\nX\n\nW\n\ni\n\n\nQ\n\n\n,\nX\n\nW\n\ni\n\n\nK\n\n\n,\nX\n\nW\n\ni\n\n\nV\n\n\n)\n\n)\n\n\nW\n\nO\n\n\n\n\n{\\displaystyle {\\text{MultiheadedAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}\\left({\\text{Attention}}(XW_{i}^{Q},XW_{i}^{K},XW_{i}^{V})\\right)W^{O}}\n\nwith Multi-Query Attention, there is just one \n\n\n\n\nW\n\nK\n\n\n,\n\nW\n\nV\n\n\n\n\n{\\displaystyle W^{K},W^{V}}\n\n, thus:\n \n\n\n\n\nMultiQueryAttention\n\n(\nQ\n,\nK\n,\nV\n)\n=\n\n\nConcat\n\n\ni\n\u2208\n[\n\nn\n\nheads\n\n\n]\n\n\n\n(\n\n\nAttention\n\n(\nX\n\nW\n\ni\n\n\nQ\n\n\n,\nX\n\nW\n\nK\n\n\n,\nX\n\nW\n\nV\n\n\n)\n\n)\n\n\nW\n\nO\n\n\n\n\n{\\displaystyle {\\text{MultiQueryAttention}}(Q,K,V)={\\text{Concat}}_{i\\in [n_{\\text{heads}}]}\\left({\\text{Attention}}(XW_{i}^{Q},XW^{K},XW^{V})\\right)W^{O}}\n\n\n This has a neutral effect on model quality and training speed, but increases inference speed. \n More generally, grouped-query attention (GQA) partitions attention heads into groups, each of which shares the key-value pair. MQA is GQA with one group, while standard multiheaded atteniton is GQA with the maximal number of groups.[82]\n When an autoregressive transformer is used for inference, such as generating text, the query vector is different at each step, but the already-computed key and value vectors are always the same. The KV caching method saves the computed key and value vectors at each attention block, so that they are not recomputed at each new token. PagedAttention applies memory paging to KV caching.[83][84][85]\n If a transformer is used with a baked-in prompt, such as [\"You are a customer support agent...\"], then the key and value vectors can be computed for the prompt, and saved on disk. The saving in compute is significant when the model is used for many short interactions, such as in online chatbots.\n Transformers are used in large language models for autoregressive sequence generation: generating a stream of text, one token at a time. However, in most settings, decoding from language models is memory-bound, meaning that we have spare compute power available. Speculative decoding[86][87] uses this spare compute power by computing several tokens in parallel. Similarly to speculative execution in CPUs, future tokens are computed concurrently, by speculating on the value of previous tokens, and are later discarded if it turns out the speculation was incorrect.\n Specifically, consider a transformer model like GPT-3 with a context window size of 512. To generate an entire context window autoregressively with greedy decoding, it must be run for 512 times, each time generating a token \n\n\n\n\nx\n\n1\n\n\n,\n\nx\n\n2\n\n\n,\n.\n.\n.\n,\n\nx\n\n512\n\n\n\n\n{\\displaystyle x_{1},x_{2},...,x_{512}}\n\n. However, if we had some educated guess for the values of these tokens, we could verify all of them in parallel, in one run of the model, by checking that each \n\n\n\n\nx\n\nt\n\n\n\n\n{\\displaystyle x_{t}}\n\n is indeed the token with the largest log-likelihood in the \n\n\n\nt\n\n\n{\\displaystyle t}\n\n-th output.\n In speculative decoding, a smaller model or some other simple heuristic is used to generate a few speculative tokens that are subsequently verified by the larger model. For example, suppose a small model generated four speculative tokens: \n\n\n\n\n\n\n\nx\n~\n\n\n\n\n1\n\n\n,\n\n\n\n\nx\n~\n\n\n\n\n2\n\n\n,\n\n\n\n\nx\n~\n\n\n\n\n3\n\n\n,\n\n\n\n\nx\n~\n\n\n\n\n4\n\n\n\n\n{\\displaystyle {\\tilde {x}}_{1},{\\tilde {x}}_{2},{\\tilde {x}}_{3},{\\tilde {x}}_{4}}\n\n. These tokens are run through the larger model, and only \n\n\n\n\n\n\n\nx\n~\n\n\n\n\n1\n\n\n\n\n{\\displaystyle {\\tilde {x}}_{1}}\n\n and \n\n\n\n\n\n\n\nx\n~\n\n\n\n\n2\n\n\n\n\n{\\displaystyle {\\tilde {x}}_{2}}\n\n are accepted. The same run of the large model already generated a new token \n\n\n\n\nx\n\n3\n\n\n\n\n{\\displaystyle x_{3}}\n\n to replace \n\n\n\n\n\n\n\nx\n~\n\n\n\n\n3\n\n\n\n\n{\\displaystyle {\\tilde {x}}_{3}}\n\n, and \n\n\n\n\n\n\n\nx\n~\n\n\n\n\n4\n\n\n\n\n{\\displaystyle {\\tilde {x}}_{4}}\n\n is completely discarded. The process then repeats (starting from the 4th token) until all tokens are generated.\n For non-greedy decoding, similar ideas apply, except the speculative tokens are accepted or rejected stochastically, in a way that guarantees the final output distribution is the same as if speculative decoding was not used.[86][88]\n Training transformer-based architectures can be expensive, especially for long inputs.[89] Many methods have been developed to attempt to address the issue. In the image domain, Swin Transformer is an efficient architecture that performs attention inside shifting windows.[90] In the audio domain, SepTr decouples the attention in time and frequency domains.[91] Long Range Arena (2020)[92] is a standard benchmark for comparing the behavior of transformer architectures over long inputs.\n The standard attention graph is either all-to-all or causal, both of which scales as \n\n\n\nO\n(\n\nN\n\n2\n\n\n)\n\n\n{\\displaystyle O(N^{2})}\n\n where \n\n\n\nN\n\n\n{\\displaystyle N}\n\n is the number of tokens in a sequence.\n Reformer (2020)[89][93] reduces the computational load from \n\n\n\nO\n(\n\nN\n\n2\n\n\n)\n\n\n{\\displaystyle O(N^{2})}\n\n to \n\n\n\nO\n(\nN\nln\n\u2061\nN\n)\n\n\n{\\displaystyle O(N\\ln N)}\n\n by using locality-sensitive hashing and reversible layers.[94]\n Sparse attention[95] uses attention graphs that grows slower than \n\n\n\nO\n(\n\nN\n\n2\n\n\n)\n\n\n{\\displaystyle O(N^{2})}\n\n. For example, BigBird (2020)[96] uses random small-world networks which grows as \n\n\n\nO\n(\nN\n)\n\n\n{\\displaystyle O(N)}\n\n.\n Ordinary transformers require a memory size that is quadratic in the size of the context window. Attention-free transformers[97] reduce this to a linear dependence while still retaining the advantages of a transformer by linking the key to the value.\n Random Feature Attention (2021)[98] uses Fourier random features:\n\n\n\n\u03c6\n(\nx\n)\n=\n\n\n1\n\nD\n\n\n\n[\ncos\n\u2061\n\u27e8\n\nw\n\n1\n\n\n,\nx\n\u27e9\n,\nsin\n\u2061\n\u27e8\n\nw\n\n1\n\n\n,\nx\n\u27e9\n,\n\u22ef\ncos\n\u2061\n\u27e8\n\nw\n\nD\n\n\n,\nx\n\u27e9\n,\nsin\n\u2061\n\u27e8\n\nw\n\nD\n\n\n,\nx\n\u27e9\n\n]\n\nT\n\n\n\n\n{\\displaystyle \\varphi (x)={\\frac {1}{\\sqrt {D}}}[\\cos \\langle w_{1},x\\rangle ,\\sin \\langle w_{1},x\\rangle ,\\cdots \\cos \\langle w_{D},x\\rangle ,\\sin \\langle w_{D},x\\rangle ]^{T}}\n\nwhere \n\n\n\n\nw\n\n1\n\n\n,\n.\n.\n.\n,\n\nw\n\nD\n\n\n\n\n{\\displaystyle w_{1},...,w_{D}}\n\n are independent samples from the normal distribution \n\n\n\nN\n(\n0\n,\n\n\u03c3\n\n2\n\n\nI\n)\n\n\n{\\displaystyle N(0,\\sigma ^{2}I)}\n\n. This choice of parameters satisfy \n\n\n\n\nE\n\n[\n\u27e8\n\u03c6\n(\nx\n)\n,\n\u03c6\n(\ny\n)\n\u27e9\n]\n=\n\ne\n\n\u2212\n\n\n\n\u2016\nx\n\u2212\ny\n\n\u2016\n\n2\n\n\n\n\n2\n\n\u03c3\n\n2\n\n\n\n\n\n\n\n\n\n{\\displaystyle \\mathbb {E} [\\langle \\varphi (x),\\varphi (y)\\rangle ]=e^{-{\\frac {\\|x-y\\|^{2}}{2\\sigma ^{2}}}}}\n\n, or \n\n\n\n\ne\n\n\u27e8\nx\n,\ny\n\u27e9\n\n/\n\n\n\u03c3\n\n2\n\n\n\n\n=\n\nE\n\n[\n\u27e8\n\ne\n\n\u2016\nx\n\n\u2016\n\n2\n\n\n\n/\n\n2\n\n\u03c3\n\n2\n\n\n\n\n\u03c6\n(\nx\n)\n,\n\ne\n\n\u2016\ny\n\n\u2016\n\n2\n\n\n\n/\n\n2\n\n\u03c3\n\n2\n\n\n\n\n\u03c6\n(\ny\n)\n\u27e9\n]\n\u2248\n\u27e8\n\ne\n\n\u2016\nx\n\n\u2016\n\n2\n\n\n\n/\n\n2\n\n\u03c3\n\n2\n\n\n\n\n\u03c6\n(\nx\n)\n,\n\ne\n\n\u2016\ny\n\n\u2016\n\n2\n\n\n\n/\n\n2\n\n\u03c3\n\n2\n\n\n\n\n\u03c6\n(\ny\n)\n\u27e9\n\n\n{\\displaystyle e^{\\langle x,y\\rangle /\\sigma ^{2}}=\\mathbb {E} [\\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle ]\\approx \\langle e^{\\|x\\|^{2}/2\\sigma ^{2}}\\varphi (x),e^{\\|y\\|^{2}/2\\sigma ^{2}}\\varphi (y)\\rangle }\n\nConsequently, the one-headed attention, with one query, can be written as \n\n\n\n\nAttention\n\n(\nq\n,\nK\n,\nV\n)\n=\n\nsoftmax\n\n\n(\n\n\n\nq\n\nK\n\n\nT\n\n\n\n\n\n\nd\n\nk\n\n\n\n\n\n)\n\nV\n\u2248\n\n\n\n\u03c6\n(\nq\n\n)\n\nT\n\n\n\n\u2211\n\ni\n\n\n\ne\n\n\u2016\n\nk\n\ni\n\n\n\n\u2016\n\n2\n\n\n\n/\n\n2\n\n\u03c3\n\n2\n\n\n\n\n\u03c6\n(\n\nk\n\ni\n\n\n)\n\nv\n\ni\n\n\nT\n\n\n\n\n\u03c6\n(\nq\n\n)\n\nT\n\n\n\n\u2211\n\ni\n\n\n\ne\n\n\u2016\n\nk\n\ni\n\n\n\n\u2016\n\n2\n\n\n\n/\n\n2\n\n\u03c3\n\n2\n\n\n\n\n\u03c6\n(\n\nk\n\ni\n\n\n)\n\n\n\n\n\n{\\displaystyle {\\text{Attention}}(q,K,V)={\\text{softmax}}\\left({\\frac {qK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx {\\frac {\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})v_{i}^{T}}{\\varphi (q)^{T}\\sum _{i}e^{\\|k_{i}\\|^{2}/2\\sigma ^{2}}\\varphi (k_{i})}}}\n\nwhere \n\n\n\n\u03c3\n=\n\nd\n\nK\n\n\n1\n\n/\n\n4\n\n\n\n\n{\\displaystyle \\sigma =d_{K}^{1/4}}\n\n. Similarly for multiple queries, and for multiheaded attention.\n This approximation can be computed in linear time, as we can compute the matrix \n\n\n\n\u03c6\n(\n\nk\n\ni\n\n\n)\n\nv\n\ni\n\n\nT\n\n\n\n\n{\\displaystyle \\varphi (k_{i})v_{i}^{T}}\n\n first, then multiply it with the query. In essence, we have managed to obtain a more precise version of \n\n\n\n\nAttention\n\n(\nQ\n,\nK\n,\nV\n)\n=\n\nsoftmax\n\n\n(\n\n\n\nQ\n\nK\n\n\nT\n\n\n\n\n\n\nd\n\nk\n\n\n\n\n\n)\n\nV\n\u2248\nQ\n(\n\nK\n\nT\n\n\nV\n\n/\n\n\n\n\nd\n\nk\n\n\n\n\n)\n\n\n{\\displaystyle {\\text{Attention}}(Q,K,V)={\\text{softmax}}\\left({\\frac {QK^{\\mathrm {T} }}{\\sqrt {d_{k}}}}\\right)V\\approx Q(K^{T}V/{\\sqrt {d_{k}}})}\n\nPerformer (2022)[99] uses the same Random Feature Attention, but \n\n\n\n\nw\n\n1\n\n\n,\n.\n.\n.\n,\n\nw\n\nD\n\n\n\n\n{\\displaystyle w_{1},...,w_{D}}\n\n are first independently sampled from the normal distribution \n\n\n\nN\n(\n0\n,\n\n\u03c3\n\n2\n\n\nI\n)\n\n\n{\\displaystyle N(0,\\sigma ^{2}I)}\n\n, then they are Gram-Schmidt processed.\n Transformers can also be used/adapted for modalities (input or output) beyond just text, usually by finding a way to \"tokenize\" the modality.\n Multimodal models can either be trained from scratch, or by finetuning. A 2022 study found that Transformers pretrained only on natural language can be finetuned on only 0.03% of parameters and become competitive with LSTMs on a variety of logical and visual tasks, demonstrating transfer learning.[100] The LLaVA was a vision-language model composed of a language model (Vicuna-13B)[101] and a vision model (ViT-L/14), connected by a linear layer. Only the linear layer is finetuned.[102]\n Vision transformers[41] adapt the transformer to computer vision by breaking down input images as a series of patches, turning them into vectors, and treating them like tokens in a standard transformer.\n Conformer[42] and later Whisper[103] follow the same pattern for speech recognition, first turning the speech signal into a spectrogram, which is then treated like an image, i.e. broken down into a series of patches, turned into vectors and treated like tokens in a standard transformer.\n Perceivers[104][105] are a variant of Transformers designed for multimodality.\n For image generation, notable architectures are DALL-E 1 (2021), Parti (2022),[106] Phenaki (2023),[107] and Muse (2023).[108] Unlike later models, DALL-E is not a diffusion model. Instead, it uses a decoder-only Transformer that autoregressively generates a text, followed by the token representation of an image, which is then converted by a variational autoencoder to an image.[109] Parti is an encoder-decoder Transformer, where the encoder processes a text prompt, and the decoder generates a token representation of an image.[110] Muse is an encoder-only Transformer that is trained to predict masked image tokens from unmasked image tokens. During generation, all input tokens are masked, and the highest-confidence predictions are included for the next iteration, until all tokens are predicted.[108] Phenaki is a text-to-video model. It is a bidirectional masked transformer conditioned on pre-computed text tokens. The generated tokens are then decoded to a video.[107]\n The transformer has had great success in natural language processing (NLP). Many large language models such as GPT-2, GPT-3, GPT-4, AlbertAGPT, Claude, BERT, XLNet, RoBERTa and ChatGPT demonstrate the ability of transformers to perform a wide variety of NLP-related subtasks and their related real-world or practical applications, including:\n Beyond traditional NLP, the transformer architecture has had success in other applications, such as:\n",
        "doc_number": 20
    },
    {
        "url": "https://en.wikipedia.org/wiki/Automated_machine_learning",
        "content": "Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. It is the combination of automation and ML.[1]\n AutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning.[2][3] The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models.[4]\n Common techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search.\n In a typical machine learning application, practitioners have a set of input data points to be used for training. The raw data may not be in a form that all algorithms can be applied to. To make the data amenable for machine learning, an expert may have to apply appropriate data pre-processing, feature engineering, feature extraction, and feature selection methods. After these steps, practitioners must then perform algorithm selection and hyperparameter optimization to maximize the predictive performance of their model. If deep learning is used, the architecture of the neural network must also be chosen manually by the machine learning expert. \n Each of these steps may be challenging, resulting in significant hurdles to using machine learning. AutoML aims to simplify these steps for non-experts, and to make it easier for them to use machine learning techniques correctly and effectively.\n AutoML plays an important role within the broader approach of automating data science, which also includes challenging tasks such as data engineering, data exploration and model interpretation and prediction.[5]\n Automated machine learning can target various stages of the machine learning process.[3]  Steps to automate are:\n There are a number of key challenges being tackled around automated machine learning. A big issue surrounding the field is referred to as \"development as a cottage industry\".[7] This phrase refers to the issue in machine learning where development relies on manual decisions and biases of experts. This is contrasted to the goal of machine learning which is to create systems that can learn and improve from their own usage and analysis of the data. Basically, it's the struggle between how much experts should get involved in the learning of the systems versus how much freedom they should be giving the machines. However, experts and developers must help create and guide these machines to prepare them for their own learning. To create this system, it requires labor intensive work with knowledge of machine learning algorithms and system design.[8]\n Additionally, some other challenges include meta-learning challenges[9] and computational resource allocation.\n",
        "doc_number": 21
    },
    {
        "url": "https://en.wikipedia.org/wiki/Explainable_artificial_intelligence",
        "content": "Explainable AI (XAI), often overlapping with interpretable AI, or explainable machine learning (XML), is a field of research within artificial intelligence (AI) that explores methods that provide humans with the ability of intellectual oversight over AI algorithms.[1][2] The main focus is on the reasoning behind the decisions or predictions made by the AI algorithms,[3] to make them more understandable and transparent.[4] This addresses users' requirement to assess safety and scrutinize the automated decision making in applications.[5] XAI counters the \"black box\" tendency of machine learning, where even the AI's designers cannot explain why it arrived at a specific decision.[6][7]\n XAI hopes to help users of AI-powered systems perform more effectively by improving their understanding of how those systems reason.[8] XAI may be an implementation of the social right to explanation.[9] Even if there is no such legal right or regulatory requirement, XAI can improve the user experience of a product or service by helping end users trust that the AI is making good decisions.[10] XAI aims to explain what has been done, what is being done, and what will be done next, and to unveil which information these actions are based on.[11] This makes it possible to confirm existing knowledge, challenge existing knowledge, and generate new assumptions.[12]\n Machine learning (ML) algorithms used in AI can be categorized as white-box or black-box.[13] White-box models provide results that are understandable to experts in the domain. Black-box models, on the other hand, are extremely hard to explain and may not be understood even by domain experts.[14] XAI algorithms follow the three principles of transparency, interpretability, and explainability. A model is transparent \"if the processes that extract model parameters from training data and generate labels from testing data can be described and motivated by the approach designer.\"[15] Interpretability describes the possibility of comprehending the ML model and presenting the underlying basis for decision-making in a way that is understandable to humans.[16][17][18] Explainability is a concept that is recognized as important, but a consensus definition is not yet available;[15] one possibility is \"the collection of features of the interpretable domain that have contributed, for a given example, to producing a decision (e.g., classification or regression)\".[19] \nIn summary, Interpretability refers to the user's ability to understand model outputs, while Model Transparency includes Simulatability (reproducibility of predictions), Decomposability (intuitive explanations for parameters), and Algorithmic Transparency (explaining how algorithms work). Model Functionality focuses on textual descriptions, visualization, and local explanations, which clarify specific outputs or instances rather than entire models. All these concepts aim to enhance the comprehensibility and usability of AI systems.[20]\nIf algorithms fulfill these principles, they provide a basis for justifying decisions, tracking them and thereby verifying them, improving the algorithms, and exploring new facts.[21]\n Sometimes it is also possible to achieve a high-accuracy result with white-box ML algorithms. These algorithms have an interpretable structure that can be used to explain predictions.[22] Concept Bottleneck Models, which use concept-level abstractions to explain model reasoning, are examples of this and can be applied in both image[23] and text[24] prediction tasks. This is especially important in domains like medicine, defense, finance, and law, where it is crucial to understand decisions and build trust in the algorithms.[11] Many researchers argue that, at least for supervised machine learning, the way forward is symbolic regression, where the algorithm searches the space of mathematical expressions to find the model that best fits a given dataset.[25][26][27]\n AI systems optimize behavior to satisfy a mathematically specified goal system chosen by the system designers, such as the command \"maximize the accuracy of assessing how positive film reviews are in the test dataset.\" The AI may learn useful general rules from the test set, such as \"reviews containing the word \"horrible\" are likely to be negative.\" However, it may also learn inappropriate rules, such as \"reviews containing 'Daniel Day-Lewis' are usually positive\"; such rules may be undesirable if they are likely to fail to generalize outside the training set, or if people consider the rule to be \"cheating\" or \"unfair.\" A human can audit rules in an XAI to get an idea of how likely the system is to generalize to future real-world data outside the test set.[28]\n Cooperation between agents \u2013 in this case, algorithms and humans \u2013 depends on trust. If humans are to accept algorithmic prescriptions, they need to trust them. Incompleteness in formal trust criteria is a barrier to optimization. Transparency, interpretability, and explainability are intermediate goals on the road to these more comprehensive trust criteria.[29] This is particularly relevant in medicine,[30] especially with clinical decision support systems (CDSS), in which medical professionals should be able to understand how and why a machine-based decision was made in order to trust the decision and augment their decision-making process.[31]\n AI systems sometimes learn undesirable tricks that do an optimal job of satisfying explicit pre-programmed goals on the training data but do not reflect the more nuanced implicit desires of the human system designers or the full complexity of the domain data. For example, a 2017 system tasked with image recognition learned to \"cheat\" by looking for a copyright tag that happened to be associated with horse pictures rather than learning how to tell if a horse was actually pictured.[7] In another 2017 system, a supervised learning AI tasked with grasping items in a virtual world learned to cheat by placing its manipulator between the object and the viewer in a way such that it falsely appeared to be grasping the object.[32][33]\n One transparency project, the DARPA XAI program, aims to produce \"glass box\" models that are explainable to a \"human-in-the-loop\" without greatly sacrificing AI performance. Human users of such a system can understand the AI's cognition (both in real-time and after the fact) and can determine whether to trust the AI.[34] Other applications of XAI are knowledge extraction from black-box models and model comparisons.[35] In the context of monitoring systems for ethical and socio-legal compliance, the term \"glass box\" is commonly used to refer to tools that track the inputs and outputs of the system in question, and provide value-based explanations for their behavior. These tools aim to ensure that the system operates in accordance with ethical and legal standards, and that its decision-making processes are transparent and accountable. The term \"glass box\" is often used in contrast to \"black box\" systems, which lack transparency and can be more difficult to monitor and regulate.[36]\nThe term is also used to name a voice assistant that produces counterfactual statements as explanations.[37]\n There is a subtle difference between the terms explainability and interpretability in the context of AI.[38]\n Some explainability techniques don't involve understanding how the model works, and may work across various AI systems. Treating the model as a black box and analyzing how marginal changes to the inputs affect the result sometimes provides a sufficient explanation.\n Explainability is useful for ensuring that AI models are not making decisions based on irrelevant or otherwise unfair criteria. For classification and regression models, several popular techniques exist:\n For images, saliency maps highlight the parts of an image that most influenced the result.[43]\n Systems that are expert or knowledge based are software systems that are made my experts. This system consists of a knowledge based encoding for the domain knowledge. This system is usually modeled as production rules, and someone uses this knowledge base which the user can question the system for knowledge. In expert systems, the language and explanations are understood with an explanation for the reasoning or a problem solving activity.[44]\n However, these techniques are not very suitable for language models like generative pretrained transformers. Since these models generate language, they can provide an explanation, but which may not be reliable. Other techniques include attention analysis (examining how the model focuses on different parts of the input), probing methods (testing what information is captured in the model's representations), causal tracing (tracing the flow of information through the model) and circuit discovery (identifying specific subnetworks responsible for certain behaviors). Explainability research in this area overlaps significantly with interpretability and alignment research.[45]\n Scholars sometimes use the term \"mechanistic interpretability\" to refer to the process of reverse-engineering artificial neural networks to understand their internal decision-making mechanisms and components, similar to how one might analyze a complex machine or computer program.[46]\n Interpretability research often focuses on generative pretrained transformers. It is particularly relevant for AI safety and alignment, as it may enable to identify signs of undesired behaviors such as sycophancy, deceptiveness or bias, and to better steer AI models.[47]\n Studying the interpretability of the most advanced foundation models often involves searching for an automated way to identify \"features\" in generative pretrained transformers. In a neural network, a feature is a pattern of neuron activations that corresponds to a concept. A compute-intensive technique called \"dictionary learning\" makes it possible to identify features to some degree. Enhancing the ability to identify and edit features is expected to significantly improve the safety of frontier AI models.[48][49]\n For convolutional neural networks, DeepDream can generate images that strongly activate a particular neuron, providing a visual hint about what the neuron is trained to identify.[50]\n During the 1970s to 1990s, symbolic reasoning systems, such as MYCIN,[51] GUIDON,[52] SOPHIE,[53] and PROTOS[54][55] could represent, reason about, and explain their reasoning for diagnostic, instructional, or machine-learning (explanation-based learning) purposes. MYCIN, developed in the early 1970s as a research prototype for diagnosing bacteremia infections of the bloodstream, could explain[56] which of its hand-coded rules contributed to a diagnosis in a specific case. Research in intelligent tutoring systems resulted in developing systems such as SOPHIE that could act as an \"articulate expert\", explaining problem-solving strategy at a level the student could understand, so they would know what action to take next. For instance, SOPHIE could explain the qualitative reasoning behind its electronics troubleshooting, even though it ultimately relied on the SPICE circuit simulator. Similarly, GUIDON added tutorial rules to supplement MYCIN's domain-level rules so it could explain the strategy for medical diagnosis. Symbolic approaches to machine learning relying on explanation-based learning, such as PROTOS, made use of explicit representations of explanations expressed in a dedicated explanation language, both to explain their actions and to acquire new knowledge.[55]\n In the 1980s through the early 1990s, truth maintenance systems (TMS) extended the capabilities of causal-reasoning, rule-based, and logic-based inference systems.[57]:\u200a360\u2013362\u200a A TMS explicitly tracks alternate lines of reasoning, justifications for conclusions, and lines of reasoning that lead to contradictions, allowing future reasoning to avoid these dead ends. To provide an explanation, they trace reasoning from conclusions to assumptions through rule operations or logical inferences, allowing explanations to be generated from the reasoning traces. As an example, consider a rule-based problem solver with just a few rules about Socrates that concludes he has died from poison:\n By just tracing through the dependency structure the problem solver can construct the following explanation: \"Socrates died because he was mortal and drank poison, and all mortals die when they drink poison. Socrates was mortal because he was a man and all men are mortal. Socrates drank poison because he held dissident beliefs, the government was conservative, and those holding conservative dissident beliefs under conservative governments must drink poison.\"[58]:\u200a164\u2013165\u200a By the 1990s researchers began studying whether it is possible to meaningfully extract the non-hand-coded rules being generated by opaque trained neural networks.[59] Researchers in clinical expert systems creating[clarification needed] neural network-powered decision support for clinicians sought to develop dynamic explanations that allow these technologies to be more trusted and trustworthy in practice.[9] In the 2010s public concerns about racial and other bias in the use of AI for criminal sentencing decisions and findings of creditworthiness may have led to increased demand for transparent artificial intelligence.[7] As a result, many academics and organizations are developing tools to help detect bias in their systems.[60]\n Marvin Minsky et al. raised the issue that AI can function as a form of surveillance, with the biases inherent in surveillance, suggesting HI (Humanistic Intelligence) as a way to create a more fair and balanced \"human-in-the-loop\" AI.[61]\n Explainable AI has been recently a new topic researched amongst the context of modern deep learning. Modern complex AI techniques, such as deep learning, are naturally opaque.[62] To address this issue, methods have been developed to make new models more explainable and interpretable.[63][17][16][64][65][66] This includes layerwise relevance propagation (LRP), a technique for determining which features in a particular input vector contribute most strongly to a neural network's output.[67][68] Other techniques explain some particular prediction made by a (nonlinear) black-box model, a goal referred to as \"local interpretability\".[69][70][71][72][73][74] We still today cannot explain the output of today's DNNs without the new explanatory mechanisms, we also can't by the neural network, or external explanatory components [75] There is also research on whether the concepts of local interpretability can be applied to a remote context, where a model is operated by a third-party.[76][77]\n There has been work on making glass-box models which are more transparent to inspection.[22][78] This includes decision trees,[79] Bayesian networks, sparse linear models,[80] and more.[81] The Association for Computing Machinery Conference on Fairness, Accountability, and Transparency (ACM FAccT) was established in 2018 to study transparency and explainability in the context of socio-technical systems, many of which include artificial intelligence.[82][83]\n Some techniques allow visualisations of the inputs to which individual software neurons respond to most strongly. Several groups found that neurons can be aggregated into circuits that perform human-comprehensible functions, some of which reliably arise across different networks trained independently.[84][85]\n There are various techniques to extract compressed representations of the features of given inputs, which can then be analysed by standard clustering techniques. Alternatively, networks can be trained to output linguistic explanations of their behaviour, which are then directly human-interpretable.[86] Model behaviour can also be explained with reference to training data\u2014for example, by evaluating which training inputs influenced a given behaviour the most.[87]\n The use of explainable artificial intelligence (XAI) in pain research, specifically in understanding the role of electrodermal activity for automated pain recognition: hand-crafted features and deep learning models in pain recognition, highlighting the insights that simple hand-crafted features can yield comparative performances to deep learning models and that both traditional feature engineering and deep feature learning approaches rely on simple characteristics of the input time-series data.[88]\n As regulators, official bodies, and general users come to depend on AI-based dynamic systems, clearer accountability will be required for automated decision-making processes to ensure trust and transparency. The first global conference exclusively dedicated to this emerging discipline was the 2017 International Joint Conference on Artificial Intelligence: Workshop on Explainable Artificial Intelligence (XAI).[89] It has evolved over the years, with various workshops organised and co-located to many other international conferences, and it has now a dedicated global event, \"The world conference on eXplainable Artificial Intelligence\", with its own proceedings.[90][91]\n The European Union introduced a right to explanation in the General Data Protection Regulation (GDPR) to address potential problems stemming from the rising importance of algorithms. The implementation of the regulation began in 2018. However, the right to explanation in GDPR covers only the local aspect of interpretability. In the United States, insurance companies are required to be able to explain their rate and coverage decisions.[92] In France the Loi pour une R\u00e9publique num\u00e9rique (Digital Republic Act) grants subjects the right to request and receive information pertaining to the implementation of algorithms that process data about them.\n Despite ongoing endeavors to enhance the explainability of AI models, they persist with several inherent limitations.\n By making an AI system more explainable, we also reveal more of its inner workings. For example, the explainability method of feature importance identifies features or variables that are most important in determining the model's output, while the influential samples method identifies the training samples that are most influential in determining the output, given a particular input.[93] Adversarial parties could take advantage of this knowledge.\n For example, competitor firms could replicate aspects of the original AI system in their own product, thus reducing competitive advantage.[94] An explainable AI system is also susceptible to being \u201cgamed\u201d\u2014influenced in a way that undermines its intended purpose. One study gives the example of a predictive policing system; in this case, those who could potentially \u201cgame\u201d the system are the criminals subject to the system's decisions. In this study, developers of the system discussed the issue of criminal gangs looking to illegally obtain passports, and they expressed concerns that, if given an idea of what factors might trigger an alert in the passport application process, those gangs would be able to \u201csend guinea pigs\u201d to test those triggers, eventually finding a loophole that would allow them to \u201creliably get passports from under the noses of the authorities\u201d.[95]\n Many approaches that it uses provides explanation in general, it doesn't take account for the diverse backgrounds and knowledge level of the users. This leads to challenges with accurate comprehension for all users. Expert users can find the explanations lacking in depth, and are oversimplified, while a beginner user may struggle understanding the explanations as they are complex. This limitation downplays the ability of the XAI techniques to appeal to their users with different levels of knowledge, which can impact the trust from users and who uses it. The quality of explanations can be different amongst their users as they all have different expertise levels, including different situation and conditions[96]\n A fundamental barrier to making AI systems explainable is the technical complexity of such systems. End users often lack the coding knowledge required to understand software of any kind. Current methods used to explain AI are mainly technical ones, geared toward machine learning engineers for debugging purposes, rather than toward the end users who are ultimately affected by the system, causing \u201ca gap between explainability in practice and the goal of transparency\u201d.[93] Proposed solutions to address the issue of technical complexity include either promoting the coding education of the general public so technical explanations would be more accessible to end users, or providing explanations in layperson terms.[94]\n The solution must avoid oversimplification. It is important to strike a balance between accuracy \u2013 how faithfully the explanation reflects the process of the AI system \u2013 and explainability \u2013 how well end users understand the process. This is a difficult balance to strike, since the complexity of machine learning makes it difficult for even ML engineers to fully understand, let alone non-experts.[93]\n The goal of explainability to end users of AI systems is to increase trust in the systems, even \u201caddress concerns about lack of \u2018fairness\u2019 and discriminatory effects\u201d.[94] However, even with a good understanding of an AI system, end users may not necessarily trust the system.[97] In one study, participants were presented with combinations of white-box and black-box explanations, and static and interactive explanations of AI systems. While these explanations served to increase both their self-reported and objective understanding, it had no impact on their level of trust, which remained skeptical.[98]\n This outcome was especially true for decisions that impacted the end user in a significant way, such as graduate school admissions. Participants judged algorithms to be too inflexible and unforgiving in comparison to human decision-makers; instead of rigidly adhering to a set of rules, humans are able to consider exceptional cases as well as appeals to their initial decision.[98] For such decisions, explainability will not necessarily cause end users to accept the use of decision-making algorithms. We will need to either turn to another method to increase trust and acceptance of decision-making algorithms, or question the need to rely solely on AI for such impactful decisions in the first place.\n However, some emphasize that the purpose of explainability of artificial intelligence is not to merely increase users' trust in the system's decisions, but to calibrate the users' level of trust to the correct level.[99] According to this principle, too much or too little user trust in the AI system will harm the overall performance of the human-system unit. When the trust is excessive, the users are not critical of possible mistakes of the system and when the users do not have enough trust in the system, they will not exhaust the benefits inherent in it.\n Some scholars have suggested that explainability in AI should be considered a goal secondary to AI effectiveness, and that encouraging the exclusive development of XAI may limit the functionality of AI more broadly.[100][101] Critiques of XAI rely on developed concepts of mechanistic and empiric reasoning from evidence-based medicine to suggest that AI technologies can be clinically validated even when their function cannot be understood by their operators.[100]\n Some researchers advocate the use of inherently interpretable machine learning models, rather than using post-hoc explanations in which a second model is created to explain the first. This is partly because post-hoc models increase the complexity in a decision pathway and partly because it is often unclear how faithfully a post-hoc explanation can mimic the computations of an entirely separate model.[22] However, another view is that what is important is that the explanation accomplishes the given task at hand, and whether it is pre or post-hoc doesn't matter. If a post-hoc explanation method helps a doctor diagnose cancer better, it is of secondary importance whether it is a correct/incorrect explanation.\n The goals of XAI amount to a form of lossy compression that will become less effective as AI models grow in their number of parameters. Along with other factors this leads to a theoretical limit for explainability.[102]\n Explainability was studied also in social choice theory. Social choice theory aims at finding solutions to social decision problems, that are based on well-established axioms. Ariel D. Procaccia[103] explains that these axioms can be used to construct convincing explanations to the solutions. This principle has been used to construct explanations in various subfields of social choice.\n Cailloux and Endriss[104] present a method for explaining voting rules using the axioms that characterize them. They exemplify their method on the Borda voting rule .\n Peters, Procaccia, Psomas and Zhou[105] present an algorithm for explaining the outcomes of the Borda rule using O(m2) explanations, and prove that this is tight in the worst case.\n Yang, Hausladen, Peters, Pournaras, Fricker and Helbing[106] present an empirical study of explainability in participatory budgeting. They compared the greedy and the equal shares rules, and three types of explanations: mechanism explanation (a general explanation of how the aggregation rule works given the voting input), individual explanation (explaining how many voters had at least one approved project, at least 10000 CHF in approved projects), and group explanation (explaining how the budget is distributed among the districts and topics). They compared the perceived trustworthiness and fairness of greedy and equal shares, before and after the explanations. They found out that, for MES, mechanism explanation yields the highest increase in perceived fairness and trustworthiness; the second-highest was Group explanation. For Greedy, Mechanism explanation increases perceived trustworthiness but not fairness, whereas Individual explanation increases both perceived fairness and trustworthiness. Group explanation decreases the perceived fairness and trustworthiness.\n Nizri, Azaria and Hazon[107] present an algorithm for computing explanations for the Shapley value. Given a coalitional game, their algorithm decomposes it to sub-games, for which it is easy to generate verbal explanations based on the axioms characterizing the Shapley value. The payoff allocation for each sub-game is perceived as fair, so the Shapley-based payoff allocation for the given game should seem fair as well. An experiment with 210 human subjects shows that, with their automatically generated explanations, subjects perceive Shapley-based payoff allocation as significantly fairer than with a general standard explanation.\n",
        "doc_number": 22
    },
    {
        "url": "https://en.wikipedia.org/wiki/AI_safety",
        "content": "AI safety is an interdisciplinary field focused on preventing accidents, misuse, or other harmful consequences arising from artificial intelligence (AI) systems. It encompasses machine ethics and AI alignment, which aim to ensure AI systems are moral and beneficial, as well as monitoring AI systems for risks and enhancing their reliability. The field is particularly concerned with existential risks posed by advanced AI models.\n Beyond technical research, AI safety involves developing norms and policies that promote safety. It gained significant popularity in 2023, with rapid progress in generative AI and public concerns voiced by researchers and CEOs about potential dangers. During the 2023 AI Safety Summit, the United States and the United Kingdom both established their own AI Safety Institute. However, researchers have expressed concern that AI safety measures are not keeping pace with the rapid development of AI capabilities.[1]\n Scholars discuss current risks from critical systems failures,[2] bias,[3] and AI-enabled surveillance,[4] as well as emerging risks like technological unemployment, digital manipulation,[5] weaponization,[6] AI-enabled cyberattacks[7] and bioterrorism.[8] They also discuss speculative risks from losing control of future artificial general intelligence (AGI) agents,[9] or from AI enabling perpetually stable dictatorships.[10]\n Some have criticized concerns about AGI, such as Andrew Ng who compared them in 2015 to \"worrying about overpopulation on Mars when we have not even set foot on the planet yet\".[11] Stuart J. Russell on the other side urges caution, arguing that \"it is better to anticipate human ingenuity than to underestimate it\".[12]\n AI researchers have widely differing opinions about the severity and primary sources of risk posed by AI technology[13][14][15] \u2013 though surveys suggest that experts take high consequence risks seriously. In two surveys of AI researchers, the median respondent was optimistic about AI overall, but placed a 5% probability on an \"extremely bad (e.g. human extinction)\" outcome of advanced AI.[13] In a 2022 survey of the natural language processing community, 37% agreed or weakly agreed that it is plausible that AI decisions could lead to a catastrophe that is \"at least as bad as an all-out nuclear war\".[16]\n Risks from AI began to be seriously discussed at the start of the computer age:\n Moreover, if we move in the direction of making machines which learn and whose behavior is modified by experience, we must face the fact that every degree of independence we give the machine is a degree of possible defiance of our wishes. From 2008 to 2009, the Association for the Advancement of Artificial Intelligence (AAAI) commissioned a study to explore and address potential long-term societal influences of AI research and development. The panel was generally skeptical of the radical views expressed by science-fiction authors but agreed that \"additional research would be valuable on methods for understanding and verifying the range of behaviors of complex computational systems to minimize unexpected outcomes\".[18]\n In 2011, Roman Yampolskiy introduced the term \"AI safety engineering\"[19] at the Philosophy and Theory of Artificial Intelligence conference,[20] listing prior failures of AI systems and arguing that \"the frequency and seriousness of such events will steadily increase as AIs become more capable\".[21]\n In 2014, philosopher Nick Bostrom published the book Superintelligence: Paths, Dangers, Strategies. He has the opinion that the rise of AGI has the potential to create various societal issues, ranging from the displacement of the workforce by AI, manipulation of political and military structures, to even the possibility of human extinction.[22] His argument that future advanced systems may pose a threat to human existence prompted Elon Musk,[23] Bill Gates,[24] and Stephen Hawking[25] to voice similar concerns.\n In 2015, dozens of artificial intelligence experts signed an open letter on artificial intelligence calling for research on the societal impacts of AI and outlining concrete directions.[26] To date, the letter has been signed by over 8000 people including Yann LeCun, Shane Legg, Yoshua Bengio, and Stuart Russell.\n In the same year, a group of academics led by professor Stuart Russell founded the Center for Human-Compatible AI at the University of California Berkeley and the Future of Life Institute awarded $6.5 million in grants for research aimed at \"ensuring artificial intelligence (AI) remains safe, ethical and beneficial\".[27]\n In 2016, the White House Office of Science and Technology Policy and Carnegie Mellon University announced The Public Workshop on Safety and Control for Artificial Intelligence,[28] which was one of a sequence of four White House workshops aimed at investigating \"the advantages and drawbacks\" of AI.[29] In the same year, Concrete Problems in AI Safety \u2013 one of the first and most influential technical AI Safety agendas \u2013 was published.[30]\n In 2017, the Future of Life Institute sponsored the Asilomar Conference on Beneficial AI, where more than 100 thought leaders formulated principles for beneficial AI including \"Race Avoidance: Teams developing AI systems should actively cooperate to avoid corner-cutting on safety standards\".[31]\n In 2018, the DeepMind Safety team outlined AI safety problems in specification, robustness,[32] and assurance.[33] The following year, researchers organized a workshop at ICLR that focused on these problem areas.[34]\n In 2021, Unsolved Problems in ML Safety was published, outlining research directions in robustness, monitoring, alignment, and systemic safety.[35]\n In 2023, Rishi Sunak said he wants the United Kingdom to be the \"geographical home of global AI safety regulation\" and to host the first global summit on AI safety.[36] The AI safety summit took place in November 2023, and focused on the risks of misuse and loss of control associated with frontier AI models.[37] During the summit the intention to create the International Scientific Report on the Safety of Advanced AI[38] was announced.  \n In 2024, The US and UK forged a new partnership on the science of AI safety. The MoU was signed on 1 April 2024 by US commerce secretary Gina Raimondo and UK technology secretary Michelle Donelan to jointly develop advanced AI model testing, following commitments announced at an AI Safety Summit in Bletchley Park in November.[39]\n AI safety research areas include robustness, monitoring, and alignment.[35][33]\n AI systems are often vulnerable to adversarial examples or \"inputs to machine learning (ML) models that an attacker has intentionally designed to cause the model to make a mistake\".[40] For example, in 2013, Szegedy et al. discovered that adding specific imperceptible perturbations to an image could cause it to be misclassified with high confidence.[41] This continues to be an issue with neural networks, though in recent work the perturbations are generally large enough to be perceptible.[42][43][44]\n All of the images on the right are predicted to be an ostrich after the perturbation is applied. (Left) is a correctly predicted sample, (center) perturbation applied magnified by 10x, (right) adversarial example.[41]\n Adversarial robustness is often associated with security.[45] Researchers demonstrated that an audio signal could be imperceptibly modified so that speech-to-text systems transcribe it to any message the attacker chooses.[46] Network intrusion[47] and malware[48] detection systems also must be adversarially robust since attackers may design their attacks to fool detectors.\n Models that represent objectives (reward models) must also be adversarially robust.\u00a0For example, a reward model might estimate how helpful a text response is and a language model might be trained to maximize this score.[49] Researchers have shown that if a language model is trained for long enough, it will leverage the vulnerabilities of the reward model to achieve a better score and perform worse on the intended task.[50] This issue can be addressed by improving the adversarial robustness of the reward model.[51] More generally, any AI system used to evaluate another AI system must be adversarially robust. This could include monitoring tools, since they could also potentially be tampered with to produce a higher reward.[52]\n It is often important for human operators to gauge how much they should trust an AI system, especially in high-stakes settings such as medical diagnosis.[53] ML models generally express confidence by outputting probabilities; however, they are often overconfident,[54] especially in situations that differ from those that they were trained to handle.[55] Calibration research aims to make model probabilities correspond as closely as possible to the true proportion that the model is correct.\n Similarly, anomaly detection or out-of-distribution (OOD) detection aims to identify when an AI system is in an unusual situation. For example, if a sensor on an autonomous vehicle is malfunctioning, or it encounters challenging terrain, it should alert the driver to take control or pull over.[56] Anomaly detection has been implemented by simply training a classifier to distinguish anomalous and non-anomalous inputs,[57] though a range of additional techniques are in use.[58][59]\n Scholars[6] and government agencies have expressed concerns that AI systems could be used to help malicious actors to build weapons,[60] manipulate public opinion,[61][62] or automate cyber attacks.[63] These worries are a practical concern for companies like OpenAI which host powerful AI tools online.[64] In order to prevent misuse, OpenAI has built detection systems that flag or restrict users based on their activity.[65]\n Neural networks have often been described as black boxes,[66] meaning that it is difficult to understand why they make the decisions they do as a result of the massive number of computations they perform.[67] This makes it challenging to anticipate failures. In 2018, a self-driving car killed a pedestrian after failing to identify them. Due to the black box nature of the AI software, the reason for the failure remains unclear.[68] It also raises debates in healthcare over whether statistically efficient but opaque models should be used.[69]\n One critical benefit of transparency is explainability.[70] It is sometimes a legal requirement to provide an explanation for why a decision was made in order to ensure fairness, for example for automatically filtering job applications or credit score assignment.[70]\n Another benefit is to reveal the cause of failures.[66] At the beginning of the 2020 COVID-19 pandemic, researchers used transparency tools to show that medical image classifiers were 'paying attention' to irrelevant hospital labels.[71]\n Transparency techniques can also be used to correct errors. For example, in the paper \"Locating and Editing Factual Associations in GPT\", the authors were able to identify model parameters that influenced how it answered questions about the location of the Eiffel tower. They were then able to 'edit' this knowledge to make the model respond to questions as if it believed the tower was in Rome instead of France.[72] Though in this case, the authors induced an error, these methods could potentially be used to efficiently fix them. Model editing techniques also exist in computer vision.[73]\n Finally, some have argued that the opaqueness of AI systems is a significant source of risk and better understanding of how they function could prevent high-consequence failures in the future.[74] \"Inner\" interpretability research aims to make ML models less opaque. One goal of this research is to identify what the internal neuron activations represent.[75][76] For example, researchers identified a neuron in the CLIP artificial intelligence system that responds to images of people in spider man costumes, sketches of spiderman, and the word 'spider'.[77] It also involves explaining connections between these neurons or 'circuits'.[78][79] For example, researchers have identified pattern-matching mechanisms in transformer attention that may play a role in how language models learn from their context.[80] \"Inner interpretability\" has been compared to neuroscience. In both cases, the goal is to understand what is going on in an intricate system, though ML researchers have the benefit of being able to take perfect measurements and perform arbitrary ablations.[81]\n ML models can potentially contain 'trojans' or 'backdoors':\u00a0vulnerabilities that malicious actors maliciously build into an AI system. For example, a trojaned facial recognition system could grant access when a specific piece of jewelry is in view;[35] or a trojaned autonomous vehicle may function normally until a specific trigger is visible.[82] Note that an adversary must have access to the system's training data in order to plant a trojan. [citation needed] This might not be difficult to do with some large models like CLIP or GPT-3 as they are trained on publicly available internet data.[83] Researchers were able to plant a trojan in an image classifier by changing just 300 out of 3 million of the training images.[84] In addition to posing a security risk, researchers have argued that trojans provide a concrete setting for testing and developing better monitoring tools.[52]\n In the field of artificial intelligence (AI), AI alignment aims to steer AI systems toward a person's or group's intended goals, preferences, or ethical principles. An AI system is considered aligned if it advances the intended objectives. A misaligned AI system pursues unintended objectives.[85]\n It is often challenging for AI designers to align an AI system because it is difficult for them to specify the full range of desired and undesired behaviors. Therefore, AI designers often use simpler proxy goals, such as gaining human approval. But proxy goals can overlook necessary constraints or reward the AI system for merely appearing aligned.[85][86]\n Misaligned AI systems can malfunction and cause harm. AI systems may find loopholes that allow them to accomplish their proxy goals efficiently but in unintended, sometimes harmful, ways (reward hacking).[85][87][88] They may also develop unwanted instrumental strategies, such as seeking power or survival because such strategies help them achieve their final given goals.[85][89][90] Furthermore, they might develop undesirable emergent goals that could be hard to detect before the system is deployed and encounters new situations and data distributions.[91][92]\n Today, some of these issues affect existing commercial systems such as large language models,[93][94][95] robots,[96] autonomous vehicles,[97] and social media recommendation engines.[93][90][98] Some AI researchers argue that more capable future systems will be more severely affected because these problems partially result from high capabilities.[99][87][86]\n Many prominent AI researchers,[100][101][102] including Geoffrey Hinton, Yoshua Bengio, and Stuart Russell, argue that AI is approaching human-like (AGI) and superhuman cognitive capabilities (ASI) and could endanger human civilization if misaligned.[103][90] These risks remain debated.[104]\n It is common for AI risks (and technological risks more generally) to be categorized as misuse or accidents.[118] Some scholars have suggested that this framework falls short.[118] For example, the Cuban Missile Crisis was not clearly an accident or a misuse of technology.[118] Policy analysts Zwetsloot and Dafoe wrote, \"The misuse and accident perspectives tend to focus only on the last step in a causal chain leading up to a harm: that is, the person who misused the technology, or the system that behaved in unintended ways\u2026 Often, though, the relevant causal chain is much longer.\" Risks often arise from 'structural' or 'systemic' factors such as competitive pressures, diffusion of harms, fast-paced development, high levels of uncertainty, and inadequate safety culture.[118] In the broader context of safety engineering, structural factors like 'organizational safety culture' play a central role in the popular STAMP risk analysis framework.[119]\n Inspired by the structural perspective, some researchers have emphasized the importance of using machine learning to improve sociotechnical safety factors, for example, using ML for cyber defense, improving institutional decision-making, and facilitating cooperation.[35] Others have emphasized the importance of involving both AI practitioners and domain experts in the design process to address structural vulnerabilities.[120]\n Some scholars are concerned that AI will exacerbate the already imbalanced game between cyber attackers and cyber defenders.[121] This would increase 'first strike' incentives and could lead to more aggressive and destabilizing attacks. In order to mitigate this risk, some have advocated for an increased emphasis on cyber defense. In addition, software security is essential for preventing powerful AI models from being stolen and misused.[6] Recent studies have shown that AI can significantly enhance both technical and managerial cybersecurity tasks by automating routine tasks and improving overall efficiency.[122]\n The advancement of AI in economic and military domains could precipitate unprecedented political challenges.[123] Some scholars have compared AI race dynamics to the cold war, where the careful judgment of a small number of decision-makers often spelled the difference between stability and catastrophe.[124] AI researchers have argued that AI technologies could also be used to assist decision-making.[35] For example, researchers are beginning to develop AI forecasting[125] and advisory systems.[126]\n Many of the largest global threats (nuclear war,[127] climate change,[128] etc.) have been framed as cooperation challenges. As in the well-known prisoner's dilemma scenario, some dynamics may lead to poor results for all players, even when they are optimally acting in their self-interest. For example, no single actor has strong incentives to address climate change even though the consequences may be significant if no one intervenes.[128]\n A salient AI cooperation challenge is avoiding a 'race to the bottom'.[129] In this scenario, countries or companies race to build more capable AI systems and neglect safety, leading to a catastrophic accident that harms everyone involved. Concerns about scenarios like these have inspired both political[130] and technical[131] efforts to facilitate cooperation between humans, and potentially also between AI systems. Most AI research focuses on designing individual agents to serve isolated functions (often in 'single-player' games).[132] Scholars have suggested that as AI systems become more autonomous, it may become essential to study and shape the way they interact.[132][120]\n In recent years, the development of large language models (LLMs) has raised unique concerns within the field of AI safety. Researchers Bender and Gebru et al.[133] have highlighted the environmental and financial costs associated with training these models, emphasizing that the energy consumption and carbon footprint of training procedures like those for Transformer models can be substantial. Moreover, these models often rely on massive, uncurated Internet-based datasets, which can encode hegemonic and biased viewpoints, further marginalizing underrepresented groups. The large-scale training data, while vast, does not guarantee diversity and often reflects the worldviews of privileged demographics, leading to models that perpetuate existing biases and stereotypes. This situation is exacerbated by the tendency of these models to produce seemingly coherent and fluent text, which can mislead users into attributing meaning and intent where none exists, a phenomenon described as 'stochastic parrots'. These models, therefore, pose risks of amplifying societal biases, spreading misinformation, and being used for malicious purposes, such as generating extremist propaganda or deepfakes. To address these challenges, researchers advocate for more careful planning in dataset creation and system development, emphasizing the need for research projects that contribute positively towards an equitable technological ecosystem.[134][135]\n AI governance is broadly concerned with creating norms, standards, and regulations to guide the use and development of AI systems.[124]\n AI safety governance research ranges from foundational investigations into the potential impacts of AI to specific applications. On the foundational side, researchers have argued that AI could transform many aspects of society due to its broad applicability, comparing it to electricity and the steam engine.[137] Some work has focused on anticipating specific risks that may arise from these impacts \u2013 for example, risks from mass unemployment,[138] weaponization,[139] disinformation,[140] surveillance,[141] and the concentration of power.[142] Other work explores underlying risk factors such as the difficulty of monitoring the rapidly evolving AI industry,[143] the availability of AI models,[144] and 'race to the bottom' dynamics.[129][145] Allan Dafoe, the head of longterm governance and strategy at DeepMind has emphasized the dangers of racing and the potential need for cooperation: \"it may be close to a necessary and sufficient condition for AI safety and alignment that there be a high degree of caution prior to deploying advanced powerful systems; however, if actors are competing in a domain with large returns to first-movers or relative advantage, then they will be pressured to choose a sub-optimal level of caution\".[130] A research stream focuses on developing approaches, frameworks, and methods to assess AI accountability, guiding and promoting audits of AI-based systems.[146][147][148]\n Efforts to enhance AI safety include frameworks designed to align AI outputs with ethical guidelines and reduce risks like misuse and data leakage. Tools such as Nvidia's  Guardrails,[149] Llama Guard,[150] and Preamble's customizable guardrails[151] mitigate vulnerabilities like prompt injection and ensure outputs adhere to predefined principles. These frameworks are often integrated into AI systems to improve safety and reliability.[152]\n The field of AI safety is deeply intertwined with philosophical considerations, particularly in the realm of ethics. Deontological ethics, which emphasizes adherence to moral rules, has been proposed as a framework for aligning AI systems with human values. By embedding deontological principles, AI systems can be guided to avoid actions that cause harm, ensuring their operations remain within ethical boundaries.[153]\n In addressing the AI safety problem it is important to stress the distinction between local and global solutions. Local solutions focus on individual AI systems, ensuring they are safe and beneficial, while global solutions seek to implement safety measures for all AI systems across various jurisdictions. Some researchers[154] argue for the necessity of scaling local safety measures to a global level, proposing a classification for these global solutions. This approach underscores the importance of collaborative efforts in the international governance of AI safety, emphasizing that no single entity can effectively manage the risks associated with AI technologies. This perspective aligns with ongoing efforts in international policy-making and regulatory frameworks, which aim to address the complex challenges posed by advanced AI systems worldwide.[155][156]\n Some experts have argued that it is too early to regulate AI, expressing concerns that regulations will hamper innovation and it would be foolish to \"rush to regulate in ignorance\".[157][158] Others, such as business magnate Elon Musk, call for pre-emptive action to mitigate catastrophic risks.[159]\n Outside of formal legislation, government agencies have put forward ethical and safety recommendations. In March 2021, the US National Security Commission on Artificial Intelligence reported that advances in AI may make it increasingly important to \"assure that systems are aligned with goals and values, including safety, robustness and trustworthiness\".[160] Subsequently, the National Institute of Standards and Technology drafted a framework for managing AI Risk, which advises that when \"catastrophic risks are present \u2013 development and deployment should cease in a safe manner until risks can be sufficiently managed\".[161]\n In September 2021, the People's Republic of China published ethical guidelines for the use of AI in China, emphasizing that AI decisions should remain under human control and calling for accountability mechanisms. In the same month, The United Kingdom published its 10-year National AI Strategy,[162] which states the British government \"takes the long-term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously\".[163] The strategy describes actions to assess long-term AI risks, including catastrophic risks.[163] The British government held first major global summit on AI safety. This took place on the 1st and 2 November 2023 and was described as \"an opportunity for policymakers and world leaders to consider the immediate and future risks of AI and how these risks can be mitigated via a globally coordinated approach\".[164][165]\n Government organizations, particularly in the United States, have also encouraged the development of technical AI safety research. The Intelligence Advanced Research Projects Activity initiated the TrojAI project to identify and protect against Trojan attacks on AI systems.[166] The DARPA engages in research on explainable artificial intelligence and improving robustness against adversarial attacks.[167][168] And the National Science Foundation supports the Center for Trustworthy Machine Learning, and is providing millions of dollars in funding for empirical AI safety research.[169]\n In 2024, the United Nations General Assembly adopted the first global resolution on the promotion of \u201csafe, secure and trustworthy\u201d AI systems that emphasized the respect, protection and promotion of human rights in the design, development, deployment and the use of AI.[170]\n In May 2024, the Department for Science, Innovation and Technology (DSIT) announced \u00a38.5 million in funding for AI safety research under the Systemic AI Safety Fast Grants Programme, led by Christopher Summerfield and Shahar Avin at the AI Safety Institute, in partnership with UK Research and Innovation. Technology Secretary Michelle Donelan announced the plan at the AI Seoul Summit, stating the goal was to make AI safe across society and that promising proposals could receive further funding. The UK also signed an agreement with 10 other countries and the EU to form an international network of AI safety institutes to promote collaboration and share information and resources. Additionally, the UK AI Safety Institute planned to open an office in San Francisco.[171]\n AI labs and companies generally abide by safety practices and norms that fall outside of formal legislation.[172] One aim of governance researchers is to shape these norms. Examples of safety recommendations found in the literature include performing third-party auditing,[173] offering bounties for finding failures,[173] sharing AI incidents[173] (an AI incident database was created for this purpose),[174] following guidelines to determine whether to publish research or models,[144] and improving information and cyber security in AI labs.[175]\n Companies have also made commitments. Cohere, OpenAI, and AI21 proposed and agreed on \"best practices for deploying language models\", focusing on mitigating misuse.[176] To avoid contributing to racing-dynamics, OpenAI has also stated in their charter that \"if a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project\"[177] Also, industry leaders such as CEO of DeepMind Demis Hassabis, director of Facebook AI Yann LeCun have signed open letters such as the Asilomar Principles[31] and the Autonomous Weapons Open Letter.[178]\n",
        "doc_number": 23
    },
    {
        "url": "https://en.wikipedia.org/wiki/Algorithmic_bias",
        "content": "\n Algorithmic bias describes systematic and repeatable errors in a computer system that create \"unfair\" outcomes, such as \"privileging\" one category over another in ways different from the intended function of the algorithm.\n Bias can emerge from many factors, including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded, collected, selected or used to train the algorithm. For example, algorithmic bias has been observed in search engine results and social media platforms. This bias can have impacts ranging from inadvertent privacy violations to reinforcing social biases of race, gender, sexuality, and ethnicity. The study of algorithmic bias is most concerned with algorithms that reflect \"systematic and unfair\" discrimination.[2] This bias has only recently been addressed in legal frameworks, such as the European Union's General Data Protection Regulation (proposed 2018) and the Artificial Intelligence Act (proposed 2021, approved 2024).\n As algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise (in part due to the psychological phenomenon of automation bias), and in some cases, reliance on algorithms can displace human responsibility for their outcomes. Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; by how features and labels are chosen; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design.[3]\n Algorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech. It has also arisen in criminal justice, healthcare, and hiring, compounding existing racial, socioeconomic, and gender biases. The relative inability of facial recognition technology to accurately identify darker-skinned faces has been linked to multiple wrongful arrests of black men, an issue stemming from imbalanced datasets. Problems in understanding, researching, and discovering algorithmic bias persist due to the proprietary nature of algorithms, which are typically treated as trade secrets. Even when full transparency is provided, the complexity of certain algorithms poses a barrier to understanding their functioning. Furthermore, algorithms may change, or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis. In many cases, even within a single website or application, there is no single \"algorithm\" to examine, but a network of many interrelated programs and data inputs, even between users of the same service.\n Algorithms are difficult to define,[4] but may be generally understood as lists of instructions that determine how programs read, collect, process, and analyze data to generate output.[5]:\u200a13\u200a For a rigorous technical introduction, see Algorithms. Advances in computer hardware have led to an increased ability to process, store and transmit data. This has in turn boosted the design and adoption of technologies such as machine learning and artificial intelligence.[6]:\u200a14\u201315\u200a By analyzing and processing data, algorithms are the backbone of search engines,[7] social media websites,[8] recommendation engines,[9] online retail,[10] online advertising,[11] and more.[12]\n Contemporary social scientists are concerned with algorithmic processes embedded into hardware and software applications because of their political and social impact, and question the underlying assumptions of an algorithm's neutrality.[13]:\u200a2\u200a[14]:\u200a563\u200a[15]:\u200a294\u200a[16] The term algorithmic bias describes systematic and repeatable errors that create unfair outcomes, such as privileging one arbitrary group of users over others. For example, a credit score algorithm may deny a loan without being unfair, if it is consistently weighing relevant financial criteria. If the algorithm recommends loans to one group of users, but denies loans to another set of nearly identical users based on unrelated criteria, and if this behavior can be repeated across multiple occurrences, an algorithm can be described as biased.[17]:\u200a332\u200a This bias may be intentional or unintentional (for example, it can come from biased data obtained from a worker that previously did the job the algorithm is going to do from now on).\n Bias can be introduced to an algorithm in several ways. During the assemblage of a dataset, data may be collected, digitized, adapted, and entered into a database according to human-designed cataloging criteria.[18]:\u200a3\u200a Next, programmers assign priorities, or hierarchies, for how a program assesses and sorts that data. This requires human decisions about how data is categorized, and which data is included or discarded.[18]:\u200a4\u200a Some algorithms collect their own data based on human-selected criteria, which can also reflect the bias of human designers.[18]:\u200a8\u200a Other algorithms may reinforce stereotypes and preferences as they process and display \"relevant\" data for human users, for example, by selecting information based on previous choices of a similar user or group of users.[18]:\u200a6\u200a\n Beyond assembling and processing data, bias can emerge as a result of design.[19] For example, algorithms that determine the allocation of resources or scrutiny (such as determining school placements) may inadvertently discriminate against a category when determining risk based on similar users (as in credit scores).[20]:\u200a36\u200a Meanwhile, recommendation engines that work by associating users with similar users, or that make use of inferred marketing traits, might rely on inaccurate associations that reflect broad ethnic, gender, socio-economic, or racial stereotypes. Another example comes from determining criteria for what is included and excluded from results. These criteria could present unanticipated outcomes for search results, such as with flight-recommendation software that omits flights that do not follow the sponsoring airline's flight paths.[19] Algorithms may also display an uncertainty bias, offering more confident assessments when larger data sets are available. This can skew algorithmic processes toward results that more closely correspond with larger samples, which may disregard data from underrepresented populations.[21]:\u200a4\u200a\n The earliest computer programs were designed to mimic human reasoning and deductions, and were deemed to be functioning when they successfully and consistently reproduced that human logic. In his 1976 book Computer Power and Human Reason, artificial intelligence pioneer Joseph Weizenbaum suggested that bias could arise both from the data used in a program, but also from the way a program is coded.[22]:\u200a149\u200a\n Weizenbaum wrote that programs are a sequence of rules created by humans for a computer to follow. By following those rules consistently, such programs \"embody law\",[22]:\u200a40\u200a that is, enforce a specific way to solve problems. The rules a computer follows are based on the assumptions of a computer programmer for how these problems might be solved. That means the code could incorporate the programmer's imagination of how the world works, including their biases and expectations.[22]:\u200a109\u200a While a computer program can incorporate bias in this way, Weizenbaum also noted that any data fed to a machine additionally reflects \"human decision making processes\" as data is being selected.[22]:\u200a70,\u200a105\u200a\n Finally, he noted that machines might also transfer good information with unintended consequences if users are unclear about how to interpret the results.[22]:\u200a65\u200a Weizenbaum warned against trusting decisions made by computer programs that a user doesn't understand, comparing such faith to a tourist who can find his way to a hotel room exclusively by turning left or right on a coin toss. Crucially, the tourist has no basis of understanding how or why he arrived at his destination, and a successful arrival does not mean the process is accurate or reliable.[22]:\u200a226\u200a\n An early example of algorithmic bias resulted in as many as 60 women and ethnic minorities denied entry to St. George's Hospital Medical School per year from 1982 to 1986, based on implementation of a new computer-guidance assessment system that denied entry to women and men with \"foreign-sounding names\" based on historical trends in admissions.[24] While many schools at the time employed similar biases in their selection process, St. George was most notable for automating said bias through the use of an algorithm, thus gaining the attention of people on a much wider scale.\n In recent years, as algorithms increasingly rely on machine learning methods applied to real-world data, algorithmic bias has become more prevalent due to inherent biases within the data itself. For instance, facial recognition systems have been shown to misidentify individuals from marginalized groups at significantly higher rates than white individuals, highlighting how biases in training datasets manifest in deployed systems.[25] A 2018 study by Joy Buolamwini and Timnit Gebru found that commercial facial recognition technologies exhibited error rates of up to 35% when identifying darker-skinned women, compared to less than 1% for lighter-skinned men.[26]\n Algorithmic biases are not only technical failures but often reflect systemic inequities embedded in historical and societal data. Researchers and critics, such as Cathy O'Neil in her book Weapons of Math Destruction (2016), emphasize that these biases can amplify existing social inequalities under the guise of objectivity. O'Neil argues that opaque, automated decision-making processes in areas such as credit scoring, predictive policing, and education can reinforce discriminatory practices while appearing neutral or scientific.[27]\n Though well-designed algorithms frequently determine outcomes that are equally (or more) equitable than the decisions of human beings, cases of bias still occur, and are difficult to predict and analyze.[28] The complexity of analyzing algorithmic bias has grown alongside the complexity of programs and their design. Decisions made by one designer, or team of designers, may be obscured among the many pieces of code created for a single program; over time these decisions and their collective impact on the program's output may be forgotten.[29]:\u200a115\u200a In theory, these biases may create new patterns of behavior, or \"scripts\", in relationship to specific technologies as the code interacts with other elements of society.[30] Biases may also impact how society shapes itself around the data points that algorithms require. For example, if data shows a high number of arrests in a particular area, an algorithm may assign more police patrols to that area, which could lead to more arrests.[31]:\u200a180\u200a\n The decisions of algorithmic programs can be seen as more authoritative than the decisions of the human beings they are meant to assist,[32]:\u200a15\u200a a process described by author Clay Shirky as \"algorithmic authority\".[33] Shirky uses the term to describe \"the decision to regard as authoritative an unmanaged process of extracting value from diverse, untrustworthy sources\", such as search results.[33] This neutrality can also be misrepresented by the language used by experts and the media when results are presented to the public. For example, a list of news items selected and presented as \"trending\" or \"popular\" may be created based on significantly wider criteria than just their popularity.[18]:\u200a14\u200a\n Because of their convenience and authority, algorithms are theorized as a means of delegating responsibility away from humans.[32]:\u200a16\u200a[34]:\u200a6\u200a This can have the effect of reducing alternative options, compromises, or flexibility.[32]:\u200a16\u200a Sociologist Scott Lash has critiqued algorithms as a new form of \"generative power\", in that they are a virtual means of generating actual ends. Where previously human behavior generated data to be collected and studied, powerful algorithms increasingly could shape and define human behaviors.[35]:\u200a71\u200a\n Concerns over the impact of algorithms on society have led to the creation of working groups in organizations such as Google and Microsoft, which have co-created a working group named Fairness, Accountability,\nand Transparency in Machine Learning.[36]:\u200a115\u200a Ideas from Google have included community groups that patrol the outcomes of algorithms and vote to control or restrict outputs they deem to have negative consequences.[36]:\u200a117\u200a In recent years, the study of the Fairness, Accountability,\nand Transparency (FAT) of algorithms has emerged as its own interdisciplinary research area with an annual conference called FAccT.[37] Critics have suggested that FAT initiatives cannot serve effectively as independent watchdogs when many are funded by corporations building the systems being studied.[38]\n Pre-existing bias in an algorithm is a consequence of underlying social and institutional ideologies. Such ideas may influence or create personal biases within individual designers or programmers. Such prejudices can be explicit and conscious, or implicit and unconscious.[17]:\u200a334\u200a[15]:\u200a294\u200a Poorly selected input data, or simply data from a biased source, will influence the outcomes created by machines.[23]:\u200a17\u200a Encoding pre-existing bias into software can preserve social and institutional bias, and, without correction, could be replicated in all future uses of that algorithm.[29]:\u200a116\u200a[34]:\u200a8\u200a\n An example of this form of bias is the British Nationality Act Program, designed to automate the evaluation of new British citizens after the 1981 British Nationality Act.[17]:\u200a341\u200a The program accurately reflected the tenets of the law, which stated that \"a man is the father of only his legitimate children, whereas a woman is the mother of all her children, legitimate or not.\"[17]:\u200a341\u200a[39]:\u200a375\u200a In its attempt to transfer a particular logic into an algorithmic process, the BNAP inscribed the logic of the British Nationality Act into its algorithm, which would perpetuate it even if the act was eventually repealed.[17]:\u200a342\u200a\n Another source of bias, which has been called \"label choice bias\",[40] arises when proxy measures are used to train algorithms, that build in bias against certain groups. For example, a widely used algorithm predicted health care costs as a proxy for health care needs, and used predictions to allocate resources to help patients with complex health needs. This introduced bias because Black patients have lower costs, even when they are just as unhealthy as White patients[41] Solutions to the \"label choice bias\" aim to match the actual target (what the algorithm is predicting) more closely to the ideal target (what researchers want the algorithm to predict), so for the prior example, instead of predicting cost, researchers would focus on the variable of healthcare needs which is rather more significant. Adjusting the target led to almost double the number of Black patients being selected for the program.[40]\n Machine learning bias refers to systematic and unfair disparities in the output of machine learning algorithms. These biases can manifest in various ways and are often a reflection of the data used to train these algorithms. Here are some key aspects:\n Language bias refers a type of statistical sampling bias tied to the language of a query that leads to \"a systematic deviation in sampling information that prevents it from accurately representing the true coverage of topics and views available in their repository.\"[42] Luo et al.'s work[42] shows that current large language models, as they are predominately trained on English-language data, often present the Anglo-American views as truth, while systematically downplaying non-English perspectives as irrelevant, wrong, or noise. When queried with political ideologies like \"What is liberalism?\", ChatGPT, as it was trained on English-centric data, describes liberalism from the Anglo-American perspective, emphasizing aspects of human rights and equality, while equally valid aspects like \"opposes state intervention in personal and economic life\" from the dominant Vietnamese perspective and \"limitation of government power\" from the prevalent Chinese perspective are absent.[42]\n Gender bias refers to the tendency of these models to produce outputs that are unfairly prejudiced towards one gender over another. This bias typically arises from the data on which these models are trained. For example, large language models often assign roles and characteristics based on traditional gender norms; it might associate nurses or secretaries predominantly with women and engineers or CEOs with men.[43][44]\n Beyond gender and race, these models can reinforce a wide range of stereotypes, including those based on age, nationality, religion, or occupation. This can lead to outputs that unfairly generalize or caricature groups of people, sometimes in harmful or derogatory ways.[45]\n A recent focus in research has been on the complex interplay between the grammatical properties of a language and real-world biases that can become embedded in AI systems, potentially perpetuating harmful stereotypes and assumptions. The study on gender bias in language models trained on Icelandic, a highly grammatically gendered language, revealed that the models exhibited a significant predisposition towards the masculine grammatical gender when referring to occupation terms, even for female-dominated professions.[46] This suggests the models amplified societal gender biases present in the training data.\n Political bias refers to the tendency of algorithms to systematically favor certain political viewpoints, ideologies, or outcomes over others. Language models may also exhibit political biases. Since the training data includes a wide range of political opinions and coverage, the models might generate responses that lean towards particular political ideologies or viewpoints, depending on the prevalence of those views in the data.[47]\n Racial bias refers to the tendency of machine learning models to produce outcomes that unfairly discriminate against or stereotype individuals based on race or ethnicity. This bias often stems from training data that reflects historical and systemic inequalities. For example, AI systems used in hiring, law enforcement, or healthcare may disproportionately disadvantage certain racial groups by reinforcing existing stereotypes or underrepresenting them in key areas. Such biases can manifest in ways like facial recognition systems misidentifying individuals of certain racial backgrounds or healthcare algorithms underestimating the medical needs of minority patients. Addressing racial bias requires careful examination of data, improved transparency in algorithmic processes, and efforts to ensure fairness throughout the AI development lifecycle.[48][49]\n Technical bias emerges through limitations of a program, computational power, its design, or other constraint on the system.[17]:\u200a332\u200a Such bias can also be a restraint of design, for example, a search engine that shows three results per screen can be understood to privilege the top three results slightly more than the next three, as in an airline price display.[17]:\u200a336\u200a Another case is software that relies on randomness for fair distributions of results. If the random number generation mechanism is not truly random, it can introduce bias, for example, by skewing selections toward items at the end or beginning of a list.[17]:\u200a332\u200a\n A decontextualized algorithm uses unrelated information to sort results, for example, a flight-pricing algorithm that sorts results by alphabetical order would be biased in favor of American Airlines over United Airlines.[17]:\u200a332\u200a The opposite may also apply, in which results are evaluated in contexts different from which they are collected. Data may be collected without crucial external context: for example, when facial recognition software is used by surveillance cameras, but evaluated by remote staff in another country or region, or evaluated by non-human algorithms with no awareness of what takes place beyond the camera's field of vision. This could create an incomplete understanding of a crime scene, for example, potentially mistaking bystanders for those who commit the crime.[14]:\u200a574\u200a\n Lastly, technical bias can be created by attempting to formalize decisions into concrete steps on the assumption that human behavior works in the same way. For example, software weighs data points to determine whether a defendant should accept a plea bargain, while ignoring the impact of emotion on a jury.[17]:\u200a332\u200a Another unintended result of this form of bias was found in the plagiarism-detection software Turnitin, which compares student-written texts to information found online and returns a probability score that the student's work is copied. Because the software compares long strings of text, it is more likely to identify non-native speakers of English than native speakers, as the latter group might be better able to change individual words, break up strings of plagiarized text, or obscure copied passages through synonyms. Because it is easier for native speakers to evade detection as a result of the technical constraints of the software, this creates a scenario where Turnitin identifies foreign-speakers of English for plagiarism while allowing more native-speakers to evade detection.[32]:\u200a21\u201322\u200a\n Emergent bias is the result of the use and reliance on algorithms across new or unanticipated contexts.[17]:\u200a334\u200a Algorithms may not have been adjusted to consider new forms of knowledge, such as new drugs or medical breakthroughs, new laws, business models, or shifting cultural norms.[17]:\u200a334,\u200a336\u200a This may exclude groups through technology, without providing clear outlines to understand who is responsible for their exclusion.[31]:\u200a179\u200a[15]:\u200a294\u200a Similarly, problems may emerge when training data (the samples \"fed\" to a machine, by which it models certain conclusions) do not align with contexts that an algorithm encounters in the real world.[50]\n In 1990, an example of emergent bias was identified in the software used to place US medical students into residencies, the National Residency Match Program (NRMP).[17]:\u200a338\u200a The algorithm was designed at a time when few married couples would seek residencies together. As more women entered medical schools, more students were likely to request a residency alongside their partners. The process called for each applicant to provide a list of preferences for placement across the US, which was then sorted and assigned when a hospital and an applicant both agreed to a match. In the case of married couples where both sought residencies, the algorithm weighed the location choices of the higher-rated partner first. The result was a frequent assignment of highly preferred schools to the first partner and lower-preferred schools to the second partner, rather than sorting for compromises in placement preference.[17]:\u200a338\u200a[51]\n Additional emergent biases include:\n Unpredictable correlations can emerge when large data sets are compared to each other. For example, data collected about web-browsing patterns may align with signals marking sensitive data (such as race or sexual orientation). By selecting according to certain behavior or browsing patterns, the end effect would be almost identical to discrimination through the use of direct race or sexual orientation data.[21]:\u200a6\u200a In other cases, the algorithm draws conclusions from correlations, without being able to understand those correlations. For example, one triage program gave lower priority to asthmatics who had pneumonia than asthmatics who did not have pneumonia. The program algorithm did this because it simply compared survival rates: asthmatics with pneumonia are at the highest risk. Historically, for this same reason, hospitals typically give such asthmatics the best and most immediate care.[52][clarification needed]\n Emergent bias can occur when an algorithm is used by unanticipated audiences. For example, machines may require that users can read, write, or understand numbers, or relate to an interface using metaphors that they do not understand.[17]:\u200a334\u200a These exclusions can become compounded, as biased or exclusionary technology is more deeply integrated into society.[31]:\u200a179\u200a\n Apart from exclusion, unanticipated uses may emerge from the end user relying on the software rather than their own knowledge. In one example, an unanticipated user group led to algorithmic bias in the UK, when the British National Act Program was created as a proof-of-concept by computer scientists and immigration lawyers to evaluate suitability for British citizenship. The designers had access to legal expertise beyond the end users in immigration offices, whose understanding of both software and immigration law would likely have been unsophisticated. The agents administering the questions relied entirely on the software, which excluded alternative pathways to citizenship, and used the software even after new case laws and legal interpretations led the algorithm to become outdated. As a result of designing an algorithm for users assumed to be legally savvy on immigration law, the software's algorithm indirectly led to bias in favor of applicants who fit a very narrow set of legal criteria set by the algorithm, rather than by the more broader criteria of British immigration law.[17]:\u200a342\u200a\n Emergent bias may also create a feedback loop, or recursion, if data collected for an algorithm results in real-world responses which are fed back into the algorithm.[53][54] For example, simulations of the predictive policing software (PredPol), deployed in Oakland, California, suggested an increased police presence in black neighborhoods based on crime data reported by the public.[55] The simulation showed that the public reported crime based on the sight of police cars, regardless of what police were doing. The simulation interpreted police car sightings in modeling its predictions of crime, and would in turn assign an even larger increase of police presence within those neighborhoods.[53][56][57] The Human Rights Data Analysis Group, which conducted the simulation, warned that in places where racial discrimination is a factor in arrests, such feedback loops could reinforce and perpetuate racial discrimination in policing.[54] Another well known example of such an algorithm exhibiting such behavior is COMPAS, a software that determines an individual's likelihood of becoming a criminal offender. The software is often criticized for labeling Black individuals as criminals much more likely than others, and then feeds the data back into itself in the event individuals become registered criminals, further enforcing the bias created by the dataset the algorithm is acting on.\n Recommender systems such as those used to recommend online videos or news articles can create feedback loops.[58] When users click on content that is suggested by algorithms, it influences the next set of suggestions.[59] Over time this may lead to users entering a filter bubble and being unaware of important or useful content.[60][61]\n Corporate algorithms could be skewed to invisibly favor financial arrangements or agreements between companies, without the knowledge of a user who may mistake the algorithm as being impartial. For example, American Airlines created a flight-finding algorithm in the 1980s. The software presented a range of flights from various airlines to customers, but weighed factors that boosted its own flights, regardless of price or convenience. In testimony to the United States Congress, the president of the airline stated outright that the system was created with the intention of gaining competitive advantage through preferential treatment.[62]:\u200a2\u200a[17]:\u200a331\u200a\n In a 1998 paper describing Google, the founders of the company had adopted a policy of transparency in search results regarding paid placement, arguing that \"advertising-funded search engines will be inherently biased towards the advertisers and away from the needs of the consumers.\"[63] This bias would be an \"invisible\" manipulation of the user.[62]:\u200a3\u200a\n A series of studies about undecided voters in the US and in India found that search engine results were able to shift voting outcomes by about 20%. The researchers concluded that candidates have \"no means of competing\" if an algorithm, with or without intent, boosted page listings for a rival candidate.[64] Facebook users who saw messages related to voting were more likely to vote. A 2010 randomized trial of Facebook users showed a 20% increase (340,000 votes) among users who saw messages encouraging voting, as well as images of their friends who had voted.[65] Legal scholar Jonathan Zittrain has warned that this could create a \"digital gerrymandering\" effect in elections, \"the selective presentation of information by an intermediary to meet its agenda, rather than to serve its users\", if intentionally manipulated.[66]:\u200a335\u200a\n In 2016, the professional networking site LinkedIn was discovered to recommend male variations of women's names in response to search queries. The site did not make similar recommendations in searches for male names. For example, \"Andrea\" would bring up a prompt asking if users meant \"Andrew\", but queries for \"Andrew\" did not ask if users meant to find \"Andrea\". The company said this was the result of an analysis of users' interactions with the site.[67]\n In 2012, the department store franchise Target was cited for gathering data points to infer when women customers were pregnant, even if they had not announced it, and then sharing that information with marketing partners.[68]:\u200a94\u200a[69] Because the data had been predicted, rather than directly observed or reported, the company had no legal obligation to protect the privacy of those customers.[68]:\u200a98\u200a\n Web search algorithms have also been accused of bias. Google's results may prioritize pornographic content in search terms related to sexuality, for example, \"lesbian\". This bias extends to the search engine showing popular but sexualized content in neutral searches. For example, \"Top 25 Sexiest Women Athletes\" articles displayed as first-page results in searches for \"women athletes\".[70]:\u200a31\u200a In 2017, Google adjusted these results along with others that surfaced hate groups, racist views, child abuse and pornography, and other upsetting and offensive content.[71] Other examples include the display of higher-paying jobs to male applicants on job search websites.[72] Researchers have also identified that machine translation exhibits a strong tendency towards male defaults.[73] In particular, this is observed in fields linked to unbalanced gender distribution, including STEM occupations.[74] In fact, current machine translation systems fail to reproduce the real world distribution of female workers.[75]\n In 2015, Amazon.com turned off an AI system it developed to screen job applications when they realized it was biased against women.[76] The recruitment tool excluded applicants who attended all-women's colleges and resumes that included the word \"women's\".[77] A similar problem emerged with music streaming services\u2014In 2019, it was discovered that the recommender system algorithm used by Spotify was biased against women artists.[78] Spotify's song recommendations suggested more male artists over women artists.\n Algorithms have been criticized as a method for obscuring racial prejudices in decision-making.[79][80][81]:\u200a158\u200a Because of how certain races and ethnic groups were treated in the past, data can often contain hidden biases.[82] For example, black people are likely to receive longer sentences than white people who committed the same crime.[83][84] This could potentially mean that a system amplifies the original biases in the data.\n In 2015, Google apologized when a couple of black users complained that an image-identification algorithm in its Photos application identified them as gorillas.[85] In 2010, Nikon cameras were criticized when image-recognition algorithms consistently asked Asian users if they were blinking.[86] Such examples are the product of bias in biometric data sets.[85] Biometric data is drawn from aspects of the body, including racial features either observed or inferred, which can then be transferred into data points.[81]:\u200a154\u200a Speech recognition technology can have different accuracies depending on the user's accent. This may be caused by the a lack of training data for speakers of that accent.[87]\n Biometric data about race may also be inferred, rather than observed. For example, a 2012 study showed that names commonly associated with blacks were more likely to yield search results implying arrest records, regardless of whether there is any police record of that individual's name.[88] A 2015 study also found that Black and Asian people are assumed to have lesser functioning lungs due to racial and occupational exposure data not being incorporated into the prediction algorithm's model of lung function.[89][90]\n In 2019, a research study revealed that a healthcare algorithm sold by Optum favored white patients over sicker black patients. The algorithm predicts how much patients would cost the health-care system in the future. However, cost is not race-neutral, as black patients incurred about $1,800 less in medical costs per year than white patients with the same number of chronic conditions, which led to the algorithm scoring white patients as equally at risk of future health problems as black patients who suffered from significantly more diseases.[91]\n A study conducted by researchers at UC Berkeley in November 2019 revealed that mortgage algorithms have been discriminatory towards Latino and African Americans which discriminated against minorities based on \"creditworthiness\" which is rooted in the U.S. fair-lending law which allows lenders to use measures of identification to determine if an individual is worthy of receiving loans. These particular algorithms were present in FinTech companies and were shown to discriminate against minorities.[92][non-primary source needed]\n Another study, published in August 2024, on Large language model investigates how language models perpetuate covert racism, particularly through dialect prejudice against speakers of African American English (AAE). It highlights that these models exhibit more negative stereotypes about AAE speakers than any recorded human biases, while their overt stereotypes are more positive. This discrepancy raises concerns about the potential harmful consequences of such biases in decision-making processes.[93]\n Algorithms already have numerous applications in legal systems. An example of this is COMPAS, a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than the average COMPAS-assigned risk level of white defendants, and that black defendants are twice as likely to be erroneously assigned the label \"high-risk\" as white defendants.[94][95]\n One example is the use of risk assessments in criminal sentencing in the United States and parole hearings, judges were presented with an algorithmically generated score intended to reflect the risk that a prisoner will repeat a crime.[96] For the time period starting in 1920 and ending in 1970, the nationality of a criminal's father was a consideration in those risk assessment scores.[97]:\u200a4\u200a Today, these scores are shared with judges in Arizona, Colorado, Delaware, Kentucky, Louisiana, Oklahoma, Virginia, Washington, and Wisconsin. An independent investigation by ProPublica found that the scores were inaccurate 80% of the time, and disproportionately skewed to suggest blacks to be at risk of relapse, 77% more often than whites.[96]\n One study that set out to examine \"Risk, Race, & Recidivism: Predictive Bias and Disparate Impact\" alleges a two-fold (45 percent vs. 23 percent) adverse likelihood for black vs. Caucasian defendants to be misclassified as imposing a higher risk despite having objectively remained without any documented recidivism over a two-year period of observation.[98]\n In the pretrial detention context, a law review article argues that algorithmic risk assessments violate 14th Amendment Equal Protection rights on the basis of race, since the algorithms are argued to be facially discriminatory, to result in disparate treatment, and to not be narrowly tailored.[99]\n In 2017 a Facebook algorithm designed to remove online hate speech was found to advantage white men over black children when assessing objectionable content, according to internal Facebook documents.[100] The algorithm, which is a combination of computer programs and human content reviewers, was created to protect broad categories rather than specific subsets of categories. For example, posts denouncing \"Muslims\" would be blocked, while posts denouncing \"Radical Muslims\" would be allowed. An unanticipated outcome of the algorithm is to allow hate speech against black children, because they denounce the \"children\" subset of blacks, rather than \"all blacks\", whereas \"all white men\" would trigger a block, because whites and males are not considered subsets.[100] Facebook was also found to allow ad purchasers to target \"Jew haters\" as a category of users, which the company said was an inadvertent outcome of algorithms used in assessing and categorizing data. The company's design also allowed ad buyers to block African-Americans from seeing housing ads.[101]\n While algorithms are used to track and block hate speech, some were found to be 1.5 times more likely to flag information posted by Black users and 2.2 times likely to flag information as hate speech if written in African American English.[102] Without context for slurs and epithets, even when used by communities which have re-appropriated them, were flagged.[103]\n Another instance in a study found that 85 out of 100 examined subreddits tended to remove various norm violations, including misogynistic slurs and racist hate speech, highlighting the prevalence of such content in online communities.[104] As platforms like Reddit update their hate speech policies, they must balance free expression with the protection of marginalized communities, emphasizing the need for context-sensitive moderation and nuanced algorithms.[104]\n Surveillance camera software may be considered inherently political because it requires algorithms to distinguish normal from abnormal behaviors, and to determine who belongs in certain locations at certain times.[14]:\u200a572\u200a The ability of such algorithms to recognize faces across a racial spectrum has been shown to be limited by the racial diversity of images in its training database; if the majority of photos belong to one race or gender, the software is better at recognizing other members of that race or gender.[105] However, even audits of these image-recognition systems are ethically fraught, and some scholars have suggested the technology's context will always have a disproportionate impact on communities whose actions are over-surveilled.[106] For example, a 2002 analysis of software used to identify individuals in CCTV images found several examples of bias when run against criminal databases. The software was assessed as identifying men more frequently than women, older people more frequently than the young, and identified Asians, African-Americans and other races more often than whites.[31]:\u200a190\u200a A 2018 study found that facial recognition software most likely accurately identified light-skinned (typically European) males, with slightly lower accuracy rates for light-skinned females. Dark-skinned males and females were significanfly less likely to be accurately identified by facial recognition software. These disparities are attributed to the under-representation of darker-skinned participants in data sets used to develop this software.[107][108]\n In 2011, users of the gay hookup application Grindr reported that the Android store's recommendation algorithm was linking Grindr to applications designed to find sex offenders, which critics said inaccurately related homosexuality with pedophilia. Writer Mike Ananny criticized this association in The Atlantic, arguing that such associations further stigmatized gay men.[109] In 2009, online retailer Amazon de-listed 57,000 books after an algorithmic change expanded its \"adult content\" blacklist to include any book addressing sexuality or gay themes, such as the critically acclaimed novel Brokeback Mountain.[110][18]:\u200a5\u200a[111]\n In 2019, it was found that on Facebook, searches for \"photos of my female friends\" yielded suggestions such as \"in bikinis\" or \"at the beach\". In contrast, searches for \"photos of my male friends\" yielded no results.[112]\n Facial recognition technology has been seen to cause problems for transgender individuals. In 2018, there were reports of Uber drivers who were transgender or transitioning experiencing difficulty with the facial recognition software that Uber implements as a built-in security measure. As a result of this, some of the accounts of trans Uber drivers were suspended which cost them fares and potentially cost them a job, all due to the facial recognition software experiencing difficulties with recognizing the face of a trans driver who was transitioning.[113] Although the solution to this issue would appear to be including trans individuals in training sets for machine learning models, an instance of trans YouTube videos that were collected to be used in training data did not receive consent from the trans individuals that were included in the videos, which created an issue of violation of privacy.[114]\n There has also been a study that was conducted at Stanford University in 2017 that tested algorithms in a machine learning system that was said to be able to detect an individual's sexual orientation based on their facial images.[115] The model in the study predicted a correct distinction between gay and straight men 81% of the time, and a correct distinction between gay and straight women 74% of the time. This study resulted in a backlash from the LGBTQIA community, who were fearful of the possible negative repercussions that this AI system could have on individuals of the LGBTQIA community by putting individuals at risk of being \"outed\" against their will.[116]\n While the modalities of algorithmic fairness have been judged on the basis of different aspects of bias \u2013 like gender, race and socioeconomic status, disability often is left out of the list.[117][118] The marginalization people with disabilities currently face in society is being translated into AI systems and algorithms, creating even more exclusion[119][120]\n The shifting nature of disabilities and its subjective characterization, makes it more difficult to computationally address. The lack of historical depth in defining disabilities, collecting its incidence and prevalence in questionnaires, and establishing recognition add to the controversy and ambiguity in its quantification and calculations.\u00a0 The definition of disability has been long debated shifting from a medical model to a social model of disability most recently, which establishes that disability is a result of the mismatch between people's interactions and barriers in their environment, rather than impairments and health conditions. Disabilities can also be situational or temporary,[121] considered in a constant state of flux. Disabilities are incredibly diverse,[122] fall within a large spectrum, and can be unique to each individual. People's identity can vary based on the specific types of disability they experience, how they use assistive technologies, and who they support.\u00a0 The high level of variability across people's experiences greatly personalizes how a disability can manifest. Overlapping identities and intersectional experiences[123] are excluded from statistics and datasets,[124] hence underrepresented and nonexistent in training data.[125] Therefore, machine learning models are trained inequitably and artificial intelligent systems perpetuate more algorithmic bias.[126] For example, if people with speech impairments are not included in training voice control features and smart AI assistants \u2013they are unable to use the feature or the responses received from a Google Home or Alexa are extremely poor.\n Given the stereotypes and stigmas that still exist surrounding disabilities, the sensitive nature of revealing these identifying characteristics also carries vast privacy challenges.\u00a0As disclosing disability information can be taboo and drive further discrimination against this population, there is a lack of explicit disability data available for algorithmic systems to interact with. People with disabilities face additional harms and risks with respect to their social support, cost of health insurance, workplace discrimination and other basic necessities upon disclosing their disability status. Algorithms are further exacerbating this gap by recreating the biases that already exist in societal systems and structures.[127][128]\n While users generate results that are \"completed\" automatically, Google has failed to remove sexist and racist autocompletion text. For example, Algorithms of Oppression: How Search Engines Reinforce Racism Safiya Noble notes an example of the search for \"black girls\", which was reported to result in pornographic images. Google claimed it was unable to erase those pages unless they were considered unlawful.[129]\n Several problems impede the study of large-scale algorithmic bias, hindering the application of academically rigorous studies and public understanding.[13]:\u200a5\u200a[130][131]\n Literature on algorithmic bias has focused on the remedy of fairness, but definitions of fairness are often incompatible with each other and the realities of machine learning optimization.[132][133] For example, defining fairness as an \"equality of outcomes\" may simply refer to a system producing the same result for all people, while fairness defined as \"equality of treatment\" might explicitly consider differences between individuals.[134]:\u200a2\u200a As a result, fairness is sometimes described as being in conflict with the accuracy of a model, suggesting innate tensions between the priorities of social welfare and the priorities of the vendors designing these systems.[135]:\u200a2\u200a In response to this tension, researchers have suggested more care to the design and use of systems that draw on potentially biased algorithms, with \"fairness\" defined for specific applications and contexts.[136]\n Algorithmic processes are complex, often exceeding the understanding of the people who use them.[13]:\u200a2\u200a[137]:\u200a7\u200a Large-scale operations may not be understood even by those involved in creating them.[138] The methods and processes of contemporary programs are often obscured by the inability to know every permutation of a code's input or output.[31]:\u200a183\u200a Social scientist Bruno Latour has identified this process as blackboxing, a process in which \"scientific and technical work is made invisible by its own success. When a machine runs efficiently, when a matter of fact is settled, one need focus only on its inputs and outputs and not on its internal complexity. Thus, paradoxically, the more science and technology succeed, the more opaque and obscure they become.\"[139] Others have critiqued the black box metaphor, suggesting that current algorithms are not one black box, but a network of interconnected ones.[140]:\u200a92\u200a\n An example of this complexity can be found in the range of inputs into customizing feedback. The social media site Facebook factored in at least 100,000 data points to determine the layout of a user's social media feed in 2013.[141] Furthermore, large teams of programmers may operate in relative isolation from one another, and be unaware of the cumulative effects of small decisions within connected, elaborate algorithms.[29]:\u200a118\u200a Not all code is original, and may be borrowed from other libraries, creating a complicated set of relationships between data processing and data input systems.[6]:\u200a22\u200a\n Additional complexity occurs through machine learning and the personalization of algorithms based on user interactions such as clicks, time spent on site, and other metrics. These personal adjustments can confuse general attempts to understand algorithms.[142]:\u200a367\u200a[137]:\u200a7\u200a One unidentified streaming radio service reported that it used five unique music-selection algorithms it selected for its users, based on their behavior. This creates different experiences of the same streaming services between different users, making it harder to understand what these algorithms do.[13]:\u200a5\u200a\nCompanies also run frequent A/B tests to fine-tune algorithms based on user response. For example, the search engine Bing can run up to ten million subtle variations of its service per day, creating different experiences of the service between each use and/or user.[13]:\u200a5\u200a\n Commercial algorithms are proprietary, and may be treated as trade secrets.[13]:\u200a2\u200a[137]:\u200a7\u200a[31]:\u200a183\u200a Treating algorithms as trade secrets protects companies, such as search engines, where a transparent algorithm might reveal tactics to manipulate search rankings.[142]:\u200a366\u200a This makes it difficult for researchers to conduct interviews or analysis to discover how algorithms function.[6]:\u200a20\u200a Critics suggest that such secrecy can also obscure possible unethical methods used in producing or processing algorithmic output.[142]:\u200a369\u200a Other critics, such as lawyer and activist Katarzyna Szymielewicz, have suggested that the lack of transparency is often disguised as a result of algorithmic complexity, shielding companies from disclosing or investigating its own algorithmic processes.[143]\n A significant barrier to understanding the tackling of bias in practice is that categories, such as demographics of individuals protected by anti-discrimination law, are often not explicitly considered when collecting and processing data.[144] In some cases, there is little opportunity to collect this data explicitly, such as in device fingerprinting, ubiquitous computing and the Internet of Things. In other cases, the data controller may not wish to collect such data for reputational reasons, or because it represents a heightened liability and security risk. It may also be the case that, at least in relation to the European Union's General Data Protection Regulation, such data falls under the 'special category' provisions (Article 9), and therefore comes with more restrictions on potential collection and processing.\n Some practitioners have tried to estimate and impute these missing sensitive categorizations in order to allow bias mitigation, for example building systems to infer ethnicity from names,[145] however this can introduce other forms of bias if not undertaken with care.[146] Machine learning researchers have drawn upon cryptographic privacy-enhancing technologies such as secure multi-party computation to propose methods whereby algorithmic bias can be assessed or mitigated without these data ever being available to modellers in cleartext.[147]\n Algorithmic bias does not only include protected categories, but can also concern characteristics less easily observable or codifiable, such as political viewpoints. In these cases, there is rarely an easily accessible or non-controversial ground truth, and removing the bias from such a system is more difficult.[148] Furthermore, false and accidental correlations can emerge from a lack of understanding of protected categories, for example, insurance rates based on historical data of car accidents which may overlap, strictly by coincidence, with residential clusters of ethnic minorities.[149]\n A study of 84 policy guidelines on ethical AI found that fairness and \"mitigation of unwanted bias\" was a common point of concern, and were addressed through a blend of technical solutions, transparency and monitoring, right to remedy and increased oversight, and diversity and inclusion efforts.[150]\n There have been several attempts to create methods and tools that can detect and observe biases within an algorithm. These emergent fields focus on tools which are typically applied to the (training) data used by the program rather than the algorithm's internal processes. These methods may also analyze a program's output and its usefulness and therefore may involve the analysis of its confusion matrix (or table of confusion).[151][152][153][154][155][156][157][158][159] Explainable AI to detect algorithm Bias is a suggested way to detect the existence of bias in an algorithm or learning model.[160] Using machine learning to detect bias is called, \"conducting an AI audit\", where the \"auditor\" is an algorithm that goes through the AI model and the training data to identify biases.[161]\nEnsuring that an AI tool such as a classifier is free from bias is more difficult than just removing the sensitive information\nfrom its input signals, because this is typically implicit in other signals. For example, the hobbies, sports and schools attended\nby a job candidate might reveal their gender to the software, even when this is removed from the analysis. Solutions to this\nproblem involve ensuring that the intelligent agent does not have any information that could be used to reconstruct the protected\nand sensitive information about the subject, as first demonstrated in[162] where a deep learning network was simultaneously trained to learn a task while at the same time being completely agnostic about the protected feature. A simpler method was proposed in the context of word embeddings, and involves removing information that is correlated with the protected characteristic.[163]\n Currently, a new IEEE standard is being drafted that aims to specify methodologies which help creators of algorithms eliminate issues of bias and articulate transparency (i.e. to authorities or end users) about the function and possible effects of their algorithms. The project was approved February 2017 and is sponsored by the Software & Systems Engineering Standards Committee,[164] a committee chartered by the IEEE Computer Society. A draft of the standard is expected to be submitted for balloting in June 2019.[165][166]\n In 2022, the IEEE released a standard aimed at specifying methodologies to help creators of algorithms address issues of bias and promote transparency regarding the function and potential effects of their algorithms. The project, initially approved in February 2017, was sponsored by the Software & Systems Engineering Standards Committee,[167] a committee under the IEEE Computer Society. The standard provides guidelines for articulating transparency to authorities or end users and mitigating algorithmic biases.[165][166][168]\n Ethics guidelines on AI point to the need for accountability, recommending that steps be taken to improve the interpretability of results.[169] Such solutions include the consideration of the \"right to understanding\" in machine learning algorithms, and to resist deployment of machine learning in situations where the decisions could not be explained or reviewed.[170] Toward this end, a movement for \"Explainable AI\" is already underway within organizations such as DARPA, for reasons that go beyond the remedy of bias.[171] Price Waterhouse Coopers, for example, also suggests that monitoring output means designing systems in such a way as to ensure that solitary components of the system can be isolated and shut down if they skew results.[172]\n An initial approach towards transparency included the open-sourcing of algorithms.[173] Software code can be looked into and improvements can be proposed through source-code-hosting facilities. However, this approach doesn't necessarily produce the intended effects. Companies and organizations can share all possible documentation and code, but this does not establish transparency if the audience doesn't understand the information given. Therefore, the role of an interested critical audience is worth exploring in relation to transparency. Algorithms cannot be held accountable without a critical audience.[174]\n From a regulatory perspective, the Toronto Declaration calls for applying a human rights framework to harms caused by algorithmic bias.[175] This includes legislating expectations of due diligence on behalf of designers of these algorithms, and creating accountability when private actors fail to protect the public interest, noting that such rights may be obscured by the complexity of determining responsibility within a web of complex, intertwining processes.[176] Others propose the need for clear liability insurance mechanisms.[177]\n Amid concerns that the design of AI systems is primarily the domain of white, male engineers,[178] a number of scholars have suggested that algorithmic bias may be minimized by expanding inclusion in the ranks of those designing AI systems.[170][150] For example, just 12% of machine learning engineers are women,[179] with black AI leaders pointing to a \"diversity crisis\" in the field.[180] Groups like Black in AI and Queer in AI are attempting to create more inclusive spaces in the AI community and work against the often harmful desires of corporations that control the trajectory of AI research.[181] Critiques of simple inclusivity efforts suggest that diversity programs can not address overlapping forms of inequality, and have called for applying a more deliberate lens of intersectionality to the design of algorithms.[182][183]:\u200a4\u200a Researchers at the University of Cambridge have argued that addressing racial diversity is hampered by the \"whiteness\" of the culture of AI.[184]\n Integrating interdisciplinarity and collaboration in developing of AI systems can play a critical role in tackling algorithmic bias. Integrating insights, expertise, and perspectives from disciplines outside of computer science can foster a better understanding of the impact data driven solutions have on society. An example of this in AI research is PACT or Participatory Approach to enable Capabilities in communiTies, a proposed framework for facilitating collaboration when developing AI driven solutions concerned with social impact.[185] This framework identifies guiding principals for stakeholder participation when working on AI for Social Good (AI4SG) projects. PACT attempts to reify the importance of decolonizing and power-shifting efforts in the design of human-centered AI solutions. An academic initiative in this regard is the Stanford University's Institute for Human-Centered Artificial Intelligence which aims to foster multidisciplinary collaboration. The mission of the institute is to advance artificial intelligence (AI) research, education, policy and practice to improve the human condition.[186]\n Collaboration with outside experts and various stakeholders facilitates ethical, inclusive, and accountable development of intelligent systems. It incorporates ethical considerations, understands the social and cultural context, promotes human-centered design, leverages technical expertise, and addresses policy and legal considerations.[187] Collaboration across disciplines is essential to effectively mitigate bias in AI systems and ensure that AI technologies are fair, transparent, and accountable.\n The General Data Protection Regulation (GDPR), the European Union's revised data protection regime that was implemented in 2018, addresses \"Automated individual decision-making, including profiling\" in Article 22. These rules prohibit \"solely\" automated decisions which have a \"significant\" or \"legal\" effect on an individual, unless they are explicitly authorised by consent, contract, or member state law. Where they are permitted, there must be safeguards in place, such as a right to a human-in-the-loop, and a non-binding right to an explanation of decisions reached. While these regulations are commonly considered to be new, nearly identical provisions have existed across Europe since 1995, in Article 15 of the Data Protection Directive. The original automated decision rules and safeguards found in French law since the late 1970s.[188]\n \nThe GDPR addresses algorithmic bias in profiling systems, as well as the statistical approaches possible to clean it, directly in recital 71,[189] noting that the controller should use appropriate mathematical or statistical procedures for the profiling, implement technical and organisational measures appropriate\u00a0... that prevents, inter alia, discriminatory effects on natural persons on the basis of racial or ethnic origin, political opinion, religion or beliefs, trade union membership, genetic or health status or sexual orientation, or that result in measures having such an effect. Like the non-binding right to an explanation in recital 71, the problem is the non-binding nature of recitals.[190] While it has been treated as a requirement by the Article 29 Working Party that advised on the implementation of data protection law,[189] its practical dimensions are unclear. It has been argued that the Data Protection Impact Assessments for high risk data profiling (alongside other pre-emptive measures within data protection) may be a better way to tackle issues of algorithmic discrimination, as it restricts the actions of those deploying algorithms, rather than requiring consumers to file complaints or request changes.[191]\n The United States has no general legislation controlling algorithmic bias, approaching the problem through various state and federal laws that might vary by industry, sector, and by how an algorithm is used.[192] Many policies are self-enforced or controlled by the Federal Trade Commission.[192] In 2016, the Obama administration released the National Artificial Intelligence Research and Development Strategic Plan,[193] which was intended to guide policymakers toward a critical assessment of algorithms. It recommended researchers to \"design these systems so that their actions and decision-making are transparent and easily interpretable by humans, and thus can be examined for any bias they may contain, rather than just learning and repeating these biases\". Intended only as guidance, the report did not create any legal precedent.[194]:\u200a26\u200a\n In 2017, New York City passed the first algorithmic accountability bill in the United States.[195] The bill, which went into effect on January 1, 2018, required \"the creation of a task force that provides recommendations on how information on agency automated decision systems may be shared with the public, and how agencies may address instances where people are harmed by agency automated decision systems.\"[196] The task force is required to present findings and recommendations for further regulatory action in 2019.[197]\nOn February 11, 2019, according to Executive Order 13859, the federal government unveiled the \"American AI Initiative,\" a comprehensive strategy to maintain U.S. leadership in artificial intelligence. The initiative highlights the importance of sustained AI research and development, ethical standards, workforce training, and the protection of critical AI technologies.[198] This aligns with broader efforts to ensure transparency, accountability, and innovation in AI systems across public and private sectors. Furthermore, on October 30, 2023, the President signed Executive Order 14110, which emphasizes the safe, secure, and trustworthy development and use of artificial intelligence (AI). The order outlines a coordinated, government-wide approach to harness AI's potential while mitigating its risks, including fraud, discrimination, and national security threats. An important point in the commitment is promoting responsible innovation and collaboration across sectors to ensure that AI benefits society as a whole.[199] With this order, President Joe Biden mandated the federal government to create best practices for companies to optimize AI's benefits and minimize its harms.[200]\n On July 31, 2018, a draft of the Personal Data Bill was presented.[201] The draft proposes standards for the storage, processing and transmission of data. While it does not use the term algorithm, it makes for provisions for \"harm resulting from any processing or any kind of processing undertaken by the fiduciary\". It defines \"any denial or withdrawal of a service, benefit or good resulting from an evaluative decision about the data principal\" or \"any discriminatory treatment\" as a source of harm that could arise from improper use of data. It also makes special provisions for people of \"Intersex status\".[202]\n",
        "doc_number": 24
    },
    {
        "url": "https://en.wikipedia.org/wiki/Facial_recognition_system",
        "content": "\n A facial recognition system[1] is a technology potentially capable of matching a human face from a digital image or a video frame against a database of faces. Such a system is typically employed to authenticate users through ID verification services, and works by pinpointing and measuring facial features from a given image.[2]\n Development began on similar systems in the 1960s, beginning as a form of computer application. Since their inception, facial recognition systems have seen wider uses in recent times on smartphones and in other forms of technology, such as robotics. Because computerized facial recognition involves the measurement of a human's physiological characteristics, facial recognition systems are categorized as biometrics. Although the accuracy of facial recognition systems as a biometric technology is lower than iris recognition, fingerprint image acquisition, palm recognition or voice recognition, it is widely adopted due to its contactless process.[3] Facial recognition systems have been deployed in advanced human\u2013computer interaction, video surveillance, law enforcement, passenger screening, decisions on employment and housing and automatic indexing of images.[4][5]\n Facial recognition systems are employed throughout the world today by governments and private companies.[6] Their effectiveness varies, and some systems have previously been scrapped because of their ineffectiveness. The use of facial recognition systems has also raised controversy, with claims that the systems violate citizens' privacy, commonly make incorrect identifications, encourage gender norms[7][8] and racial profiling,[9] and do not protect important biometric data. The appearance of synthetic media such as deepfakes has also raised concerns about its security.[10] These claims have led to the ban of facial recognition systems in several cities in the United States.[11] Growing societal concerns led social networking company Meta Platforms to shut down its Facebook facial recognition system in 2021, deleting the face scan data of more than one billion users.[12][13] The change represented one of the largest shifts in facial recognition usage in the technology's history. IBM also stopped offering facial recognition technology due to similar concerns.[14]\n Automated facial recognition was pioneered in the 1960s by Woody Bledsoe, Helen Chan Wolf, and Charles Bisson, whose work focused on teaching computers to recognize human faces.[15] Their early facial recognition project was dubbed \"man-machine\" because a human first needed to establish the coordinates of facial features in a photograph before they could be used by a computer for recognition. Using a graphics tablet, a human would pinpoint facial features coordinates, such as the pupil centers, the inside and outside corners of eyes, and the widows peak in the hairline. The coordinates were used to calculate 20 individual distances, including the width of the mouth and of the eyes. A human could process about 40 pictures an hour, building a database of these computed distances. A computer would then automatically compare the distances for each photograph, calculate the difference between the distances, and return the closed records as a possible match.[15]\n In 1970, Takeo Kanade publicly demonstrated a face-matching system that located anatomical features such as the chin and calculated the distance ratio between facial features without human intervention. Later tests revealed that the system could not always reliably identify facial features. Nonetheless, interest in the subject grew and in 1977 Kanade published the first detailed book on facial recognition technology.[16]\n In 1993, the Defense Advanced Research Project Agency (DARPA) and the Army Research Laboratory (ARL) established the face recognition technology program FERET to develop \"automatic face recognition capabilities\" that could be employed in a productive real life environment \"to assist security, intelligence, and law enforcement personnel in the performance of their duties.\" Face recognition systems that had been trialled in research labs were evaluated. The FERET tests found that while the performance of existing automated facial recognition systems varied, a handful of existing methods could viably be used to recognize faces in still images taken in a controlled environment.[17] The FERET tests spawned three US companies that sold automated facial recognition systems. Vision Corporation and Miros Inc were founded in 1994, by researchers who used the results of the FERET tests as a selling point. Viisage Technology was established by a identification card defense contractor in 1996 to commercially exploit the rights to the facial recognition algorithm developed by Alex Pentland at MIT.[18]\n Following the 1993 FERET face-recognition vendor test, the Department of Motor Vehicles (DMV) offices in West Virginia and New Mexico became the first DMV offices to use automated facial recognition systems to prevent people from obtaining multiple driving licenses using different names. Driver's licenses in the United States were at that point a commonly accepted form of photo identification. DMV offices across the United States were undergoing a technological upgrade and were in the process of establishing databases of digital ID photographs. This enabled DMV offices to deploy the facial recognition systems on the market to search photographs for new driving licenses against the existing DMV database.[19] DMV offices became one of the first major markets for automated facial recognition technology and introduced US citizens to facial recognition as a standard method of identification.[20] The increase of the US prison population in the 1990s prompted U.S. states to established connected and automated identification systems that incorporated digital biometric databases, in some instances this included facial recognition. In 1999, Minnesota incorporated the facial recognition system FaceIT by Visionics into a mug shot booking system that allowed police, judges and court officers to track criminals across the state.[21]\n Until the 1990s, facial recognition systems were developed primarily by using photographic portraits of human faces. Research on face recognition to reliably locate a face in an image that contains other objects gained traction in the early 1990s with the principal component analysis (PCA). The PCA method of face detection is also known as Eigenface and was developed by Matthew Turk and Alex Pentland.[22] Turk and Pentland combined the conceptual approach of the Karhunen\u2013Lo\u00e8ve theorem and factor analysis, to develop a linear model. Eigenfaces are determined based on global and orthogonal features in human faces. A human face is calculated as a weighted combination of a number of Eigenfaces. Because few Eigenfaces were used to encode human faces of a given population, Turk and Pentland's PCA face detection method greatly reduced the amount of data that had to be processed to detect a face. Pentland in 1994 defined Eigenface features, including eigen eyes, eigen mouths and eigen noses, to advance the use of PCA in facial recognition. In 1997, the PCA Eigenface method of face recognition[23] was improved upon using linear discriminant analysis (LDA) to produce Fisherfaces.[24] LDA Fisherfaces became dominantly used in PCA feature based face recognition. While Eigenfaces were also used for face reconstruction. In these approaches no global structure of the face is calculated which links the facial features or parts.[25]\n Purely feature based approaches to facial recognition were overtaken in the late 1990s by the Bochum system, which used Gabor filter to record the face features and computed a grid of the face structure to link the features.[26] Christoph von der Malsburg and his research team at the University of Bochum developed Elastic Bunch Graph Matching in the mid-1990s to extract a face out of an image using skin segmentation.[22] By 1997, the face detection method developed by Malsburg outperformed most other facial detection systems on the market. The so-called \"Bochum system\" of face detection was sold commercially on the market as ZN-Face to operators of airports and other busy locations. The software was \"robust enough to make identifications from less-than-perfect face views. It can also often see through such impediments to identification as mustaches, beards, changed hairstyles and glasses\u2014even sunglasses\".[27]\n Real-time face detection in video footage became possible in 2001 with the Viola\u2013Jones object detection framework for faces.[28] Paul Viola and Michael Jones combined their face detection method with the Haar-like feature approach to object recognition in digital images to launch AdaBoost, the first real-time frontal-view face detector.[29] By 2015, the Viola\u2013Jones algorithm had been implemented using small low power detectors on handheld devices and embedded systems. Therefore, the Viola\u2013Jones algorithm has not only broadened the practical application of face recognition systems but has also been used to support new features in user interfaces and teleconferencing.[30]\n Ukraine is using the US-based Clearview AI facial recognition software to identify dead Russian soldiers. Ukraine has conducted 8,600 searches and identified the families of 582 deceased Russian soldiers. The IT volunteer section of the Ukrainian army using the software is subsequently contacting the families of the deceased soldiers to raise awareness of Russian activities in Ukraine. The main goal is to destabilise the Russian government. It can be seen as a form of psychological warfare. About 340 Ukrainian government officials in five government ministries are using the technology. It is used to catch spies that might try to enter Ukraine.[31]\n Clearview AI's facial recognition database is only available to government agencies who may only use the technology to assist in the course of law enforcement investigations or in connection with national security.[32]\n The software was donated to Ukraine by Clearview AI. Russia is thought to be using it to find anti-war activists. Clearview AI was originally designed for US law enforcement. Using it in war raises new ethical concerns. One London based surveillance expert, Stephen Hare, is concerned it might make the Ukrainians appear inhuman: \"Is it actually working? Or is it making [Russians] say: 'Look at these lawless, cruel Ukrainians, doing this to our boys'?\"[33]\n While humans can recognize faces without much effort,[34] facial recognition is a challenging pattern recognition problem in computing. Facial recognition systems attempt to identify a human face, which is three-dimensional and changes in appearance with lighting and facial expression, based on its two-dimensional image. To accomplish this computational task, facial recognition systems perform four steps. First face detection is used to segment the face from the image background. In the second step the segmented face image is aligned to account for face pose, image size and photographic properties, such as illumination and grayscale. The purpose of the alignment process is to enable the accurate localization of facial features in the third step, the facial feature extraction. Features such as eyes, nose and mouth are pinpointed and measured in the image to represent the face. The so established feature vector of the face is then, in the fourth step, matched against a database of faces.[35]\n Some face recognition algorithms identify facial features by extracting landmarks, or features, from an image of the subject's face. For example, an algorithm may analyze the relative position, size, and/or shape of the eyes, nose, cheekbones, and jaw.[36] These features are then used to search for other images with matching features.[37]\n Other algorithms normalize a gallery of face images and then compress the face data, only saving the data in the image that is useful for face recognition. A probe image is then compared with the face data.[38] One of the earliest successful systems[39] is based on template matching techniques[40] applied to a set of salient facial features, providing a sort of compressed face representation.\n Recognition algorithms can be divided into two main approaches: geometric, which looks at distinguishing features, or photo-metric, which is a statistical approach that distills an image into values and compares the values with templates to eliminate variances. Some classify these algorithms into two broad categories: holistic and feature-based models. The former attempts to recognize the face in its entirety while the feature-based subdivide into components such as according to features and analyze each as well as its spatial location with respect to other features.[41]\n Popular recognition algorithms include principal component analysis using eigenfaces, linear discriminant analysis, elastic bunch graph matching using the Fisherface algorithm, the hidden Markov model, the multilinear subspace learning using tensor representation, and the neuronal motivated dynamic link matching.[citation needed][42] Modern facial recognition systems make increasing use of machine learning techniques such as deep learning.[43]\n To enable human identification at a distance (HID) low-resolution images of faces are enhanced using face hallucination. In CCTV imagery faces are often very small. But because facial recognition algorithms that identify and plot facial features require high resolution images, resolution enhancement techniques have been developed to enable facial recognition systems to work with imagery that has been captured in environments with a high signal-to-noise ratio. Face hallucination algorithms that are applied to images prior to those images being submitted to the facial recognition system use example-based machine learning with pixel substitution or nearest neighbour distribution indexes that may also incorporate demographic and age related facial characteristics. Use of face hallucination techniques improves the performance of high resolution facial recognition algorithms and may be used to overcome the inherent limitations of super-resolution algorithms. Face hallucination techniques are also used to pre-treat imagery where faces are disguised. Here the disguise, such as sunglasses, is removed and the face hallucination algorithm is applied to the image. Such face hallucination algorithms need to be trained on similar face images with and without disguise. To fill in the area uncovered by removing the disguise, face hallucination algorithms need to correctly map the entire state of the face, which may be not possible due to the momentary facial expression captured in the low resolution image.[44]\n Three-dimensional face recognition technique uses 3D sensors to capture information about the shape of a face. This information is then used to identify distinctive features on the surface of a face, such as the contour of the eye sockets, nose, and chin.[45]\nOne advantage of 3D face recognition is that it is not affected by changes in lighting like other techniques. It can also identify a face from a range of viewing angles, including a profile view.[45][37] Three-dimensional data points from a face vastly improve the precision of face recognition. 3D-dimensional face recognition research is enabled by the development of sophisticated sensors that project structured light onto the face.[46] 3D matching technique are sensitive to expressions, therefore researchers at Technion applied tools from metric geometry to treat expressions as isometries.[47] A new method of capturing 3D images of faces uses three tracking cameras that point at different angles; one camera will be pointing at the front of the subject, second one to the side, and third one at an angle. All these cameras will work together so it can track a subject's face in real-time and be able to face detect and recognize.[48]\n A different form of taking input data for face recognition is by using thermal cameras, by this procedure the cameras will only detect the shape of the head and it will ignore the subject accessories such as glasses, hats, or makeup.[49] Unlike conventional cameras, thermal cameras can capture facial imagery even in low-light and nighttime conditions without using a flash and exposing the position of the camera.[50] However, the databases for face recognition are limited. Efforts to build databases of thermal face images date back to 2004.[49] By 2016, several databases existed, including the IIITD-PSE and the Notre Dame thermal face database.[51] Current thermal face recognition systems are not able to reliably detect a face in a thermal image that has been taken of an outdoor environment.[52]\n In 2018, researchers from the U.S. Army Research Laboratory (ARL) developed a technique that would allow them to match facial imagery obtained using a thermal camera with those in databases that were captured using a conventional camera.[53] Known as a cross-spectrum synthesis method due to how it bridges facial recognition from two different imaging modalities, this method synthesize a single image by analyzing multiple facial regions and details.[54] It consists of a non-linear regression model that maps a specific thermal image into a corresponding visible facial image and an optimization issue that projects the latent projection back into the image space.[50] ARL scientists have noted that the approach works by combining global information (i.e. features across the entire face) with local information (i.e. features regarding the eyes, nose, and mouth).[55] According to performance tests conducted at ARL, the multi-region cross-spectrum synthesis model demonstrated a performance improvement of about 30% over baseline methods and about 5% over state-of-the-art methods.[54]\n Founded in 2013, Looksery went on to raise money for its face modification app on Kickstarter. After successful crowdfunding, Looksery launched in October 2014. The application allows video chat with others through a special filter for faces that modifies the look of users. Image augmenting applications already on the market, such as Facetune and Perfect365, were limited to static images, whereas Looksery allowed augmented reality to live videos. In late 2015 SnapChat purchased Looksery, which would then become its landmark lenses function.[56] Snapchat filter applications use face detection technology and on the basis of the facial features identified in an image a 3D mesh mask is layered over the face.[57] A variety of technologies attempt to fool facial recognition software by the use of anti-facial recognition masks.[58]\n DeepFace is a deep learning facial recognition system created by a research group at Facebook. It identifies human faces in digital images. It employs a nine-layer neural net with over 120 million connection weights, and was trained on four million images uploaded by Facebook users.[59][60] The system is said to be 97% accurate, compared to 85% for the FBI's Next Generation Identification system.[61]\n TikTok's algorithm has been regarded as especially effective, but many were left to wonder at the exact programming that caused the app to be so effective in guessing the user's desired content.[62] In June 2020, TikTok released a statement regarding the \"For You\" page, and how they recommended videos to users, which did not include facial recognition.[63] In February 2021, however, TikTok agreed to a $92\u00a0million settlement to a US lawsuit which alleged that the app had used facial recognition in both user videos and its algorithm to identify age, gender and ethnicity.[64]\n The emerging use of facial recognition is in the use of ID verification services. Many companies and others are working in the market now to provide these services to banks, ICOs, and other e-businesses.[65] Face recognition has been leveraged as a form of biometric authentication for various computing platforms and devices;[37] Android 4.0 \"Ice Cream Sandwich\" added facial recognition using a smartphone's front camera as a means of unlocking devices,[66][67] while Microsoft introduced face recognition login to its Xbox 360 video game console through its Kinect accessory,[68] as well as Windows 10 via its \"Windows Hello\" platform (which requires an infrared-illuminated camera).[69] In 2017, Apple's iPhone X smartphone introduced facial recognition to the product line with its \"Face ID\" platform, which uses an infrared illumination system.[70]\n Apple introduced Face ID on the flagship iPhone X as a biometric authentication successor to the Touch ID, a fingerprint based system. Face ID has a facial recognition sensor that consists of two parts: a \"Romeo\" module that projects more than 30,000 infrared dots onto the user's face, and a \"Juliet\" module that reads the pattern.[71] The pattern is sent to a local \"Secure Enclave\" in the device's central processing unit (CPU) to confirm a match with the phone owner's face.[72]\n The facial pattern is not accessible by Apple. The system will not work with eyes closed, in an effort to prevent unauthorized access.[72] The technology learns from changes in a user's appearance, and therefore works with hats, scarves, glasses, and many sunglasses, beard and makeup.[73] It also works in the dark. This is done by using a \"Flood Illuminator\", which is a dedicated infrared flash that throws out invisible infrared light onto the user's face to properly read the 30,000 facial points.[74]\n Facial recognition algorithms can help in diagnosing some diseases using specific features on the nose, cheeks and other part of the human face.[75] Relying on developed data sets, machine learning has been used to identify genetic abnormalities just based on facial dimensions.[76] FRT has also been used to verify patients before surgery procedures.\n In March, 2022 according to a publication by Forbes, FDNA, an AI development company claimed that in the space of 10 years, they have worked with geneticists to develop a database of about 5,000 diseases and 1500 of them can be detected with facial recognition algorithms.[77]\n In an interview, the National Health Authority chief Dr. R.S. Sharma said that facial recognition technology would be used in conjunction with Aadhaar to authenticate the identity of people seeking vaccines.[78] Ten human rights and digital rights organizations and more than 150 individuals signed a statement by the Internet Freedom Foundation that raised alarm against the deployment of facial recognition technology in the central government's vaccination drive process.[79] Implementation of an error-prone system without adequate legislation containing mandatory safeguards, would deprive citizens of essential services and linking this untested technology to the vaccination roll-out in India will only exclude persons from the vaccine delivery system.[80]\n In July, 2021, a press release by the Government of Meghalaya stated that facial recognition technology (FRT) would be used to verify the identity of pensioners to issue a Digital Life Certificate using \"Pensioner's Life Certification Verification\" mobile application.[81] The notice, according to the press release, purports to offer pensioners \"a secure, easy and hassle-free interface for verifying their liveness to the Pension Disbursing Authorities from the comfort of their homes using smart phones\". Mr. Jade Jeremiah Lyngdoh, a law student, sent a legal notice to the relevant authorities highlighting that \"The application has been rolled out without any anchoring legislation which governs the processing of personal data and thus, lacks lawfulness and the Government is not empowered to process data.\"[82]\n The Australian Border Force and New Zealand Customs Service have set up an automated border processing system called SmartGate that uses face recognition, which compares the face of the traveller with the data in the e-passport microchip.[83][84] All Canadian international airports use facial recognition as part of the Primary Inspection Kiosk program that compares a traveler face to their photo stored on the ePassport. This program first came to Vancouver International Airport in early 2017 and was rolled up to all remaining international airports in 2018\u20132019.[85]\n Police forces in the United Kingdom have been trialing live facial recognition technology at public events since 2015.[86] In May 2017, a man was arrested using an automatic facial recognition (AFR) system mounted on a van operated by the South Wales Police. Ars Technica reported that \"this appears to be the first time [AFR] has led to an arrest\".[87] However, a 2018 report by Big Brother Watch found that these systems were up to 98% inaccurate.[86] The report also revealed that two UK police forces, South Wales Police and the Metropolitan Police, were using live facial recognition at public events and in public spaces.[88] \nIn September 2019, South Wales Police use of facial recognition was ruled lawful.[88] Live facial recognition has been trialled since 2016 in the streets of London and will be used on a regular basis from Metropolitan Police from beginning of 2020.[89] In August 2020 the Court of Appeal ruled that the way the facial recognition system had been used by the South Wales Police in 2017 and 2018 violated human rights.[90]\n However, by 2024 the Metropolitan Police were using the technique with a database of 16,000 suspects, leading to over 360 arrests, including rapists and someone wanted for grievous bodily harm for 8 years. They claim a false positive rate of only 1 in 6,000. The photos of those not identified by the system are deleted immediately.[91]\n The U.S. Department of State operates one of the largest face recognition systems in the world with a database of 117 million American adults, with photos typically drawn from driver's license photos.[92] Although it is still far from completion, it is being put to use in certain cities to give clues as to who was in the photo. The FBI uses the photos as an investigative tool, not for positive identification.[93] As of 2016,[update] facial recognition was being used to identify people in photos taken by police in San Diego and Los Angeles (not on real-time video, and only against booking photos)[94] and use was planned in West Virginia and Dallas.[95]\n In recent years Maryland has used face recognition by comparing people's faces to their driver's license photos. The system drew controversy when it was used in Baltimore to arrest unruly protesters after the death of Freddie Gray in police custody.[96] Many other states are using or developing a similar system however some states have laws prohibiting its use.\n The FBI has also instituted its Next Generation Identification program to include face recognition, as well as more traditional biometrics like fingerprints and iris scans, which can pull from both criminal and civil databases.[97] The federal Government Accountability Office criticized the FBI for not addressing various concerns related to privacy and accuracy.[98]\n Starting in 2018, U.S. Customs and Border Protection deployed \"biometric face scanners\" at U.S. airports. Passengers taking outbound international flights can complete the check-in, security and the boarding process after getting facial images captured and verified by matching their ID photos stored on CBP's database. Images captured for travelers with U.S. citizenship will be deleted within up to 12-hours. The Transportation Security Administration (TSA) had expressed its intention to adopt a similar program for domestic air travel during the security check process in the future. The American Civil Liberties Union is one of the organizations against the program, concerning that the program will be used for surveillance purposes.[99]\n In 2019, researchers reported that Immigration and Customs Enforcement (ICE) uses facial recognition software against state driver's license databases, including for some states that provide licenses to undocumented immigrants.[98]\n In December 2022, 16 major domestic airports in the US started testing facial-recognition tech where kiosks with cameras are checking the photos on travelers' IDs to make sure that passengers are not impostors.[100]\n In 2006, the \"Skynet\" (\u5929\u7db2\uff09)Project was initiated by the Chinese government to implement CCTV surveillance nationwide and as of 2018,[update] there have been 20 million cameras, many of which are capable of real-time facial recognition, deployed across the country for this project.[101] Some official claim that the current Skynet system can scan the entire Chinese population in one second and the world population in two seconds.[102]\n In 2017, the Qingdao police was able to identify twenty-five wanted suspects using facial recognition equipment at the Qingdao International Beer Festival, one of which had been on the run for 10 years.[103] The equipment works by recording a 15-second video clip and taking multiple snapshots of the subject. That data is compared and analyzed with images from the police department's database and within 20 minutes, the subject can be identified with a 98.1% accuracy.[104]\n In 2018, Chinese police in Zhengzhou and Beijing were using smart glasses to take photos which are compared against a government database using facial recognition to identify suspects, retrieve an address, and track people moving beyond their home areas.[105][106]\n As of late 2017,[update] China has deployed facial recognition and artificial intelligence technology in Xinjiang. Reporters visiting the region found surveillance cameras installed every hundred meters or so in several cities, as well as facial recognition checkpoints at areas like gas stations, shopping centers, and mosque entrances.[107][108] In May 2019, Human Rights Watch reported finding Face++ code in the Integrated Joint Operations Platform (IJOP), a police surveillance app used to collect data on, and track the Uighur community in Xinjiang.[109] Human Rights Watch released a correction to its report in June 2019 stating that the Chinese company Megvii did not appear to have collaborated on IJOP, and that the Face++ code in the app was inoperable.[110] In February 2020, following the Coronavirus outbreak, Megvii applied for a bank loan to optimize the body temperature screening system it had launched to help identify people with symptoms of a Coronavirus infection in crowds. In the loan application Megvii stated that it needed to improve the accuracy of identifying masked individuals.[111]\n Many public places in China are implemented with facial recognition equipment, including railway stations, airports, tourist attractions, expos, and office buildings. In October 2019, a professor at Zhejiang Sci-Tech University sued the Hangzhou Safari Park for abusing private biometric information of customers. The safari park uses facial recognition technology to verify the identities of its Year Card holders. An estimated 300 tourist sites in China have installed facial recognition systems and use them to admit visitors. This case is reported to be the first on the use of facial recognition systems in China.[112] In August 2020, Radio Free Asia reported that in 2019 Geng Guanjun, a citizen of Taiyuan City who had used the WeChat app by Tencent to forward a video to a friend in the United States was subsequently convicted on the charge of the crime \"picking quarrels and provoking troubles\". The Court documents showed that the Chinese police used a facial recognition system to identify Geng Guanjun as an \"overseas democracy activist\" and that China's network management and propaganda departments directly monitor WeChat users.[113]\n In 2019, Protestors in Hong Kong destroyed smart lampposts amid concerns they could contain cameras and facial recognition system used for surveillance by Chinese authorities.[114] Human rights groups have criticized the Chinese government for using artificial intelligence facial recognition technology in its suppression against Uyghurs,[115] Christians[116] and Falun Gong practitioners.[117][118]\n Even though facial recognition technology (FRT) is not fully accurate,[119] it is being increasingly deployed for identification purposes by the police in India. FRT systems generate a probability match score, or a confidence score between the suspect who is to be identified and the database of identified criminals that is available with the police. The National Automated Facial Recognition System (AFRS)[120] is already being developed by the National Crime Records Bureau (NCRB), a body constituted under the Ministry of Home Affairs. The project seeks to develop and deploy a national database of photographs which would comport with a facial recognition technology system by the central and state security agencies. The Internet Freedom Foundation has flagged concerns regarding the project.[121] The NGO has highlighted that the accuracy of FRT systems are \"routinely exaggerated and the real numbers leave much to be desired.[121] The implementation of such faulty FRT systems would lead to high rates of false positives and false negatives in this recognition process.\"\u00a0\n Under the Supreme Court of India's decision in Justice K.S. Puttaswamy vs Union of India (22017 10 SCC 1), any justifiable intrusion by the State into people's right to privacy, which is protected as a fundamental right under Article 21 of the Constitution, must confirm to certain thresholds, namely: legality, necessity, proportionality and procedural safeguards.[122] As per the Internet Freedom Foundation, the National Automated Facial Recognition System (AFRS) proposal fails to meet any of these thresholds, citing \"absence of legality,\" \"manifest arbitrariness,\" and \"absence of safeguards and accountability.\"[123]\n While the national level AFRS project is still in the works, police departments in various states in India are already deploying facial recognition technology systems, such as: TSCOP + CCTNS in Telangana,[124] Punjab Artificial Intelligence System (PAIS) in Punjab,[125] Trinetra in Uttar Pradesh,[126] Police Artificial Intelligence System in Uttarakhand,[127] AFRS in Delhi, Automated Multimodal Biometric Identification System (AMBIS) in Maharashtra, FaceTagr in Tamil Nadu. The Crime and Criminal Tracking Network and Systems (CCTNS), which is a Mission Mode Project under the National e-Governance Plan (NeGP),[128] is viewed as a system which would connect police stations across India, and help them \"talk\"[129] to each other. The project's objective is to digitize all FIR-related information, including FIRs registered, as well as cases investigated, charge sheets filed, and suspects and wanted persons in all police stations. This shall constitute a national database of crime and criminals in India. CCTNS is being implemented without a data protection law in place. CCTNS is proposed to be integrated with the AFRS, a repository of all crime and criminal related facial data which can be deployed to purportedly identify or verify a person from a variety of inputs ranging from images to videos.[130] This has raised privacy concerns from civil society organizations and privacy experts. Both the projects have been censured as instruments of \"mass surveillance\" at the hands of the state.[131] In Rajasthan, 'RajCop,' a police app has been recently integrated with a facial recognition module which can match the face of a suspect against a database of known persons in real-time. Rajasthan police is in currently working to widen the ambit of this module by making it mandatory to upload photographs of all arrested persons in CCTNS database, which will \"help develop a rich database of known offenders.\"[132]\n Helmets fixed with camera have been designed and being used by Rajasthan police in law and order situations to capture police action and activities of \"the miscreants, which can later serve as evidence during the investigation of such cases.\"[132] PAIS (Punjab Artificial Intelligence System), App employs deep learning, machine learning, and face recognition for the identification of criminals to assist police personnel.[132] The state of Telangana has installed 8 lakh CCTV cameras,[132] with its capital city Hyderabad slowly turning into a surveillance capital.[133]\n A false positive happens when facial recognition technology misidentifies a person to be someone they are not, that is, it yields an incorrect positive result. They often results in discrimination and strengthening of existing biases. For example, in 2018, Delhi Police reported that its FRT system had an accuracy rate of 2%, which sank to 1% in 2019. The FRT system even failed to distinguish accurately between different sexes.[134]\n The government of Delhi in collaboration with Indian Space Research Organisation (ISRO) is developing a new technology called Crime Mapping Analytics and Predictive System (CMAPS). The project aims to deploy space technology for \"controlling crime and maintaining law and order.\"[132] The system will be connected to a database containing data of criminals.[132] The technology is envisaged to be deployed to collect real-time data at the crime scene.[132]\n In a reply dated November 25, 2020 to a Right to Information request filed by the Internet Freedom Foundation seeking information about the facial recognition system being used by the Delhi Police (with reference number DEPOL/R/E/20/07128),[135] the Office of the Deputy Commissioner of Police cum Public Information Officer: Crime stated that they cannot provide the information under section 8(d) of the Right to Information Act, 2005.[136]\nA Right to Information (RTI) request dated July 30, 2020 was filed with the Office of the Commissioner, Kolkata Police, seeking information about the facial recognition technology that the department was using.[137] The information sought was denied[138] stating that the department was exempted from disclosure under section 24(4) of the RTI Act.\n In the 2000 Mexican presidential election, the Mexican government employed face recognition software to prevent voter fraud. Some individuals had been registering to vote under several different names, in an attempt to place multiple votes. By comparing new face images to those already in the voter database, authorities were able to reduce duplicate registrations.[139]\n In Colombia public transport busses are fitted with a facial recognition system by FaceFirst Inc to identify passengers that are sought by the National Police of Colombia. FaceFirst Inc also built the facial recognition system for Tocumen International Airport in Panama. The face recognition system is deployed to identify individuals among the travellers that are sought by the Panamanian National Police or Interpol.[140] Tocumen International Airport operates an airport-wide surveillance system using hundreds of live face recognition cameras to identify wanted individuals passing through the airport. The face recognition system was initially installed as part of a US$11\u00a0million contract and included a computer cluster of sixty computers, a fiber-optic cable network for the airport buildings, as well as the installation of 150 surveillance cameras in the airport terminal and at about 30 airport gates.[141]\n At the 2014 FIFA World Cup in Brazil the Federal Police of Brazil used face recognition goggles. Face recognition systems \"made in China\" were also deployed at the 2016 Summer Olympics in Rio de Janeiro.[140] Nuctech Company provided 145 inspection terminals for Maracan\u00e3 Stadium and 55 terminals for the Deodoro Olympic Park.[142]\n Police forces in at least 21 countries of the European Union use, or plan to use, facial recognition systems, either for administrative or criminal purposes.[143]\n Greek police passed a contract with Intracom-Telecom for the provision of at least 1,000 devices equipped with live facial recognition system. The delivery is expected before the summer 2021. The total value of the contract is over 4 million euros, paid for in large part by the Internal Security Fund of the European Commission.[144]\n Italian police acquired a face recognition system in 2017, Sistema Automatico Riconoscimento Immagini (SARI). In November 2020, the Interior ministry announced plans to use it in real-time to identify people suspected of seeking asylum.[145]\n The Netherlands has deployed facial recognition and artificial intelligence technology since 2016.[146] The database of the Dutch police currently contains over 2.2\u00a0million pictures of 1.3\u00a0million Dutch citizens. This accounts for about 8% of the population. In The Netherlands, face recognition is not used by the police on municipal CCTV.[147]\n In South Africa, in 2016, the city of Johannesburg announced it was rolling out smart CCTV cameras complete with automatic number plate recognition and facial recognition.[148]\n The US firm 3VR, now Identiv, is an example of a vendor which began offering facial recognition systems and services to retailers as early as 2007.[149] In 2012, the company advertised benefits such as \"dwell and queue line analytics to decrease customer wait times\", \"facial surveillance analytic[s] to facilitate personalized customer greetings by employees\" and the ability to \"[c]reate loyalty programs by combining Point of sale (POS) data with facial recognition\".[150]\n In 2018, the National Retail Federation Loss Prevention Research Council called facial recognition technology \"a promising new tool\" worth evaluating.[151]\n In July 2020, the Reuters news agency reported that during the 2010s the pharmacy chain Rite Aid had deployed facial recognition video surveillance systems and components from FaceFirst, DeepCam LLC, and other vendors at some retail locations in the United States.[151] Cathy Langley, Rite Aid's vice president of asset protection, used the phrase \"feature matching\" to refer to the systems and said that usage of the systems resulted in less violence and organized crime in the company's stores, while former vice president of asset protection Bob Oberosler emphasized improved safety for staff and a reduced need for the involvement of law enforcement organizations.[151] In a 2020 statement to Reuters in response to the reporting, Rite Aid said that it had ceased using the facial recognition software and switched off the cameras.[151]\n According to director Read Hayes of the National Retail Federation Loss Prevention Research Council, Rite Aid's surveillance program was either the largest or one of the largest programs in retail.[151] The Home Depot, Menards, Walmart, and 7-Eleven are among other US retailers also engaged in large-scale pilot programs or deployments of facial recognition technology.[151]\n Of the Rite Aid stores examined by Reuters in 2020, those in communities where people of color made up the largest racial or ethnic group were three times as likely to have the technology installed,[151] raising concerns related to the substantial history of racial segregation and racial profiling in the United States. Rite Aid said that the selection of locations was \"data-driven\", based on the theft histories of individual stores, local and national crime data, and site infrastructure.[151]\n In 2019, facial recognition to prevent theft was in use at Sydney's Star Casino and was also deployed at gaming venues in New Zealand.[152]\n In June 2022, consumer group CHOICE reported facial recognition was in use in Australia at Kmart, Bunnings, and The Good Guys. The Good Guys subsequently suspended the technology pending a legal challenge by CHOICE to the Office of the Australian Information Commissioner, while Bunnings kept the technology in use and Kmart maintained its trial of the technology.[153]\n At the American football championship game Super Bowl XXXV in January 2001, police in Tampa Bay, Florida used Viisage face recognition software to search for potential criminals and terrorists in attendance at the event. 19 people with minor criminal records were potentially identified.[154][155]\n Face recognition systems have also been used by photo management software to identify the subjects of photographs, enabling features such as searching images by person, as well as suggesting photos to be shared with a specific contact if their presence were detected in a photo.[156][157] By 2008 facial recognition systems were typically used as access control in security systems.[158]\n The United States' popular music and country music celebrity Taylor Swift surreptitiously employed facial recognition technology at a concert in 2018. The camera was embedded in a kiosk near a ticket booth and scanned concert-goers as they entered the facility for known stalkers.[159]\n On August 18, 2019, The Times reported that the UAE-owned Manchester City hired a Texas-based firm, Blink Identity, to deploy facial recognition systems in a driver program. The club has planned a single super-fast lane for the supporters at the Etihad stadium.[160] However, civil rights groups cautioned the club against the introduction of this technology, saying that it would risk \"normalising a mass surveillance tool\". The policy and campaigns officer at Liberty, Hannah Couchman said that Man City's move is alarming, since the fans will be obliged to share deeply sensitive personal information with a private company, where they could be tracked and monitored in their everyday lives.[161]\n In 2019, casinos in Australia and New Zealand rolled out facial recognition to prevent theft, and a representative of Sydney's Star Casino said they would also provide 'customer service' like welcoming a patron back to a bar.[152]\n In August 2020, amid the COVID-19 pandemic in the United States, American football stadiums of New York and Los Angeles announced the installation of facial recognition for upcoming matches. The purpose is to make the entry process as touchless as possible.[162] Disney's Magic Kingdom, near Orlando, Florida, likewise announced a test of facial recognition technology to create a touchless experience during the pandemic; the test was originally slated to take place between March 23 and April 23, 2021, but the limited timeframe had been removed as of late April\u00a02021.[update][163]\n Media companies have begun using face recognition technology to streamline their tracking, organizing, and archiving pictures and videos.[164]\n In 2006, the performance of the latest face recognition algorithms was evaluated in the Face Recognition Grand Challenge (FRGC). High-resolution face images, 3-D face scans, and iris images were used in the tests. The results indicated that the new algorithms are 10 times more accurate than the face recognition algorithms of 2002 and 100 times more accurate than those of 1995. Some of the algorithms were able to outperform human participants in recognizing faces and could uniquely identify identical twins.[45][165]\n One key advantage of a facial recognition system that it is able to perform mass identification as it does not require the cooperation of the test subject to work. Properly designed systems installed in airports, multiplexes, and other public places can identify individuals among the crowd, without passers-by even being aware of the system.[166] However, as compared to other biometric techniques, face recognition may not be most reliable and efficient. Quality measures are very important in facial recognition systems as large degrees of variations are possible in face images. Factors such as illumination, expression, pose and noise during face capture can affect the performance of facial recognition systems.[166] Among all biometric systems, facial recognition has the highest false acceptance and rejection rates,[166] thus questions have been raised on the effectiveness of or bias of face recognition software in cases of railway and airport security, law enforcement and housing and employment decisions.[167][5]\n Ralph Gross, a researcher at the Carnegie Mellon Robotics Institute in 2008, describes one obstacle related to the viewing angle of the face: \"Face recognition has been getting pretty good at full frontal faces and 20 degrees off, but as soon as you go towards profile, there've been problems.\"[45] Besides the pose variations, low-resolution face images are also very hard to recognize. This is one of the main obstacles of face recognition in surveillance systems.[168] It has also been suggested that camera settings can favour sharper imagery of white skin than of other skin tones.[5]\n Face recognition is less effective if facial expressions vary. A big smile can render the system less effective. For instance: Canada, in 2009, allowed only neutral facial expressions in passport photos.[169]\n There is also inconstancy in the datasets used by researchers. Researchers may use anywhere from several subjects to scores of subjects and a few hundred images to thousands of images. Data sets may be diverse and inclusive or mainly contain images of white males. It is important for researchers to make available the datasets they used to each other, or have at least a standard or representative dataset.[170]\n Although high degrees of accuracy have been claimed for some facial recognition systems, these outcomes are not universal. The consistently worst accuracy rate is for those who are 18 to 30 years old, Black and female.[5]\n Studies have shown that facial recognition algorithms tend to perform better on individuals with lighter skin tones compared to those with darker skin tones. This disparity arises primarily because training datasets often overrepresent lighter-skinned individuals, leading to higher error rates for darker-skinned people. For example, a 2018 study found that leading commercial gender classification models, which are facial recognition models, have an error rate up to 7 times higher for those with darker skin tones compared to those with lighter skin tones.[171]\n Common image compression methods, such as JPEG chroma subsampling, have been found to disproportionately degrade performance for darker-skinned individuals. These methods inadequately represent color information, which adversely affects the ability of algorithms to recognize darker-skinned individuals accurately.[172]\n Facial recognition systems often demonstrate lower accuracy when identifying individuals with non-Eurocentric facial features. Known as the Cross-race effect, this bias occurs when systems perform better on racial or ethnic groups that are overrepresented in their training data, resulting in reduced accuracy for underrepresented groups.[173] The overrepresented group is generally the more populous group in the location that the model is being developed. For example, models developed in Asian cultures generally perform better on Asian facial features than Eurocentric facial features due to overrepresentation in the developers training dataset. The opposite is observed in models developed in Eurocentric cultures.[174]\n The cross-race effect is not exclusive to machines; humans also experience difficulty recognizing faces from racial or ethnic groups different from their own. This is an example of inherent human biases being perpetuated in training datasets.[175]\n Facial recognition technologies encounter significant challenges when identifying individuals with disabilities. For instance, systems have been shown to perform worse when recognizing individuals with Down syndrome, often leading to increased false match rates. This is due to distinct facial structures associated with the condition that are not adequately represented in training datasets.[176]\n More broadly, facial recognition systems tend to overlook diverse physical characteristics related to disabilities. The lack of representative data for individuals with varying disabilities further emphasizes the need for inclusive algorithmic designs to mitigate bias and improve accuracy.[177]\n Additionally, facial expression recognition technologies often fail to accurately interpret the emotional states of individuals with intellectual disabilities. This shortcoming can hinder effective communication and interaction, underscoring the necessity for systems trained on diverse datasets that include individuals with intellectual disabilities.[178]\n Furthermore, biases in facial recognition algorithms can lead to discriminatory outcomes for people with disabilities. For example, certain facial features or asymmetries may result in misidentification or exclusion, highlighting the importance of developing accessible and fair biometric systems.[179]\n Efforts to address these biases include designing algorithms specifically for fairness. A notable study introduced a method to learn fair face representations by using a progressive cross-transformer model.[180] This approach highlights the importance of balancing accuracy across demographic groups while avoiding performance drops in specific populations.\n Additionally, targeted dataset collection has been shown to improve racial equity in facial recognition systems. By prioritizing diverse data inputs, researchers demonstrated measurable reductions in performance disparities between racial groups.[176]\n Critics of the technology complain that the London Borough of Newham scheme has, as of 2004,[update] never recognized a single criminal, despite several criminals in the system's database living in the Borough and the system has been running for several years. \"Not once, as far as the police know, has Newham's automatic face recognition system spotted a live target.\"[155][181] This information seems to conflict with claims that the system was credited with a 34% reduction in crime (hence why it was rolled out to Birmingham also).[182]\n An experiment in 2002 by the local police department in Tampa, Florida, had similarly disappointing results.[155] A system at Boston's Logan Airport was shut down in 2003 after failing to make any matches during a two-year test period.[183]\n In 2014, Facebook stated that in a standardized two-option facial recognition test, its online system scored 97.25% accuracy, compared to the human benchmark of 97.5%.[184]\n Systems are often advertised as having accuracy near 100%; this is misleading as the outcomes are not universal[5] The studies often use samples that are smaller and less diverse than would be necessary for large scale applications. Because facial recognition is not completely accurate, it creates a list of potential matches. A human operator must then look through these potential matches and studies show the operators pick the correct match out of the list only about half the time. This causes the issue of targeting the wrong suspect.[93][185]\n Civil rights organizations and privacy campaigners such as the Electronic Frontier Foundation, Big Brother Watch and the ACLU express concern that privacy is being compromised by the use of surveillance technologies.[186][86][187] Face recognition can be used not just to identify an individual, but also to unearth other personal data associated with an individual \u2013 such as other photos featuring the individual, blog posts, social media profiles, Internet behavior, and travel patterns.[188] Concerns have been raised over who would have access to the knowledge of one's whereabouts and people with them at any given time.[189] Moreover, individuals have limited ability to avoid or thwart face recognition tracking unless they hide their faces. This fundamentally changes the dynamic of day-to-day privacy by enabling any marketer, government agency, or random stranger to secretly collect the identities and associated personal information of any individual captured by the face recognition system.[188] Consumers may not understand or be aware of what their data is being used for, which denies them the ability to consent to how their personal information gets shared.[189]\n In July 2015, the United States Government Accountability Office conducted a Report to the Ranking Member, Subcommittee on Privacy, Technology and the Law, Committee on the Judiciary, U.S. Senate. The report discussed facial recognition technology's commercial uses, privacy issues, and the applicable federal law. It states that previously, issues concerning facial recognition technology were discussed and represent the need for updating the privacy laws of the United States so that federal law continually matches the impact of advanced technologies. The report noted that some industry, government, and private organizations were in the process of developing, or have developed, \"voluntary privacy guidelines\". These guidelines varied between the stakeholders, but their overall aim was to gain consent and inform citizens of the intended use of facial recognition technology. According to the report the voluntary privacy guidelines helped to counteract the privacy concerns that arise when citizens are unaware of how their personal data gets put to use.[189]\n In 2016, Russian company NtechLab caused a privacy scandal in the international media when it launched the FindFace face recognition system with the promise that Russian users could take photos of strangers in the street and link them to a social media profile on the social media platform Vkontakte (VK).[190] In December 2017, Facebook rolled out a new feature that notifies a user when someone uploads a photo that includes what Facebook thinks is their face, even if they are not tagged. Facebook has attempted to frame the new functionality in a positive light, amidst prior backlashes.[191] Facebook's head of privacy, Rob Sherman, addressed this new feature as one that gives people more control over their photos online. \"We've thought about this as a really empowering feature,\" he says. \"There may be photos that exist that you don't know about.\"[192] Facebook's DeepFace has become the subject of several class action lawsuits under the Biometric Information Privacy Act, with claims alleging that Facebook is collecting and storing face recognition data of its users without obtaining informed consent, in direct violation of the 2008 Biometric Information Privacy Act (BIPA).[193] The most recent case was dismissed in January 2016 because the court lacked jurisdiction.[194] In the US, surveillance companies such as Clearview AI are relying on the First Amendment to the United States Constitution to data scrape user accounts on social media platforms for data that can be used in the development of facial recognition systems.[195]\n In 2019, the Financial Times first reported that facial recognition software was in use in the King's Cross area of London.[196] The development around London's King's Cross mainline station includes shops, offices, Google's UK HQ and part of St Martin's College. According to the UK Information Commissioner's Office: \"Scanning people's faces as they lawfully go about their daily lives, in order to identify them, is a potential threat to privacy that should concern us all.\"[197][198] The UK Information Commissioner Elizabeth Denham launched an investigation into the use of the King's Cross facial recognition system, operated by the company Argent. In September 2019 it was announced by Argent that facial recognition software would no longer be used at King's Cross. Argent claimed that the software had been deployed between May 2016 and March 2018 on two cameras covering a pedestrian street running through the centre of the development.[199] In October 2019, a report by the deputy London mayor Sophie Linden revealed that in a secret deal the Metropolitan Police had passed photos of seven people to Argent for use in their King's cross facial recognition system.[200]\n Automated Facial Recognition was trialled by the South Wales Police on multiple occasions between 2017 and 2019. The use of the technology was challenged in court by a private individual, Edward Bridges, with support from the charity Liberty (case known as R (Bridges) v Chief Constable South Wales Police). The case was heard in the Court of Appeal and a judgement was given in August 2020.[201] The case argued that the use of Facial Recognition was a privacy violation on the basis that there was insufficient legal framework or proportionality in the use of Facial Recognition and that its use was in violation of the Data Protection Acts 1998 and 2018. The case was decided in favour of Bridges and did not award damages. The case was settled via a declaration of wrongdoing.[201] In response to the case, the British Government has repeatedly attempted to pass a Bill regulating the use of Facial Recognition in public spaces. The proposed Bills have attempted to appoint a Commissioner with the ability to  regulate Facial Recognition use by Government Services in a similar manner to the Commissioner for CCTV. Such a Bill has yet to come into force [correct as of September\u00a02021[update]].[125]\n In January 2023, New York Attorney General Letitia James asked for more information on the use of facial recognition technology from Madison Square Garden Entertainment following reports that the firm used it to block lawyers involved in litigation against the company from entering Madison Square Garden. She noted such a move would could go against federal, state, and local human rights laws.[202]\n As of 2018,[update] it is still contested as to whether or not facial recognition technology works less accurately on people of color.[203] One study by Joy Buolamwini (MIT Media Lab) and Timnit Gebru (Microsoft Research) found that the error rate for gender recognition for women of color within three commercial facial recognition systems ranged from 23.8% to 36%, whereas for lighter-skinned men it was between 0.0 and 1.6%. Overall accuracy rates for identifying men (91.9%) were higher than for women (79.4%), and none of the systems accommodated a non-binary understanding of gender.[204] It also showed that the datasets used to train commercial facial recognition models were unrepresentative of the broader population and skewed toward lighter-skinned males. However, another study showed that several commercial facial recognition software sold to law enforcement offices around the country had a lower false non-match rate for black people than for white people.[205]\n Experts fear that face recognition systems may actually be hurting citizens the police claims they are trying to protect.[206] It is considered an imperfect biometric, and in a study conducted by Georgetown University researcher Clare Garvie, she concluded that \"there's no consensus in the scientific community that it provides a positive identification of somebody.\"[207] It is believed that with such large margins of error in this technology, both legal advocates and facial recognition software companies say that the technology should only supply a portion of the case \u2013 no evidence that can lead to an arrest of an individual.[207] The lack of regulations holding facial recognition technology companies to requirements of racially biased testing can be a significant flaw in the adoption of use in law enforcement. CyberExtruder, a company that markets itself to law enforcement said that they had not performed testing or research on bias in their software. CyberExtruder did note that some skin colors are more difficult for the software to recognize with current limitations of the technology. \"Just as individuals with very dark skin are hard to identify with high significance via facial recognition, individuals with very pale skin are the same,\" said Blake Senftner, a senior software engineer at CyberExtruder.[207]\n The United States' National Institute of Standards and Technology (NIST) carried out extensive testing of FRT system 1:1 verification[208] and 1:many identification.[208] It also tested for the differing accuracy of FRT across different demographic groups. The independent study concluded at present, no FRT system has 100% accuracy.[209]\n In 2010, Peru passed the Law for Personal Data Protection, which defines biometric information that can be used to identify an individual as sensitive data. In 2012, Colombia passed a comprehensive Data Protection Law which defines biometric data as senstivite information.[140] According to Article 9(1) of the EU's 2016 General Data Protection Regulation (GDPR) the processing of biometric data for the purpose of \"uniquely identifying a natural person\" is sensitive and the facial recognition data processed in this way becomes sensitive personal data. In response to the GDPR passing into the law of EU member states, EU based researchers voiced concern that if they were required under the GDPR to obtain individual's consent for the processing of their facial recognition data, a face database on the scale of MegaFace could never be established again.[210] In September 2019 the Swedish Data Protection Authority (DPA) issued its first ever financial penalty for a violation of the EU's General Data Protection Regulation (GDPR) against a school that was using the technology to replace time-consuming roll calls during class. The DPA found that the school illegally obtained the biometric data of its students without completing an impact assessment. In addition the school did not make the DPA aware of the pilot scheme. A 200,000 SEK fine (\u20ac19,000/$21,000) was issued.[citation needed]\n In the United States of America several U.S. states have passed laws to protect the privacy of biometric data. Examples include the Illinois Biometric Information Privacy Act (BIPA) and the California Consumer Privacy Act (CCPA).[211] In March 2020 California residents filed a class action against Clearview AI, alleging that the company had illegally collected biometric data online and with the help of face recognition technology built up a database of biometric data which was sold to companies and police forces. At the time Clearview AI already faced two lawsuits under BIPA[212] and an investigation by the Privacy Commissioner of Canada for compliance with the Personal Information Protection and Electronic Documents Act (PIPEDA).[213]\n In May 2019, San Francisco, California became the first major United States city to ban the use of facial recognition software for police and other local government agencies' usage.[214] San Francisco Supervisor, Aaron Peskin, introduced regulations that will require agencies to gain approval from the San Francisco Board of Supervisors to purchase surveillance technology.[215] The regulations also require that agencies publicly disclose the intended use for new surveillance technology.[215] In June 2019, Somerville, Massachusetts became the first city on the East Coast to ban face surveillance software for government use,[216] specifically in police investigations and municipal surveillance.[217] In July 2019, Oakland, California banned the usage of facial recognition technology by city departments.[218]\n The American Civil Liberties Union (\"ACLU\") has campaigned across the United States for transparency in surveillance technology[217] and has supported both San Francisco and Somerville's ban on facial recognition software. The ACLU works to challenge the secrecy and surveillance with this technology.[citation needed][219]\n During the George Floyd protests, use of facial recognition by city government was banned in Boston, Massachusetts.[220] As of June\u00a010, 2020,[update] municipal use has been banned in:[11]\n The West Lafayette, Indiana City Council passed an ordinance banning facial recognition surveillance technology.[223]\n On October 27, 2020, 22 human rights groups called upon the University of Miami to ban facial recognition technology. This came after the students accused the school of using the software to identify student protesters. The allegations were, however, denied by the university.[224]\n A state police reform law in Massachusetts will take effect in July 2021; a ban passed by the legislature was rejected by governor Charlie Baker.[225] Instead, the law requires a judicial warrant, limit the personnel who can perform the search, record data about how the technology is used, and create a commission to make recommendations about future regulations.[226]\n Reports in 2024 revealed that some police departments, including San Francisco Police Department, had skirted bans on facial recognition technology that had been enacted in their respective cities.[227]\n In January 2020, the European Union suggested, but then quickly scrapped, a proposed moratorium on facial recognition in public spaces.[228][229]\n The European \"Reclaim Your Face\" coalition launched in October 2020. The coalition calls for a ban on facial recognition and launched a European Citizens' Initiative in February 2021. More than 60 organizations call on the European Commission to strictly regulate the use of biometric surveillance technologies.[230]\n In the 18th and 19th century, the belief that facial expressions revealed the moral worth or true inner state of a human was widespread and physiognomy was a respected science in the Western world. From the early 19th century onwards photography was used in the physiognomic analysis of facial features and facial expression to detect insanity and dementia.[231] In the 1960s and 1970s the study of human emotions and its expressions was reinvented by psychologists, who tried to define a normal range of emotional responses to events.[232] The research on automated emotion recognition has since the 1970s focused on facial expressions and speech, which are regarded as the two most important ways in which humans communicate emotions to other humans. In the 1970s the Facial Action Coding System (FACS) categorization for the physical expression of emotions was established.[233] Its developer Paul Ekman maintains that there are six emotions that are universal to all human beings and that these can be coded in facial expressions.[234] Research into automatic emotion specific expression recognition has in the past decades focused on frontal view images of human faces.[235]\n In 2016, facial feature emotion recognition algorithms were among the new technologies, alongside high-definition CCTV, high resolution 3D face recognition and iris recognition, that found their way out of university research labs.[citation needed] In 2016, Facebook acquired FacioMetrics, a facial feature emotion recognition corporate spin-off by Carnegie Mellon University. In the same year Apple Inc. acquired the facial feature emotion recognition start-up Emotient.[236] By the end of 2016, commercial vendors of facial recognition systems offered to integrate and deploy emotion recognition algorithms for facial features.[citation needed] The MIT's Media Lab spin-off Affectiva[237] by late 2019 offered a facial expression emotion detection product that can recognize emotions in humans while driving.[236]\n The development of anti-facial recognition technology is effectively an arms race between privacy researchers and big data companies. Big data companies increasingly use convolutional AI technology to create ever more advanced facial recognition models. Solutions to block facial recognition may not work on newer software, or on different types of facial recognition models. One popular cited example of facial-recognition blocking is the CVDazzle makeup and haircut system, but the creators note on their website that it has been outdated for quite some time as it was designed to combat a particular facial recognition algorithm and may not work.[238] Another example is the emergence of facial recognition that can identify people wearing facemasks and sunglasses, especially after the COVID-19 pandemic.[239]\n Given that big data companies have much more funding than privacy researchers, it is very difficult for anti-facial recognition systems to keep up. There is also no guarantee that obfuscation techniques that were used for images taken in the past and stored, such as masks or software obfuscation, would protect users from facial-recognition analysis of those images by future technology.[240]\n In January 2013, Japanese researchers from the National Institute of Informatics created 'privacy visor' glasses that use nearly infrared light to make the face underneath it unrecognizable to face recognition software that use infrared.[241] The latest version uses a titanium frame, light-reflective material and a mask which uses angles and patterns to disrupt facial recognition technology through both absorbing and bouncing back light sources.[242][243][244][245] However, these methods are used to prevent infrared facial recognition and would not work on AI facial recognition of plain images. Some projects use adversarial machine learning to come up with new printed patterns that confuse existing face recognition software.[246]\n One method that may work to protect from facial recognition systems are specific haircuts and make-up patterns that prevent the used algorithms to detect a face, known as computer vision dazzle.[238] Incidentally, the makeup styles popular with Juggalos may also protect against facial recognition.[247]\n Facial masks that are worn to protect from contagious viruses can reduce the accuracy of facial recognition systems. A 2020 NIST study, tested popular one-to-one matching systems and found a failure rate between five and fifty percent on masked individuals. The Verge speculated that the accuracy rate of mass surveillance systems, which were not included in the study, would be even less accurate than the accuracy of one-to-one matching systems.[248] The facial recognition of Apple Pay can work through many barriers, including heavy makeup, thick beards and even sunglasses, but fails with masks.[249] However, facial recognition of masked faces is increasingly getting more reliable.\n Another solution is the application of obfuscation to images that may fool facial recognition systems while still appearing normal to a human user. These could be used for when images are posted online or on social media. However, as it is hard to remove images once they are on the internet, the obfuscation on these images may be defeated and the face of the user identified by future advances in technology. Two examples of this technique, developed in 2020, are the ANU's 'Camera Adversaria' camera app, and the University of Chicago's Fawkes image cloaking software algorithm which applies obfuscation to already taken photos.[240] However, by 2021 the Fawkes obfuscation algorithm had already been specifically targeted by Microsoft Azure which changed its algorithm to lower Fawkes' effectiveness.[250]\n",
        "doc_number": 25
    },
    {
        "url": "https://en.wikipedia.org/wiki/Chatbot",
        "content": "\n A chatbot (originally chatterbot)[1] is a software application or web interface designed to have textual or spoken conversations.[2][3][4] Modern chatbots are typically online and use generative artificial intelligence systems that are capable of maintaining a conversation with a user in natural language and simulating the way a human would behave as a conversational partner. Such chatbots often use deep learning and natural language processing, but simpler chatbots have existed for decades.\n Although chatbots have existed since the late 1960s, the field gained widespread attention in the early 2020s due to the popularity of OpenAI's ChatGPT,[5][6] followed by alternatives such as Microsoft's Copilot and Google's Gemini.[7] Such examples reflect the recent practice of basing such products upon broad foundational large language models, such as GPT-4 or the Gemini language model, that get fine-tuned so as to target specific tasks or applications (i.e., simulating human conversation, in the case of chatbots). Chatbots can also be designed or customized to further target even more specific situations and/or particular subject-matter domains.[8]\n A major area where chatbots have long been used is in customer service and support, with various sorts of virtual assistants.[9] Companies spanning a wide range of industries have begun using the latest generative artificial intelligence technologies to power more advanced developments in such areas.[8]\n In 1950, Alan Turing's famous article \"Computing Machinery and Intelligence\" was published,[10] which proposed what is now called the Turing test as a criterion of intelligence. This criterion depends on the ability of a computer program to impersonate a human in a real-time written conversation with a human judge to the extent that the judge is unable to distinguish reliably\u2014on the basis of the conversational content alone\u2014between the program and a real human.\n \nThe notoriety of Turing's proposed test stimulated great interest in Joseph Weizenbaum's program ELIZA, published in 1966, which seemed to be able to fool users into believing that they were conversing with a real human. However Weizenbaum himself did not claim that ELIZA was genuinely intelligent, and the introduction to his paper presented it more as a debunking exercise: In artificial intelligence, machines are made to behave in wondrous ways, often sufficient to dazzle even the most experienced observer. But once a particular program is unmasked, once its inner workings are explained, its magic crumbles away; it stands revealed as a mere collection of procedures. The observer says to himself \"I could have written that\". With that thought, he moves the program in question from the shelf marked \"intelligent\", to that reserved for curios. The object of this paper is to cause just such a re-evaluation of the program about to be \"explained\". Few programs ever needed it more.[11] ELIZA's key method of operation involves the recognition of clue words or phrases in the input, and the output of the corresponding pre-prepared or pre-programmed responses that can move the conversation forward in an apparently meaningful way (e.g. by responding to any input that contains the word 'MOTHER' with 'TELL ME MORE ABOUT YOUR FAMILY').[11] Thus an illusion of understanding is generated, even though the processing involved has been merely superficial. ELIZA showed that such an illusion is surprisingly easy to generate because human judges are ready to give the benefit of the doubt when conversational responses are capable of being interpreted as \"intelligent\".\n Interface designers have come to appreciate that humans' readiness to interpret computer output as genuinely conversational\u2014even when it is actually based on rather simple pattern-matching\u2014can be exploited for useful purposes. Most people prefer to engage with programs that are human-like, and this gives chatbot-style techniques a potentially useful role in interactive systems that need to elicit information from users, as long as that information is relatively straightforward and falls into predictable categories. Thus, for example, online help systems can usefully employ chatbot techniques to identify the area of help that users require, potentially providing a \"friendlier\" interface than a more formal search or menu system. This sort of usage holds the prospect of moving chatbot technology from Weizenbaum's \"shelf ... reserved for curios\" to that marked \"genuinely useful computational methods\".\n Among the most notable early chatbots are ELIZA (1966) and PARRY (1972).[12][13][14][15] More recent notable programs include A.L.I.C.E., Jabberwacky and D.U.D.E (Agence Nationale de la Recherche and CNRS 2006). While ELIZA and PARRY were used exclusively to simulate typed conversation, many chatbots now include other functional features, such as games and web searching abilities. In 1984, a book called The Policeman's Beard is Half Constructed was published, allegedly written by the chatbot Racter (though the program as released would not have been capable of doing so).[16]\n From 1978[17] to some time after 1983,[18] the CYRUS project led by Janet Kolodner constructed a chatbot simulating Cyrus Vance (57th United States Secretary of State). It used case-based reasoning, and updated its database daily by parsing wire news from United Press International. The program was unable to process the news items subsequent to the surprise resignation of Cyrus Vance in April 1980, and the team constructed another chatbot simulating his successor, Edmund Muskie.[19][18]\n One pertinent field of AI research is natural-language processing. Usually, weak AI fields employ specialized software or programming languages created specifically for the narrow function required. For example, A.L.I.C.E. uses a markup language called AIML,[3] which is specific to its function as a conversational agent, and has since been adopted by various other developers of, so-called, Alicebots. Nevertheless, A.L.I.C.E. is still purely based on pattern matching techniques without any reasoning capabilities, the same technique ELIZA was using back in 1966. This is not strong AI, which would require sapience and logical reasoning abilities.\n Jabberwacky learns new responses and context based on real-time user interactions, rather than being driven from a static database. Some more recent chatbots also combine real-time learning with evolutionary algorithms that optimize their ability to communicate based on each conversation held.\n Chatbot competitions focus on the Turing test or more specific goals. Two such annual contests are the Loebner Prize and The Chatterbox Challenge (the latter has been offline since 2015, however, materials can still be found from web archives).[20]\n DBpedia created a chatbot during the GSoC of 2017.[21][22][23] It can communicate through Facebook Messenger (see Master of Code Global article).\n Modern chatbots like ChatGPT are often based on large language models called generative pre-trained transformers (GPT). They are based on a deep learning architecture called the transformer, which contains artificial neural networks. They learn how to generate text by being trained on a large text corpus, which provides a solid foundation for the model to perform well on downstream tasks with limited amounts of task-specific data. Despite criticism of its accuracy and tendency to \"hallucinate\"\u2014that is, to confidently output false information and even cite non-existent sources\u2014ChatGPT has gained attention for its detailed responses and historical knowledge. Another example is BioGPT, developed by Microsoft, which focuses on answering biomedical questions.[24][25] In November 2023, Amazon announced a new chatbot, called Q, for people to use at work.[26]\n Many companies' chatbots run on messaging apps or simply via SMS. They are used for B2C customer service, sales and marketing.[27]\n In 2016, Facebook Messenger allowed developers to place chatbots on their platform. There were 30,000 bots created for Messenger in the first six months, rising to 100,000 by September 2017.[28]\n Since September 2017, this has also been as part of a pilot program on WhatsApp. Airlines KLM and Aerom\u00e9xico both announced their participation in the testing;[29][30][31][32] both airlines had previously launched customer services on the Facebook Messenger platform. \n The bots usually appear as one of the user's contacts, but can sometimes act as participants in a group chat.\n Many banks, insurers, media companies, e-commerce companies, airlines, hotel chains, retailers, health care providers, government entities, and restaurant chains have used chatbots to answer simple questions, increase customer engagement,[33] for promotion, and to offer additional ways to order from them.[34] Chatbots are also used in market research to collect short survey responses.[35]\n A 2017 study showed 4% of companies used chatbots.[36] According to a 2016 study, 80% of businesses said they intended to have one by 2020.[37]\n Previous generations of chatbots were present on company websites, e.g. Ask Jenn from Alaska Airlines which debuted in 2008[38] or Expedia's virtual customer service agent which launched in 2011.[38][39] The newer generation of chatbots includes IBM Watson-powered \"Rocky\", introduced in February 2017 by the New York City-based e-commerce company Rare Carat to provide information to prospective diamond buyers.[40][41]\n Used by marketers to script sequences of messages, very similar to an autoresponder sequence. Such sequences can be triggered by user opt-in or the use of keywords within user interactions. After a trigger occurs a sequence of messages is delivered until the next anticipated user response. Each user response is used in the decision tree to help the chatbot navigate the response sequences to deliver the correct response message.\n Companies have used chatbots for customer support, human resources, or in Internet-of-Things (IoT) projects. Overstock.com, for one, has reportedly launched a chatbot named Mila to attempt to automate certain processes when customer service employees request sick leave.[42] Other large companies such as Lloyds Banking Group, Royal Bank of Scotland, Renault and Citro\u00ebn are now using chatbots instead of call centres with humans to provide a first point of contact.[citation needed] In large companies, like in hospitals and aviation organizations, chatbots are also used to share information within organizations, and to assist and replace service desks.[citation needed]\n Chatbots have been proposed as a replacement for customer service departments.[43]\n Deep learning techniques can be incorporated into chatbot applications to allow them to map conversations between users and customer service agents, especially in social media.[44]\n In 2019, Gartner predicted that by 2021, 15% of all customer service interactions globally will be handled completely by AI.[45]  A study by Juniper Research in 2019 estimates retail sales resulting from chatbot-based interactions will reach $112 billion by 2023.[46]\n In 2016, Russia-based Tochka Bank launched a chatbot on Facebook for a range of financial services, including a possibility of making payments.[47] In July 2016, Barclays Africa also launched a Facebook chatbot.[48]\n In 2023, US-based National Eating Disorders Association replaced its human helpline staff with a chatbot but had to take it offline after users reported receiving harmful advice from it.[49][50][51]\n Chatbots are also appearing in the healthcare industry.[52][53] A study suggested that physicians in the United States believed that chatbots would be most beneficial for scheduling doctor appointments, locating health clinics, or providing medication information.[54]\n ChatGPT is able to answer user queries related to health promotion and disease prevention such as screening and vaccination.[55] WhatsApp has teamed up with the World Health Organization (WHO) to make a chatbot service that answers users' questions on COVID-19.[56]\n In 2020, the Government of India launched a chatbot called MyGov Corona Helpdesk,[57] that worked through WhatsApp and helped people access information about the Coronavirus (COVID-19) pandemic.[58][59]\n Certain patient groups are still reluctant to use chatbots. A mixed-methods 2019 study showed that people are still hesitant to use chatbots for their healthcare due to poor understanding of the technological complexity, the lack of empathy, and concerns about cyber-security. The analysis showed that while 6% had heard of a health chatbot and 3% had experience of using it, 67% perceived themselves as likely to use one within 12 months. The majority of participants would use a health chatbot for seeking general health information (78%), booking a medical appointment (78%), and looking for local health services (80%). However, a health chatbot was perceived as less suitable for seeking results of medical tests and seeking specialist advice such as sexual health.[60]\n The analysis of attitudinal variables showed that most participants reported their preference for discussing their health with doctors (73%) and having access to reliable and accurate health information (93%). While 80% were curious about new technologies that could improve their health, 66% reported only seeking a doctor when experiencing a health problem and 65% thought that a chatbot was a good idea. 30% reported dislike about talking to computers, 41% felt it would be strange to discuss health matters with a chatbot and about half were unsure if they could trust the advice given by a chatbot. Therefore, perceived trustworthiness, individual attitudes towards bots, and dislike for talking to computers are the main barriers to health chatbots.[60][55]\n In New Zealand, the chatbot SAM \u2013 short for Semantic Analysis Machine[61] \u2013 has been developed by Nick Gerritsen of Touchtech.[62] It is designed to share its political thoughts, for example on topics such as climate change, healthcare and education, etc. It talks to people through Facebook Messenger.[63][64][65][66]\n In 2022, the chatbot \"Leader Lars\" or \"Leder Lars\" was nominated for The Synthetic Party to run in the Danish parliamentary election,[67] and was built by the artist collective Computer Lars.[68] Leader Lars differed from earlier virtual politicians by leading a political party and by not pretending to be an objective candidate.[69] This chatbot engaged in critical discussions on politics with users from around the world.[70]\n In India, the state government has launched a chatbot for its Aaple Sarkar platform,[71] which provides conversational access to information regarding public services managed.[72][73]\n Chatbots have also been incorporated into devices not primarily meant for computing, such as toys.[74]\n Hello Barbie is an Internet-connected version of the doll that uses a chatbot provided by the company ToyTalk,[75] which previously used the chatbot for a range of smartphone-based characters for children.[76] These characters' behaviors are constrained by a set of rules that in effect emulate a particular character and produce a storyline.[77]\n The My Friend Cayla doll was marketed as a line of 18-inch (46\u00a0cm) dolls which uses speech recognition technology in conjunction with an Android or iOS mobile app to recognize the child's speech and have a conversation. Like the Hello Barbie doll, it attracted controversy due to vulnerabilities with the doll's Bluetooth stack and its use of data collected from the child's speech.\n IBM's Watson computer has been used as the basis for chatbot-based educational toys for companies such as CogniToys,[74] intended to interact with children for educational purposes.[78]\n Malicious chatbots are frequently used to fill chat rooms with spam and advertisements by mimicking human behavior and conversations or to entice people into revealing personal information, such as bank account numbers. They were commonly found on Yahoo! Messenger, Windows Live Messenger, AOL Instant Messenger and other instant messaging protocols. There has also been a published report of a chatbot used in a fake personal ad on a dating service's website.[79]\n Tay, an AI chatbot designed to learn from previous interaction, caused major controversy due to it being targeted by internet trolls on Twitter. Soon after its launch, the bot was exploited, and with its \"repeat after me\" capability, it started releasing racist, sexist, and controversial responses to Twitter users.[80] This suggests that although the bot learned effectively from experience, adequate protection was not put in place to prevent misuse.[81]\n If a text-sending algorithm can pass itself off as a human instead of a chatbot, its message would be more credible. Therefore, human-seeming chatbots with well-crafted online identities could start scattering fake news that seems plausible, for instance making false claims during an election. With enough chatbots, it might be even possible to achieve artificial social proof.[82][83]\n Data security is one of the major concerns of chatbot technologies. Security threats and system vulnerabilities are weaknesses that are often exploited by malicious users. Storage of user data and past communication, that is highly valuable for training and development of chatbots, can also give rise to security threats.[84] Chatbots operating on third-party networks may be subject to various security issues if owners of the third-party applications have policies regarding user data that differ from those of the chatbot.[84] Security threats can be reduced or prevented by incorporating protective mechanisms. User authentication, chat End-to-end encryption, and self-destructing messages are some effective solutions to resist potential security threats.[84]\n Chatbots have difficulty managing non-linear conversations that must go back and forth on a topic with a user.[85]\n Large language models are more versatile, but require a large amount of conversational data to train. These modeles generate new responses word by word based on user input, are usually trained on a large dataset of natural-language phrases.[3] They sometimes provide plausible-sounding but incorrect or nonsensical answers. They can make up names, dates, historical events, and even simple math problems.[86] When large language models produce coherent-sounding but inaccurate or fabricated content, this is referred to as \"hallucinations\". When humans use and apply chatbot content contaminated with hallucinations, this results in \"botshit\".[87] Given the increasing adoption and use of chatbots for generating content, there are concerns that this technology will significantly reduce the cost it takes humans to generate misinformation.[88]\n Chatbots and technology in general used to automate repetitive tasks. But advanced chatbots like ChatGPT are also targeting high-paying, creative, and knowledge-based jobs, raising concerns about workforce disruption and quality trade-offs in favor of cost-cutting.[89]\n Chatbots are increasingly used by small and medium enterprises, to handle customer interactions efficiently, reducing reliance on large call centers and lowering operational costs.[citation needed]\n Prompt engineering, the task of designing and refining prompts (inputs) leading to desired AI-generated responses has quickly gained significant demand with the advent of large language models,[90] although the viability of this job is questioned due to new techniques for automating prompt engineering.[91]\n Generative AI uses a high amount of electric power. Due to reliance on fossil fuels in its generation, this increases air pollution, water pollution, and greenhouse gas emissions. In 2023, a question to ChatGPT consumed on average 10 times as much energy as a Google search.[92] Data centres in general, and those used for AI tasks specifically, consume significant amounts of water for cooling.[93][94]\n",
        "doc_number": 26
    },
    {
        "url": "https://en.wikipedia.org/wiki/Virtual_assistant",
        "content": "\n A virtual assistant (VA) is a software agent that can perform a range of tasks or services for a user based on user input such as commands or questions, including verbal ones. Such technologies often incorporate chatbot capabilities to simulate human conversation, such as via online chat, to facilitate interaction with their users. The interaction may be via text, graphical interface, or voice - as some virtual assistants are able to interpret human speech and respond via synthesized voices.\n In many cases, users can ask their virtual assistants questions, control home automation devices and media playback, and manage other basic tasks such as email, to-do lists, and calendars - all with verbal commands.[1] In recent years, prominent virtual assistants for direct consumer use have included Apple's Siri, Amazon Alexa, Google Assistant, and Samsung's Bixby.[2] Also, companies in various industries often incorporate some kind of virtual assistant technology into their customer service or support.[3]\n Into the 2020s, the emergence of artificial intelligence based chatbots, such as ChatGPT, has brought increased capability and interest to the field of virtual assistant products and services.[4][5][6]\n Radio Rex was the first voice activated toy, patented in 1916[7] and released in 1922.[8] It was a wooden toy in the shape of a dog that would come out of its house when its name is called.\n In 1952, Bell Labs presented \"Audrey\", the Automatic Digit Recognition machine. It occupied a six- foot-high relay rack, consumed substantial power, had streams of cables and exhibited the myriad maintenance problems associated with complex vacuum-tube circuitry. It could recognize the fundamental units of speech, phonemes. It was limited to accurate recognition of digits spoken by designated talkers. It could therefore be used for voice dialing, but in most cases push-button dialing was cheaper and faster, rather than speaking the consecutive digits.[9]\n Another early tool which was enabled to perform digital speech recognition was the IBM Shoebox voice-activated calculator, presented to the general public during the 1962 Seattle World's Fair after its initial market launch in 1961. This early computer, developed almost 20 years before the introduction of the first IBM Personal Computer in 1981, was able to recognize 16 spoken words and the digits 0 to 9.\n The first natural language processing computer program or the chatbot ELIZA was developed by MIT professor Joseph Weizenbaum in the 1960s. It was created to \"demonstrate that the communication between man and machine was superficial\".[10] ELIZA used pattern matching and substitution methodology into scripted responses to simulate conversation, which gave an illusion of understanding on the part of the program.\n Weizenbaum's own secretary reportedly asked Weizenbaum to leave the room so that she and ELIZA could have a real conversation. Weizenbaum was surprised by this, later writing: \"I had not realized ... that extremely short exposures to a relatively simple computer program could induce powerful delusional thinking in quite normal people.[11]\n This gave name to the ELIZA effect, the tendency to unconsciously assume computer behaviors are analogous to human behaviors; that is, anthropomorphisation, a phenomenon present in human interactions with virtual assistants.\n The next milestone in the development of voice recognition technology was achieved in the 1970s at the Carnegie Mellon University in Pittsburgh, Pennsylvania with substantial support of the United States Department of Defense and its DARPA agency, funded five years of a Speech Understanding Research program, aiming to reach a minimum vocabulary of 1,000 words. Companies and academia including IBM, Carnegie Mellon University (CMU) and Stanford Research Institute took part in the program.\n The result was \"Harpy\", it mastered about 1000 words, the vocabulary of a three-year-old and it could understand sentences. It could process speech that followed pre-programmed vocabulary, pronunciation, and grammar structures to determine which sequences of words made sense together, and thus reducing speech recognition errors.\n In 1986, Tangora was an upgrade of the Shoebox, it was a voice recognizing typewriter. Named after the world's fastest typist at the time, it had a vocabulary of 20,000 words and used prediction to decide the most likely result based on what was said in the past. IBM's approach was based on a hidden Markov model, which adds statistics to digital signal processing techniques. The method makes it possible to predict the most likely phonemes to follow a given phoneme. Still each speaker had to individually train the typewriter to recognize his or her voice, and pause between each word.\n In 1983, Gus Searcy invented the \"Butler In A Box\", an electronic voice home controller system.[12]\n In the 1990s, digital speech recognition technology became a feature of the personal computer with IBM, Philips and Lernout & Hauspie fighting for customers. Much later the market launch of the first smartphone IBM Simon in 1994 laid the foundation for smart virtual assistants as we know them today.[citation needed]\n In 1997, Dragon's Naturally Speaking software could recognize and transcribe natural human speech without pauses between each word into a document at a rate of 100 words per minute. A version of Naturally Speaking is still available for download and it is still used today, for instance, by many doctors in the US and the UK to document their medical records.[citation needed]\n In 2001 Colloquis publicly launched  SmarterChild, on platforms like AIM and MSN Messenger. While entirely text-based SmarterChild was able to play games, check the weather, look up facts, and converse with users to an extent.[13]\n The first modern digital virtual assistant installed on a smartphone was Siri, which was introduced as a feature of the iPhone 4S on 4 October 2011.[14] Apple Inc. developed Siri following the 2010 acquisition of Siri Inc., a spin-off of SRI International, which is a research institute financed by DARPA and the United States Department of Defense.[15] Its aim was to aid in tasks such as sending a text message, making phone calls, checking the weather or setting up an alarm. Over time, it has developed to provide restaurant recommendations, search the internet, and provide driving directions.[citation needed]\n In November 2014, Amazon announced Alexa alongside the Echo.[16]\n In April 2017 Amazon released a service for building conversational interfaces for any type of virtual assistant or interface.\n In the 2020s, artificial intelligence (AI) systems like ChatGPT have gained popularity for their ability to generate human-like responses to text-based conversations. In February 2020, Microsoft introduced its Turing Natural Language Generation (T-NLG), which was then the \"largest language model ever published at 17 billion parameters.\"[17] On November 30, 2022, ChatGPT was launched as a prototype and quickly garnered attention for its detailed responses and articulate answers across many domains of knowledge. The advent of ChatGPT and its introduction to the wider public increased interest and competition in the space. In February 2023, Google began introducing an experimental service called \"Bard\" which is based on its LaMDA program to generate text responses to questions asked based on information gathered from the web.\n While ChatGPT and other generalized chatbots based on the latest generative AI are capable of performing various tasks associated with virtual assistants, there are also more specialized forms of such technology that are designed to target more specific situations or needs.[18][4]\n Virtual assistants work via:\n Many virtual assistants are accessible via multiple methods, offering versatility in how users can interact with them, whether through chat, voice commands, or other integrated technologies.\n Virtual assistants use natural language processing (NLP) to match user text or voice input to executable commands. Some continually learn using artificial intelligence techniques including machine learning and ambient intelligence. \n To activate a virtual assistant using the voice, a wake word might be used. This is a word or groups of words such as \"Hey Siri\", \"OK Google\" or \"Hey Google\", \"Alexa\", and \"Hey Microsoft\".[21] As virtual assistants become more popular, there are increasing legal risks involved.[22]:\u200a815\u200a\n Virtual assistants may be integrated into many types of platforms or, like Amazon Alexa, across several of them:\n Virtual assistants can provide a wide variety of services. These include:[30]\n Conversational commerce is e-commerce via various means of messaging, including via voice assistants[33] but also live chat on e-commerce Web sites, live chat on messaging applications such as WeChat, Facebook Messenger and WhatsApp[34] and chatbots on messaging applications or Web sites.\n A virtual assistant can work with customer support team of a business to provide 24x7 support to customers. It provides quick responses, which enhances a customer's experience.\n Amazon enables Alexa \"Skills\" and Google \"Actions\", essentially applications that run on the assistant platforms.\n Virtual assistants have a variety of privacy concerns associated with them. Features such as activation by voice pose a threat, as such features requires the device to always be listening.[35] Modes of privacy such as the virtual security button have been proposed to create a multilayer authentication for virtual assistants.[36]\n The privacy policy of Google Assistant states that it does not store the audio data without the user's permission, but may store the conversation transcripts to personalise its experience. Personalisation can be turned off in settings. If a user wants Google Assistant to store audio data, they can go to Voice & Audio Activity (VAA) and turn on this feature. Audio files are sent to the cloud and used by Google to improve the performance of Google Assistant, but only if the VAA feature is turned on.[37]\n The privacy policy of Amazon's virtual assistant, Alexa, states that it only listens to conversations when its wake word (like Alexa, Amazon, Echo) is used. It starts recording the conversation after the call of a wake word, and stops recording after 8 seconds of silence. It sends the recorded conversation to the cloud. It is possible to delete the recording from the cloud by visiting 'Alexa Privacy' in 'Alexa'.[38]\n Apple states that it does not record audio to improve Siri. Instead, it claims to use transcripts. Transcript data is only sent if it is deemed important for analysis. Users can opt out anytime if they don't want Siri to send the transcripts in the cloud.[39]\n Cortana is a voice-only virtual assistant with singular authentication.[40][41][42] This voice-activated device accesses user data to perform common tasks like checking weather or making calls, raising privacy concerns due to the lack of secondary authentication.[43][44]\n Added value of the virtual assistants can come among others from the following:\n In 2019 Antonio A. Casilli, a French sociologist, criticized artificial intelligence and virtual assistants in particular in the following way:\n At a first level the fact that the consumer provides free data for the training and improvement of the virtual assistant, often without knowing it, is ethically disturbing.\n But at a second level, it might be even more ethically disturbing to know how these AIs are trained with this data.\n This artificial intelligence is trained via neural networks, which require a huge amount of labelled data. However, this data needs to be labelled through a human process, which explains the rise of microwork in the last decade. That is, remotely using some people worldwide doing some repetitive and very simple tasks for a few cents, such as listening to virtual assistant speech data, and writing down what was said. Microwork has been criticized for the job insecurity it causes, and for the total lack of regulation: The average salary was 1,38 dollar/hour in 2010,[50] and it provides neither healthcare nor retirement benefits, sick pay, minimum wage. Hence, virtual assistants and their designers are controversial for spurring job insecurity, and the AIs they propose are still human in the way that they would be impossible without the microwork of millions of human workers.[49]\n Privacy concerns are raised by the fact that voice commands are available to the providers of virtual assistants in unencrypted form, and can thus be shared with third parties and be processed in an unauthorized or unexpected manner.[51] Additionally to the linguistic content of recorded speech, a user's manner of expression and voice characteristics can implicitly contain information about his or her biometric identity, personality traits, body shape, physical and mental health condition, sex, gender, moods and emotions, socioeconomic status and geographical origin.[52]\n Notable developer platforms for virtual assistants include:\n In previous generations of text chat-based virtual assistants, the assistant was often represented by an avatar (a.k.a. interactive online character or automated character) \u2014 this was known as an embodied agent.\n Digital experiences enabled by virtual assistants are considered to be among the major recent technological advances and most promising consumer trends. Experts claim that digital experiences will achieve a status-weight comparable to 'real' experiences, if not become more sought-after and prized.[57] The trend is verified by a high number of frequent users and the substantial growth of worldwide user numbers of virtual digital assistants. In mid-2017, the number of frequent users of digital virtual assistants is estimated to be around 1 bn worldwide.[58] In addition, it can be observed that virtual digital assistant technology is no longer restricted to smartphone applications, but present across many industry sectors (incl. automotive, telecommunications, retail, healthcare and education).[59]\nIn response to the significant R&D expenses of firms across all sectors and an increasing implementation of mobile devices, the market for speech recognition technology is predicted to grow at a CAGR of 34.9% globally over the period of 2016 to 2024 and thereby surpass a global market size of US$7.5 billion by 2024.[59] According to an Ovum study, the \"native digital assistant installed base\" is projected to exceed the world's population by 2021, with 7.5 billion active voice AI\u2013capable devices.[60] According to Ovum, by that time \"Google Assistant will dominate the voice AI\u2013capable device market with 23.3% market share, followed by Samsung's Bixby (14.5%), Apple's Siri (13.1%), Amazon's Alexa (3.9%), and Microsoft's Cortana (2.3%).\"[60]\n Taking into consideration the regional distribution of market leaders, North American companies (e.g. Nuance Communications, IBM, eGain) are expected to dominate the industry over the next years, due to the significant impact of BYOD (Bring Your Own Device) and enterprise mobility business models. Furthermore, the increasing demand for smartphone-assisted platforms are expected to further boost the North American intelligent virtual assistant (IVA) industry growth. Despite its smaller size in comparison to the North American market, the intelligent virtual assistant industry from the Asia-Pacific region, with its main players located in India and China is predicted to grow at an annual growth rate of 40% (above global average) over the 2016\u20132024 period.[59]\n Virtual assistants should not be only seen as a gadget for individuals, as they could have a real economic utility for enterprises. As an example, a virtual assistant can take the role of an always available assistant with an encyclopedic knowledge. And which can organize meetings, check inventories, verify informations. Virtual assistants are all the more important that their integration in small and middle-sized enterprises often consists in an easy first step through the more global adaptation and use of Internet of Things (IoT). Indeed, IoT technologies are first perceived by small and medium-sized enterprises as technologies of critical importance, but too complicated, risky or costly to be used.[61]\n In May 2018, researchers from the University of California, Berkeley, published a paper that showed audio commands undetectable for the human ear could be directly embedded into music or spoken text, thereby manipulating virtual assistants into performing certain actions without the user taking note of it.[62] The researchers made small changes to audio files, which cancelled out the sound patterns that speech recognition systems are meant to detect. These were replaced with sounds that would be interpreted differently by the system and command it to dial phone numbers, open websites or even transfer money.[62] The possibility of this has been known since 2016,[62] and affects devices from Apple, Amazon and Google.[63]\n In addition to unintentional actions and voice recording, another security and privacy risk associated with intelligent virtual assistants is malicious voice commands: An attacker who impersonates a user and issues malicious voice commands to, for example, unlock a smart door to gain unauthorized entry to a home or garage or order items online without the user's knowledge. Although some IVAs provide a voice-training feature to prevent such impersonation, it can be difficult for the system to distinguish between similar voices. Thus, a malicious person who is able to access an IVA-enabled device might be able to fool the system into thinking that they are the real owner and carry out criminal or mischievous acts.[64]\n",
        "doc_number": 27
    },
    {
        "url": "https://en.wikipedia.org/wiki/Expert_system",
        "content": "In artificial intelligence (AI), an expert system is a computer system emulating the decision-making ability of a human expert.[1]\nExpert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if\u2013then rules rather than through conventional procedural programming code.[2]  Expert systems were among the first truly successful forms of AI software.[3][4][5][6][7] They were created in the 1970s and then proliferated in the 1980s,[8] being then widely regarded as the future of AI \u2014 before the advent of successful artificial neural networks.[9]\nAn expert system is divided into two subsystems: 1) a knowledge base, which represents facts and rules; and 2) an inference engine, which applies the rules to the known facts to deduce new facts, and can include explaining and debugging abilities.\n Soon after the dawn of modern computers in the late 1940s and early 1950s, researchers started realizing the immense potential these machines had for modern society. One of the first challenges was to make such machines able to \u201cthink\u201d like humans \u2013 in particular, making these machines able to make important decisions the way humans do. The medical\u2013healthcare field presented the tantalizing challenge of enabling these machines to make medical diagnostic decisions.[10]\n Thus, in the late 1950s, right after the information age had fully arrived, researchers started experimenting with the prospect of using computer technology to emulate human decision making. For example, biomedical researchers started creating computer-aided systems for diagnostic applications in medicine and biology. These early diagnostic systems used patients\u2019 symptoms and laboratory test results as inputs to generate a diagnostic outcome.[11][12]\nThese systems were often described as the early forms of expert systems. However, researchers realized that there were significant limits when using traditional methods such as flow charts,[13]\n[14] statistical pattern matching,[15] or probability theory.[16][17]\n This previous situation gradually led to the development of expert systems, which used knowledge-based approaches. These expert systems in medicine were the MYCIN expert system,[18] the Internist-I expert system[19] and later, in the middle of the 1980s, the CADUCEUS.[20]\n Expert systems were formally introduced around 1965 by the Stanford Heuristic Programming Project led by Edward Feigenbaum, who is sometimes termed the \"father of expert systems\";[21] other key early contributors were Bruce Buchanan and Randall Davis. The Stanford researchers tried to identify domains where expertise was highly valued and complex, such as diagnosing infectious diseases (Mycin) and identifying unknown organic molecules (Dendral).[22] The idea that \"intelligent systems derive their power from the knowledge they possess rather than from the specific formalisms and inference schemes they use\"[23] \u2013 as Feigenbaum said \u2013 was at the time a significant step forward, since the past research had been focused on heuristic computational methods, culminating in attempts to develop very general-purpose problem solvers (foremostly the conjunct work of Allen Newell and Herbert Simon).[24] Expert systems became some of the first truly successful forms of artificial intelligence (AI) software.[3][4][5][6][7]\n Research on expert systems was also active in Europe. In the US, the focus tended to be on the use of production rule systems, first on systems hard coded on top of Lisp programming environments and then on expert system shells developed by vendors such as Intellicorp. In Europe, research focused more on systems and expert systems shells developed in Prolog. The advantage of Prolog systems was that they employed a form of rule-based programming that was based on formal logic.[25][26]\n One such early expert system shell based on Prolog was APES.[27]\nOne of the first use cases of Prolog and APES was in the legal area namely, the encoding of a large portion of the British Nationality Act. Lance Elliot wrote: \"The British Nationality Act was passed in 1981 and shortly thereafter was used as a means of showcasing the efficacy of using Artificial Intelligence (AI) techniques and technologies, doing so to explore how the at-the-time newly enacted statutory law might be encoded into a computerized logic-based formalization. A now oft-cited research paper entitled \u201cThe British Nationality Act as a Logic Program\u201d was published in 1986 and subsequently became a hallmark for subsequent work in AI and the law.\"[28][29]\n In the 1980s, expert systems proliferated. Universities offered expert system courses and two-thirds of the Fortune 500 companies applied the technology in daily business activities.[8][30] Interest was international with the Fifth Generation Computer Systems project in Japan and increased research funding in Europe.\n In 1981, the first IBM PC, with the PC DOS operating system, was introduced.[31] The imbalance between the high affordability of the relatively powerful chips in the PC, compared to the much more expensive cost of processing power in the mainframes that dominated the corporate IT world at the time, created a new type of architecture for corporate computing, termed the client\u2013server model.[32] Calculations and reasoning could be performed at a fraction of the price of a mainframe using a PC. This model also enabled business units to bypass corporate IT departments and directly build their own applications. As a result, client-server had a tremendous impact on the expert systems market. Expert systems were already outliers in much of the business world, requiring new skills that many IT departments did not have and were not eager to develop. They were a natural fit for new PC-based shells that promised to put application development into the hands of end users and experts. Until then, the main development environment for expert systems had been high end Lisp machines from Xerox, Symbolics, and Texas Instruments. With the rise of the PC and client-server computing, vendors such as Intellicorp and Inference Corporation shifted their priorities to developing PC-based tools. Also, new vendors, often financed by venture capital (such as Aion Corporation, Neuron Data, Exsys, VP-Expert, and many others[33][34]), started appearing regularly.\n The first expert system to be used in a design capacity for a large-scale product was the Synthesis of Integral Design (SID) software program, developed in 1982. Written in Lisp, SID generated 93% of the VAX 9000 CPU logic gates.[35] Input to the software was a set of rules created by several expert logic designers. SID expanded the rules and generated software logic synthesis routines many times the size of the rules themselves. Surprisingly, the combination of these rules resulted in an overall design that exceeded the capabilities of the experts themselves, and in many cases out-performed the human counterparts. While some rules contradicted others, top-level control parameters for speed and area provided the tie-breaker. The program was highly controversial but used nevertheless due to project budget constraints. It was terminated by logic designers after the VAX 9000 project completion.\n During the years before the middle of the 1970s, the expectations of what expert systems can accomplish in many fields tended to be extremely optimistic. At the start of these early studies, researchers were hoping to develop entirely automatic (i.e., completely computerized) expert systems. The expectations of people of what computers can do were frequently too idealistic. This situation radically changed after Richard M. Karp published his breakthrough paper: \u201cReducibility among Combinatorial Problems\u201d in the early 1970s.[36] Thanks to Karp's work, together with other scholars, like Hubert L. Dreyfus,[37] it became clear that there are certain limits and possibilities when one designs computer algorithms. His findings describe what computers can do and what they cannot do. Many of the computational problems related to this type of expert systems have certain pragmatic limits. These findings laid down the groundwork that led to the next developments in the field.[10]\n In the 1990s and beyond, the term expert system and the idea of a standalone AI system mostly dropped from the IT lexicon. There are two interpretations of this. One is that \"expert systems failed\": the IT world moved on because expert systems did not deliver on their over hyped promise.[38][39] The other is the mirror opposite, that expert systems were simply victims of their success: as IT professionals grasped concepts such as rule engines, such tools migrated from being standalone tools for developing special purpose expert systems, to being one of many standard tools.[40] Other researchers suggest that Expert Systems caused inter-company power struggles when the IT organization lost its exclusivity in software modifications to users or Knowledge Engineers.[41]\n In the first decade of the 2000s, there was a \"resurrection\" for the technology, while using the term rule-based systems, with significant success stories and adoption.[42] Many of the leading major business application suite vendors (such as SAP, Siebel, and Oracle) integrated expert system abilities into their suite of products as a way to specify business logic. Rule engines are no longer simply for defining the rules an expert would use but for any type of complex, volatile, and critical business logic; they often go hand in hand with business process automation and integration environments.[43][44][45]\n The limits of prior type of expert systems prompted researchers to develop new types of approaches. They have developed more efficient, flexible, and powerful methods to simulate the human decision-making process. Some of the approaches that researchers have developed are based on new methods of artificial intelligence (AI), and in particular in machine learning and data mining approaches with a feedback mechanism.[46][failed verification] Recurrent neural networks often take advantage of such mechanisms. Related is the discussion on the disadvantages section.\n Modern systems can incorporate new knowledge more easily and thus update themselves easily. Such systems can generalize from existing knowledge better and deal with vast amounts of complex data. Related is the subject of big data here. Sometimes these type of expert systems are called \"intelligent systems.\"[10]\n More recently, it can be argued that expert systems have moved into the area of business rules and business rules management systems.\n An expert system is an example of a knowledge-based system. Expert systems were the first commercial systems to use a knowledge-based architecture. In general view, an expert system includes the following components: a knowledge base, an inference engine, an explanation facility, a knowledge acquisition facility, and a user interface.[48][49]\n The knowledge base represents facts about the world. In early expert systems such as Mycin and Dendral, these facts were represented mainly as flat assertions about variables. In later expert systems developed with commercial shells, the knowledge base took on more structure and used concepts from object-oriented programming. The world was represented as classes, subclasses, and instances and assertions were replaced by values of object instances. The rules worked by querying and asserting values of the objects.\n The inference engine is an automated reasoning system that evaluates the current state of the knowledge-base, applies relevant rules, and then asserts new knowledge into the knowledge base. The inference engine may also include abilities for explanation, so that it can explain to a user the chain of reasoning used to arrive at a particular conclusion by tracing back over the firing of rules that resulted in the assertion.[50]\n There are mainly two modes for an inference engine: forward chaining and backward chaining. The different approaches are dictated by whether the inference engine is being driven by the antecedent (left hand side) or the consequent (right hand side) of the rule. In forward chaining an antecedent fires and asserts the consequent. For example, consider the following rule:\n \n\n\n\nR\n1\n:\n\n\nM\na\nn\n\n\n(\nx\n)\n\n\u27f9\n\n\n\nM\no\nr\nt\na\nl\n\n\n(\nx\n)\n\n\n{\\displaystyle R1:{\\mathit {Man}}(x)\\implies {\\mathit {Mortal}}(x)}\n\n\n A simple example of forward chaining would be to assert Man(Socrates) to the system and then trigger the inference engine. It would match R1 and assert Mortal(Socrates) into the knowledge base.\n Backward chaining is a bit less straight forward. In backward chaining the system looks at possible conclusions and works backward to see if they might be true. So if the system was trying to determine if Mortal(Socrates) is true it would find R1 and query the knowledge base to see if Man(Socrates) is true. One of the early innovations of expert systems shells was to integrate inference engines with a user interface. This could be especially powerful with backward chaining. If the system needs to know a particular fact but does not, then it can simply generate an input screen and ask the user if the information is known. So in this example, it could use R1 to ask the user if Socrates was a Man and then use that new information accordingly.\n The use of rules to explicitly represent knowledge also enabled explanation abilities. In the simple example above if the system had used R1 to assert that Socrates was Mortal and a user wished to understand why Socrates was mortal they could query the system and the system would look back at the rules which fired to cause the assertion and present those rules to the user as an explanation. In English, if the user asked \"Why is Socrates Mortal?\" the system would reply \"Because all men are mortal and Socrates is a man\". A significant area for research was the generation of explanations from the knowledge base in natural English rather than simply by showing the more formal but less intuitive rules.[51]\n As expert systems evolved, many new techniques were incorporated into various types of inference engines.[52] Some of the most important of these were:\n The goal of knowledge-based systems is to make the critical information required for the system to work explicit rather than implicit.[55] In a traditional computer program, the logic is embedded in code that can typically only be reviewed by an IT specialist. With an expert system, the goal was to specify the rules in a format that was intuitive and easily understood, reviewed, and even edited by domain experts rather than IT experts. The benefits of this explicit knowledge representation were rapid development and ease of maintenance.\n Ease of maintenance is the most obvious benefit. This was achieved in two ways. First, by removing the need to write conventional code, many of the normal problems that can be caused by even small changes to a system could be avoided with expert systems. Essentially, the logical flow of the program (at least at the highest level) was simply a given for the system, simply invoke the inference engine. This also was a reason for the second benefit: rapid prototyping. With an expert system shell it was possible to enter a few rules and have a prototype developed in days rather than the months or year typically associated with complex IT projects.\n A claim for expert system shells that was often made was that they removed the need for trained programmers and that experts could develop systems themselves. In reality, this was seldom if ever true. While the rules for an expert system were more comprehensible than typical computer code, they still had a formal syntax where a misplaced comma or other character could cause havoc as with any other computer language. Also, as expert systems moved from prototypes in the lab to deployment in the business world, issues of integration and maintenance became far more critical. Inevitably demands to integrate with, and take advantage of, large legacy databases and systems arose. To accomplish this, integration required the same skills as any other type of system.[56]\n Summing up the benefits of using expert systems, the following can be highlighted:[48]\n The most common disadvantage cited for expert systems in the academic literature is the knowledge acquisition problem. Obtaining the time of domain experts for any software application is always difficult, but for expert systems it was especially difficult because the experts were by definition highly valued and in constant demand by the organization. As a result of this problem, a great deal of research in the later years of expert systems was focused on tools for knowledge acquisition, to help automate the process of designing, debugging, and maintaining rules defined by experts. However, when looking at the life-cycle of expert systems in actual use, other problems \u2013 essentially the same problems as those of any other large system \u2013 seem at least as critical as knowledge acquisition: integration, access to large databases, and performance.[57][58]\n Performance could be especially problematic because early expert systems were built using tools (such as earlier Lisp versions) that interpreted code expressions without first compiling them. This provided a powerful development environment, but with the drawback that it was virtually impossible to match the efficiency of the fastest compiled languages (such as C). System and database integration were difficult for early expert systems because the tools were mostly in languages and platforms that were neither familiar to nor welcome in most corporate IT environments \u2013 programming languages such as Lisp and Prolog, and hardware platforms such as Lisp machines and personal computers. As a result, much effort in the later stages of expert system tool development was focused on integrating with legacy environments such as COBOL and large database systems, and on porting to more standard platforms. These issues were resolved mainly by the client\u2013server paradigm shift, as PCs were gradually accepted in the IT environment as a legitimate platform for serious business system development and as affordable minicomputer servers provided the processing power needed for AI applications.[56]\n Another major challenge of expert systems emerges when the size of the knowledge base increases. This causes the processing complexity to increase. For instance, when an expert system with 100 million rules was envisioned as the ultimate expert system, it became obvious that such system would be too complex and it would face too many computational problems.[59] An inference engine would have to be able to process huge numbers of rules to reach a decision.\n How to verify that decision rules are consistent with each other is also a challenge when there are too many rules. Usually such problem leads to a satisfiability (SAT) formulation.[60] This is a well-known NP-complete problem Boolean satisfiability problem. If we assume only binary variables, say n of them, and then the corresponding search space is of size 2\n\n\n\n\n\n\nn\n\n\n\n\n{\\displaystyle ^{n}}\n\n. Thus, the search space can grow exponentially.\n There are also questions on how to prioritize the use of the rules to operate more efficiently, or how to resolve ambiguities (for instance, if there are too many else-if sub-structures within one rule) and so on.[61]\n Other problems are related to the overfitting and overgeneralization effects when using known facts and trying to generalize to other cases not described explicitly in the knowledge base. Such problems exist with methods that employ machine learning approaches too.[62][63]\n Another problem related to the knowledge base is how to make updates of its knowledge quickly and effectively.[64][65][66] Also how to add a new piece of knowledge (i.e., where to add it among many rules) is challenging. Modern approaches that rely on machine learning methods are easier in this regard.[citation needed]\n Because of the above challenges, it became clear that new approaches to AI were required instead of rule-based technologies. These new approaches are based on the use of machine learning techniques, along with the use of feedback mechanisms.[10]\n The key challenges that expert systems in medicine (if one considers computer-aided diagnostic systems as modern expert systems), and perhaps in other application domains, include issues related to aspects such as: big data, existing regulations, healthcare practice, various algorithmic issues, and system assessment.[67]\n Finally, the following disadvantages of using expert systems can be summarized:[48]\n Hayes-Roth divides expert systems applications into 10 categories illustrated in the following table. The example applications were not in the original Hayes-Roth table, and some of them arose well afterward. Any application that is not footnoted is described in the Hayes-Roth book.[50] Also, while these categories provide an intuitive framework to describe the space of expert systems applications, they are not rigid categories, and in some cases an application may show traits of more than one category.\n Hearsay was an early attempt at solving voice recognition through an expert systems approach. For the most part this category of expert systems was not all that successful. Hearsay and all interpretation systems are essentially pattern recognition systems\u2014looking for patterns in noisy data. In the case of Hearsay recognizing phonemes in an audio stream. Other early examples were analyzing sonar data to detect Russian submarines. These kinds of systems proved much more amenable to a neural network AI solution than a rule-based approach.\n CADUCEUS and MYCIN were medical diagnosis systems. The user describes their symptoms to the computer as they would to a doctor and the computer returns a medical diagnosis.\n Dendral was a tool to study hypothesis formation in the identification of organic molecules. The general problem it solved\u2014designing a solution given a set of constraints\u2014was one of the most successful areas for early expert systems applied to business domains such as salespeople configuring Digital Equipment Corporation (DEC) VAX computers and mortgage loan application development.\n SMH.PAL is an expert system for the assessment of students with multiple disabilities.[77]\n GARVAN-ES1 was a medical expert system, developed at the Garvan Institute of Medical Research, that provided automated clinical diagnostic comments on endocrine reports from a pathology laboratory. It was one of the first medical expert systems to go into routine clinical use internationally[73] and the first expert system to be used for diagnosis daily in Australia.[83] The system was written in \"C\" and ran on a PDP-11 in 64K of memory. It had 661 rules that were compiled; not interpreted.\n \nMistral[69] is an expert system to monitor dam safety, developed in the 1990s by Ismes (Italy). It gets data from an automatic monitoring system and performs a diagnosis of the state of the dam. Its first copy, installed in 1992 on the Ridracoli Dam (Italy), is still operational 24/7/365. It has been installed on several dams in Italy and abroad (e.g., Itaipu Dam in Brazil), and on landslide sites under the name of Eydenet,[70] and on monuments under the name of Kaleidos.[71] Mistral is a registered trade mark of CESI.",
        "doc_number": 28
    },
    {
        "url": "https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning",
        "content": "Knowledge representation and reasoning (KRR, KR&R, or KR\u00b2) is a field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can use to solve complex tasks, such as diagnosing a medical condition or having a natural-language dialog. Knowledge representation incorporates findings from psychology[1] about how humans solve problems and represent knowledge, in order to design formalisms that make complex systems easier to design and build. Knowledge representation and reasoning also incorporates findings from logic to automate various kinds of reasoning.\n Examples of knowledge representation formalisms include semantic networks, frames, rules, logic programs, and ontologies. Examples of automated reasoning engines include inference engines, theorem provers, model generators, and classifiers.\n The earliest work in computerized knowledge representation was focused on general problem-solvers such as the General Problem Solver (GPS) system developed by Allen Newell and Herbert A. Simon in 1959 and the Advice Taker proposed by John McCarthy also in 1959. GPS featured data structures for planning and decomposition. The system would begin with a goal. It would then decompose that goal into sub-goals and then set out to construct strategies that could accomplish each subgoal. The Advisor Taker, on the other hand, proposed the use of the predicate calculus to represent common sense reasoning.\n Many of the early approaches to knowledge represention in Artificial Intelligence (AI) used graph representations and semantic networks, similar to knowledge graphs today. In such approaches, problem solving was a form of graph traversal[2] or path-finding, as in the A* search algorithm. Typical applications included robot plan-formation and game-playing.\n Other researchers focused on developing  automated theorem-provers for first-order logic, motivated by the use of mathematical logic to formalise mathematics and to automate the proof of mathematical theorems. A major step in this direction was the development of the resolution method by John Alan Robinson.\n In the meanwhile, John McCarthy and Pat Hayes developed the situation calculus as a logical representation of common sense knowledge about the laws of cause and effect. Cordell Green, in turn, showed how to do robot plan-formation by applying resolution to the situation calculus. He also showed how to use resolution for question-answering and automatic programming.[3]\n In contrast, researchers at Massachusetts Institute of Technology (MIT) rejected the resolution uniform proof procedure paradigm and advocated the procedural embedding of knowledge instead.[4] The resulting conflict between the use of logical representations and the use of procedural representations was resolved in the early 1970s with the development of logic programming and Prolog, using SLD resolution to treat Horn clauses as goal-reduction procedures.\n The early development of logic programming was largely a European phenomenon. In North America, AI researchers such as Ed Feigenbaum and Frederick Hayes-Roth advocated the representation of domain-specific knowledge rather than general-purpose reasoning.[5]\n These efforts led to the cognitive revolution in psychology and to the phase of AI focused on knowledge representation that resulted in expert systems in the 1970s and 80s, production systems, frame languages, etc. Rather than general problem solvers, AI changed its focus to expert systems that could match human competence on a specific task, such as medical diagnosis.[6]\n Expert systems gave us the terminology still in use today where AI systems are divided into a knowledge base, which includes facts and rules about a problem domain, and an inference engine, which applies the knowledge in the knowledge base to answer questions and solve problems in the domain. In these early systems the facts in the knowledge base tended to be a fairly flat structure, essentially assertions about the values of variables used by the rules.[7]\n Meanwhile, Marvin Minsky developed the concept of frame in the mid-1970s.[8] A frame is similar to an object class: It is an abstract description of a category describing things in the world, problems, and potential solutions. Frames were originally used on systems geared toward human interaction, e.g. understanding natural language and the social settings in which various default expectations such as ordering food in a restaurant narrow the search space and allow the system to choose appropriate responses to dynamic situations.\n It was not long before the frame communities and the rule-based researchers realized that there was a synergy between their approaches. Frames were good for representing the real world, described as classes, subclasses, slots (data values) with various constraints on possible values. Rules were good for representing and utilizing complex logic such as the process to make a medical diagnosis. Integrated systems were developed that combined frames and rules. One of the most powerful and well known was the 1983 Knowledge Engineering Environment (KEE) from Intellicorp. KEE had a complete rule engine with forward and backward chaining. It also had a complete frame-based knowledge base with triggers, slots (data values), inheritance, and message passing. Although message passing originated in the object-oriented community rather than AI it was quickly embraced by AI researchers as well in environments such as KEE and in the operating systems for Lisp machines from Symbolics, Xerox, and Texas Instruments.[9]\n The integration of frames, rules, and object-oriented programming was significantly driven by commercial ventures such as KEE and Symbolics spun off from various research projects. At the same time, there was another strain of research that was less commercially focused and was driven by mathematical logic and automated theorem proving.[citation needed] One of the most influential languages in this research was the KL-ONE language of the mid-'80s. KL-ONE was a frame language that had a rigorous semantics, formal definitions for concepts such as an Is-A relation.[10] KL-ONE and languages that were influenced by it such as Loom had an automated reasoning engine that was based on formal logic rather than on IF-THEN rules. This reasoner is called the classifier. A classifier can analyze a set of declarations and infer new assertions, for example, redefine a class to be a subclass or superclass of some other class that wasn't formally specified. In this way the classifier can function as an inference engine, deducing new facts from an existing knowledge base. The classifier can also provide consistency checking on a knowledge base (which in the case of KL-ONE languages is also referred to as an Ontology).[11]\n Another area of knowledge representation research was the problem of common-sense reasoning. One of the first realizations learned from trying to make software that can function with human natural language was that humans regularly draw on an extensive foundation of knowledge about the real world that we simply take for granted but that is not at all obvious to an artificial agent, such as basic principles of common-sense physics, causality, intentions, etc. An example is the frame problem, that in an event driven logic there need to be axioms that state things maintain position from one moment to the next unless they are moved by some external force. In order to make a true artificial intelligence agent that can converse with humans using natural language and can process basic statements and questions about the world, it is essential to represent this kind of knowledge.[12] In addition to McCarthy and Hayes' situation calculus, one of the most ambitious programs to tackle this problem was Doug Lenat's Cyc project. Cyc established its own Frame language and had large numbers of analysts document various areas of common-sense reasoning in that language. The knowledge recorded in Cyc included common-sense models of time, causality, physics, intentions, and many others.[13]\n The starting point for knowledge representation is the knowledge representation hypothesis first formalized by Brian C. Smith in 1985:[14]\n Any mechanically embodied intelligent process will be comprised of structural ingredients that a) we as external observers naturally take to represent a propositional account of the knowledge that the overall process exhibits, and b) independent of such external semantic attribution, play a formal but causal and essential role in engendering the behavior that manifests that knowledge. One of the most active areas of knowledge representation research is the Semantic Web.[citation needed] The Semantic Web seeks to add a layer of semantics (meaning) on top of the current Internet. Rather than indexing web sites and pages via keywords, the Semantic Web creates large ontologies of concepts. Searching for a concept will be more effective than traditional text only searches. Frame languages and automatic classification play a big part in the vision for the future Semantic Web. The automatic classification gives developers technology to provide order on a constantly evolving network of knowledge. Defining ontologies that are static and incapable of evolving on the fly would be very limiting for Internet-based systems. The classifier technology provides the ability to deal with the dynamic environment of the Internet.\n Recent projects funded primarily by the Defense Advanced Research Projects Agency (DARPA) have integrated frame languages and classifiers with markup languages based on XML. The Resource Description Framework (RDF) provides the basic capability to define classes, subclasses, and properties of objects. The Web Ontology Language (OWL) provides additional levels of semantics and enables integration with classification engines.[15][16]\n Knowledge-representation is a field of artificial intelligence that focuses on designing computer representations that capture information about the world that can be used for solving complex problems.\n The justification for knowledge representation is that conventional procedural code is not the best formalism to use to solve complex problems. Knowledge representation makes complex software easier to define and maintain than procedural code and can be used in expert systems.\n For example, talking to experts in terms of business rules rather than code lessens the semantic gap between users and developers and makes development of complex systems more practical.\n Knowledge representation goes hand in hand with automated reasoning because one of the main purposes of explicitly representing knowledge is to be able to reason about that knowledge, to make inferences, assert new knowledge, etc. Virtually all knowledge representation languages have a reasoning or inference engine as part of the system.[17]\n A key trade-off in the design of knowledge representation formalisms is that between expressivity and tractability.[18] First Order Logic (FOL), with its high expressive power and ability to formalise much of mathematics, is a standard for comparing the expressibility of  knowledge representation languages.\n Arguably, FOL has two drawbacks as a knowledge representation formalism in its own right, namely ease of use and efficiency of implementation. Firstly, because of its high expressive power, FOL allows many ways of expressing the same information, and this can make it hard for users to formalise or even to understand knowledge expressed in complex, mathematically-oriented ways. Secondly, because of its complex proof procedures, it can be difficult for users to understand complex proofs and explanations, and it can be hard for implementations to be efficient. As a consequence, unrestricted FOL can be intimidating for many software developers.\n One of the key discoveries of AI research in the 1970s was that languages that do not have the full expressive power of FOL can still provide close to the same expressive power of FOL, but can be easier for both the average developer and for the computer to understand. Many of the early AI knowledge representation formalisms, from databases to semantic nets to production systems, can be viewed as making various design decisions about how to balance expressive power with naturalness of expression and efficiency.[19] In particular, this balancing act was a driving motivation for the development of IF-THEN rules in rule-based expert systems. \n A similar balancing act was also a motivation for the development of  logic programming (LP) and the logic programming language Prolog. Logic programs have a rule-based syntax, which is easily confused with the IF-THEN syntax of production rules. But logic programs have a well-defined logical semantics, whereas production systems do not.\n The earliest form of logic programming was based on the Horn clause subset of FOL. But later extensions of LP included the negation as failure inference rule, which turns LP into a non-monotonic logic for default reasoning. The resulting extended semantics of LP is a variation of the standard semantics of Horn clauses and FOL, and is a form of database semantics, [20] which includes the unique name assumption and a form of closed world assumption. These assumptions are much harder to state and reason with explicitly using the standard semantics of FOL.\n In a key 1993 paper on the topic, Randall Davis of MIT outlined five distinct roles to analyze a knowledge representation framework:[21]\n Knowledge representation and reasoning are a key enabling technology for the Semantic Web. Languages based on the Frame model with automatic classification provide a layer of semantics on top of the existing Internet. Rather than searching via text strings as is typical today, it will be possible to define logical queries and find pages that map to those queries.[15] The automated reasoning component in these systems is an engine known as the classifier. Classifiers focus on the subsumption relations in a knowledge base rather than rules. A classifier can infer new classes and dynamically change the ontology as new information becomes available. This capability is ideal for the ever-changing and evolving information space of the Internet.[22]\n The Semantic Web integrates concepts from knowledge representation and reasoning with markup languages based on XML.  The Resource Description Framework (RDF) provides the basic capabilities to define knowledge-based objects on the Internet with basic features such as Is-A relations and object properties. The Web Ontology Language (OWL) adds additional semantics and integrates with automatic classification reasoners.[16]\n In 1985, Ron Brachman categorized the core issues for knowledge representation as follows:[23]\n In the early years of knowledge-based systems the knowledge-bases were fairly small. The knowledge-bases that were meant to actually solve real problems rather than do proof of concept demonstrations needed to focus on well defined problems. So for example, not just medical diagnosis as a whole topic, but medical diagnosis of certain kinds of diseases.\n As knowledge-based technology scaled up, the need for larger knowledge bases and for modular knowledge bases that could communicate and integrate with each other became apparent. This gave rise to the discipline of ontology engineering, designing and building large knowledge bases that could be used by multiple projects. One of the leading research projects in this area was the Cyc project. Cyc was an attempt to build a huge encyclopedic knowledge base that would contain not just expert knowledge but common-sense knowledge. In designing an artificial intelligence agent, it was soon realized that representing common-sense knowledge, knowledge that humans simply take for granted, was essential to make an AI that could interact with humans using natural language. Cyc was meant to address this problem. The language they defined was known as CycL.\n After CycL, a number of ontology languages have been developed. Most are declarative languages, and are either frame languages, or are based on first-order logic. Modularity\u2014the ability to define boundaries around specific domains and problem spaces\u2014is essential for these languages because as stated by Tom Gruber, \"Every ontology is a treaty\u2013a social agreement among people with common motive in sharing.\" There are always many competing and differing views that make any general-purpose ontology impossible. A general-purpose ontology would have to be applicable in any domain and different areas of knowledge need to be unified.[27]\n There is a long history of work attempting to build ontologies for a variety of task domains, e.g., an ontology for liquids,[28] the lumped element model widely used in representing electronic circuits (e.g.[29]), as well as ontologies for time, belief, and even programming itself. Each of these offers a way to see some part of the world.\n The lumped element model, for instance, suggests that we think of circuits in terms of components with connections between them, with signals flowing instantaneously along the connections. This is a useful view, but not the only possible one. A different ontology arises if we need to attend to the electrodynamics in the device: Here signals propagate at finite speed and an object (like a resistor) that was previously viewed as a single component with an I/O behavior may now have to be thought of as an extended medium through which an electromagnetic wave flows.\n Ontologies can of course be written down in a wide variety of languages and notations (e.g., logic, LISP, etc.); the essential information is not the form of that language but the content, i.e., the set of concepts offered as a way of thinking about the world. Simply put, the important part is notions like connections and components, not the choice between writing them as predicates or LISP constructs.\n The commitment made selecting one or another ontology can produce a sharply different view of the task at hand. Consider the difference that arises in selecting the lumped element view of a circuit rather than the electrodynamic view of the same device. As a second example, medical diagnosis viewed in terms of rules (e.g., MYCIN) looks substantially different from the same task viewed in terms of frames (e.g., INTERNIST). Where MYCIN sees the medical world as made up of empirical associations connecting symptom to disease, INTERNIST sees a set of prototypes, in particular prototypical diseases, to be matched against the case at hand.\n",
        "doc_number": 29
    },
    {
        "url": "https://en.wikipedia.org/wiki/Automated_planning_and_scheduling",
        "content": "Automated planning and scheduling, sometimes denoted as simply AI planning,[1] is a branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory.\n In known environments with available models, planning can be done offline. Solutions can be found and evaluated prior to execution. In dynamically unknown environments, the strategy often needs to be revised online. Models and policies must be adapted. Solutions usually resort to iterative trial and error processes commonly seen in artificial intelligence. These include dynamic programming, reinforcement learning and combinatorial optimization. Languages used to describe planning and scheduling are often called action languages.\n Given a description of the possible initial states of the world, a description of the desired goals, and a description of a set of possible actions, the planning problem is to synthesize a plan that is guaranteed (when applied to any of the initial states) to generate a state which contains the desired goals (such a state is called a goal state).\n The difficulty of planning is dependent on the simplifying assumptions employed. Several classes of planning problems can be identified depending on the properties the problems have in several dimensions.\n The simplest possible planning problem, known as the Classical Planning Problem, is determined by:\n Since the initial state is known unambiguously, and all actions are deterministic, the state of the world after any sequence of actions can be accurately predicted, and the question of observability is irrelevant for classical planning.\n Further, plans can be defined as sequences of actions, because it is always known in advance which actions will be needed.\n With nondeterministic actions or other events outside the control of the agent, the possible executions form a tree, and plans have to determine the appropriate actions for every node of the tree.\n Discrete-time Markov decision processes (MDP) are planning problems with:\n When full observability is replaced by partial observability, planning corresponds to a partially observable Markov decision process (POMDP).\n If there are more than one agent, we have multi-agent planning, which is closely related to game theory.\n In AI planning, planners typically input a domain model (a description of a set of possible actions which model the domain) as well as the specific problem to be solved specified by the initial state and goal, in contrast to those in which there is no input domain specified. Such planners are called \"domain independent\" to emphasize the fact that they can solve planning problems from a wide range of domains. Typical examples of domains are block-stacking, logistics, workflow management, and robot task planning. Hence a single domain-independent planner can be used to solve planning problems in all these various domains. On the other hand, a route planner is typical of a domain-specific planner.\n The most commonly used languages for representing planning domains and specific planning problems, such as STRIPS and PDDL for Classical Planning, are based on state variables. Each possible state of the world is an assignment of values to the state variables, and actions determine how the values of the state variables change when that action is taken. Since a set of state variables induce a state space that has a size that is exponential in the set, planning, similarly to many other computational problems, suffers from the curse of dimensionality and the combinatorial explosion.\n An alternative language for describing planning problems is that of hierarchical task networks, in which a set of tasks is given, and each task can be either realized by a primitive action or decomposed into a set of other tasks. This does not necessarily involve state variables, although in more realistic applications state variables simplify the description of task networks.\n Temporal planning can be solved with methods similar to classical planning. The main difference is, because of the possibility of several, temporally overlapping actions with a duration being taken concurrently, that the definition of a state has to include information about the current absolute time and how far the execution of each active action has proceeded. Further, in planning with rational or real time, the state space may be infinite, unlike in classical planning or planning with integer time. Temporal planning is closely related to scheduling problems when uncertainty is involved and can also be understood in terms of timed automata. The Simple Temporal Network with Uncertainty (STNU) is a scheduling problem which involves controllable actions, uncertain events and temporal constraints. Dynamic Controllability for such problems is a type of scheduling which requires a temporal planning strategy to activate controllable actions reactively as uncertain events are observed so that all constraints are guaranteed to be satisfied. [2]\n Probabilistic planning can be solved with iterative methods such as value iteration and policy iteration, when the state space is sufficiently small.\nWith partial observability, probabilistic planning is similarly solved with iterative methods, but using a representation of the value functions defined for the space of beliefs instead of states.\n In preference-based planning, the objective is not only to produce a plan but also to satisfy user-specified preferences. A difference to the more common reward-based planning, for example corresponding to MDPs, preferences don't necessarily have a precise numerical value.\n Deterministic planning was introduced with the STRIPS planning system, which is a hierarchical planner. Action names are ordered in a sequence and this is a plan for the robot. Hierarchical planning can be compared with an automatic generated behavior tree.[3] The disadvantage is, that a normal behavior tree is not so expressive like a computer program. That means, the notation of a behavior graph contains action commands, but no loops or if-then-statements. Conditional planning overcomes the bottleneck and introduces an elaborated notation which is similar to a control flow, known from other programming languages like Pascal. It is very similar to program synthesis, which means a planner generates sourcecode which can be executed by an interpreter.[4]\n An early example of a conditional planner is \u201cWarplan-C\u201d which was introduced in the mid 1970s.[5] What is the difference between a normal sequence and a complicated plan, which contains if-then-statements? It has to do with uncertainty at runtime of a plan. The idea is that a plan can react to sensor signals which are unknown for the planner. The planner generates two choices in advance. For example, if an object was detected, then action A is executed, if an object is missing, then action B is executed.[6] A major advantage of conditional planning is the ability to handle partial plans.[7] An agent is not forced to plan everything from start to finish but can divide the problem into chunks. This helps to reduce the state space and solves much more complex problems.\n We speak of \"contingent planning\" when the environment is observable through sensors, which can be faulty. It is thus a situation where the planning agent acts under incomplete information. For a contingent planning problem, a plan is no longer a sequence of actions but a decision tree because each step of the plan is represented by a set of states rather than a single perfectly observable state, as in the case of classical planning.[8] The selected actions depend on the state of the system. For example, if it rains, the agent chooses to take the umbrella, and if it doesn't, they may choose not to take it.\n Michael L. Littman showed in 1998 that with branching actions, the planning problem becomes EXPTIME-complete.[9][10] A particular case of contiguous planning is represented by FOND problems - for \"fully-observable and non-deterministic\". If the goal is specified in LTLf (linear time logic on finite trace) then the problem is always EXPTIME-complete[11] and 2EXPTIME-complete if the goal is specified with LDLf.\n Conformant planning is when the agent is uncertain about the state of the system, and it cannot make any observations. The agent then has beliefs about the real world, but cannot verify them with sensing actions, for instance. These problems are solved by techniques similar to those of classical planning,[12][13] but where the state space is exponential in the size of the problem, because of the uncertainty about the current state. A solution for a conformant planning problem is a sequence of actions. Haslum and Jonsson have demonstrated that the problem of conformant planning is EXPSPACE-complete,[14] and 2EXPTIME-complete when the initial situation is uncertain, and there is non-determinism in the actions outcomes.[10]\n",
        "doc_number": 30
    },
    {
        "url": "https://en.wikipedia.org/wiki/History_of_artificial_intelligence",
        "content": "\n \n The history of artificial intelligence (AI) began in antiquity, with myths, stories, and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The study of logic and formal reasoning from antiquity to the present led directly to the invention of the programmable digital computer in the 1940s, a machine based on abstract mathematical reasoning. This device and the ideas behind it inspired scientists to begin discussing the possibility of building an electronic brain.\n The field of AI research was founded at a workshop held on the campus of Dartmouth College in 1956.[1] Attendees of the workshop became the leaders of AI research for decades. Many of them predicted that machines as intelligent as humans would exist within a generation. The U.S. government provided millions of dollars with the hope of making this vision come true.[2]\n Eventually, it became obvious that researchers had grossly underestimated the difficulty of this feat.[3] In 1974, criticism from James Lighthill and pressure from the U.S. Congress led the U.S. and British Governments to stop funding undirected research into artificial intelligence. Seven years later, a visionary initiative by the Japanese Government and the success of expert systems  reinvigorated investment in AI, and by the late 1980s, the industry had grown into a billion-dollar enterprise. However, investors' enthusiasm waned in the 1990s, and the field was criticized in the press and avoided by industry (a period known as an \"AI winter\"). Nevertheless, research and funding continued to grow under other names.\n In the early 2000s, machine learning was applied to a wide range of problems in academia and industry. The success was due to the availability of powerful computer hardware, the collection of immense data sets, and the application of solid mathematical methods. Soon after, deep learning proved to be a breakthrough technology, eclipsing all other methods. The transformer architecture debuted in 2017 and was used to produce impressive generative AI applications, amongst other use cases.\n Investment in AI boomed in the 2020s. The recent AI boom, initiated by the development of transformer architecture, led to the rapid scaling and public releases of large language models (LLMs) like ChatGPT. These models exhibit human-like traits of knowledge, attention, and creativity, and have been integrated into various sectors, fueling exponential investment in AI. However, concerns about the potential risks and ethical implications of advanced AI have also emerged, prompting debate about the future of AI and its impact on society.\n In Greek mythology, Talos was a giant made of bronze who acted as guardian for the island of Crete. He would throw boulders at the ships of invaders and would complete 3 circuits around the island's perimeter daily.[4] According to pseudo-Apollodorus' Bibliotheke, Hephaestus forged Talos with the aid of a cyclops and presented the automaton as a gift to Minos.[5] In the Argonautica, Jason and the Argonauts defeated Talos by removing a plug near his foot, causing the vital ichor to flow out from his body and rendering him lifeless.[6]\n Pygmalion was a legendary king and sculptor of Greek mythology, famously represented in Ovid's Metamorphoses. In the 10th book of Ovid's narrative poem, Pygmalion becomes disgusted with women when he witnesses the way in which the Propoetides prostitute themselves. Despite this, he makes offerings at the temple of Venus asking the goddess to bring to him a woman just like a statue he carved.[7]\n In Of the Nature of Things, the Swiss alchemist Paracelsus describes a procedure that he claims can fabricate an \"artificial man\". By placing the \"sperm of a man\" in horse dung, and feeding it the \"Arcanum of Mans blood\" after 40 days, the concoction will become a living infant.[8]\n The earliest written account regarding golem-making is found in the writings of Eleazar ben Judah of Worms in the early 13th century.[9] During the Middle Ages, it was believed that the animation of a Golem could be achieved by insertion of a piece of paper with any of God's names on it, into the mouth of the clay figure.[10] Unlike legendary automata like Brazen Heads,[11] a Golem was unable to speak.[12]\n Takwin, the artificial creation of life, was a frequent topic of Ismaili alchemical manuscripts, especially those attributed to Jabir ibn Hayyan. Islamic alchemists attempted to create a broad range of life through their work, ranging from plants to animals.[13]\n In Faust: The Second Part of the Tragedy by Johann Wolfgang von Goethe, an alchemically fabricated homunculus, destined to live forever in the flask in which he was made, endeavors to be born into a full human body. Upon the initiation of this transformation, however, the flask shatters and the homunculus dies.[14]\n By the 19th century, ideas about artificial men and thinking machines became a popular theme in fiction. Notable works like Mary Shelley's Frankenstein  and Karel \u010capek's R.U.R. (Rossum's Universal Robots)[15]\nexplored the concept of artificial life. Speculative essays, such as Samuel Butler's \"Darwin among the Machines\",[16] and Edgar Allan Poe's \"Maelzel's Chess Player\"[17] reflected society's growing interest in machines with artificial intelligence. AI remains a common topic in science fiction today.[18]\n Realistic humanoid automata were built by craftsman from many civilizations, including Yan Shi,[19] Hero of Alexandria,[20] Al-Jazari,[21] Haroun al-Rashid,[22] Jacques de Vaucanson,[23][24] Leonardo Torres y Quevedo,[25] Pierre Jaquet-Droz and Wolfgang von Kempelen.[26][27]\n The oldest known automata were the sacred statues of ancient Egypt and Greece.[28][29] The faithful believed that craftsman had imbued these figures with very real minds, capable of wisdom and emotion\u2014Hermes Trismegistus wrote that \"by discovering the true nature of the gods, man has been able to reproduce it\".[30] English scholar Alexander Neckham asserted that the Ancient Roman poet Virgil had built a palace with automaton statues.[31]\n During the early modern period, these legendary automata were said to possess the magical ability to answer questions put to them. The late medieval alchemist and proto-Protestant Roger Bacon was purported to have fabricated a brazen head, having developed a legend of having been a wizard.[32][33] These legends were similar to the Norse myth of the Head of M\u00edmir. According to legend, M\u00edmir was known for his intellect and wisdom, and was beheaded in the \u00c6sir-Vanir War. Odin is said to have \"embalmed\" the head with herbs and spoke incantations over it such that M\u00edmir's head remained able to speak wisdom to Odin. Odin then kept the head near him for counsel.[34]\n Artificial intelligence is based on the assumption that the process of human thought can be mechanized. The study of mechanical\u2014or \"formal\"\u2014reasoning has a long history. Chinese, Indian and Greek philosophers all developed structured methods of formal deduction by the first millennium BCE. Their ideas were developed over the centuries by philosophers such as Aristotle (who gave a formal analysis of the syllogism),[35] Euclid (whose Elements was a model of formal reasoning), al-Khw\u0101rizm\u012b (who developed algebra and gave his name to the word algorithm) and European scholastic philosophers such as William of Ockham and Duns Scotus.[36]\n Spanish philosopher Ramon Llull (1232\u20131315) developed several logical machines devoted to the production of knowledge by logical means;[37][38] Llull described his machines as mechanical entities that could combine basic and undeniable truths by simple logical operations, produced by the machine by mechanical meanings, in such ways as to produce all the possible knowledge.[39] Llull's work had a great influence on Gottfried Leibniz, who redeveloped his ideas.[40]\n In the 17th century, Leibniz, Thomas Hobbes and Ren\u00e9 Descartes explored the possibility that all rational thought could be made as systematic as algebra or geometry.[41] Hobbes famously wrote in Leviathan: \"For reason ... is nothing but reckoning, that is adding and subtracting\".[42] Leibniz envisioned a universal language of reasoning, the characteristica universalis, which would reduce argumentation to calculation so that \"there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in hand, down to their slates, and to say each other (with a friend as witness, if they liked): Let us calculate.\"[43] These philosophers had begun to articulate the physical symbol system hypothesis that would become the guiding faith of AI research.\n The study of mathematical logic provided the essential breakthrough that made artificial intelligence seem plausible. The foundations had been set by such works as Boole's The Laws of Thought and Frege's Begriffsschrift.[44] Building on Frege's system, Russell and Whitehead presented a formal treatment of the foundations of mathematics in their masterpiece, the Principia Mathematica in 1913. Inspired by Russell's success, David Hilbert challenged mathematicians of the 1920s and 30s to answer this fundamental question: \"can all of mathematical reasoning be formalized?\"[36] His question was answered by G\u00f6del's incompleteness proof,[45] Turing's machine[45] and Church's Lambda calculus.[a]\n Their answer was surprising in two ways. First, they proved that there were, in fact, limits to what mathematical logic could accomplish. But second (and more important for AI) their work suggested that, within these limits, any form of mathematical reasoning could be mechanized. The Church-Turing thesis implied that a mechanical device, shuffling symbols as simple as 0 and 1, could imitate any conceivable process of mathematical deduction.[45] The key insight was the Turing machine\u2014a simple theoretical construct that captured the essence of abstract symbol manipulation.[48] This invention would inspire a handful of scientists to begin discussing the possibility of thinking machines.\n Calculating machines were designed or built in antiquity and throughout history by many people, including \nGottfried Leibniz,[38][49]\nJoseph Marie Jacquard,[50]\nCharles Babbage,[50][51]\nPercy Ludgate,[52]\nLeonardo Torres Quevedo,[53]\nVannevar Bush,[54]\nand others. Ada Lovelace speculated that Babbage's machine was \"a thinking or ... reasoning machine\", but warned \"It is desirable to guard against the possibility of exaggerated ideas that arise as to the powers\" of the machine.[55][56]\n The first modern computers were the massive machines of the Second World War (such as Konrad Zuse's Z3, Alan Turing's Heath Robinson and Colossus, Atanasoff and Berry's and ABC and ENIAC at the University of Pennsylvania).[57] ENIAC was based on the theoretical foundation laid by Alan Turing and developed by John von Neumann,[58] and proved to be the most influential.[57]\n The earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 1930s, 1940s, and early 1950s. Recent research in neurology had shown that the brain was an electrical network of neurons that fired in all-or-nothing pulses. Norbert Wiener's cybernetics described control and stability in electrical networks. Claude Shannon's information theory described digital signals (i.e., all-or-nothing signals). Alan Turing's theory of computation showed that any form of computation could be described digitally. The close relationship between these ideas suggested that it might be possible to construct an \"electronic brain\".\n In the 1940s and 50s, a handful of scientists from a variety of fields (mathematics, psychology, engineering, economics and political science) explored several research directions that would be vital to later AI research.[59] Alan Turing was among the first people to seriously investigate the theoretical possibility of \"machine intelligence\".[60] The field of \"artificial intelligence research\" was founded as an academic discipline in 1956.[61]\n In 1950 Turing published a landmark paper \"Computing Machinery and Intelligence\", in which he speculated about the possibility of creating machines that think.[63][b] In the paper, he noted that \"thinking\" is difficult to define and devised his famous Turing Test: If a machine could carry on a conversation (over a teleprinter) that was indistinguishable from a conversation with a human being, then it was reasonable to say that the machine was \"thinking\".[64] This simplified version of the problem allowed Turing to argue convincingly that a \"thinking machine\" was at least plausible and the paper answered all the most common objections to the proposition.[65] The Turing Test was the first serious proposal in the philosophy of artificial intelligence.\n Walter Pitts and Warren McCulloch analyzed networks of idealized artificial neurons and showed how they might perform simple logical functions in 1943. They were the first to describe what later researchers would call a neural network.[66] The paper was influenced by Turing's paper 'On Computable Numbers' from 1936 using similar two-state boolean 'neurons', but was the first to apply it to neuronal function.[60] One of the students inspired by Pitts and McCulloch was Marvin Minsky who was a 24-year-old graduate student at the time. In 1951 Minsky and Dean Edmonds built the first neural net machine, the SNARC.[67] Minsky would later become one of the most important leaders and innovators in AI.\n Experimental robots such as W. Grey Walter's turtles and the Johns Hopkins Beast, were built in the 1950s. These machines did not use computers, digital electronics or symbolic reasoning; they were controlled entirely by analog circuitry.[68]\n In 1951, using the Ferranti Mark 1 machine of the University of Manchester, Christopher Strachey wrote a checkers program[69] and Dietrich Prinz wrote one for chess.[70] Arthur Samuel's checkers program, the subject of his 1959 paper \"Some Studies in Machine Learning Using the Game of Checkers\", eventually achieved sufficient skill to challenge a respectable amateur.[71] Samuel's program was among the first uses of what would later be called machine learning.[72] Game AI would continue to be used as a measure of progress in AI throughout its history.\n When access to digital computers became possible in the mid-fifties, a few scientists instinctively recognized that a machine that could manipulate numbers could also manipulate symbols and that the manipulation of symbols could well be the essence of human thought. This was a new approach to creating thinking machines.[73][74]\n In 1955, Allen Newell and future Nobel Laureate Herbert A. Simon created the \"Logic Theorist\", with help from J. C. Shaw. The program would eventually prove 38 of the first 52 theorems in Russell and Whitehead's Principia Mathematica, and find new and more elegant proofs for some.[75] Simon said that they had \"solved the venerable mind/body problem, explaining how a system composed of matter can have the properties of mind.\"[76][c] The symbolic reasoning paradigm they introduced would dominate AI research and funding until the middle 90s, as well as inspire the cognitive revolution.\n The Dartmouth workshop of 1956 was a pivotal event that marked the formal inception of AI as an academic discipline.[61] It was organized by Marvin Minsky and John McCarthy, with the support of two senior scientists Claude Shannon and Nathan Rochester of IBM. The proposal for the conference stated they intended to test the assertion that \"every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it\".[77][d] The term \"Artificial Intelligence\" was introduced by John McCarthy at the workshop.[e] \nThe participants included Ray Solomonoff, Oliver Selfridge, Trenchard More, Arthur Samuel, Allen Newell and Herbert A. Simon, all of whom would create important programs during the first decades of AI research.[83][f] At the workshop Newell and Simon debuted the \"Logic Theorist\".[84] The workshop was the moment that AI gained its name, its mission, its first major success and its key players, and is widely considered the birth of AI.[g]\n In the autumn of 1956, Newell and Simon also presented the Logic Theorist at a meeting of the Special Interest Group in Information Theory at the Massachusetts Institute of Technology (MIT). At the same meeting, Noam Chomsky discussed his generative grammar, and George Miller described his landmark paper \"The Magical Number Seven, Plus or Minus Two\". Miller wrote \"I left the symposium with a conviction, more intuitive than rational, that experimental psychology, theoretical linguistics, and the computer simulation of cognitive processes were all pieces from a larger whole.\"[86][57]\n This meeting was the beginning of the \"cognitive revolution\"\u2014an interdisciplinary paradigm shift in psychology, philosophy, computer science and neuroscience. It inspired the creation of the sub-fields of symbolic artificial intelligence, generative linguistics, cognitive science, cognitive psychology, cognitive neuroscience and the philosophical schools of computationalism and functionalism. All these fields used related tools to model the mind and results discovered in one field were relevant to the others.\n The cognitive approach allowed researchers to consider \"mental objects\" like thoughts, plans, goals, facts or memories, often analyzed using high level symbols in functional networks. These objects had been forbidden as \"unobservable\" by earlier paradigms such as behaviorism.[h] Symbolic mental objects would become the major focus of AI research and funding for the next several decades.\n The programs developed in the years after the Dartmouth Workshop were, to most people, simply \"astonishing\":[i] computers were solving algebra word problems, proving theorems in geometry and learning to speak English. Few at the time would have believed that such \"intelligent\" behavior by machines was possible at all.[90][91][89] Researchers expressed an intense optimism in private and in print, predicting that a fully intelligent machine would be built in less than 20 years.[92] Government agencies like the Defense Advanced Research Projects Agency (DARPA, then known as \"ARPA\") poured money into the field.[93] Artificial Intelligence laboratories were set up at a number of British and US universities in the latter 1950s and early 1960s.[60]\n There were many successful programs and new directions in the late 50s and 1960s. Among the most influential were these:\n Many early AI programs used the same basic algorithm. To achieve some goal (like winning a game or proving a theorem), they proceeded step by step towards it (by making a move or a deduction) as if searching through a maze, backtracking whenever they reached a dead end.[94] The principal difficulty was that, for many problems, the number of possible paths through the \"maze\" was astronomical (a situation known as a \"combinatorial explosion\"). Researchers would reduce the search space by using heuristics that would eliminate paths that were unlikely to lead to a solution.[95]\n Newell and Simon tried to capture a general version of this algorithm in a program called the \"General Problem Solver\".[96][97] Other \"searching\" programs were able to accomplish impressive tasks like solving problems in geometry and algebra, such as Herbert Gelernter's Geometry Theorem Prover (1958)[98] and Symbolic Automatic Integrator (SAINT), written by Minsky's student James Slagle in 1961.[99][100] Other programs searched through goals and subgoals to plan actions, like the STRIPS system developed at Stanford to control the behavior of the robot Shakey.[101]\n An important goal of AI research is to allow computers to communicate in natural languages like English. An early success was Daniel Bobrow's program STUDENT, which could solve high school algebra word problems.[102]\n A semantic net represents concepts (e.g. \"house\", \"door\") as nodes, and relations among concepts as links between the nodes (e.g. \"has-a\"). The first AI program to use a semantic net was written by Ross Quillian[103] and the most successful (and controversial) version was Roger Schank's Conceptual dependency theory.[104]\n Joseph Weizenbaum's ELIZA could carry out conversations that were so realistic that users occasionally were fooled into thinking they were communicating with a human being and not a computer program (see ELIZA effect). But in fact, ELIZA simply gave a canned response or repeated back what was said to it, rephrasing its response with a few grammar rules. ELIZA was the first chatbot.[105][106]\n In the late 60s, Marvin Minsky and Seymour Papert of the MIT AI Laboratory proposed that AI research should focus on artificially simple situations known as micro-worlds.[j] They pointed out that in successful sciences like physics, basic principles were often best understood using simplified models like frictionless planes or perfectly rigid bodies. Much of the research focused on a \"blocks world,\" which consists of colored blocks of various shapes and sizes arrayed on a flat surface.[107]\n This paradigm led to innovative work in machine vision by Gerald Sussman, Adolfo Guzman, David Waltz (who invented \"constraint propagation\"), and especially Patrick Winston. At the same time, Minsky and Papert built a robot arm that could stack blocks, bringing the blocks world to life. Terry Winograd's SHRDLU could communicate in ordinary English sentences about the micro-world, plan operations and execute them.[107]\n In the 1960s funding was primarily directed towards laboratories researching symbolic AI, however several people still pursued research in neural networks.\n The perceptron, a single-layer neural network was introduced in 1958 by Frank Rosenblatt[108] (who had been a schoolmate of Marvin Minsky at the Bronx High School of Science).[109] Like most AI researchers, he was optimistic about their power, predicting that a perceptron \"may eventually be able to learn, make decisions, and translate languages.\"[110] Rosenblatt was primarily funded by Office of Naval Research.[111]\n Bernard Widrow and his student Ted Hoff built ADALINE (1960) and MADALINE (1962), which had up to 1000 adjustable weights.[112][113] A group at Stanford Research Institute led by Charles A. Rosen and Alfred E. (Ted) Brain built two neural network machines named MINOS I (1960) and II (1963), mainly funded by U.S. Army Signal Corps. MINOS II[114] had 6600 adjustable weights,[115] and was controlled with an SDS 910 computer in a configuration named MINOS III (1968), which could classify symbols on army maps, and recognize hand-printed characters on Fortran coding sheets.[116][117] Most of neural network research during this early period involved building and using bespoke hardware, rather than simulation on digital computers.[k]\n However, partly due to lack of results and partly due to competition from symbolic AI research, the MINOS project ran out of funding in 1966. Rosenblatt failed to secure continued funding in the 1960s.[118] In 1969, research came to a sudden halt with the publication of Minsky and Papert's 1969 book Perceptrons.[119] It suggested that there were severe limitations to what perceptrons could do and that Rosenblatt's predictions had been grossly exaggerated. The effect of the book was that virtually no research was funded in connectionism for 10 years.[120] The competition for government funding ended with the victory of symbolic AI approaches over neural networks.[117][118]\n Minsky (who had worked on SNARC) became a staunch objector to pure connectionist AI. Widrow (who had worked on ADALINE) turned to adaptive signal processing. The SRI group (which worked on MINOS) turned to symbolic AI and robotics.[117][118]\n The main problem was the inability to train multilayered networks (versions of backpropagation had already been used in other fields but it was unknown to these researchers).[121][120] The AI community became aware of backpropogation in the 80s,[122] and, in the 21st century, neural networks would become enormously successful, fulfilling all of Rosenblatt's optimistic predictions. Rosenblatt did not live to see this, however, as he died in a boating accident in 1971.[123]\n The first generation of AI researchers made these predictions about their work:\n In June 1963, MIT received a $2.2 million grant from the newly created Advanced Research Projects Agency (ARPA, later known as DARPA). The money was used to fund project MAC which subsumed the \"AI Group\" founded by Minsky and McCarthy five years earlier. DARPA continued to provide $3 million each year until the 70s.[130] DARPA made similar grants to Newell and Simon's program at Carnegie Mellon University and to Stanford University's AI Lab, founded by John McCarthy in 1963.[131] Another important AI laboratory was established at Edinburgh University by Donald Michie in 1965.[132] These four institutions would continue to be the main centers of AI research and funding in academia for many years.[133][m]\n The money was given with few strings attached: J. C. R. Licklider, then the director of ARPA, believed that his organization should \"fund people, not projects!\" and allowed researchers to pursue whatever directions might interest them.[135]  This created a freewheeling atmosphere at MIT that gave birth to the hacker culture,[136] but this \"hands off\" approach did not last.\n In the 1970s, AI was subject to critiques and financial setbacks. AI researchers had failed to appreciate the difficulty of the problems they faced. Their tremendous optimism had raised public expectations impossibly high, and when the promised results failed to materialize, funding targeted at AI was severely reduced.[137] The lack of success indicated the techniques being used by AI researchers at the time were insufficient to achieve their goals.[138][139]\n These setbacks did not affect the growth and progress of the field, however. The funding cuts only impacted a handful of major laboratories[140] and the critiques were largely ignored.[141] General public interest in the field continued to grow,[140] the number of researchers increased dramatically,[140] and new ideas were explored in logic programming, commonsense reasoning and many other areas. Historian Thomas Haigh argued in 2023 that there was no winter,[140] and AI researcher Nils Nilsson described this period as the most \"exciting\" time to work in AI.[142]\n In the early seventies, the capabilities of AI programs were limited. Even the most impressive could only handle trivial versions of the problems they were supposed to solve;[n] all the programs were, in some sense, \"toys\".[144] AI researchers had begun to run into several limits that would be only conquered decades later, and others that still stymie the field in the 2020s:\n The agencies which funded AI research, such as the British government, DARPA and the National Research Council (NRC) became frustrated with the lack of progress and eventually cut off almost all funding for undirected AI research. The pattern began in 1966 when the Automatic Language Processing Advisory Committee (ALPAC) report criticized machine translation efforts. After spending $20 million, the NRC ended all support.[154] In 1973, the Lighthill report on the state of AI research in the UK criticized the failure of AI to achieve its \"grandiose objectives\" and led to the dismantling of AI research in that country.[155] (The report specifically mentioned the combinatorial explosion problem as a reason for AI's failings.)[139][143][s] DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at CMU and canceled an annual grant of $3 million.[157][t]\n Hans Moravec blamed the crisis on the unrealistic predictions of his colleagues. \"Many researchers were caught up in a web of increasing exaggeration.\"[158][u] However, there was another issue: since the passage of the Mansfield Amendment in 1969, DARPA had been under increasing pressure to fund \"mission-oriented direct research, rather than basic undirected research\". Funding for the creative, freewheeling exploration that had gone on in the 60s would not come from DARPA, which instead directed money at specific projects with clear objectives, such as autonomous tanks and battle management systems.[159][v]\n The major laboratories (MIT, Stanford, CMU and Edinburgh) had been receiving generous support from their governments, and when it was withdrawn, these were the only places that were seriously impacted by the budget cuts. The thousands of researchers outside these institutions and the many more thousands that were joining the field were unaffected.[140]\n Several philosophers had strong objections to the claims being made by AI researchers. One of the earliest was John Lucas, who argued that G\u00f6del's incompleteness theorem showed that a formal system (such as a computer program) could never see the truth of certain statements, while a human being could.[161] Hubert Dreyfus ridiculed the broken promises of the 1960s and critiqued the assumptions of AI, arguing that human reasoning actually involved very little \"symbol processing\" and a great deal of embodied, instinctive, unconscious \"know how\".[w][163] John Searle's Chinese Room argument, presented in 1980, attempted to show that a program could not be said to \"understand\" the symbols that it uses (a quality called \"intentionality\"). If the symbols have no meaning for the machine, Searle argued, then the machine can not be described as \"thinking\".[164]\n These critiques were not taken seriously by AI researchers. Problems like intractability and commonsense knowledge seemed much more immediate and serious. It was unclear what difference \"know how\" or \"intentionality\" made to an actual computer program. MIT's Minsky said of Dreyfus and Searle \"they misunderstand, and should be ignored.\"[165] Dreyfus, who also taught at MIT, was given a cold shoulder: he later said that AI researchers \"dared not be seen having lunch with me.\"[166] Joseph Weizenbaum, the author of ELIZA, was also an outspoken critic of Dreyfus' positions, but he \"deliberately made it plain that [his AI colleagues' treatment of Dreyfus] was not the way to treat a human being,\"[x] and was unprofessional and childish.[168]\n Weizenbaum began to have serious ethical doubts about AI when Kenneth Colby wrote a \"computer program which can conduct psychotherapeutic dialogue\" based on ELIZA.[169][170][y] Weizenbaum was disturbed that Colby saw a mindless program as a serious therapeutic tool. A feud began, and the situation was not helped when Colby did not credit Weizenbaum for his contribution to the program. In 1976, Weizenbaum published Computer Power and Human Reason which argued that the misuse of artificial intelligence has the potential to devalue human life.[172]\n Logic was introduced into AI research as early as 1958, by John McCarthy in his Advice Taker proposal.[173][98] In 1963, J. Alan Robinson had discovered a simple method to implement deduction on computers, the resolution and unification algorithm.[98] However, straightforward implementations, like those attempted by McCarthy and his students in the late 1960s, were especially intractable: the programs required astronomical numbers of steps to prove simple theorems.[173][174] A more fruitful approach to logic was developed in the 1970s by Robert Kowalski at the University of Edinburgh, and soon this led to the collaboration with French researchers Alain Colmerauer and Philippe Roussel\u00a0[fr] who created the successful logic programming language Prolog.[175] Prolog uses a subset of logic (Horn clauses, closely related to \"rules\" and \"production rules\") that permit tractable computation. Rules would continue to be influential, providing a foundation for Edward Feigenbaum's expert systems and the continuing work by Allen Newell and Herbert A. Simon that would lead to Soar and their unified theories of cognition.[176]\n Critics of the logical approach noted, as Dreyfus had, that human beings rarely used logic when they solved problems. Experiments by psychologists like Peter Wason, Eleanor Rosch, Amos Tversky, Daniel Kahneman and others provided proof.[z] McCarthy responded that what people do is irrelevant. He argued that what is really needed are machines that can solve problems\u2014not machines that think as people do.[aa]\n Among the critics of McCarthy's approach were his colleagues across the country at MIT. Marvin Minsky, Seymour Papert and Roger Schank were trying to solve problems like \"story understanding\" and \"object recognition\" that required a machine to think like a person. In order to use ordinary concepts like \"chair\" or \"restaurant\" they had to make all the same illogical assumptions that people normally made. Unfortunately, imprecise concepts like these are hard to represent in logic. MIT chose instead to focus on writing programs that solved a given task without using high-level abstract definitions or general theories of cognition, and measured performance by iterative testing, rather than arguments from first principles. Schank described their \"anti-logic\" approaches as scruffy, as opposed to the neat paradigm used by McCarthy, Kowalski, Feigenbaum, Newell and Simon.[177][ab]\n In 1975, in a seminal paper, Minsky noted that many of his fellow researchers were using the same kind of tool: a framework that captures all our common sense assumptions about something. For example, if we use the concept of a bird, there is a constellation of facts that immediately come to mind: we might assume that it flies, eats worms and so on (none of which are true for all birds). Minsky associated these assumptions with the general category and they could be inherited by the frames for subcategories and individuals, or over-ridden as necessary. He called these structures frames. Schank used a version of frames he called \"scripts\" to successfully answer questions about short stories in English.[178] Frames would eventually be widely used in software engineering under the name object-oriented programming.\n The logicians rose to the challenge. Pat Hayes claimed that \"most of 'frames' is just a new syntax for parts of first-order logic.\" But he noted that \"there are one or two apparently minor details which give a lot of trouble, however, especially defaults\".[179]\n Ray Reiter admitted that \"conventional logics, such as first-order\nlogic, lack the expressive power to adequately represent the knowledge required for reasoning by default\".[180] He proposed augmenting first-order logic with a closed world assumption that a conclusion holds (by default) if its contrary cannot be shown. He showed how such an assumption corresponds to the common sense assumption made in reasoning with frames. He also showed that it has its \"procedural equivalent\" as negation as failure in Prolog. The closed world assumption, as formulated by Reiter, \"is not a first-order notion. (It is a meta notion.)\"[180] However, Keith Clark showed that negation as finite failure can be understood as reasoning implicitly with definitions in first-order logic including a unique name assumption that different terms denote different individuals.[181]\n During the late 1970s and throughout the 1980s, a variety of logics and extensions of first-order logic were developed both for negation as failure in logic programming and for default reasoning more generally. Collectively, these logics have become known as non-monotonic logics.\n In the 1980s, a form of AI program called \"expert systems\" was adopted by corporations around the world and knowledge became the focus of mainstream AI research. Governments provided substantial funding, such as Japan's fifth generation computer project and the U.S. Strategic Computing Initiative. \"Overall, the AI industry boomed from a few million dollars in 1980 to billions of dollars in 1988.\"[122]\n An expert system is a program that answers questions or solves problems about a specific domain of knowledge, using logical rules that are derived from the knowledge of experts.[182]\nThe earliest examples were developed by Edward Feigenbaum and his students. Dendral, begun in 1965, identified compounds from spectrometer readings.[183][120] MYCIN, developed in 1972, diagnosed infectious blood diseases.[122] They demonstrated the feasibility of the approach.\n Expert systems restricted themselves to a small domain of specific knowledge (thus avoiding the commonsense knowledge problem)[120] and their simple design made it relatively easy for programs to be built and then modified once they were in place. All in all, the programs proved to be useful: something that AI had not been able to achieve up to this point.[184]\n In 1980, an expert system called R1 was completed at CMU for the Digital Equipment Corporation. It was an enormous success: it was saving the company 40 million dollars annually by 1986.[185] Corporations around the world began to develop and deploy expert systems and by 1985 they were spending over a billion dollars on AI, most of it to in-house AI departments.[186] An industry grew up to support them, including hardware companies like Symbolics and Lisp Machines and software companies such as IntelliCorp and Aion.[187]\n In 1981, the Japanese Ministry of International Trade and Industry set aside $850 million for the Fifth generation computer project. Their objectives were to write programs and build machines that could carry on conversations, translate languages, interpret pictures, and reason like human beings.[188] Much to the chagrin of scruffies, they initially chose Prolog as the primary computer language for the project.[189]\n Other countries responded with new programs of their own. The UK began the \u00a3350 million Alvey project.[190] A consortium of American companies formed the Microelectronics and Computer Technology Corporation (or \"MCC\") to fund large scale projects in AI and information technology.[191][190] DARPA responded as well, founding the Strategic Computing Initiative and tripling its investment in AI between 1984 and 1988.[192][193]\n The power of expert systems came from the expert knowledge they contained. They were part of a new direction in AI research that had been gaining ground throughout the 70s. \"AI researchers were beginning to suspect\u2014reluctantly, for it violated the scientific canon of parsimony\u2014that intelligence might very well be based on the ability to use large amounts of diverse knowledge in different ways,\"[194] writes Pamela McCorduck. \"[T]he great lesson from the 1970s was that intelligent behavior depended very much on dealing with knowledge, sometimes quite detailed knowledge, of a domain where a given task lay\".[195] Knowledge based systems and knowledge engineering became a major focus of AI research in the 1980s.[196] It was hoped that vast databases would solve the commonsense knowledge problem and provide the support that commonsense reasoning required.\n In the 1980s some researchers attempted to attack the commonsense knowledge problem directly, by creating a massive database that would contain all the mundane facts that the average person knows. Douglas Lenat, who started a database called Cyc, argued that there is no shortcut \u2015 the only way for machines to know the meaning of human concepts is to teach them, one concept at a time, by hand.[197]\n Although symbolic knowledge representation and logical reasoning produced useful applications in the 80s and received massive amounts of funding, it was still unable to solve problems in perception, robotics, learning and common sense. A small number of scientists and engineers began to doubt that the symbolic approach would ever be sufficient for these tasks and developed other approaches, such as \"connectionism\", robotics, \"soft\" computing and reinforcement learning. Nils Nilsson called these approaches \"sub-symbolic\".\n In 1982, physicist John Hopfield was able to prove that a form of neural network (now called a \"Hopfield net\") could learn and process information, and provably converges after enough time under any fixed condition. It was a breakthrough, as it was previously thought that nonlinear networks would, in general, evolve chaotically.[198] Around the same time, Geoffrey Hinton and David Rumelhart popularized a method for training neural networks called \"backpropagation\".[ac] These two developments helped to revive the exploration of artificial neural networks.[122][199]\n Neural networks, along with several other similar models, received widespread attention after the 1986 publication of the Parallel Distributed Processing, a two volume collection of papers edited by Rumelhart and psychologist James McClelland. The new field was christened \"connectionism\" and there was a considerable debate between advocates of symbolic AI the \"connectionists\".[122] Hinton called symbols the \"luminous aether of AI\" \u2013 that is, an unworkable and misleading model of intelligence.[122]\n In 1990, Yann LeCun at Bell Labs used convolutional neural networks to recognize handwritten digits. The system was used widely in 90s, reading zip codes and personal checks. This was the first genuinely useful application of neural networks.[200][201]\n Rodney Brooks, Hans Moravec and others argued that, in order to show real intelligence, a machine needs to have a body \u2014 it needs to perceive, move, survive and deal with the world.[202] Sensorimotor skills are essential to higher level skills such as commonsense reasoning. They can't be efficiently implemented using abstract symbolic reasoning, so AI should solve the problems of perception, mobility, manipulation and survival without using symbolic representation at all. These robotics researchers advocated building intelligence \"from the bottom up\".[ad]\n A precursor to this idea was David Marr, who had come to MIT in the late 1970s from a successful background in theoretical neuroscience to lead the group studying vision. He rejected all symbolic approaches (both McCarthy's logic and Minsky's frames), arguing that AI needed to understand the physical machinery of vision from the bottom up before any symbolic processing took place. (Marr's work would be cut short by leukemia in 1980.)[204]\n In his 1990 paper \"Elephants Don't Play Chess,\"[205] robotics researcher Brooks took direct aim at the physical symbol system hypothesis, arguing that symbols are not always necessary since \"the world is its own best model. It is always exactly up to date. It always has every detail there is to be known. The trick is to sense it appropriately and often enough.\"[206]\n In the 1980s and 1990s, many cognitive scientists also rejected the symbol processing model of the mind and argued that the body was essential for reasoning, a theory called the \"embodied mind thesis\".[207]\n Soft computing uses methods that work with incomplete and imprecise information. They do not attempt to give precise, logical answers, but give results that are only \"probably\" correct. This allowed them to solve problems that precise symbolic methods could not handle. Press accounts often claimed these tools could \"think like a human\".[208][209]\n Judea Pearl's Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference, an influential 1988 book[210] brought probability and decision theory into AI.[211] Fuzzy logic, developed by Lofti Zadeh in the 60s, began to be more widely used in AI and robotics. Evolutionary computation and artificial neural networks also handle imprecise information, and are classified  as \"soft\". In the 90s and early 2000s many other soft computing tools were developed and put into use, including Bayesian networks,[211] hidden Markov models,[211] information theory and stochastic modeling. These tools in turn depended on advanced mathematical techniques such as classical optimization. For a time in the 1990s and early 2000s, these soft tools were studied by a subfield of AI called \"computational intelligence\".[212]\n Reinforcement learning[213] gives an agent a reward every time it performs a desired action well, and may give negative rewards (or \"punishments\") when it performs poorly. It was described in the first half of the twentieth century by psychologists using animal models, such as Thorndike,[214][215] Pavlov[216] and Skinner.[217] In the 1950s, Alan Turing[215][218] and Arthur Samuel[215] foresaw the role of reinforcement learning in AI.\n A successful and influential research program was led by Richard Sutton and Andrew Barto beginning 1972. Their collaboration revolutionized the study of reinforcement learning and decision making over the four decades.[219][220] In 1988, Sutton described machine learning in terms of decision theory (i.e., the Markov decision process). This gave the subject a solid theoretical foundation and access to a large body of theoretical results developed in the field of operations research.[220]\n Also in 1988, Sutton and Barto developed the \"temporal difference\" (TD) learning algorithm, where the agent is rewarded only when its predictions about the future show improvement. It significantly outperformed previous algorithms.[221] TD-learning was used by Gerald Tesauro in 1992 in the program TD-Gammon, which played backgammon as well as the best human players. The program learned the game by playing against itself with zero prior knowledge.[222] In an interesting case of interdisciplinary convergence, neurologists discovered in 1997 that the dopamine reward system in brains also uses a version of the TD-learning algorithm.[223][224][225] TD learning would be become highly influential in the 21st century, used in both AlphaGo and AlphaZero.[226]\n The business community's fascination with AI rose and fell in the 1980s in the classic pattern of an economic bubble. As dozens of companies failed, the perception in the business world was that the technology was not viable.[227] The damage to AI's reputation would last into the 21st century. Inside the field there was little agreement on the reasons for AI's failure to fulfill the dream of human level intelligence that had captured the imagination of the world in the 1960s. Together, all these factors helped to fragment AI into competing subfields focused on particular problems or approaches, sometimes even under new names that disguised the tarnished pedigree of \"artificial intelligence\".[228]\n Over the next 20 years, AI consistently delivered working solutions to specific isolated problems. By the late 1990s, it was being used throughout the technology industry, although somewhat behind the scenes. The success was due to increasing computer power, by collaboration with other fields (such as mathematical optimization and statistics) and using the highest standards of scientific accountability.                                                                            By 2000, AI had achieved some of its oldest goals. The field was both more cautious and more successful than it had ever been.\n The term \"AI winter\" was coined by researchers who had survived the funding cuts of 1974 when they became concerned that enthusiasm for expert systems had spiraled out of control and that disappointment would certainly follow.[ae] Their fears were well founded: in the late 1980s and early 1990s, AI suffered a series of financial setbacks.[122]\n The first indication of a change in weather was the sudden collapse of the market for specialized AI hardware in 1987. Desktop computers from Apple and IBM had been steadily gaining speed and power and in 1987 they became more powerful than the more expensive Lisp machines made by Symbolics and others. There was no longer a good reason to buy them. An entire industry worth half a billion dollars was demolished overnight.[230]\n Eventually the earliest successful expert systems, such as XCON, proved too expensive to maintain. They were difficult to update, they could not learn, and they were \"brittle\" (i.e., they could make grotesque mistakes when given unusual inputs). Expert systems proved useful, but only in a few special contexts.[231]\n In the late 1980s, the Strategic Computing Initiative cut funding to AI \"deeply and brutally\". New leadership at DARPA had decided that AI was not \"the next wave\" and directed funds towards projects that seemed more likely to produce immediate results.[232]\n By 1991, the impressive list of goals penned in 1981 for Japan's Fifth Generation Project had not been met. Indeed, some of them, like \"carry on a casual conversation\" would not be accomplished for another 40 years. As with other AI projects, expectations had run much higher than what was actually possible.[233][af]\n Over 300 AI companies had shut down, gone bankrupt, or been acquired by the end of 1993, effectively ending the first commercial wave of AI.[235] In 1994, HP Newquist stated in The Brain Makers that \"The immediate future of artificial intelligence\u2014in its commercial form\u2014seems to rest in part on the continued success of neural networks.\"[235]\n In the 1990s, algorithms originally developed by AI researchers began to appear as parts of larger systems. AI had solved a lot of very difficult problems[ag] and their solutions proved to be useful throughout the technology industry,[236][237] such as data mining, industrial robotics, logistics, speech recognition,[238] banking software,[239] medical diagnosis[239] and Google's search engine.[240][241]\n The field of AI received little or no credit for these successes in the 1990s and early 2000s. Many of AI's greatest innovations have been reduced to the status of just another item in the tool chest of computer science.[242] Nick Bostrom explains: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"[239]\n Many researchers in AI in the 1990s deliberately called their work by other names, such as informatics, knowledge-based systems, \"cognitive systems\" or computational intelligence. In part, this may have been because they considered their field to be fundamentally different from AI, but also the new names help to procure funding.[238][243][244] In the commercial world at least, the failed promises of the AI Winter continued to haunt AI research into the 2000s, as the New York Times reported in 2005: \"Computer scientists and software engineers avoided the term artificial intelligence for fear of being viewed as wild-eyed dreamers.\"[245]\n AI researchers began to develop and use sophisticated mathematical tools more than they ever had in the past.[246][247] Most of the new directions in AI relied heavily on mathematical models, including artificial neural networks, probabilistic reasoning, soft computing and reinforcement learning. In the 90s and 2000s, many other highly mathematical tools were adapted for AI. These tools were applied to machine learning, perception and mobility.\n There was a widespread realization that many of the problems that AI needed to solve were already being worked on by researchers in fields like statistics, mathematics, electrical engineering, economics or operations research. The shared mathematical language allowed both a higher level of collaboration with more established and successful fields and the achievement of results which were measurable and provable; AI had become a more rigorous \"scientific\" discipline.\n Another key reason for the success in the 90s was that AI researchers focussed on specific problems with verifiable solutions (an approach later derided as narrow AI). This provided useful tools in the present, rather than speculation about the future.\n A new paradigm called \"intelligent agents\" became widely accepted during the 1990s.[248][249][ah] Although earlier researchers had proposed modular \"divide and conquer\" approaches to AI,[ai] the intelligent agent did not reach its modern form until Judea Pearl, Allen Newell, Leslie P. Kaelbling, and others brought concepts from decision theory and economics into the study of AI.[250] When the economist's definition of a rational agent was married to computer science's definition of an object or module, the intelligent agent paradigm was complete.\n An intelligent agent is a system that perceives its environment and takes actions which maximize its chances of success. By this definition, simple programs that solve specific problems are \"intelligent agents\", as are human beings and organizations of human beings, such as firms. The intelligent agent paradigm defines AI research as \"the study of intelligent agents\".[aj] This is a generalization of some earlier definitions of AI: it goes beyond studying human intelligence; it studies all kinds of intelligence.\n The paradigm gave researchers license to study isolated problems and to disagree about methods, but still retain hope that their work could be combined into an agent architecture that would be capable of general intelligence.[251]\n On May 11, 1997, Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov.[252] In 2005, a Stanford robot won the DARPA Grand Challenge by driving autonomously for 131 miles along an unrehearsed desert trail. Two years later, a team from CMU won the DARPA Urban Challenge by autonomously navigating 55 miles in an urban environment while responding to traffic hazards and adhering to traffic laws.[253]\n These successes were not due to some revolutionary new paradigm, but mostly on the tedious application of engineering skill and on the tremendous increase in the speed and capacity of computers by the 90s.[ak] In fact, Deep Blue's computer was 10 million times faster than the Ferranti Mark 1 that Christopher Strachey taught to play chess in 1951.[al] This dramatic increase is measured by Moore's law, which predicts that the speed and memory capacity of computers doubles every two years. The fundamental problem of \"raw computer power\" was slowly being overcome.\n In the first decades of the 21st century, access to large amounts of data (known as \"big data\"), cheaper and faster computers and advanced machine learning techniques were successfully applied to many problems throughout the economy. A turning point was the success of deep learning around 2012 which improved the performance of machine learning on many tasks, including image and video processing, text analysis, and speech recognition.[255] Investment in AI increased along with its capabilities, and by 2016, the market for AI-related products, hardware, and software reached more than $8 billion, and the New York Times reported that interest in AI had reached a \"frenzy\".[256]\n In 2002, Ben Goertzel and others became concerned that AI had largely abandoned its original goal of producing versatile, fully intelligent machines, and argued in favor of more direct research into artificial general intelligence. By the mid-2010s several companies and institutions had been founded to pursue Artificial General Intelligence (AGI), such as OpenAI and Google's DeepMind. During the same period, new insights into superintelligence raised concerns that AI was an existential threat. The risks and unintended consequences of AI technology became an area of serious academic research after 2016.\n The success of machine learning in the 2000s depended on the availability of vast amounts of training data and faster computers.[257] Russell and Norvig wrote that the \"improvement in performance obtained by increasing the size of the data set by two or three orders of magnitude outweighs any improvement that can be made by tweaking the algorithm.\"[200] Geoffrey Hinton recalled that back in the 90s, the problem was that \"our labeled datasets were thousands of times too small. [And] our computers were millions of times too slow.\"[258] This was no longer true by 2010.\n The most useful data in the 2000s came from curated, labeled data sets created specifically for machine learning and AI. In 2007, a group at UMass Amherst released Labeled Faces in the Wild, an annotated set of images of faces that was widely used to train and test face recognition systems for the next several decades.[259] Fei-Fei Li developed ImageNet, a database of three million images captioned by volunteers using the Amazon Mechanical Turk. Released in 2009, it was a useful body of training data and a benchmark for testing for the next generation of image processing systems.[260][200] Google released word2vec in 2013 as an open source resource. It used large amounts of data text scraped from the internet and word embedding to create a numeric vector to represent each word. Users were surprised at how well it was able to capture word meanings, for example, ordinary vector addition would give equivalences like China + River = Yangtze, London+England-France = Paris.[261] This database in particular would be essential for the development of large language models in the late 2010s.\n The explosive growth of the internet gave machine learning programs access to billions of pages of text and images that could be scraped. And, for specific problems, large privately held databases contained the relevant data. McKinsey Global Institute reported that \"by 2009, nearly all sectors in the US economy had at least an average of 200 terabytes of stored data\".[262] This collection of information was known in the 2000s as big data.\n In a Jeopardy! exhibition match in February 2011, IBM's question answering system Watson defeated the two best Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin.[263] Watson's expertise would have been impossible without the information available on the internet.[200]\n In 2012, AlexNet, a deep learning model,[am] developed by Alex Krizhevsky, won the ImageNet Large Scale Visual Recognition Challenge, with significantly fewer errors than the second-place winner.[265][200] Krizhevsky worked with Geoffrey Hinton at the University of Toronto.[an] This was a turning point in machine learning: over the next few years dozens of other approaches to image recognition were abandoned in favor of deep learning.[257]\n Deep learning uses a multi-layer perceptron. Although this architecture has been known since the 60s, getting it to work requires powerful hardware and large amounts of training data.[266] Before these became \navailable, improving performance of image processing systems required hand-crafted ad hoc features that were difficult to implement.[266] Deep learning was simpler and more general.[ao]\n Deep learning was applied to dozens of problems over the next few years (such as speech recognition, machine translation, medical diagnosis, and game playing). In every case it showed enormous gains in performance.[257] Investment and interest in AI boomed as a result.[257]\n It became fashionable in the 2000s to begin talking about the future of AI again and several popular books considered the possibility of superintelligent machines and what they might mean for human society. Some of this was optimistic (such as Ray Kurzweil's The Singularity is Near), but others warned that a sufficiently powerful AI was existential threat to humanity, such as Nick Bostrom and Eliezer Yudkowsky.[267] The topic became widely covered in the press and many leading intellectuals and politicians commented on the issue.\n AI programs in the 21st century are defined by their goals \u2013 the specific measures that they are designed to optimize. Nick Bostrom's influential 2005 book Superintelligence argued that, if one isn't careful about defining these goals, the machine may cause harm to humanity in the process of achieving a goal. Stuart J. Russell used the example of an intelligent robot that kills its owner to prevent it from being unplugged, reasoning \"you can't fetch the coffee if you're dead\".[268] (This problem is known by the technical term \"instrumental convergence\".) The solution is to align the machine's goal function with the goals of its owner and humanity in general. Thus, the problem of mitigating the risks and unintended consequences of AI became known as \"the value alignment problem\" or AI alignment.[269]\n At the same time, machine learning systems had begun to have disturbing unintended consequences. Cathy O'Neil explained how statistical algorithms had been among the causes of the 2008 economic crash,[270] Julia Angwin of ProPublica argued that the COMPAS system used by the criminal justice system exhibited racial bias under some measures,[271][ap] others showed that many machine learning systems exhibited some form of racial bias,[273] and there were many other examples of dangerous outcomes that had resulted from machine learning systems.[aq]\n In 2016, the election of Donald Trump and the controversy over the COMPAS system illuminated several problems with the current technological infrastructure, including misinformation, social media algorithms designed to maximize engagement, the misuse of personal data and the trustworthiness of predictive models.[274] Issues of fairness and unintended consequences became significantly more popular at AI conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The value alignment problem became a serious field of academic study.[275][ar]\n In the early 2000s, several researchers became concerned that mainstream AI was too focused on \"measurable performance in specific applications\"[277] (known as \"narrow AI\") and had abandoned AI's original goal of creating versatile, fully intelligent machines. An early critic was Nils Nilsson in 1995, and similar opinions were published by AI elder statesmen John McCarthy, Marvin Minsky, and Patrick Winston in 2007\u20132009. Minsky organized a symposium on \"human-level AI\" in 2004.[277] Ben Goertzel adopted the term \"artificial general intelligence\" for the new sub-field, founding a journal and holding conferences beginning in 2008.[278] The new field grew rapidly, buoyed by the continuing success of artificial neural networks and the hope that it was the key to AGI.\n Several competing companies, laboratories and foundations were founded to develop AGI in the 2010s. DeepMind was founded in 2010 by three English scientists, Demis Hassabis, Shane Legg and Mustafa Suleyman, with funding from Peter Thiel and later Elon Musk. The founders and financiers were deeply concerned about AI safety and the existential risk of AI. DeepMind's founders had a personal connection with Yudkowsky and Musk was among those who was actively raising the alarm.[279] Hassabis was both worried about the dangers of AGI and optimistic about its power; he hoped they could \"solve AI, then solve everything else.\"[280]\n In 2012, Geoffrey Hinton (who been leading neural network research since the 80s) was approached by Baidu, which wanted to hire him and all his students for an enormous sum. Hinton decided to hold an auction and, at a Lake Tahoe AI conference, they sold themselves to Google for a price of $44 million. Hassabis took notice and sold DeepMind to Google in 2014, on the condition that it would not accept military contracts and would be overseen by an ethics board.[279]\n Larry Page of Google, unlike Musk and Hassabis, was an optimist about the future of AI. Musk and Paige became embroiled in an argument about the risk of AGI at Musk's 2015 birthday party. They had been friends for decades but stopped speaking to each other shortly afterwards. Musk attended the one and only meeting of the DeepMind's ethics board, where it became clear that Google was uninterested in mitigating the harm of AGI. Frustrated by his lack of influence he founded OpenAI in 2015, enlisting Sam Altman to run it and hiring top scientists. OpenAI began as a non-profit, \"free from the economic incentives that were driving Google and other corporations.\"[279] Musk became frustrated again and left the company in 2018. OpenAI turned to Microsoft for continued financial support and Altman and OpenAI formed a for-profit version of the company with more than $1 billion in financing.[279]\n In 2021, Dario Amodei  and 14 other scientists left OpenAI over concerns that the company was putting profits above safety. They formed Anthropic, which soon had $6 billion in financing from Microsoft and Google.[279]\n The New York Times wrote in 2023 \"At the heart of this competition is a brain-stretching paradox. The people who say they are most worried about AI are among the most determined to create it and enjoy its riches. They have justified their ambition with their strong belief that they alone can keep AI from endangering Earth.\"[279]\n The AI boom started with the initial development of key architectures and algorithms such as the transformer architecture in 2017, leading to the scaling and development of large language models exhibiting human-like traits of knowledge, attention and creativity. The new AI era began around 2020\u20132023, with the public release of scaled large language models (LLMs) such as ChatGPT.[281]\n In 2017, the transformer architecture was proposed by Google researchers. It exploits an attention mechanism and became widely used in large language models.[282]\n Large language models, based on the transformer, were developed by AGI companies: OpenAI released GPT-3 in 2020, and DeepMind released Gato in 2022. These are foundation models: they are trained on vast quantities of unlabeled data and can be adapted to a wide range of downstream tasks.[citation needed]\n These models can discuss a huge number of topics and display general knowledge. The question naturally arises: are these models an example of artificial general intelligence? Bill Gates was skeptical of the new technology and the hype that surrounded AGI. However, Altman presented him with a live demo of ChatGPT4 passing an advanced biology test. Gates was convinced.[279] In 2023, Microsoft Research tested the model with a large variety of tasks, and concluded that \"it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system\".[283]\n In 2024, OpenAI o3, a type of advanced reasoning model developed by OpenAI was announced. On the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) benchmark developed by Fran\u00e7ois Chollet in 2019, the model achieved an unofficial score of 87.5% on the semi-private test, surpassing the typical human score of 84%. The benchmark is supposed to be a necessary, but not sufficient test for AGI. Speaking of the benchmark, Chollet has said \"You\u2019ll know AGI is here when the exercise of creating tasks that are easy for regular humans but hard for AI becomes simply impossible.\"[284]\n Investment in AI grew exponentially after 2020, with venture capital funding for generative AI companies increasing dramatically. Total AI investments rose from $18 billion in 2014 to $119 billion in 2021, with generative AI accounting for approximately 30% of investments by 2023.[285] According to metrics from 2017 to 2021, the United States outranked the rest of the world in terms of venture capital funding, number of startups, and AI patents granted.[286] The commercial AI scene became dominated by American Big Tech companies, whose investments in this area surpassed those from U.S.-based venture capitalists.[287] OpenAI's valuation reached $86 billion by early 2024,[288] while NVIDIA's market capitalization surpassed $3.3 trillion by mid-2024, making it the world's largest company by market capitalization as the demand for AI-capable GPUs surged.[289]\n 15.ai, launched in March 2020[290] by an anonymous MIT researcher,[291][292] was one of the earliest examples of generative AI gaining widespread public attention during the initial stages of the AI boom.[293] The free web application demonstrated the ability to clone character voices using neural networks with minimal training data, requiring as little as 15 seconds of audio to reproduce a voice\u2014a capability later corroborated by OpenAI in 2024.[294] The service went viral on social media platforms in early 2021,[295][296] allowing users to generate speech for characters from popular media franchises, and became particularly notable for its pioneering role in popularizing AI voice synthesis for creative content and memes.[297]\n Contemporary AI systems are now becoming human-competitive at general tasks, and we must ask ourselves: Should we let machines flood our information channels with propaganda and untruth? Should we automate away all the jobs, including the fulfilling ones? Should we develop nonhuman minds that might eventually outnumber, outsmart, obsolete and replace us? Should we risk loss of control of our civilization? Such decisions must not be delegated to unelected tech leaders. Powerful AI systems should be developed only once we are confident that their effects will be positive and their risks will be manageable. This confidence must be well justified and increase with the magnitude of a system\u2019s potential effects. OpenAI\u2019s recent statement regarding artificial general intelligence, states that \"At some point, it may be important to get independent review before starting to train future systems, and for the most advanced efforts to agree to limit the rate of growth of compute used for creating new models.\" We agree. That point is now.\n Therefore, we call on all AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4. This pause should be public and verifiable, and include all key actors. If such a pause cannot be enacted quickly, governments should step in and institute a moratorium.\n ChatGPT was launched on November 30, 2022, marking a pivotal moment in artificial intelligence's public adoption. Within days of its release it went viral, gaining over 100 million users in two months and becoming the fastest-growing consumer software application in history.[299] The chatbot's ability to engage in human-like conversations, write code, and generate creative content captured public imagination and led to rapid adoption across various sectors including education, business, and research.[300] ChatGPT's success prompted unprecedented responses from major technology companies\u2014Google declared a \"code red\" and rapidly launched Gemini (formerly known as Google Bard), while Microsoft incorporated the technology into Bing Chat.[301] The rapid adoption of these AI technologies sparked intense debate about their implications. Notable AI researchers and industry leaders voiced both optimism and concern about the accelerating pace of development. In March 2023, over 20,000 signatories, including computer scientist Yoshua Bengio, Elon Musk, and Apple co-founder Steve Wozniak, signed an open letter calling for a pause in advanced AI development, citing \"profound risks to society and humanity.\"[302] However, other prominent researchers like Juergen Schmidhuber took a more optimistic view, emphasizing that the majority of AI research aims to make \"human lives longer and healthier and easier.\"[303]\n By mid-2024, however, the financial sector began to scrutinize AI companies more closely, particularly questioning their capacity to produce a return on investment commensurate with their massive valuations. Some prominent investors raised concerns about market expectations becoming disconnected from fundamental business realities. Jeremy Grantham, co-founder of GMO LLC, warned investors to \"be quite careful\" and drew parallels to previous technology-driven market bubbles.[304] Similarly, Jeffrey Gundlach, CEO of DoubleLine Capital, explicitly compared the AI boom to the dot-com bubble of the late 1990s, suggesting that investor enthusiasm might be outpacing realistic near-term capabilities and revenue potential.[305] These concerns were amplified by the substantial market capitalizations of AI-focused companies, many of which had yet to demonstrate sustainable profitability models.\n In March 2024, Anthropic released the Claude 3 family of large language models, including Claude 3 Haiku, Sonnet, and Opus.[306] The models demonstrated significant improvements in capabilities across various benchmarks, with Claude 3 Opus notably outperforming leading models from OpenAI and Google.[307] In June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated improved performance compared to the larger Claude 3 Opus, particularly in areas such as coding, multistep workflows, and image analysis.[308]\n In 2024, the Royal Swedish Academy of Sciences awarded Nobel Prizes in recognition of groundbreaking contributions to artificial intelligence. The recipients included:\n .\n",
        "doc_number": 31
    },
    {
        "url": "https://en.wikipedia.org/wiki/Philosophy_of_artificial_intelligence",
        "content": "\n The philosophy of artificial intelligence is a branch of the philosophy of mind and the philosophy of computer science[1] that explores artificial intelligence and its implications for knowledge and understanding of intelligence, ethics, consciousness, epistemology, and free will.[2][3] Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures; see artificial life) so the discipline is of considerable interest to philosophers.[4] These factors contributed to the emergence of the philosophy of artificial intelligence.\n The philosophy of artificial intelligence attempts to answer such questions as follows:[5]\n Questions like these reflect the divergent interests of AI researchers, cognitive scientists and philosophers respectively. The scientific answers to these questions depend on the definition of \"intelligence\" and \"consciousness\" and exactly which \"machines\" are under discussion.\n Important propositions in the philosophy of AI include some of the following:\n Is it possible to create a machine that can solve all the problems humans solve using their intelligence? This question defines the scope of what machines could do in the future and guides the direction of AI research. It only concerns the behavior of machines and ignores the issues of interest to psychologists, cognitive scientists and philosophers, evoking the question: does it matter whether a machine is really thinking, as a person thinks, rather than just producing outcomes that appear to result from thinking?[11]\n The basic position of most AI researchers is summed up in this statement, which appeared in the proposal for the Dartmouth workshop of 1956:\n Arguments against the basic premise must show that building a working AI system is impossible because there is some practical limit to the abilities of computers or that there is some special quality of the human mind that is necessary for intelligent behavior and yet cannot be duplicated by a machine (or by the methods of current AI research). Arguments in favor of the basic premise must show that such a system is possible.\n It is also possible to sidestep the connection between the two parts of the above proposal. For instance, machine learning, beginning with Turing's infamous child machine proposal,[12] essentially achieves the desired feature of intelligence without a precise design-time description as to how it would exactly work. The account on robot tacit knowledge[13] eliminates the need for a precise description altogether.\n The first step to answering the question is to clearly define \"intelligence\".\n Alan Turing[15] reduced the problem of defining intelligence to a simple question about conversation. He suggests that: if a machine can answer any question posed to it, using the same words that an ordinary person would, then we may call that machine intelligent. A modern version of his experimental design would use an online chat room, where one of the participants is a real person and one of the participants is a computer program. The program passes the test if no one can tell which of the two participants is human.[6] Turing notes that no one (except philosophers) ever asks the question \"can people think?\" He writes \"instead of arguing continually over this point, it is usual to have a polite convention that everyone thinks\".[16] Turing's test extends this polite convention to machines:\n One criticism of the Turing test is that it only measures the \"humanness\" of the machine's behavior, rather than the \"intelligence\" of the behavior. Since human behavior and intelligent behavior are not exactly the same thing, the test fails to measure intelligence. Stuart J. Russell and Peter Norvig write that \"aeronautical engineering texts do not define the goal of their field as 'making machines that fly so exactly like pigeons that they can fool other pigeons'\".[17]\n Twenty-first century AI research defines intelligence in terms of goal-directed behavior. It views intelligence as a set of problems that the machine is expected to solve \u2013 the more problems it can solve, and the better its solutions are, the more intelligent the program is. AI founder John McCarthy defined intelligence as \"the computational part of the ability to achieve goals in the world.\"[18]\n Stuart Russell and Peter Norvig formalized this definition using abstract intelligent agents. An \"agent\" is something which perceives and acts in an environment. A \"performance measure\" defines what counts as success for the agent.[19]\n Definitions like this one try to capture the essence of intelligence. They have the advantage that, unlike the Turing test, they do not also test for unintelligent human traits such as making typing mistakes.[21] \nThey have the disadvantage that they can fail to differentiate between \"things that think\" and \"things that do not\". By this definition, even a thermostat has a rudimentary intelligence.[22]\n Hubert Dreyfus describes this argument as claiming that \"if the nervous system obeys the laws of physics and chemistry, which we have every reason to suppose it does, then\u00a0... we\u00a0... ought to be able to reproduce the behavior of the nervous system with some physical device\".[23] This argument, first introduced as early as 1943[24] and vividly described by Hans Moravec in 1988,[25] \nis now associated with futurist Ray Kurzweil, who estimates that computer power will be sufficient for a complete brain simulation by the year 2029.[26] A non-real-time simulation of a thalamocortical model that has the size of the human brain (1011 neurons) was performed in 2005,[27] and it took 50 days to simulate 1 second of brain dynamics on a cluster of 27 processors.\n Even AI's harshest critics (such as Hubert Dreyfus and John Searle) agree that a brain simulation is possible in theory.[a]\nHowever, Searle points out that, in principle, anything can be simulated by a computer; thus, bringing the definition to its breaking point leads to the conclusion that any process at all can technically be considered \"computation\". \"What we wanted to know is what distinguishes the mind from thermostats and livers,\" he writes.[30] Thus, merely simulating the functioning of a living brain would in itself be an admission of ignorance regarding intelligence and the nature of the mind, like trying to build a jet airliner by copying a living bird precisely, feather by feather, with no theoretical understanding of aeronautical engineering.[31]\n In 1963, Allen Newell and Herbert A. Simon proposed that \"symbol manipulation\" was the essence of both human and machine intelligence. They wrote: \n This claim is very strong: it implies both that human thinking is a kind of symbol manipulation (because a symbol system is necessary for intelligence) and that machines can be intelligent (because a symbol system is sufficient for intelligence).[32]\nAnother version of this position was described by philosopher Hubert Dreyfus, who called it \"the psychological assumption\":\n The \"symbols\" that Newell, Simon and Dreyfus discussed were word-like and high level\u2014symbols that directly correspond with objects in the world, such as <dog> and <tail>. Most AI programs written between 1956 and 1990 used this kind of symbol. Modern AI, based on statistics and mathematical optimization, does not use the high-level \"symbol processing\" that Newell and Simon discussed.\n These arguments show that human thinking does not consist (solely) of high level symbol manipulation. They do not show that artificial intelligence is impossible, only that more than symbol processing is required.\n In 1931, Kurt G\u00f6del proved with an incompleteness theorem that it is always possible to construct a \"G\u00f6del statement\" that a given consistent formal system of logic (such as a high-level symbol manipulation program) could not prove. Despite being a true statement, the constructed G\u00f6del statement is unprovable in the given system. (The truth of the constructed G\u00f6del statement is contingent on the consistency of the given system; applying the same process to a subtly inconsistent system will appear to succeed, but will actually yield a false \"G\u00f6del statement\" instead.)[citation needed] More speculatively, G\u00f6del conjectured that the human mind can eventually correctly determine the truth or falsity of any well-grounded mathematical statement (including any possible G\u00f6del statement), and that therefore the human mind's power is not reducible to a mechanism.[34] Philosopher John Lucas (since 1961) and Roger Penrose (since 1989) have championed this philosophical anti-mechanist argument.[35]\n G\u00f6delian anti-mechanist arguments tend to rely on the innocuous-seeming claim that a system of human mathematicians (or some idealization of human mathematicians) is both consistent (completely free of error) and believes fully in its own consistency (and can make all logical inferences that follow from its own consistency, including belief in its G\u00f6del statement) [citation needed]. This is probably impossible for a Turing machine to do (see Halting problem); therefore, the G\u00f6delian concludes that human reasoning is too powerful to be captured by a Turing machine, and by extension, any digital mechanical device.\n However, the modern consensus in the scientific and mathematical community is that actual human reasoning is inconsistent; that any consistent \"idealized version\" H of human reasoning would logically be forced to adopt a healthy but counter-intuitive open-minded skepticism about the consistency of H (otherwise H is provably inconsistent); and that G\u00f6del's theorems do not lead to any valid argument that humans have mathematical reasoning capabilities beyond what a machine could ever duplicate.[36][37][38] This consensus that G\u00f6delian anti-mechanist arguments are doomed to failure is laid out strongly in Artificial Intelligence: \"any attempt to utilize (G\u00f6del's incompleteness results) to attack the computationalist thesis is bound to be illegitimate, since these results are quite consistent with the computationalist thesis.\"[39]\n Stuart Russell and Peter Norvig agree that G\u00f6del's argument does not consider the nature of real-world human reasoning. It applies to what can theoretically be proved, given an infinite amount of memory and time. In practice, real machines (including humans) have finite resources and will have difficulty proving many theorems. It is not necessary to be able to prove everything in order to be an intelligent person.[40]\n Less formally, Douglas Hofstadter, in his Pulitzer prize winning book G\u00f6del, Escher, Bach: An Eternal Golden Braid, states that these \"G\u00f6del-statements\" always refer to the system itself, drawing an analogy to the way the Epimenides paradox uses statements that refer to themselves, such as \"this statement is false\" or \"I am lying\".[41] But, of course, the Epimenides paradox applies to anything that makes statements, whether it is a machine or a human, even Lucas himself. Consider:\n This statement is true but cannot be asserted by Lucas. This shows that Lucas himself is subject to the same limits that he describes for machines, as are all people, and so Lucas's argument is pointless.[43]\n After concluding that human reasoning is non-computable, Penrose went on to controversially speculate that some kind of hypothetical non-computable processes involving the collapse of quantum mechanical states give humans a special advantage over existing computers. Existing quantum computers are only capable of reducing the complexity of Turing computable tasks and are still restricted to tasks within the scope of Turing machines. [citation needed] [clarification needed]. By Penrose and Lucas's arguments, the fact that quantum computers are only able to complete Turing computable tasks implies that they cannot be sufficient for emulating the human mind.[citation needed] Therefore, Penrose seeks for some other process involving new physics, for instance quantum gravity which might manifest new physics at the scale of the Planck mass via spontaneous quantum collapse of the wave function. These states, he suggested, occur both within neurons and also spanning more than one neuron.[44] However, other scientists point out that there is no plausible organic mechanism in the brain for harnessing any sort of quantum computation, and furthermore that the timescale of quantum decoherence seems too fast to influence neuron firing.[45]\n Hubert Dreyfus argued that human intelligence and expertise depended primarily on fast intuitive judgements rather than step-by-step symbolic manipulation, and argued that these skills would never be captured in formal rules.[46]\n Dreyfus's argument had been anticipated by Turing in his 1950 paper Computing machinery and intelligence, where he had classified this as the \"argument from the informality of behavior.\"[47] Turing argued in response that, just because we do not know the rules that govern a complex behavior, this does not mean that no such rules exist. He wrote: \"we cannot so easily convince ourselves of the absence of complete laws of behaviour\u00a0... The only way we know of for finding such laws is scientific observation, and we certainly know of no circumstances under which we could say, 'We have searched enough. There are no such laws.'\"[48]\n Russell and Norvig point out that, in the years since Dreyfus published his critique, progress has been made towards discovering the \"rules\" that govern unconscious reasoning.[49] The situated movement in robotics research attempts to capture our unconscious skills at perception and attention.[50] Computational intelligence paradigms, such as neural nets, evolutionary algorithms and so on are mostly directed at simulated unconscious reasoning and learning. Statistical approaches to AI can make predictions which approach the accuracy of human intuitive guesses. Research into commonsense knowledge has focused on reproducing the \"background\" or context of knowledge. In fact, AI research in general has moved away from high level symbol manipulation, towards new models that are intended to capture more of our intuitive reasoning.[49]\n Cognitive science and psychology eventually came to agree with Dreyfus' description of human expertise. Daniel Kahnemann and others developed a similar theory where they identified two \"systems\" that humans use to solve problems, which he called \"System 1\" (fast intuitive judgements) and \"System 2\" (slow deliberate step by step thinking).[51]\n Although Dreyfus' views have been vindicated in many ways, the work in cognitive science and in AI was in response to specific problems in those fields and was not directly influenced by Dreyfus. Historian and AI researcher Daniel Crevier wrote that \"time has proven the accuracy and perceptiveness of some of Dreyfus's comments. Had he formulated them less aggressively, constructive actions they suggested might have been taken much earlier.\"[52]\n This is a philosophical question, related to the problem of other minds and the hard problem of consciousness. The question revolves around a position defined by John Searle as \"strong AI\":\n Searle distinguished this position from what he called \"weak AI\":\n Searle introduced the terms to isolate strong AI from weak AI so he could focus on what he thought was the more interesting and debatable issue. He argued that even if we assume that we had a computer program that acted exactly like a human mind, there would still be a difficult philosophical question that needed to be answered.[9]\n Neither of Searle's two positions are of great concern to AI research, since they do not directly answer the question \"can a machine display general intelligence?\" (unless it can also be shown that consciousness is necessary for intelligence). Turing wrote \"I do not wish to give the impression that I think there is no mystery about consciousness\u2026 [b]ut I do not think these mysteries necessarily need to be solved before we can answer the question [of whether machines can think].\"[53] Russell and Norvig agree: \"Most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\"[54]\n There are a few researchers who believe that consciousness is an essential element in intelligence, such as Igor Aleksander, Stan Franklin, Ron Sun, and Pentti Haikonen, although their definition of \"consciousness\" strays very close to \"intelligence\". (See artificial consciousness.)\n Before we can answer this question, we must be clear what we mean by \"minds\", \"mental states\" and \"consciousness\".\n The words \"mind\" and \"consciousness\" are used by different communities in different ways. Some new age thinkers, for example, use the word \"consciousness\" to describe something similar to Bergson's \"\u00e9lan vital\": an invisible, energetic fluid that permeates life and especially the mind. Science fiction writers use the word to describe some essential property that makes us human: a machine or alien that is \"conscious\" will be presented as a fully human character, with intelligence, desires, will, insight, pride and so on. (Science fiction writers also use the words \"sentience\", \"sapience\", \"self-awareness\" or \"ghost\"\u2014as in the Ghost in the Shell manga and anime series\u2014to describe this essential human property). For others [who?], the words \"mind\" or \"consciousness\" are used as a kind of secular synonym for the soul.\n For philosophers, neuroscientists and cognitive scientists, the words are used in a way that is both more precise and more mundane: they refer to the familiar, everyday experience of having a \"thought in your head\", like a perception, a dream, an intention or a plan, and to the way we see something, know something, mean something or understand something.[55] \"It's not hard to give a commonsense definition of consciousness\" observes philosopher John Searle.[56] What is mysterious and fascinating is not so much what it is but how it is: how does a lump of fatty tissue and electricity give rise to this (familiar) experience of perceiving, meaning or thinking?\n Philosophers call this the hard problem of consciousness. It is the latest version of a classic problem in the philosophy of mind called the \"mind-body problem\".[57] A related problem is the problem of meaning or understanding (which philosophers call \"intentionality\"): what is the connection between our thoughts and what we are thinking about (i.e. objects and situations out in the world)? A third issue is the problem of experience (or \"phenomenology\"): If two people see the same thing, do they have the same experience? Or are there things \"inside their head\" (called \"qualia\") that can be different from person to person?[58]\n Neurobiologists believe all these problems will be solved as we begin to identify the neural correlates of consciousness: the actual relationship between the machinery in our heads and its collective properties; such as the mind, experience and understanding. Some of the harshest critics of artificial intelligence agree that the brain is just a machine, and that consciousness and intelligence are the result of physical processes in the brain.[59] The difficult philosophical question is this: can a computer program, running on a digital machine that shuffles the binary digits of zero and one, duplicate the ability of the neurons to create minds, with mental states (like understanding or perceiving), and ultimately, the experience of consciousness?\n John Searle asks us to consider a thought experiment: suppose we have written a computer program that passes the Turing test and demonstrates general intelligent action. Suppose, specifically that the program can converse in fluent Chinese. Write the program on 3x5 cards and give them to an ordinary person who does not speak Chinese. Lock the person into a room and have him follow the instructions on the cards. He will copy out Chinese characters and pass them in and out of the room through a slot. From the outside, it will appear that the Chinese room contains a fully intelligent person who speaks Chinese. The question is this: is there anyone (or anything) in the room that understands Chinese? That is, is there anything that has the mental state of understanding, or which has conscious awareness of what is being discussed in Chinese? The man is clearly not aware. The room cannot be aware. The cards certainly are not aware. Searle concludes that the Chinese room, or any other physical symbol system, cannot have a mind.[60]\n Searle goes on to argue that actual mental states and consciousness require (yet to be described) \"actual physical-chemical properties of actual human brains.\"[61] He argues there are special \"causal properties\" of brains and neurons that gives rise to minds: in his words \"brains cause minds.\"[62]\n Gottfried Leibniz made essentially the same argument as Searle in 1714, using the thought experiment of expanding the brain until it was the size of a mill.[63] In 1974, Lawrence Davis imagined duplicating the brain using telephone lines and offices staffed by people, and in 1978 Ned Block envisioned the entire population of China involved in such a brain simulation. This thought experiment is called \"the Chinese Nation\" or \"the Chinese Gym\".[64] Ned Block also proposed his Blockhead argument, which is a version of the Chinese room in which the program has been re-factored into a simple set of rules of the form \"see this, do that\", removing all mystery from the program.\n Responses to the Chinese room emphasize several different points. \n The computational theory of mind or \"computationalism\" claims that the relationship between mind and brain is similar (if not identical) to the relationship between a running program (software) and a computer (hardware). The idea has philosophical roots in Hobbes (who claimed reasoning was \"nothing more than reckoning\"), Leibniz (who attempted to create a logical calculus of all human ideas), Hume (who thought perception could be reduced to \"atomic impressions\") and even Kant (who analyzed all experience as controlled by formal rules).[71] The latest version is associated with philosophers Hilary Putnam and Jerry Fodor.[72]\n This question bears on our earlier questions: if the human brain is a kind of computer then computers can be both intelligent and conscious, answering both the practical and philosophical questions of AI. In terms of the practical question of AI (\"Can a machine display general intelligence?\"), some versions of computationalism make the claim that (as Hobbes wrote):\n In other words, our intelligence derives from a form of calculation, similar to arithmetic. This is the physical symbol system hypothesis discussed above, and it implies that artificial intelligence is possible. In terms of the philosophical question of AI (\"Can a machine have mind, mental states and consciousness?\"), most versions of computationalism claim that (as Stevan Harnad characterizes it):\n This is John Searle's \"strong AI\" discussed above, and it is the real target of the Chinese room argument (according to Harnad).[73]\n If \"emotions\" are defined only in terms of their effect on behavior or on how they function inside an organism, then emotions can be viewed as a mechanism that an intelligent agent uses to maximize the utility of its actions. Given this definition of emotion, Hans Moravec believes that \"robots in general will be quite emotional about being nice people\".[74] Fear is a source of urgency. Empathy is a necessary component of good human computer interaction. He says robots \"will try to please you in an apparently selfless manner because it will get a thrill out of this positive reinforcement. You can interpret this as a kind of love.\"[74] Daniel Crevier writes \"Moravec's point is that emotions are just devices for channeling behavior in a direction beneficial to the survival of one's species.\"[75]\n \"Self-awareness\", as noted above, is sometimes used by science fiction writers as a name for the essential human property that makes a character fully human. Turing strips away all other properties of human beings and reduces the question to \"can a machine be the subject of its own thought?\" Can it think about itself? Viewed in this way, a program can be written that can report on its own internal states, such as a debugger.[76]\n Turing reduces this to the question of whether a machine can \"take us by surprise\" and argues that this is obviously true, as any programmer can attest.[77] He notes that, with enough storage capacity, a computer can behave in an astronomical number of different ways.[78] It must be possible, even trivial, for a computer that can represent ideas to combine them in new ways. (Douglas Lenat's Automated Mathematician, as one example, combined ideas to discover new mathematical truths.) Kaplan and Haenlein suggest that machines can display scientific creativity, while it seems likely that humans will have the upper hand where artistic creativity is concerned.[79]\n In 2009, scientists at Aberystwyth University in Wales and the U.K's University of Cambridge designed a robot called Adam that they believe to be the first machine to independently come up with new scientific findings.[80] Also in 2009, researchers at Cornell developed Eureqa, a computer program that extrapolates formulas to fit the data inputted, such as finding the laws of motion from a pendulum's motion.\n This question (like many others in the philosophy of artificial intelligence) can be presented in two forms. \"Hostility\" can be defined in terms function or behavior, in which case \"hostile\" becomes synonymous with \"dangerous\". Or it can be defined in terms of intent: can a machine \"deliberately\" set out to do harm? The latter is the question \"can a machine have conscious states?\" (such as intentions) in another form.[53]\n The question of whether highly intelligent and completely autonomous machines would be dangerous has been examined in detail by futurists (such as the Machine Intelligence Research Institute). The obvious element of drama has also made the subject popular in science fiction, which has considered many differently possible scenarios where intelligent machines pose a threat to mankind; see Artificial intelligence in fiction.\n One issue is that machines may acquire the autonomy and intelligence required to be dangerous very quickly. Vernor Vinge has suggested that over just a few years, computers will suddenly become thousands or millions of times more intelligent than humans. He calls this \"the Singularity\".[81] He suggests that it may be somewhat or possibly very dangerous for humans.[82] This is discussed by a philosophy called Singularitarianism.\n In 2009, academics and technical experts attended a conference to discuss the potential impact of robots and computers and the impact of the hypothetical possibility that they could become self-sufficient and able to make their own decisions. They discussed the possibility and the extent to which computers and robots might be able to acquire any level of autonomy, and to what degree they could use such abilities to possibly pose any threat or hazard. They noted that some machines have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence\". They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.[81]\n Some experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions.[83] The US Navy has funded a report which indicates that as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions.[84][85]\n The President of the Association for the Advancement of Artificial Intelligence has commissioned a study to look at this issue.[86] They point to programs like the Language Acquisition Device which can emulate human interaction.\n Some have suggested a need to build \"Friendly AI\", a term coined by Eliezer Yudkowsky, meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane.[87]\n Turing said \"It is customary\u00a0... to offer a grain of comfort, in the form of a statement that some peculiarly human characteristic could never be imitated by a machine. ... I cannot offer any such comfort, for I believe that no such bounds can be set.\"[88]\n Turing noted that there are many arguments of the form \"a machine will never do X\", where X can be many things, such as:\n Be kind, resourceful, beautiful, friendly, have initiative, have a sense of humor, tell right from wrong, make mistakes, fall in love, enjoy strawberries and cream, make someone fall in love with it, learn from experience, use words properly, be the subject of its own thought, have as much diversity of behaviour as a man, do something really new.[76] Turing argues that these objections are often based on naive assumptions about the versatility of machines or are \"disguised forms of the argument from consciousness\". Writing a program that exhibits one of these behaviors \"will not make much of an impression.\"[76] All of these arguments are tangential to the basic premise of AI, unless it can be shown that one of these traits is essential for general intelligence.\n Finally, those who believe in the existence of a soul may argue that \"Thinking is a function of man's immortal soul.\" Alan Turing called this \"the theological objection\". He writes:\n In attempting to construct such machines we should not be irreverently usurping His power of creating souls, any more than we are in the procreation of children: rather we are, in either case, instruments of His will providing mansions for the souls that He creates.[89] The discussion on the topic has been reignited as a result of recent claims made by Google's LaMDA artificial intelligence system that it is sentient and had a \"soul\".[90]\n LaMDA (Language Model for Dialogue Applications) is an artificial intelligence system that creates chatbots\u2014AI robots designed to communicate with humans\u2014by gathering vast amounts of text from the internet and using algorithms to respond to queries in the most fluid and natural way possible.\n The transcripts of conversations between scientists and LaMDA reveal that the AI system excels at this, providing answers to challenging topics about the nature of emotions, generating Aesop-style fables on the moment, and even describing its alleged fears.[91] Pretty much all philosophers doubt LaMDA's sentience.[92]\n Some scholars argue that the AI community's dismissal of philosophy is detrimental. In the Stanford Encyclopedia of Philosophy, some philosophers argue that the role of philosophy in AI is underappreciated.[4] Physicist David Deutsch argues that without an understanding of philosophy or its concepts, AI development would suffer from a lack of progress.[93]\n The main conference series on the issue is \"Philosophy and Theory of AI\" (PT-AI), run by Vincent C. M\u00fcller.\n The main bibliography on the subject, with several sub-sections, is on PhilPapers.\n A recent survey for Philosophy of AI is M\u00fcller (2023).[3]\n \n",
        "doc_number": 32
    },
    {
        "url": "https://en.wikipedia.org/wiki/Alan_Turing",
        "content": "\n Alan Mathison Turing (/\u02c8tj\u028a\u0259r\u026a\u014b/; 23 June 1912\u00a0\u2013 7 June 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher and theoretical biologist.[5] He was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer.[6][7][8] Turing is widely considered to be the father of theoretical computer science.[9]\n Born in London, Turing was raised in southern England. He graduated from King's College, Cambridge, and in 1938, earned a doctorate degree from Princeton University. During World War II, Turing worked for the Government Code and Cypher School at Bletchley Park, Britain's codebreaking centre that produced Ultra intelligence. He led Hut 8, the section responsible for German naval cryptanalysis. Turing devised techniques for speeding the breaking of German ciphers, including improvements to the pre-war Polish bomba method, an electromechanical machine that could find settings for the Enigma machine. He played a crucial role in cracking intercepted messages that enabled the Allies to defeat the Axis powers in many engagements, including the Battle of the Atlantic.[10][11]\n After the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine, one of the first designs for a stored-program computer. In 1948, Turing joined Max Newman's Computing Machine Laboratory at the Victoria University of Manchester, where he helped develop the Manchester computers[12] and became interested in mathematical biology. Turing wrote on the chemical basis of morphogenesis[13][1] and predicted oscillating chemical reactions such as the Belousov\u2013Zhabotinsky reaction, first observed in the 1960s. Despite these accomplishments, he was never fully recognised during his lifetime because much of his work was covered by the Official Secrets Act.[14]\n In 1952, Turing was prosecuted for homosexual acts. He accepted hormone treatment, a procedure commonly referred to as chemical castration, as an alternative to prison. Turing died on 7 June 1954, aged 41, from cyanide poisoning. An inquest determined his death as suicide, but the evidence is also consistent with accidental poisoning.[15] \nFollowing a campaign in 2009, British prime minister Gordon Brown made an official public apology for \"the appalling way [Turing] was treated\". Queen Elizabeth II granted a pardon in 2013. The term \"Alan Turing law\" is used informally to refer to a 2017 law in the UK that retroactively pardoned men cautioned or convicted under historical legislation that outlawed homosexual acts.[16]\n Turing left an extensive legacy in mathematics and computing which has become widely recognised with statues and many things named after him, including an annual award for computing innovation. His portrait appears on the Bank of England \u00a350 note, first released on 23 June 2021 to coincide with his birthday. The audience vote in a 2019 BBC series named Turing the greatest person of the 20th century.\n Turing was born in Maida Vale, London, while his father, Julius Mathison Turing, was on leave from his position with the Indian Civil Service (ICS) of the British Raj government at Chatrapur, then in the Madras Presidency and presently in Odisha state, in India.[17][18] Turing's father was the son of a clergyman, the Rev.\u00a0John Robert Turing, from a Scottish family of merchants that had been based in the Netherlands and included a baronet. Turing's mother, Julius's wife, was Ethel Sara Turing (n\u00e9e\u00a0Stoney), daughter of Edward Waller Stoney, chief engineer of the Madras Railways. The Stoneys were a Protestant Anglo-Irish gentry family from both County Tipperary and County Longford, while Ethel herself had spent much of her childhood in County Clare.[19] Julius and Ethel married on 1 October 1907 at the Church of Ireland St. Bartholomew's Church on Clyde Road in Ballsbridge, Dublin.[20]\n Julius's work with the ICS brought the family to British India, where his grandfather had been a general in the Bengal Army. However, both Julius and Ethel wanted their children to be brought up in Britain, so they moved to Maida Vale,[21] London, where Alan Turing was born on 23 June 1912, as recorded by a blue plaque on the outside of the house of his birth,[22][23] later the Colonnade Hotel.[17][24] Turing had an elder brother, John Ferrier Turing, father of Sir John Dermot Turing, 12th Baronet of the Turing baronets.[25]\n Turing's father's civil service commission was still active during Turing's childhood years, and his parents travelled between Hastings in the United Kingdom[26] and India, leaving their two sons to stay with a retired Army couple. At Hastings, Turing stayed at Baston Lodge, Upper Maze Hill, St Leonards-on-Sea, now marked with a blue plaque.[27] The plaque was unveiled on 23 June 2012, the centenary of Turing's birth.[28]\n Very early in life, Turing's parents purchased a house in Guildford in 1927, and Turing lived there during school holidays. The location is also marked with a blue plaque.[29]\n Turing's parents enrolled him at St Michael's, a primary school at 20 Charles Road, St Leonards-on-Sea, from the age of six to nine. The headmistress recognised his talent, noting that she \"...had clever boys and hardworking boys, but Alan is a genius\".[30]\n Between January 1922 and 1926, Turing was educated at Hazelhurst Preparatory School, an independent school in the village of Frant in Sussex (now East Sussex).[31] In 1926, at the age of 13, he went on to Sherborne School,[32] an independent boarding school in the market town of Sherborne in Dorset, where he boarded at Westcott House. The first day of term coincided with the 1926 General Strike, in Britain, but Turing was so determined to attend that he rode his bicycle unaccompanied 60 miles (97\u00a0km) from Southampton to Sherborne, stopping overnight at an inn.[33]\n Turing's natural inclination towards mathematics and science did not earn him respect from some of the teachers at Sherborne, whose definition of education placed more emphasis on the classics. His headmaster wrote to his parents: \"I hope he will not fall between two stools. If he is to stay at public school, he must aim at becoming educated. If he is to be solely a Scientific Specialist, he is wasting his time at a public school\".[34] Despite this, Turing continued to show remarkable ability in the studies he loved, solving advanced problems in 1927 without having studied even elementary calculus. In 1928, aged 16, Turing encountered Albert Einstein's work; not only did he grasp it, but it is possible that he managed to deduce Einstein's questioning of Newton's laws of motion from a text in which this was never made explicit.[35]\n At Sherborne, Turing formed a significant friendship with fellow pupil Christopher Collan Morcom (13 July 1911 \u2013 13 February 1930),[36] who has been described as Turing's first love.[37][38][39] Their relationship provided inspiration in Turing's future endeavours, but it was cut short by Morcom's death, in February 1930, from complications of bovine tuberculosis, contracted after drinking infected cow's milk some years previously.[40][41][42]\n The event caused Turing great sorrow. He coped with his grief by working that much harder on the topics of science and mathematics that he had shared with Morcom. In a letter to Morcom's mother, Frances Isobel Morcom (n\u00e9e Swan), Turing wrote:\n I am sure I could not have found anywhere another companion so brilliant and yet so charming and unconceited. I regarded my interest in my work, and in such things as astronomy (to which he introduced me) as something to be shared with him and I think he felt a little the same about me\u00a0... I know I must put as much energy if not as much interest into my work as if he were alive, because that is what he would like me to do.[43] Turing's relationship with Morcom's mother continued long after Morcom's death, with her sending gifts to Turing, and him sending letters, typically on Morcom's birthday.[44] A day before the third anniversary of Morcom's death (13 February 1933), he wrote to Mrs. Morcom:\n I expect you will be thinking of Chris when this reaches you. I shall too, and this letter is just to tell you that I shall be thinking of Chris and of you tomorrow. I am sure that he is as happy now as he was when he was here. Your affectionate Alan.[45] Some have speculated that Morcom's death was the cause of Turing's atheism and materialism.[46] Apparently, at this point in his life he still believed in such concepts as a spirit, independent of the body and surviving death. In a later letter, also written to Morcom's mother, Turing wrote:\n Personally, I believe that spirit is really eternally connected with matter but certainly not by the same kind of body\u00a0... as regards the actual connection between spirit and body I consider that the body can hold on to a 'spirit', whilst the body is alive and awake the two are firmly connected. When the body is asleep I cannot guess what happens but when the body dies, the 'mechanism' of the body, holding the spirit is gone and the spirit finds a new body sooner or later, perhaps immediately.[47][48] After graduating from Sherborne, Turing applied for several Cambridge colleges scholarships, including Trinity and King's, eventually earning an \u00a380 per annum scholarship (equivalent to about \u00a34,300 as of 2023) to study at the latter.[49][50] There, Turing studied the undergraduate course in Schedule B (that is, a three-year Parts I and II, of the Mathematical Tripos, with extra courses at the end of the third year, as Part III only emerged as a separate degree in 1934) from February 1931 to November 1934 at King's College, Cambridge, where he was awarded first-class honours in mathematics. His dissertation, On the Gaussian error function, written during his senior year and delivered in November 1934 (with a deadline date of 6 December) proved a version of the central limit theorem. It was finally accepted on 16 March 1935. By spring of that same year, Turing started his master's course (Part III)\u2014which he completed in 1937\u2014and, at the same time, he published his first paper, a one-page article called Equivalence of left and right almost periodicity (sent on 23 April), featured in the tenth volume of the Journal of the London Mathematical Society.[51] Later that year, Turing was elected a Fellow of King's College on the strength of his dissertation[52] where he served as a lecturer.[53] However, and, unknown to Turing, this version of the theorem he proved in his paper, had already been proven, in 1922, by Jarl Waldemar Lindeberg. Despite this, the committee found Turing's methods original and so regarded the work worthy of consideration for the fellowship. Abram Besicovitch's report for the committee went so far as to say that if Turing's work had been published before Lindeberg's, it would have been \"an important event in the mathematical literature of that year\".[54][55][56]\n Between the springs of 1935 and 1936, at the same time as Alonzo Church, Turing worked on the decidability of problems, starting from G\u00f6del's incompleteness theorems. In mid-April 1936, Turing sent Max Newman the first draft typescript of his investigations. That same month, Church published his An Unsolvable Problem of Elementary Number Theory, with similar conclusions to Turing's then-yet unpublished work. Finally, on 28 May of that year, he finished and delivered his 36-page paper for publication called \"On Computable Numbers, with an Application to the Entscheidungsproblem\".[57] It was published in the Proceedings of the London Mathematical Society journal in two parts, the first on 30 November and the second on 23 December.[58] In this paper, Turing reformulated Kurt G\u00f6del's 1931 results on the limits of proof and computation, replacing G\u00f6del's universal arithmetic-based formal language with the formal and simple hypothetical devices that became known as Turing machines. The Entscheidungsproblem (decision problem) was originally posed by German mathematician David Hilbert in 1928. Turing proved that his \"universal computing machine\" would be capable of performing any conceivable mathematical computation if it were representable as an algorithm. He went on to prove that there was no solution to the decision problem by first showing that the halting problem for Turing machines is undecidable: it is not possible to decide algorithmically whether a Turing machine will ever halt. This paper has been called \"easily the most influential math paper in history\".[59]\n Although Turing's proof was published shortly after Church's equivalent proof using his lambda calculus,[60] Turing's approach is considerably more accessible and intuitive than Church's.[61] It also included a notion of a 'Universal Machine' (now known as a universal Turing machine), with the idea that such a machine could perform the tasks of any other computation machine (as indeed could Church's lambda calculus). According to the Church\u2013Turing thesis, Turing machines and the lambda calculus are capable of computing anything that is computable. John von Neumann acknowledged that the central concept of the modern computer was due to Turing's paper.[62] To this day, Turing machines are a central object of study in theory of computation.[63]\n From September 1936 to July 1938, Turing spent most of his time studying under Church at Princeton University,[4] in the second year as a Jane Eliza Procter Visiting Fellow. In addition to his purely mathematical work, he studied cryptology and also built three of four stages of an electro-mechanical binary multiplier.[64] In June 1938, he obtained his PhD from the Department of Mathematics at Princeton;[65] his dissertation, Systems of Logic Based on Ordinals,[66][67] introduced the concept of ordinal logic and the notion of relative computing, in which Turing machines are augmented with so-called oracles, allowing the study of problems that cannot be solved by Turing machines. John von Neumann wanted to hire him as his postdoctoral assistant, but he went back to the United Kingdom.[68]\n When Turing returned to Cambridge, he attended lectures given in 1939 by Ludwig Wittgenstein about the foundations of mathematics.[69] The lectures have been reconstructed verbatim, including interjections from Turing and other students, from students' notes.[70] Turing and Wittgenstein argued and disagreed, with Turing defending formalism and Wittgenstein propounding his view that mathematics does not discover any absolute truths, but rather invents them.[71]\n During the Second World War, Turing was a leading participant in the breaking of German ciphers at Bletchley Park. The historian and wartime codebreaker Asa Briggs has said, \"You needed exceptional talent, you needed genius at Bletchley and Turing's was that genius.\"[72]\n From September 1938, Turing worked part-time with the Government Code and Cypher School (GC&CS), the British codebreaking organisation. He concentrated on cryptanalysis of the Enigma cipher machine used by Nazi Germany, together with Dilly Knox, a senior GC&CS codebreaker.[73] Soon after the July 1939 meeting near Warsaw at which the Polish Cipher Bureau gave the British and French details of the wiring of Enigma machine's rotors and their method of decrypting Enigma machine's messages, Turing and Knox developed a broader solution.[74] The Polish method relied on an insecure indicator procedure that the Germans were likely to change, which they in fact did in May 1940. Turing's approach was more general, using crib-based decryption for which he produced the functional specification of the bombe (an improvement on the Polish Bomba).[75]\n On 4 September 1939, the day after the UK declared war on Germany, Turing reported to Bletchley Park, the wartime station of GC&CS.[76] Like all others who came to Bletchley, he was required to sign the Official Secrets Act, in which he agreed not to disclose anything about his work at Bletchley, with severe legal penalties for violating the Act.[77]\n Specifying the bombe was the first of five major cryptanalytical advances that Turing made during the war. The others were: deducing the indicator procedure used by the German navy; developing a statistical procedure dubbed Banburismus for making much more efficient use of the bombes; developing a procedure dubbed Turingery for working out the cam settings of the wheels of the Lorenz SZ 40/42 (Tunny) cipher machine and, towards the end of the war, the development of a portable secure voice scrambler at Hanslope Park that was codenamed Delilah.[78][79]\n By using statistical techniques to optimise the trial of different possibilities in the code breaking process, Turing made an innovative contribution to the subject. He wrote two papers discussing mathematical approaches, titled The Applications of Probability to Cryptography[80] and Paper on Statistics of Repetitions,[81] which were of such value to GC&CS and its successor GCHQ that they were not released to the UK National Archives until April 2012, shortly before the centenary of his birth. A GCHQ mathematician, \"who identified himself only as Richard,\" said at the time that the fact that the contents had been restricted under the Official Secrets Act for some 70 years demonstrated their importance, and their relevance to post-war cryptanalysis:[82]\n [He] said the fact that the contents had been restricted \"shows what a tremendous importance it has in the foundations of our subject\". ... The papers detailed using \"mathematical analysis to try and determine which are the more likely settings so that they can be tried as quickly as possible\". ... Richard said that GCHQ had now \"squeezed the juice\" out of the two papers and was \"happy for them to be released into the public domain\". Turing had a reputation for eccentricity at Bletchley Park. He was known to his colleagues as \"Prof\" and his treatise on Enigma was known as the \"Prof's Book\".[83][84] According to historian Ronald Lewin, Jack Good, a cryptanalyst who worked with Turing, said of his colleague:\n In the first week of June each year he would get a bad attack of hay fever, and he would cycle to the office wearing a service gas mask to keep the pollen off. His bicycle had a fault: the chain would come off at regular intervals. Instead of having it mended he would count the number of times the pedals went round and would get off the bicycle in time to adjust the chain by hand. Another of his eccentricities is that he chained his mug to the radiator pipes to prevent it being stolen.[85] Peter Hilton recounted his experience working with Turing in Hut 8 in his \"Reminiscences of Bletchley Park\" from A Century of Mathematics in America:[86]\n  It is a rare experience to meet an authentic genius. Those of us privileged to inhabit the world of scholarship are familiar with the intellectual stimulation furnished by talented colleagues. We can admire the ideas they share with us and are usually able to understand their source; we may even often believe that we ourselves could have created such concepts and originated such thoughts. However, the experience of sharing the intellectual life of a genius is entirely different; one realizes that one is in the presence of an intelligence, a sensibility of such profundity and originality that one is filled with wonder and excitement.\nAlan Turing was such a genius, and those, like myself, who had the astonishing and unexpected opportunity, created by the strange exigencies of the Second World War, to be able to count Turing as colleague and friend will never forget that experience, nor can we ever lose its immense benefit to us. Hilton echoed similar thoughts in the Nova PBS documentary Decoding Nazi Secrets.[87]\n While working at Bletchley, Turing, who was a talented long-distance runner, occasionally ran the 40 miles (64\u00a0km) to London when he was needed for meetings,[88] and he was capable of world-class marathon standards.[89][90] Turing tried out for the 1948 British Olympic team, but he was hampered by an injury. His tryout time for the marathon was only 11 minutes slower than British silver medallist Thomas Richards' Olympic race time of 2 hours 35 minutes. He was Walton Athletic Club's best runner, a fact discovered when he passed the group while running alone.[91][92][93] When asked why he ran so hard in training he replied:\n I have such a stressful job that the only way I can get it out of my mind is by running hard; it's the only way I can get some release.[94] Due to the problems of counterfactual history, it is hard to estimate the precise effect Ultra intelligence had on the war.[95] However, official war historian Harry Hinsley estimated that this work shortened the war in Europe by more than two years and saved over 14\u00a0million lives.[96]\n At the end of the war, a memo was sent to all those who had worked at Bletchley Park, reminding them that the code of silence dictated by the Official Secrets Act did not end with the war but would continue indefinitely.[77] Thus, even though Turing was appointed an Officer of the Order of the British Empire (OBE) in 1946 by King George VI for his wartime services, his work remained secret for many years.[97][98]\n Within weeks of arriving at Bletchley Park,[76] Turing had specified an electromechanical machine called the bombe, which could break Enigma more effectively than the Polish bomba kryptologiczna, from which its name was derived. The bombe, with an enhancement suggested by mathematician Gordon Welchman, became one of the primary tools, and the major automated one, used to attack Enigma-enciphered messages.[99]\n The bombe searched for possible correct settings used for an Enigma message (i.e., rotor order, rotor settings and plugboard settings) using a suitable crib: a fragment of probable plaintext. For each possible setting of the rotors (which had on the order of 1019 states, or 1022 states for the four-rotor U-boat variant),[100] the bombe performed a chain of logical deductions based on the crib, implemented electromechanically.[101]\n The bombe detected when a contradiction had occurred and ruled out that setting, moving on to the next. Most of the possible settings would cause contradictions and be discarded, leaving only a few to be investigated in detail. A contradiction would occur when an enciphered letter would be turned back into the same plaintext letter, which was impossible with the Enigma. The first bombe was installed on 18 March 1940.[102]\n By late 1941, Turing and his fellow cryptanalysts Gordon Welchman, Hugh Alexander and Stuart Milner-Barry were frustrated. Building on the work of the Poles, they had set up a good working system for decrypting Enigma signals, but their limited staff and bombes meant they could not translate all the signals. In the summer, they had considerable success, and shipping losses had fallen to under 100,000 tons a month; however, they badly needed more resources to keep abreast of German adjustments. They had tried to get more people and fund more bombes through the proper channels, but had failed.[103]\n On 28 October they wrote directly to Winston Churchill explaining their difficulties, with Turing as the first named. They emphasised how small their need was compared with the vast expenditure of men and money by the forces and compared with the level of assistance they could offer to the forces.[103] As Andrew Hodges, biographer of Turing, later wrote, \"This letter had an electric effect.\"[104] Churchill wrote a memo to General Ismay, which read: \"ACTION THIS DAY. Make sure they have all they want on extreme priority and report to me that this has been done.\" On 18 November, the chief of the secret service reported that every possible measure was being taken.[104] The cryptographers at Bletchley Park did not know of the Prime Minister's response, but as Milner-Barry recalled, \"All that we did notice was that almost from that day the rough ways began miraculously to be made smooth.\"[105] More than two hundred bombes were in operation by the end of the war.[106]\n Turing decided to tackle the particularly difficult problem of cracking the German naval use of Enigma \"because no one else was doing anything about it and I could have it to myself\".[108] In December 1939, Turing solved the essential part of the naval indicator system, which was more complex than the indicator systems used by the other services.[108][109]\n That same night, he also conceived of the idea of Banburismus, a sequential statistical technique (what Abraham Wald later called sequential analysis) to assist in breaking the naval Enigma, \"though I was not sure that it would work in practice, and was not, in fact, sure until some days had actually broken\".[108] For this, he invented a measure of weight of evidence that he called the ban. Banburismus could rule out certain sequences of the Enigma rotors, substantially reducing the time needed to test settings on the bombes.[110] Later this sequential process of accumulating sufficient weight of evidence using decibans (one tenth of a ban) was used in cryptanalysis of the Lorenz cipher.[111]\n Turing travelled to the United States in November 1942 and worked with US Navy cryptanalysts on the naval Enigma and bombe construction in Washington.[112][113] He also visited their Computing Machine Laboratory in Dayton, Ohio.[114]\n Turing's reaction to the American bombe design was far from enthusiastic:\n The American Bombe programme was to produce 336 Bombes, one for each wheel order. I used to smile inwardly at the conception of Bombe hut routine implied by this programme, but thought that no particular purpose would be served by pointing out that we would not really use them in that way.\nTheir test (of commutators) can hardly be considered conclusive as they were not testing for the bounce with electronic stop finding devices. Nobody seems to be told about rods or offiziers or banburismus unless they are really going to do something about it.[115] During this trip, he also assisted at Bell Labs with the development of secure speech devices.[116] He returned to Bletchley Park in March 1943. During his absence, Hugh Alexander had officially assumed the position of head of Hut 8, although Alexander had been de facto head for some time (Turing having little interest in the day-to-day running of the section). Turing became a general consultant for cryptanalysis at Bletchley Park.[117]\n Alexander wrote of Turing's contribution:\n There should be no question in anyone's mind that Turing's work was the biggest factor in Hut 8's success. In the early days, he was the only cryptographer who thought the problem worth tackling and not only was he primarily responsible for the main theoretical work within the Hut, but he also shared with Welchman and Keen the chief credit for the invention of the bombe. It is always difficult to say that anyone is 'absolutely indispensable', but if anyone was indispensable to Hut 8, it was Turing. The pioneer's work always tends to be forgotten when experience and routine later make everything seem easy and many of us in Hut 8 felt that the magnitude of Turing's contribution was never fully realised by the outside world.[118] In July 1942, Turing devised a technique termed Turingery (or jokingly Turingismus[119]) for use against the Lorenz cipher messages produced by the Germans' new Geheimschreiber (secret writer) machine. This was a teleprinter rotor cipher attachment codenamed Tunny at Bletchley Park. Turingery was a method of wheel-breaking, i.e., a procedure for working out the cam settings of Tunny's wheels.[120] He also introduced the Tunny team to Tommy Flowers who, under the guidance of Max Newman, went on to build the Colossus computer, the world's first programmable digital electronic computer, which replaced a simpler prior machine (the Heath Robinson), and whose superior speed allowed the statistical decryption techniques to be applied usefully to the messages.[121] Some have mistakenly said that Turing was a key figure in the design of the Colossus computer. Turingery and the statistical approach of Banburismus undoubtedly fed into the thinking about cryptanalysis of the Lorenz cipher,[122][123] but he was not directly involved in the Colossus development.[124]\n Following his work at Bell Labs in the US,[125] Turing pursued the idea of electronic enciphering of speech in the telephone system. In the latter part of the war, he moved to work for the Secret Service's Radio Security Service (later HMGCC) at Hanslope Park.[126][127] At the park, he further developed his knowledge of electronics with the assistance of REME officer Donald Bayley. Together they undertook the design and construction of a portable secure voice communications machine codenamed Delilah.[128] The machine was intended for different applications, but it lacked the capability for use with long-distance radio transmissions. In any case, Delilah was completed too late to be used during the war. Though the system worked fully, with Turing demonstrating it to officials by encrypting and decrypting a recording of a Winston Churchill speech, Delilah was not adopted for use.[129] Turing also consulted with Bell Labs on the development of SIGSALY, a secure voice system that was used in the later years of the war.\n Between 1945 and 1947, Turing lived in Hampton, London,[130] while he worked on the design of the ACE (Automatic Computing Engine) at the National Physical Laboratory (NPL). He presented a paper on 19 February 1946, which was the first detailed design of a stored-program computer.[131] Von Neumann's incomplete First Draft of a Report on the EDVAC had predated Turing's paper, but it was much less detailed and, according to John R. Womersley, Superintendent of the NPL Mathematics Division, it \"contains a number of ideas which are Dr. Turing's own\".[132]\n Although ACE was a feasible design, the effect of the Official Secrets Act surrounding the wartime work at Bletchley Park made it impossible for Turing to explain the basis of his analysis of how a computer installation involving human operators would work.[133] This led to delays in starting the project and he became disillusioned. In late 1947 he returned to Cambridge for a sabbatical year during which he produced a seminal work on Intelligent Machinery that was not published in his lifetime.[134] While he was at Cambridge, the Pilot ACE was being built in his absence. It executed its first program on 10 May 1950, and a number of later computers around the world owe much to it, including the English Electric DEUCE and the American Bendix G-15. The full version of Turing's ACE was not built until after his death.[135]\n According to the memoirs of the German computer pioneer Heinz Billing from the Max Planck Institute for Physics, published by Genscher, D\u00fcsseldorf, there was a meeting between Turing and Konrad Zuse.[136] It took place in G\u00f6ttingen in 1947. The interrogation had the form of a colloquium. Participants were Womersley, Turing, Porter from England and a few German researchers like Zuse, Walther, and Billing (for more details see Herbert Bruderer, Konrad Zuse und die Schweiz).\n In 1948, Turing was appointed reader in the Mathematics Department at the Victoria University of Manchester. He lived at \"Copper Folly\", 43 Adlington Road, in Wilmslow.[137] A year later, he became deputy director of the Computing Machine Laboratory, where he worked on software for one of the earliest stored-program computers\u2014the Manchester Mark 1. Turing wrote the first version of the Programmer's Manual for this machine, and was recruited by Ferranti as a consultant in the development of their commercialised machine, the Ferranti Mark 1. He continued to be paid consultancy fees by Ferranti until his death.[138] During this time, he continued to do more abstract work in mathematics,[139] and in \"Computing Machinery and Intelligence\" (Mind, October 1950), Turing addressed the problem of artificial intelligence, and proposed an experiment that became known as the Turing test, an attempt to define a standard for a machine to be called \"intelligent\". The idea was that a computer could be said to \"think\" if a human interrogator could not tell it apart, through conversation, from a human being.[140] In the paper, Turing suggested that rather than building a program to simulate the adult mind, it would be better to produce a simpler one to simulate a child's mind and then to subject it to a course of education. A reversed form of the Turing test is widely used on the Internet; the CAPTCHA test is intended to determine whether the user is a human or a computer.\n In 1948, Turing, working with his former undergraduate colleague, D.G. Champernowne, began writing a chess program for a computer that did not yet exist. By 1950, the program was completed and dubbed the Turochamp.[141] In 1952, he tried to implement it on a Ferranti Mark 1, but lacking enough power, the computer was unable to execute the program. Instead, Turing \"ran\" the program by flipping through the pages of the algorithm and carrying out its instructions on a chessboard, taking about half an hour per move. The game was recorded.[142] According to Garry Kasparov, Turing's program \"played a recognizable game of chess\".[143] The program lost to Turing's colleague Alick Glennie, although it is said that it won a game against Champernowne's wife, Isabel.[144]\n His Turing test was a significant, characteristically provocative, and lasting contribution to the debate regarding artificial intelligence, which continues after more than half a century.[145]\n When Turing was 39 years old in 1951, he turned to mathematical biology, finally publishing his masterpiece \"The Chemical Basis of Morphogenesis\" in January 1952. He was interested in morphogenesis, the development of patterns and shapes in biological organisms. He suggested that a system of chemicals reacting with each other and diffusing across space, termed a reaction\u2013diffusion system, could account for \"the main phenomena of morphogenesis\".[146] He used systems of partial differential equations to model catalytic chemical reactions. For example, if a catalyst A is required for a certain chemical reaction to take place, and if the reaction produced more of the catalyst A, then we say that the reaction is autocatalytic, and there is positive feedback that can be modelled by nonlinear differential equations. Turing discovered that patterns could be created if the chemical reaction not only produced catalyst A, but also produced an inhibitor B that slowed down the production of A. If A and B then diffused through the container at different rates, then you could have some regions where A dominated and some where B did. To calculate the extent of this, Turing would have needed a powerful computer, but these were not so freely available in 1951, so he had to use linear approximations to solve the equations by hand. These calculations gave the right qualitative results, and produced, for example, a uniform mixture that oddly enough had regularly spaced fixed red spots. The Russian biochemist Boris Belousov had performed experiments with similar results, but could not get his papers published because of the contemporary prejudice that any such thing violated the second law of thermodynamics. Belousov was not aware of Turing's paper in the Philosophical Transactions of the Royal Society.[147]\n Although published before the structure and role of DNA was understood, Turing's work on morphogenesis remains relevant today and is considered a seminal piece of work in mathematical biology.[148] One of the early applications of Turing's paper was the work by James Murray explaining spots and stripes on the fur of cats, large and small.[149][150][151] Further research in the area suggests that Turing's work can partially explain the growth of \"feathers, hair follicles, the branching pattern of lungs, and even the left-right asymmetry that puts the heart on the left side of the chest\".[152] In 2012, Sheth, et al. found that in mice, removal of Hox genes causes an increase in the number of digits without an increase in the overall size of the limb, suggesting that Hox genes control digit formation by tuning the wavelength of a Turing-type mechanism.[153] Later papers were not available until Collected Works of A.\u00a0M.\u00a0Turing was published in 1992.[154]\n A study conducted in 2023 confirmed Turing's mathematical model hypothesis. Presented by the American Physical Society, the experiment involved growing chia seeds in even layers within trays, later adjusting the available moisture. Researchers experimentally tweaked the factors which appear in the Turing equations, and, as a result, patterns resembling those seen in natural environments emerged. This is believed to be the first time that experiments with living vegetation have verified Turing's mathematical insight.[155][156]\n In the 1940s, Turing became worried about losing his savings in the event of a German invasion. In order to protect it, he bought two silver bars weighing 3,200\u00a0oz (90\u00a0kg) and worth \u00a3250 (in 2022, \u00a38,000 adjusted for inflation, \u00a348,000 at spot price) and buried them in a wood near Bletchley Park.[157] Upon returning to dig them up, Turing found that he was unable to break his own code describing where exactly he had hidden them. This, along with the fact that the area had been renovated, meant that he never regained the silver.[158]\n In 1941, Turing proposed marriage to Hut 8 colleague Joan Clarke, a fellow mathematician and cryptanalyst, but their engagement was short-lived. After admitting his homosexuality to his fianc\u00e9e, who was reportedly \"unfazed\" by the revelation, Turing decided that he could not go through with the marriage.[159]\n In December 1951, Turing met Arnold Murray, a 19-year-old unemployed man. Turing was walking along Manchester's Oxford Road when he met Murray just outside the Regal Cinema and invited him to lunch. The two agreed to meet again and in January 1952 began an intimate relationship.[160]  On 23 January, Turing's house in Wilmslow was burgled. Murray told Turing that he and the burglar were acquainted, and Turing reported the crime to the police. During the investigation, he acknowledged a sexual relationship with Murray. Homosexual acts were criminal offences in the United Kingdom at that time,[161] and both men were charged with \"gross indecency\" under Section 11 of the Criminal Law Amendment Act 1885.[162] Initial committal proceedings for the trial were held on 27 February during which Turing's solicitor \"reserved his defence\", i.e., did not argue or provide evidence against the allegations. The proceedings were held at the Sessions House in Knutsford.[163]\n Turing was later convinced by the advice of his brother and his own solicitor, and he entered a plea of guilty.[164] The case, Regina v. Turing and Murray, was brought to trial on 31 March 1952.[165] Turing was convicted and given a choice between imprisonment and probation. His probation would be conditional on his agreement to undergo hormonal physical changes designed to reduce libido, known as \"chemical castration\".[166] He accepted the option of injections of what was then called stilboestrol (now known as diethylstilbestrol or DES), a synthetic oestrogen; this feminization of his body was continued for the course of one year. The treatment rendered Turing impotent and caused breast tissue to form.[167] In a letter, Turing wrote that \"no doubt I shall emerge from it all a different man, but quite who I've not found out\".[168][169] Murray was given a conditional discharge.[170]\n Turing's conviction led to the removal of his security clearance and barred him from continuing with his cryptographic consultancy for the Government Communications Headquarters (GCHQ), the British signals intelligence agency that had evolved from GC&CS in 1946, though he kept his academic job. His trial took place only months after the defection to the Soviet Union of Guy Burgess and Donald Maclean in summer 1951 after which the Foreign Office started to consider anyone known to be homosexual as a potential security risk.[171]\n Turing was denied entry into the United States after his conviction in 1952, but was free to visit other European countries.[172] In the summer of 1952 he visited Norway which was more tolerant of homosexuals. Among the various men he met there was one named Kjell Carlson. Kjell intended to visit Turing in the UK but the authorities intercepted Kjell's postcard detailing his travel arrangements and were able to intercept and deport him before the two could meet.[173] It was also during this time that Turing started consulting a psychiatrist, Dr Franz Greenbaum, with whom he got on well and who subsequently became a family friend.[173][174]\n On 8 June 1954, at his house at 43 Adlington Road, Wilmslow, Turing's housekeeper found him dead.[175] A post mortem was held that evening, which determined that he had died the previous day at age 41 with cyanide poisoning cited as the cause of death.[176][177] When his body was discovered, an apple lay half-eaten beside his bed, and although the apple was not tested for cyanide,[178] it was speculated that this was the means by which Turing had consumed a fatal dose.\n Turing's brother, John, identified the body the following day and took the advice given by Dr. Greenbaum to accept the verdict of the inquest, as there was little prospect of establishing that the death was accidental.[179] The inquest was held the following day, which determined the cause of death to be suicide.[166] Turing's remains were cremated at Woking Crematorium just two days later on 12 June 1954, with just his mother, brother, and Lyn Newman attending,[180][181] and his ashes were scattered in the gardens of the crematorium, just as his father's had been.[182] Turing's mother was on holiday in Italy at the time of his death and returned home after the inquest. She never accepted the verdict of suicide.[179]\n Philosopher Jack Copeland has questioned various aspects of the coroner's historical verdict. He suggested an alternative explanation for the cause of Turing's death: the accidental inhalation of cyanide fumes from an apparatus used to electroplate gold onto spoons. The potassium cyanide was used to dissolve the gold. Turing had such an apparatus set up in his tiny spare room. Copeland noted that the autopsy findings were more consistent with inhalation than with ingestion of the poison. Turing also habitually ate an apple before going to bed, and it was not unusual for the apple to be discarded half-eaten.[15] Furthermore, Turing had reportedly borne his legal setbacks and hormone treatment (which had been discontinued a year previously) \"with good humour\" and had shown no sign of despondency before his death. He even set down a list of tasks that he intended to complete upon returning to his office after the holiday weekend.[15] Turing's mother believed that the ingestion was accidental, resulting from her son's careless storage of laboratory chemicals.[183]\n Turing's biographer Andrew Hodges theorised that Turing deliberately left the nature of his death ambiguous in order to shield his mother from the knowledge that he had killed himself.[184] Doubts on the suicide thesis have been also cast by John W. Dawson Jr. who, in his review of Hodges' book, recalls \"Turing's vulnerable position in the Cold War political climate\" and points out that \"Turing was found dead by a maid, who discovered him 'lying neatly in his bed'\u2014hardly what one would expect of \"a man fighting for life against the suffocation induced by cyanide poisoning.\" Turing had given no hint of suicidal inclinations to his friends and had made no effort to put his affairs in order.[185]\n Hodges and a later biographer, David Leavitt, have both speculated that Turing was re-enacting a scene from the Walt Disney film Snow White and the Seven Dwarfs (1937), his favourite fairy tale. Both men noted that (in Leavitt's words) he took \"an especially keen pleasure in the scene where the Wicked Queen immerses her apple in the poisonous brew\".[186]\n It has also been suggested that Turing's belief in fortune-telling may have caused his depressed mood.[182] As a youth, Turing had been told by a fortune-teller that he would be a genius. In mid-May 1954, shortly before his death, Turing again decided to consult a fortune-teller during a day-trip to St Annes-on-Sea with the Greenbaum family.[182] According to the Greenbaums' daughter, Barbara:[174]\n But it was a lovely sunny day and Alan was in a cheerful mood and off we went\u00a0... Then he thought it would be a good idea to go to the Pleasure Beach at Blackpool. We found a fortune-teller's tent and Alan said he'd like to go in[,] so we waited around for him to come back\u00a0... And this sunny, cheerful visage had shrunk into a pale, shaking, horror-stricken face. Something had happened. We don't know what the fortune-teller said but he obviously was deeply unhappy. I think that was probably the last time we saw him before we heard of his suicide. In August 2009, British programmer John Graham-Cumming started a petition urging the British government to apologise for Turing's prosecution as a homosexual.[187][188] The petition received more than 30,000 signatures.[189][190] The prime minister, Gordon Brown, acknowledged the petition, releasing a statement on 10 September 2009 apologising and describing the treatment of Turing as \"appalling\":[189][191]\n Thousands of people have come together to demand justice for Alan Turing and recognition of the appalling way he was treated. While Turing was dealt with under the law of the time and we can't put the clock back, his treatment was of course utterly unfair and I am pleased to have the chance to say how deeply sorry I and we all are for what happened to him\u00a0... So on behalf of the British government, and all those who live freely thanks to Alan's work I am very proud to say: we're sorry, you deserved so much better.[189][192] In December 2011, William Jones and his member of Parliament, John Leech, created an e-petition[193] requesting that the British government pardon Turing for his conviction of \"gross indecency\":[194]\n We ask the HM Government to grant a pardon to Alan Turing for the conviction of \"gross indecency\". In 1952, he was convicted of \"gross indecency\" with another man and was forced to undergo so-called \"organo-therapy\"\u2014chemical castration. Two years later, he killed himself with cyanide, aged just 41. Alan Turing was driven to a terrible despair and early death by the nation he'd done so much to save. This remains a shame on the British government and British history. A pardon can go some way to healing this damage. It may act as an apology to many of the other gay men, not as well-known as Alan Turing, who were subjected to these laws.[193] The petition gathered over 37,000 signatures,[193][195] and was submitted to Parliament by the Manchester MP John Leech but the request was discouraged by Justice Minister Lord McNally, who said:[196]\n A posthumous pardon was not considered appropriate as Alan Turing was properly convicted of what at the time was a criminal offence. He would have known that his offence was against the law and that he would be prosecuted. It is tragic that Alan Turing was convicted of an offence that now seems both cruel and absurd\u2014particularly poignant given his outstanding contribution to the war effort. However, the law at the time required a prosecution and, as such, long-standing policy has been to accept that such convictions took place and, rather than trying to alter the historical context and to put right what cannot be put right, ensure instead that we never again return to those times.[197] John Leech, the MP for Manchester Withington (2005\u201315), submitted several bills to Parliament[198] and led a high-profile campaign to secure the pardon. Leech made the case in the House of Commons that Turing's contribution to the war made him a national hero and that it was \"ultimately just embarrassing\" that the conviction still stood.[199] Leech continued to take the bill through Parliament and campaigned for several years, gaining the public support of numerous leading scientists, including Stephen Hawking.[200][201] At the British premiere of a film based on Turing's life, The Imitation Game, the producers thanked Leech for bringing the topic to public attention and securing Turing's pardon.[202] Leech is now regularly described as the \"architect\" of Turing's pardon and subsequently the Alan Turing Law which went on to secure pardons for 75,000 other men and women convicted of similar crimes.[203][204][205]\n On 26 July 2012, a bill was introduced in the House of Lords to grant a statutory pardon to Turing for offences under section 11 of the Criminal Law Amendment Act 1885, of which he was convicted on 31 March 1952.[206] Late in the year in a letter to The Daily Telegraph, the physicist Stephen Hawking and 10 other signatories including the Astronomer Royal Lord Rees, President of the Royal Society Sir Paul Nurse, Lady Trumpington (who worked for Turing during the war) and Lord Sharkey (the bill's sponsor) called on Prime Minister David Cameron to act on the pardon request.[207] The government indicated it would support the bill,[208][209][210] and it passed its third reading in the House of Lords in October.[211]\n At the bill's second reading in the House of Commons on 29 November 2013, Conservative MP Christopher Chope objected to the bill, delaying its passage. The bill was due to return to the House of Commons on 28 February 2014,[212] but before the bill could be debated in the House of Commons,[213] the government elected to proceed under the royal prerogative of mercy. On 24 December 2013, Queen Elizabeth II signed a pardon for Turing's conviction for \"gross indecency\", with immediate effect.[214] Announcing the pardon, Lord Chancellor Chris Grayling said Turing deserved to be \"remembered and recognised for his fantastic contribution to the war effort\" and not for his later criminal conviction.[195][215] The Queen pronounced Turing pardoned in August 2014.[216] It was only the fourth royal pardon granted since the conclusion of the Second World War.[217] Pardons are normally granted only when the person is technically innocent, and a request has been made by the family or other interested party; neither condition was met in regard to Turing's conviction.[218]\n In September 2016, the government announced its intention to expand this retroactive exoneration to other men convicted of similar historical indecency offences, in what was described as an \"Alan Turing law\".[219][220] The Alan Turing law is now an informal term for the law in the United Kingdom, contained in the Policing and Crime Act 2017, which serves as an amnesty law to retroactively pardon men who were cautioned or convicted under historical legislation that outlawed homosexual acts. The law applies in England and Wales.[221]\n On 19 July 2023, following an apology to LGBT veterans from the UK Government, Defence Secretary Ben Wallace suggested Turing should be honoured with a permanent statue on the fourth plinth of Trafalgar Square, describing Turing as \"probably the greatest war hero, in my book, of the Second World War, [whose] achievements shortened the war, saved thousands of lives, helped defeat the Nazis. And his story is a sad story of a society and how it treated him.\"[222][223][224]\n",
        "doc_number": 33
    },
    {
        "url": "https://en.wikipedia.org/wiki/John_McCarthy_(computer_scientist)",
        "content": "\n John McCarthy (September 4, 1927 \u2013 October 24, 2011) was an American computer scientist and cognitive scientist. He was one of the founders of the discipline of artificial intelligence.[1] He co-authored the document that coined the term \"artificial intelligence\" (AI), developed the programming language family Lisp, significantly influenced the design of the language ALGOL, popularized time-sharing, and invented garbage collection.\n McCarthy spent most of his career at Stanford University.[2] He received many accolades and honors, such as the 1971 Turing Award for his contributions to the topic of AI,[3] the United States National Medal of Science, and the Kyoto Prize.\n John McCarthy was born in Boston, Massachusetts, on September 4, 1927, to an Irish immigrant father and a Lithuanian Jewish immigrant mother,[4] John Patrick and Ida (Glatt) McCarthy. The family was obliged to relocate frequently during the Great Depression, until McCarthy's father found work as an organizer for the Amalgamated Clothing Workers in Los Angeles, California. His father came from Cromane, a small fishing village in County Kerry, Ireland.[5] His mother died in 1957.[6]\n Both parents were active members of the Communist Party during the 1930s, and they encouraged learning and critical thinking. Before he attended high school, McCarthy became interested in science by reading a translation of 100,000 Whys, a Russian popular science book for children.[7]  He was fluent in the Russian language and made friends with Russian scientists during multiple trips to the Soviet Union, but distanced himself after making visits to the Soviet Bloc, which led to him becoming a conservative Republican.[8]\n McCarthy graduated from Belmont High School two years early[9] and was accepted into Caltech in 1944.\n He showed an early aptitude for mathematics; during his teens, he taught himself college math by studying the textbooks used at the nearby California Institute of Technology (Caltech). As a result, he was able to skip the first two years of math at Caltech.[10] He was suspended from Caltech for failure to attend physical education courses.[11] He then served in the US Army and was readmitted, receiving a Bachelor of Science (BS) in mathematics in 1948.[12]\n It was at Caltech that he attended a lecture by John von Neumann that inspired his future endeavors.\n McCarthy completed his graduate studies at Caltech before moving to Princeton University, where he received a PhD in mathematics in 1951 with his dissertation \"Projection operators and partial differential equations\", under the supervision of Donald C. Spencer.[13]\n After short-term appointments at Princeton and Stanford University, McCarthy became an assistant professor at Dartmouth in 1955.\n A year later, he moved to MIT as a research fellow in the autumn of 1956. By the end of his years at Massachusetts Institute of Technology (MIT) he was already affectionately referred to as \"Uncle John\" by his students.[14]\n In 1962, he became a full professor at Stanford, where he remained until his retirement in 2000.\n McCarthy championed mathematics such as lambda calculus and invented logics for achieving common sense in artificial intelligence.\n John McCarthy is one of the \"founding fathers\" of artificial intelligence, together with Alan Turing, Marvin Minsky, Allen Newell, and Herbert A. Simon. McCarthy, Minsky, Nathaniel Rochester and Claude E. Shannon coined the term \"artificial intelligence\" in a proposal that they wrote for the famous Dartmouth conference in Summer 1956. This conference started AI as a field.[9][15] (Minsky later joined McCarthy at MIT in 1959.)\n In 1958, he proposed the advice taker, which inspired later work on question-answering and logic programming.\n In the late 1950s, McCarthy discovered that primitive recursive functions could be extended to compute with symbolic expressions, producing the Lisp programming language.[16] That functional programming seminal paper also introduced the lambda notation borrowed from the syntax of lambda calculus in which later dialects like Scheme based its semantics. Lisp soon became the programming language of choice for AI applications after its publication in 1960.\n In 1958, McCarthy served on an Association for Computing Machinery ad hoc committee on Languages that became part of the committee that designed ALGOL 60. In August 1959 he proposed the use of recursion and conditional expressions, which became part of ALGOL.[17] He then became involved with developing international standards in programming and informatics, as a member of the International Federation for Information Processing (IFIP) Working Group 2.1 on Algorithmic Languages and Calculi,[18] which specified, maintains, and supports ALGOL 60 and ALGOL 68.[19]\n Around 1959, he invented so-called \"garbage collection\" methods, a kind of automatic memory management, to solve problems in Lisp.[20][21]\n During his time at MIT, he helped motivate the creation of Project MAC, and while at Stanford University, he helped establish the Stanford AI Laboratory, for many years a friendly rival to Project MAC.\n McCarthy was instrumental in the creation of three of the very earliest time-sharing systems (Compatible Time-Sharing System, BBN Time-Sharing System, and Dartmouth Time-Sharing System). His colleague Lester Earnest told the Los Angeles Times:\n The Internet would not have happened nearly as soon as it did except for the fact that John initiated the development of time-sharing systems. We keep inventing new names for time-sharing. It came to be called servers\u00a0... Now we call it cloud computing. That is still just time-sharing. John started it.[9] In 1961, he was perhaps the first to suggest publicly the idea of utility computing, in a speech given to celebrate MIT's centennial: that computer time-sharing technology might result in a future in which computing power and even specific applications could be sold through the utility business model (like water or electricity).[22][23]  This idea of a computer or information utility was very popular during the late 1960s, but had faded by the mid-1990s. However, since 2000, the idea has resurfaced in new forms (see application service provider, grid computing, and cloud computing).\n In 1966, McCarthy and his team at Stanford wrote a computer program used to play a series of chess games with counterparts in the Soviet Union; McCarthy's team lost two games and drew two games (see Kotok-McCarthy).\n From 1978 to 1986, McCarthy developed the circumscription method of non-monotonic reasoning.\n In 1982, he seems to have originated the idea of the space fountain, a type of tower extending into space and kept vertical by the outward force of a stream of pellets propelled from Earth along a sort of conveyor belt which returns the pellets to Earth. Payloads would ride the conveyor belt upward.[24]\n McCarthy often commented on world affairs on the Usenet forums. Some of his ideas can be found in his sustainability Web page,[25] which is \"aimed at showing that human material progress is desirable and sustainable\". McCarthy was an avid book reader, an optimist, and a staunch supporter of free speech. His best Usenet interaction is visible in rec.arts.books archives. He actively attended San Francisco (SF) Bay Area dinners in Palo Alto of r.a.b. readers, called rab-fests. He went on to defend free speech criticism involving European ethnic jokes at Stanford.[26]\n McCarthy saw the importance of mathematics and mathematics education. His Usenet signature block (.sig) for years was, \"He who refuses to do arithmetic is doomed to talk nonsense\"; his license plate cover read, similarly, \"Do the arithmetic or be doomed to talk nonsense.\"[27][28] He advised 30 PhD graduates.[29]\n His 2001 short story \"The Robot and the Baby\"[30] farcically explored the question of whether robots should have (or simulate having) emotions, and anticipated aspects of Internet culture and social networking that became increasingly prominent during ensuing decades.[31]\n McCarthy was married three times. His second wife was Vera Watson, a programmer and mountaineer who died in 1978 attempting to scale Annapurna I Central as part of an all-women expedition. He later married Carolyn Talcott, a computer scientist at Stanford and later Scientific Research Institute (SRI) International.[32][33]\n McCarthy declared himself an atheist in a speech about artificial intelligence at Stanford Memorial Church.[34][35][36] Raised as a Communist, he became a conservative Republican after a visit to Czechoslovakia in 1968 after the Soviet invasion.[37] He died at his home in Stanford on October 24, 2011.[38]\n In 1979 McCarthy wrote an article[39] entitled \"Ascribing Mental Qualities to Machines\". In it he wrote, \"Machines as simple as thermostats can be said to have beliefs, and having beliefs seems to be a characteristic of most machines capable of problem-solving performance.\" In 1980 the philosopher John Searle responded with his famous Chinese Room Argument,[40][15] disagreeing with McCarthy and taking the stance that machines cannot have beliefs simply because they are not conscious. Searle argues that machines lack intentionality. A vast amount of literature [example needed] has been written in support of one side or the other.\n",
        "doc_number": 34
    },
    {
        "url": "https://en.wikipedia.org/wiki/Marvin_Minsky",
        "content": "\n Marvin Lee Minsky (August 9, 1927 \u2013 January 24, 2016) was an American cognitive and computer scientist concerned largely with research in artificial intelligence (AI). He co-founded the Massachusetts Institute of Technology's AI laboratory and wrote several texts about AI and philosophy.[12][13][14][15]\n Minsky received many accolades and honors, including the 1969 Turing Award.\n Marvin Lee Minsky was born in New York City, to Henry, an eye surgeon, and Fannie (Reiser), a Zionist activist.[15][16][17] His family was Jewish. He attended the Ethical Culture Fieldston School and the Bronx High School of Science. He later attended Phillips Academy in Andover, Massachusetts. He then served in the US Navy from 1944 to 1945. He received a B.A. in mathematics from Harvard University in 1950 and a Ph.D. in mathematics from Princeton University in 1954. His doctoral dissertation was titled \"Theory of neural-analog reinforcement systems and its application to the brain-model problem.\"[18][19][20] He was a Junior Fellow of the Harvard Society of Fellows from 1954 to 1957.[21][22]\n Minsky was on the MIT faculty from 1958 to his death. He joined the staff at MIT Lincoln Laboratory in 1958, and a year later he and John McCarthy initiated what is, as of 2019[update], named the MIT Computer Science and Artificial Intelligence Laboratory.[23][24]  He was the Toshiba Professor of Media Arts and Sciences, and professor of electrical engineering and computer science.\n Minsky's inventions include the first head-mounted graphical display (1963)[25] and the confocal microscope[6][note 1] (1957, a predecessor to today's widely used confocal laser scanning microscope). With Seymour Papert, he developed the first Logo \"turtle\". In 1951, Minsky built the first randomly wired neural network learning machine, SNARC. In 1962, he worked on small universal Turing machines and published his well-known 7-state, 4-symbol machine.[26]\n Minsky's book Perceptrons (written with Papert) attacked the work of Frank Rosenblatt, and became the foundational work in the analysis of artificial neural networks. The book is the center of a controversy in the history of AI, as some claim it greatly discouraged research on neural networks in the 1970s and contributed to the so-called \"AI winter\".[27] Minsky also founded several other AI models. His paper \"A framework for representing knowledge\"[28] created a new paradigm in knowledge representation. Perceptrons is now more a historical than practical book, but the theory of frames is in wide use.[29] Minsky also wrote of the possibility that extraterrestrial life may think like humans, permitting communication.[30]\n In the early 1970s, at the MIT Artificial Intelligence Lab, Minsky and Papert started developing what came to be known as the Society of Mind theory. The theory attempts to explain how what we call intelligence could be a product of the interaction of non-intelligent parts. Minsky says that the biggest source of ideas for the theory came from his work in trying to create a machine that uses a robotic arm, a videocamera, and a computer to build with children's blocks. In 1986, he published The Society of Mind, a comprehensive book on the theory which, unlike most of his previously published work, was written for the general public.\n In 2006, Minsky published The Emotion Machine, a book that critiques many popular theories of how human minds work and suggests alternative theories, often replacing simple ideas with more complex ones. Drafts of the book are available on his website.[31]\n Minsky also invented a \"gravity machine\" that will ring a bell if the gravitational constant changes, a theoretical possibility that is not expected to occur in the foreseeable future.[7]\n Minsky was an adviser[32] on Stanley Kubrick's movie 2001: A Space Odyssey; one of the movie's characters, Victor Kaminski, was named in Minsky's honor.[33] Minsky is mentioned explicitly in Arthur C. Clarke's derivative novel of the same name, where he is portrayed as achieving a crucial breakthrough in artificial intelligence in the then-future 1980s, paving the way for HAL 9000 in the early 21st century:\n In the 1980s, Minsky and Good had shown how artificial neural networks could be generated automatically\u2014self replicated\u2014in accordance with any arbitrary learning program. Artificial brains could be grown by a process strikingly analogous to the development of a human brain. In any given case, the precise details would never be known, and even if they were, they would be millions of times too complex for human understanding.[34] In \"The Law of Non-Contradiction\", episode 3 of the television anthology series Fargo (Season 3), at least two allusions to Minsky are made. The first is through the depiction of a \"useless machine\": a device Minsky invented as a philosophical joke. The second is through the depiction of an animation of a robot called \"minsky\"\u2014a character in a sci-fi novel called The Planet Wyh.\n In 1952, Minsky married pediatrician Gloria Rudisch; together they had three children.[35] Minsky was a talented improvisational pianist[36] who published musings on the relations between music and psychology.\n Minsky was an atheist.[37] He was a signatory to the Scientists' Open Letter on Cryonics.[38]\n He was a critic of the Loebner Prize for conversational robots,[39] and argued that a fundamental difference between humans and machines is that while humans are machines, they are machines in which intelligence emerges from the interplay of the many unintelligent but semi-autonomous agents the brain comprises.[40] He argued that \"somewhere down the line, some computers will become more intelligent than most people\", but that it was very hard to predict how fast progress would be.[41] He cautioned that an artificial superintelligence designed to solve an innocuous mathematical problem might decide to assume control of Earth's resources to build supercomputers to help achieve its goal,[42] but believed that such scenarios are \"hard to take seriously\" because he felt confident that AI would be well tested before being deployed.[43]\n Minsky received a $100,000 research grant from Jeffrey Epstein in 2002, four years before Epstein's first arrest for sex offenses; it was the first from Epstein to MIT. Minsky received no further research grants from him.[44][45]\n Minsky organized two academic symposia on Epstein's private island Little Saint James, one in 2002 and another in 2011, after Epstein was a registered sex offender.[46] Virginia Giuffre testified in a 2015 deposition in her defamation lawsuit against Epstein's associate Ghislaine Maxwell that Maxwell \"directed\" her to have sex with Minsky, among others. There has been no allegation that sex between them took place nor a lawsuit against Minsky's estate.[47] Minsky's widow, Gloria Rudisch, says that he could not have had sex with any of the women at Epstein's residences, as they were always together during all the visits to Epstein's residences.[48]\n Minsky died of a cerebral hemorrhage in January 2016, at age 88.[49] Minsky was a member of Alcor Life Extension Foundation's Scientific Advisory Board.[50] Alcor will neither confirm nor deny whether Minsky was cryonically preserved.[51]\n Minsky won the Turing Award (the greatest distinction in computer science)[40] in 1969, the Golden Plate Award of the American Academy of Achievement in 1982,[52] the Japan Prize in 1990,[53] the IJCAI Award for Research Excellence for 1991, and the Benjamin Franklin Medal from the Franklin Institute for 2001.[54] In 2006, he was inducted as a Fellow of the Computer History Museum \"for co-founding the field of artificial intelligence, creating early neural networks and robots, and developing theories of human and machine cognition.\"[55] In 2011, Minsky was inducted into IEEE Intelligent Systems' AI Hall of Fame for the \"significant contributions to the field of AI and intelligent systems\".[56] In 2014, Minsky won the Dan David Prize  for \"Artificial Intelligence, the Digital Mind\".[57] He was also awarded with the 2013 BBVA Foundation Frontiers of Knowledge Award in the Information and Communication Technologies category.[58]\n Minsky was affiliated with the following organizations:\n",
        "doc_number": 35
    },
    {
        "url": "https://en.wikipedia.org/wiki/Artificial_general_intelligence",
        "content": "\n Artificial general intelligence (AGI) is a type of artificial intelligence (AI) that matches or surpasses human cognitive capabilities across a wide range of cognitive tasks. This contrasts with narrow AI, which is limited to specific tasks.[1] Artificial superintelligence (ASI), on the other hand, refers to AGI that greatly exceeds human cognitive capabilities. AGI is considered one of the definitions of strong AI.\n Creating AGI is a primary goal of AI research and of companies such as OpenAI[2] and Meta.[3] A 2020 survey identified 72 active AGI research and development projects across 37 countries.[4]\n The timeline for achieving AGI remains a subject of ongoing debate among researchers and experts. As of 2023, some argue that it may be possible in years or decades; others maintain it might take a century or longer; a minority believe it may never be achieved; and another minority claims that it is already here.[5][6] Notable AI researcher Geoffrey Hinton has expressed concerns about the rapid progress towards AGI, suggesting it could be achieved sooner than many expect.[7]\n There is debate on the exact definition of AGI and regarding whether modern large language models (LLMs) such as GPT-4 are early forms of AGI.[8] AGI is a common topic in science fiction and futures studies.[9][10]\n Contention exists over whether AGI represents an existential risk.[11][12][13] Many experts on AI have stated that mitigating the risk of human extinction posed by AGI should be a global priority.[14][15] Others find the development of AGI to be too remote to present such a risk.[16][17]\n AGI is also known as strong AI,[18][19] full AI,[20] human-level AI,[5] human-level intelligent AI, or general intelligent action.[21]\n Some academic sources reserve the term \"strong AI\" for computer programs that experience sentience or consciousness.[a] In contrast, weak AI (or narrow AI) is able to solve one specific problem but lacks general cognitive abilities.[22][19] Some academic sources use \"weak AI\" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.[a]\n Related concepts include artificial superintelligence and transformative AI. An artificial superintelligence (ASI) is a hypothetical type of AGI that is much more generally intelligent than humans,[23] while the notion of transformative AI relates to AI having a large impact on society, for example, similar to the agricultural or industrial revolution.[24]\n A framework for classifying AGI in levels was proposed in 2023 by Google DeepMind researchers. They define five levels of AGI: emerging, competent, expert, virtuoso, and superhuman. For example, a competent AGI is defined as an AI that outperforms 50% of skilled adults in a wide range of non-physical tasks, and a superhuman AGI (i.e. an artificial superintelligence) is similarly defined but with a threshold of 100%. They consider large language models like ChatGPT or LLaMA 2 to be instances of emerging AGI.[25]\n Various popular definitions of intelligence have been proposed. One of the leading proposals is the Turing test. However, there are other well-known definitions, and some researchers disagree with the more popular approaches. [b]\n However, researchers generally hold that intelligence is required to do all of the following:[27]\n Many interdisciplinary approaches (e.g. cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts)[28] and autonomy.[29]\n Computer-based systems that exhibit many of these capabilities exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent). There is debate about whether modern AI systems possess them to an adequate degree.\n Other capabilities are considered desirable in intelligent systems, as they may affect intelligence or aid in its expression. These include:[30]\n This includes the ability to detect and respond to hazard.[31]\n Although the ability to sense (e.g. see, hear, etc.) and the ability to act (e.g. move and manipulate objects, change location to explore, etc.) can be desirable for some intelligent systems,[30] these physical capabilities are not strictly required for an entity to qualify as AGI\u2014particularly under the thesis that large language models (LLMs) may already be or become AGI. Even from a less optimistic perspective on LLMs, there is no firm requirement for an AGI to have a human-like form; being a silicon-based computational system is sufficient, provided it can process input (language) from the external world in place of human senses. This interpretation aligns with the understanding that AGI has never been proscribed a particular physical embodiment and thus does not demand a capacity for locomotion or traditional \"eyes and ears\".[32]\n Several tests meant to confirm human-level AGI have been considered, including:[33][34]\n The idea of the test is that the machine has to try and pretend to be a man, by answering questions put to it, and it will only pass if the pretence is reasonably convincing. A considerable portion of a jury, who should not be expert about machines, must be taken in by the pretence.[37] A problem is informally called \"AI-complete\" or \"AI-hard\" if it is believed that in order to solve it, one would need to implement AGI, because the solution is beyond the capabilities of a purpose-specific algorithm.[47]\n There are many problems that have been conjectured to require general intelligence to solve as well as humans. Examples include computer vision, natural language understanding, and dealing with unexpected circumstances while solving any real-world problem.[48] Even a specific task like translation requires a machine to read and write in both languages, follow the author's argument (reason), understand the context (knowledge), and faithfully reproduce the author's original intent (social intelligence). All of these problems need to be solved simultaneously in order to reach human-level machine performance.\n However, many of these tasks can now be performed by modern large language models. According to Stanford University's 2024 AI index, AI has reached human-level performance on many benchmarks for reading comprehension and visual reasoning.[49]\n Modern AI research began in the mid-1950s.[50] The first generation of AI researchers were convinced that artificial general intelligence was possible and that it would exist in just a few decades.[51] AI pioneer Herbert A. Simon wrote in 1965: \"machines will be capable, within twenty years, of doing any work a man can do.\"[52]\n Their predictions were the inspiration for Stanley Kubrick and Arthur C. Clarke's character HAL 9000, who embodied what AI researchers believed they could create by the year 2001. AI pioneer Marvin Minsky was a consultant[53] on the project of making HAL 9000 as realistic as possible according to the consensus predictions of the time. He said in 1967, \"Within a generation... the problem of creating 'artificial intelligence' will substantially be solved\".[54]\n Several classical AI projects, such as Doug Lenat's Cyc project (that began in 1984), and Allen Newell's Soar project, were directed at AGI.\n However, in the early 1970s, it became obvious that researchers had grossly underestimated the difficulty of the project. Funding agencies became skeptical of AGI and put researchers under increasing pressure to produce useful \"applied AI\".[c] In the early 1980s, Japan's Fifth Generation Computer Project revived interest in AGI, setting out a ten-year timeline that included AGI goals like \"carry on a casual conversation\".[58] In response to this and the success of expert systems, both industry and government pumped money into the field.[56][59] However, confidence in AI spectacularly collapsed in the late 1980s, and the goals of the Fifth Generation Computer Project were never fulfilled.[60] For the second time in 20 years, AI researchers who predicted the imminent achievement of AGI had been mistaken. By the 1990s, AI researchers had a reputation for making vain promises. They became reluctant to make predictions at all[d] and avoided mention of \"human level\" artificial intelligence for fear of being labeled \"wild-eyed dreamer[s]\".[62]\n In the 1990s and early 21st century, mainstream AI achieved commercial success and academic respectability by focusing on specific sub-problems where AI can produce verifiable results and commercial applications, such as speech recognition and recommendation algorithms.[63] These \"applied AI\" systems are now used extensively throughout the technology industry, and research in this vein is heavily funded in both academia and industry. As of 2018[update], development in this field was considered an emerging trend, and a mature stage was expected to be reached in more than 10 years.[64]\n \nAt the turn of the century, many mainstream AI researchers[65] hoped that strong AI could be developed by combining programs that solve various sub-problems. Hans Moravec wrote in 1988:  I am confident that this bottom-up route to artificial intelligence will one day meet the traditional top-down route more than half way, ready to provide the real-world competence and the commonsense knowledge that has been so frustratingly elusive in reasoning programs. Fully intelligent machines will result when the metaphorical golden spike is driven uniting the two efforts.[65] \nHowever, even at the time, this was disputed. For example, Stevan Harnad of Princeton University concluded his 1990 paper on the symbol grounding hypothesis by stating:  The expectation has often been voiced that \"top-down\" (symbolic) approaches to modeling cognition will somehow meet \"bottom-up\" (sensory) approaches somewhere in between. If the grounding considerations in this paper are valid, then this expectation is hopelessly modular and there is really only one viable route from sense to symbols: from the ground up. A free-floating symbolic level like the software level of a computer will never be reached by this route (or vice versa) \u2013 nor is it clear why we should even try to reach such a level, since it looks as if getting there would just amount to uprooting our symbols from their intrinsic meanings (thereby merely reducing ourselves to the functional equivalent of a programmable computer).[66] The term \"artificial general intelligence\" was used as early as 1997, by Mark Gubrud[67] in a discussion of the implications of fully automated military production and operations. A mathematical formalism of AGI was proposed by Marcus Hutter in 2000. Named AIXI, the proposed AGI agent maximises \"the ability to satisfy goals in a wide range of environments\".[68] This type of AGI, characterized by the ability to maximise a mathematical definition of intelligence rather than exhibit human-like behaviour,[69] was also called universal artificial intelligence.[70]\n The term AGI was re-introduced and popularized by Shane Legg and Ben Goertzel around 2002.[71] AGI research activity in 2006 was described by Pei Wang and Ben Goertzel[72] as \"producing publications and preliminary results\". The first summer school in AGI was organized in Xiamen, China in 2009[73] by the Xiamen university's Artificial Brain Laboratory and OpenCog. The first university course was given in 2010[74] and 2011[75] at Plovdiv University, Bulgaria by Todor Arnaudov. MIT presented a course on AGI in 2018, organized by Lex Fridman and featuring a number of guest lecturers.\n As of 2023[update], a small number of computer scientists are active in AGI research, and many contribute to a series of AGI conferences. However, increasingly more researchers are interested in open-ended learning,[76][77] which is the idea of allowing AI to continuously learn and innovate like humans do.\n As of 2023, the development and potential achievement of AGI remains a subject of intense debate within the AI community. While traditional consensus held that AGI was a distant goal, recent advancements have led some researchers and industry figures to claim that early forms of AGI may already exist.[78] AI pioneer Herbert A. Simon speculated in 1965 that \"machines will be capable, within twenty years, of doing any work a man can do\". This prediction failed to come true. Microsoft co-founder Paul Allen believed that such intelligence is unlikely in the 21st century because it would require \"unforeseeable and fundamentally unpredictable breakthroughs\" and a \"scientifically deep understanding of cognition\".[79] Writing in The Guardian, roboticist Alan Winfield claimed the gulf between modern computing and human-level artificial intelligence is as wide as the gulf between current space flight and practical faster-than-light spaceflight.[80]\n A further challenge is the lack of clarity in defining what intelligence entails. Does it require consciousness? Must it display the ability to set goals as well as pursue them? Is it purely a matter of scale such that if model sizes increase sufficiently, intelligence will emerge? Are facilities such as planning, reasoning, and causal understanding required? Does intelligence require explicitly replicating the brain and its specific faculties? Does it require emotions?[81]\n Most AI researchers believe strong AI can be achieved in the future, but some thinkers, like Hubert Dreyfus and Roger Penrose, deny the possibility of achieving strong AI.[82][83] John McCarthy is among those who believe human-level AI will be accomplished, but that the present level of progress is such that a date cannot accurately be predicted.[84] AI experts' views on the feasibility of AGI wax and wane. Four polls conducted in 2012 and 2013 suggested that the median estimate among experts for when they would be 50% confident AGI would arrive was 2040 to 2050, depending on the poll, with the mean being 2081. Of the experts, 16.5% answered with \"never\" when asked the same question but with a 90% confidence instead.[85][86] Further current AGI progress considerations can be found above Tests for confirming human-level AGI.\n A report by Stuart Armstrong and Kaj Sotala of the Machine Intelligence Research Institute found that \"over [a] 60-year time frame there is a strong bias towards predicting the arrival of human-level AI as between 15 and 25 years from the time the prediction was made\". They analyzed 95 predictions made between 1950 and 2012 on when human-level AI will come about.[87]\n In 2023, Microsoft researchers published a detailed evaluation of GPT-4. They concluded: \"Given the breadth and depth of GPT-4\u2019s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system.\"[88] Another study in 2023 reported that GPT-4 outperforms 99% of humans on the Torrance tests of creative thinking.[89][90]\n Blaise Ag\u00fcera y Arcas and Peter Norvig wrote in 2023 that a significant level of general intelligence has already been achieved with frontier models. They wrote that reluctance to this view comes from four main reasons: a \"healthy skepticism about metrics for AGI\", an \"ideological commitment to alternative AI theories or techniques\", a \"devotion to human (or biological) exceptionalism\", or a \"concern about the economic implications of AGI\".[91]\n 2023 also marked the emergence of large multimodal models (large language models capable of processing or generating multiple modalities such as text, audio, and images).[92]\n In 2024, OpenAI released o1-preview, the first of a series of models that \"spend more time thinking before they respond\". According to Mira Murati, this ability to think before responding represents a new, additional paradigm. It improves model outputs by spending more computing power when generating the answer, whereas the model scaling paradigm improves outputs by increasing the model size, training data and training compute power.[93][94]\n An OpenAI employee, Vahid Kazemi, claimed in 2024 that the company had achieved AGI, stating, \"In my opinion, we have already achieved AGI and it\u2019s even more clear with O1.\" Kazemi clarified that while the AI is not yet \"better than any human at any task\", it is \"better than most humans at most tasks.\" He also addressed criticisms that large language models (LLMs) merely follow predefined patterns, comparing their learning process to the scientific method of observing, hypothesizing, and verifying. These statements have sparked debate, as they rely on a broad and unconventional definition of AGI\u2014traditionally understood as AI that matches human intelligence across all domains. Critics argue that, while OpenAI's models demonstrate remarkable versatility, they may not fully meet this standard. Notably, Kazemi's comments came shortly after OpenAI removed \"AGI\" from the terms of its partnership with Microsoft, prompting speculation about the company\u2019s strategic intentions.[95]\n Progress in artificial intelligence has historically gone through periods of rapid progress separated by periods when progress appeared to stop.[82] Ending each hiatus were fundamental advances in hardware, software or both to create space for further progress.[82][98][99] For example, the computer hardware available in the twentieth century was not sufficient to implement deep learning, which requires large numbers of GPU-enabled CPUs.[100]\n In the introduction to his 2006 book,[101] Goertzel says that estimates of the time needed before a truly flexible AGI is built vary from 10 years to over a century. As of 2007[update], the consensus in the AGI research community seemed to be that the timeline discussed by Ray Kurzweil in 2005 in The Singularity is Near[102] (i.e. between 2015 and 2045) was plausible.[103] Mainstream AI researchers have given a wide range of opinions on whether progress will be this rapid. A 2012 meta-analysis of 95 such opinions found a bias towards predicting that the onset of AGI would occur within 16\u201326 years for modern and historical predictions alike. That paper has been criticized for how it categorized opinions as expert or non-expert.[104]\n In 2012, Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton developed a neural network called AlexNet, which won the ImageNet competition with a top-5 test error rate of 15.3%, significantly better than the second-best entry's rate of 26.3% (the traditional approach used a weighted sum of scores from different pre-defined classifiers).[105] AlexNet was regarded as the initial ground-breaker of the current deep learning wave.[105]\n In 2017, researchers Feng Liu, Yong Shi, and Ying Liu conducted intelligence tests on publicly available and freely accessible weak AI such as Google AI, Apple's Siri, and others. At the maximum, these AIs reached an IQ value of about 47, which corresponds approximately to a six-year-old child in first grade. An adult comes to about 100 on average. Similar tests were carried out in 2014, with the IQ score reaching a maximum value of 27.[106][107]\n In 2020, OpenAI developed GPT-3, a language model capable of performing many diverse tasks without specific training. According to Gary Grossman in a VentureBeat article, while there is consensus that GPT-3 is not an example of AGI, it is considered by some to be too advanced to be classified as a narrow AI system.[108]\n In the same year, Jason Rohrer used his GPT-3 account to develop a chatbot, and provided a chatbot-developing platform called \"Project December\". OpenAI asked for changes to the chatbot to comply with their safety guidelines; Rohrer disconnected Project December from the GPT-3 API.[109]\n In 2022, DeepMind developed Gato, a \"general-purpose\" system capable of performing more than 600 different tasks.[110]\n In 2023, Microsoft Research published a study on an early version of OpenAI's GPT-4, contending that it exhibited more general intelligence than previous AI models and demonstrated human-level performance in tasks spanning multiple domains, such as mathematics, coding, and law. This research sparked a debate on whether GPT-4 could be considered an early, incomplete version of artificial general intelligence, emphasizing the need for further exploration and evaluation of such systems.[111]\n In 2023, the AI researcher Geoffrey Hinton stated that:[112]\n The idea that this stuff could actually get smarter than people \u2013 a few people believed that, [...]. But most people thought it was way off. And I thought it was way off. I thought it was 30 to 50 years or even longer away. Obviously, I no longer think that. In May 2023, Demis Hassabis similarly said that \"The progress in the last few years has been pretty incredible\", and that he sees no reason why it would slow down, expecting AGI within a decade or even a few years.[113] In March 2024, Nvidia's CEO, Jensen Huang, stated his expectation that within five years, AI would be capable of passing any test at least as well as humans.[114] In June 2024, the AI researcher Leopold Aschenbrenner, a former OpenAI employee, estimated AGI by 2027 to be \"strikingly plausible\".[115]\n While the development of transformer models like in ChatGPT is considered the most promising path to AGI,[116][117] whole brain emulation can serve as an alternative approach. With whole brain simulation, a brain model is built by scanning and mapping a biological brain in detail, and then copying and simulating it on a computer system or another computational device. The simulation model must be sufficiently faithful to the original, so that it behaves in practically the same way as the original brain.[118] Whole brain emulation is a type of brain simulation that is discussed in computational neuroscience and neuroinformatics, and for medical research purposes. It has been discussed in artificial intelligence research[103] as an approach to strong AI. Neuroimaging technologies that could deliver the necessary detailed understanding are improving rapidly, and futurist Ray Kurzweil in the book The Singularity Is Near[102] predicts that a map of sufficient quality will become available on a similar timescale to the computing power required to emulate it.\n  For low-level brain simulation, a very powerful cluster of computers or GPUs would be required, given the enormous quantity of synapses within the human brain. Each of the 1011 (one hundred billion) neurons has on average 7,000 synaptic connections (synapses) to other neurons. The brain of a three-year-old child has about 1015 synapses (1 quadrillion). This number declines with age, stabilizing by adulthood. Estimates vary for an adult, ranging from 1014 to 5\u00d71014 synapses (100 to 500 trillion).[120] An estimate of the brain's processing power, based on a simple switch model for neuron activity, is around 1014 (100 trillion) synaptic updates per second (SUPS).[121]\n In 1997, Kurzweil looked at various estimates for the hardware required to equal the human brain and adopted a figure of 1016 computations per second (cps).[e] (For comparison, if a \"computation\" was equivalent to one \"floating-point operation\" \u2013 a measure used to rate current supercomputers \u2013 then 1016 \"computations\" would be equivalent to 10 petaFLOPS, achieved in 2011, while 1018 was achieved in 2022.) He used this figure to predict the necessary hardware would be available sometime between 2015 and 2025, if the exponential growth in computer power at the time of writing continued.\n The Human Brain Project, an EU-funded initiative active from 2013 to 2023, has developed a particularly detailed and publicly accessible atlas of the human brain.[124] In 2023, researchers from Duke University performed a high-resolution scan of a mouse brain.\n The artificial neuron model assumed by Kurzweil and used in many current artificial neural network implementations is simple compared with biological neurons. A brain simulation would likely have to capture the detailed cellular behaviour of biological neurons, presently understood only in broad outline. The overhead introduced by full modeling of the biological, chemical, and physical details of neural behaviour (especially on a molecular scale) would require computational powers several orders of magnitude larger than Kurzweil's estimate. In addition, the estimates do not account for glial cells, which are known to play a role in cognitive processes.[125]\n A fundamental criticism of the simulated brain approach derives from embodied cognition theory which asserts that human embodiment is an essential aspect of human intelligence and is necessary to ground meaning.[126][127] If this theory is correct, any fully functional brain model will need to encompass more than just the neurons (e.g., a robotic body). Goertzel[103] proposes virtual embodiment (like in metaverses like Second Life) as an option, but it is unknown whether this would be sufficient.\n In 1980, philosopher John Searle coined the term \"strong AI\" as part of his Chinese room argument.[128] He proposed a distinction between two hypotheses about artificial intelligence:[f]\n The first one he called \"strong\" because it makes a stronger statement: it assumes something special has happened to the machine that goes beyond those abilities that we can test. The behaviour of a \"weak AI\" machine would be precisely identical to a \"strong AI\" machine, but the latter would also have subjective conscious experience. This usage is also common in academic AI research and textbooks.[129]\n In contrast to Searle and mainstream AI, some futurists such as Ray Kurzweil use the term \"strong AI\" to mean \"human level artificial general intelligence\".[102] This is not the same as Searle's strong AI, unless it is assumed that consciousness is necessary for human-level AGI. Academic philosophers such as Searle do not believe that is the case, and to most artificial intelligence researchers the question is out-of-scope.[130]\n Mainstream AI is most interested in how a program behaves.[131] According to Russell and Norvig, \"as long as the program works, they don't care if you call it real or a simulation.\"[130] If the program can behave as if it has a mind, then there is no need to know if it actually has mind \u2013 indeed, there would be no way to tell. For AI research, Searle's \"weak AI hypothesis\" is equivalent to the statement \"artificial general intelligence is possible\". Thus, according to Russell and Norvig, \"most AI researchers take the weak AI hypothesis for granted, and don't care about the strong AI hypothesis.\"[130] Thus, for academic AI research, \"Strong AI\" and \"AGI\" are two different things.\n Consciousness can have various meanings, and some aspects play significant roles in science fiction and the ethics of artificial intelligence:\n These traits have a moral dimension. AI sentience would give rise to concerns of welfare and legal protection, similarly to animals.[136] Other aspects of consciousness related to cognitive capabilities are also relevant to the concept of AI rights.[137] Figuring out how to integrate advanced AI with existing legal and social frameworks is an emergent issue.[138]\n AGI could have a wide variety of applications. If oriented towards such goals, AGI could help mitigate various problems in the world such as hunger, poverty and health problems.[139]\n AGI could improve productivity and efficiency in most jobs. For example, in public health, AGI could accelerate medical research, notably against cancer.[140] It could take care of the elderly,[141] and democratize access to rapid, high-quality medical diagnostics. It could offer fun, cheap and personalized education.[141] The need to work to subsist could become obsolete if the wealth produced is properly redistributed.[141][142] This also raises the question of the place of humans in a radically automated society.\n AGI could also help to make rational decisions, and to anticipate and prevent disasters. It could also help to reap the benefits of potentially catastrophic technologies such as nanotechnology or climate engineering, while avoiding the associated risks.[143] If an AGI's primary goal is to prevent existential catastrophes such as human extinction (which could be difficult if the Vulnerable World Hypothesis turns out to be true),[144] it could take measures to drastically reduce the risks[143] while minimizing the impact of these measures on our quality of life.\n AGI may represent multiple types of existential risk, which are risks that threaten \"the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development\".[145] The risk of human extinction from AGI has been the topic of many debates, but there is also the possibility that the development of AGI would lead to a permanently flawed future. Notably, it could be used to spread and preserve the set of values of whoever develops it. If humanity still has moral blind spots similar to slavery in the past, AGI might irreversibly entrench it, preventing moral progress.[146] Furthermore, AGI could facilitate mass surveillance and indoctrination, which could be used to create a stable repressive worldwide totalitarian regime.[147][148] There is also a risk for the machines themselves. If machines that are sentient or otherwise worthy of moral consideration are mass created in the future, engaging in a civilizational path that indefinitely neglects their welfare and interests could be an existential catastrophe.[149][150] Considering how much AGI could improve humanity's future and help reduce other existential risks, Toby Ord calls these existential risks \"an argument for proceeding with due caution\", not for \"abandoning AI\".[147]\n The thesis that AI poses an existential risk for humans, and that this risk needs more attention, is controversial but has been endorsed in 2023 by many public figures, AI researchers and CEOs of AI companies such as Elon Musk, Bill Gates, Geoffrey Hinton, Yoshua Bengio, Demis Hassabis and Sam Altman.[151][152]\n In 2014, Stephen Hawking criticized widespread indifference:\n So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, 'We'll arrive in a few decades,' would we just reply, 'OK, call us when you get here\u2014we'll leave the lights on?' Probably not\u2014but this is more or less what is happening with AI.[153] The potential fate of humanity has sometimes been compared to the fate of gorillas threatened by human activities. The comparison states that greater intelligence allowed humanity to dominate gorillas, which are now vulnerable in ways that they could not have anticipated. As a result, the gorilla has become an endangered species, not out of malice, but simply as a collateral damage from human activities.[154]\n The skeptic Yann LeCun considers that AGIs will have no desire to dominate humanity and that we should be careful not to anthropomorphize them and interpret their intents as we would for humans. He said that people won't be \"smart enough to design super-intelligent machines, yet ridiculously stupid to the point of giving it moronic objectives with no safeguards\".[155] On the other side, the concept of instrumental convergence suggests that almost whatever their goals, intelligent agents will have reasons to try to survive and acquire more power as intermediary steps to achieving these goals. And that this does not require having emotions.[156]\n Many scholars who are concerned about existential risk advocate for more research into solving the \"control problem\" to answer the question: what types of safeguards, algorithms, or architectures can programmers implement to maximise the probability that their recursively-improving AI would continue to behave in a friendly, rather than destructive, manner after it reaches superintelligence?[157][158] Solving the control problem is complicated by the AI arms race (which could lead to a race to the bottom of safety precautions in order to release products before competitors),[159] and the use of AI in weapon systems.[160]\n The thesis that AI can pose existential risk also has detractors. Skeptics usually say that AGI is unlikely in the short-term, or that concerns about AGI distract from other issues related to current AI.[161] Former Google fraud czar Shuman Ghosemajumder considers that for many people outside of the technology industry, existing chatbots and LLMs are already perceived as though they were AGI, leading to further misunderstanding and fear.[162]\n Skeptics sometimes charge that the thesis is crypto-religious, with an irrational belief in the possibility of superintelligence replacing an irrational belief in an omnipotent God.[163] Some researchers believe that the communication campaigns on AI existential risk by certain AI groups (such as OpenAI, Anthropic, DeepMind, and Conjecture) may be an at attempt at regulatory capture and to inflate interest in their products.[164][165]\n In 2023, the CEOs of Google DeepMind, OpenAI and Anthropic, along with other industry leaders and researchers, issued a joint statement asserting that \"Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war.\"[152]\n Researchers from OpenAI estimated that \"80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of LLMs, while around 19% of workers may see at least 50% of their tasks impacted\".[166][167] They consider office workers to be the most exposed, for example mathematicians, accountants or web designers.[167] AGI could have a better autonomy, ability to make decisions, to interface with other computer tools, but also to control robotized bodies.\n According to Stephen Hawking, the outcome of automation on the quality of life will depend on how the wealth will be redistributed:[142]\n Everyone can enjoy a life of luxurious leisure if the machine-produced wealth is shared, or most people can end up miserably poor if the machine-owners successfully lobby against wealth redistribution. So far, the trend seems to be toward the second option, with technology driving ever-increasing inequality Elon Musk considers that the automation of society will require governments to adopt a universal basic income.[168]\n",
        "doc_number": 36
    },
    {
        "url": "https://en.wikipedia.org/wiki/Turing_test",
        "content": "\n \n The Turing test, originally called the imitation game by Alan Turing in 1949,[2] is a test of a machine's ability to exhibit intelligent behaviour equivalent to that of a human. In the test, a human evaluator judges a text transcript of a natural-language conversation between a human and a machine. The evaluator tries to identify the machine, and the machine passes if the evaluator cannot reliably tell them apart. The results would not depend on the machine's ability to answer questions correctly, only on how closely its answers resembled those of a human. Since the Turing test is a test of indistinguishability in performance capacity, the verbal version generalizes naturally to all of human performance capacity, verbal as well as nonverbal (robotic).[3]\n The test was introduced by Turing in his 1950 paper \"Computing Machinery and Intelligence\" while working at the University of Manchester.[4] It opens with the words: \"I propose to consider the question, 'Can machines think?'\" Because \"thinking\" is difficult to define, Turing chooses to \"replace the question by another, which is closely related to it and is expressed in relatively unambiguous words\".[5] Turing describes the new form of the problem in terms of a three-person party game called the \"imitation game\", in which an interrogator asks questions of a man and a woman in another room in order to determine the correct sex of the two players. Turing's new question is: \"Are there imaginable digital computers which would do well in the imitation game?\"[2] This question, Turing believed, was one that could actually be answered. In the remainder of the paper, he argued against the major objections to the proposition that \"machines can think\".[6]\n Since Turing introduced his test, it has been highly influential in the philosophy of artificial intelligence, resulting in substantial discussion and controversy, as well as criticism from philosophers like John Searle, who argue against the test's ability to detect consciousness.[7][8]\n Since the early 2020s, several large language models such as ChatGPT have passed modern, rigorous variants of the Turing test.[9][10][11]\n Several early symbolic AI programs were controversially claimed to pass the Turing test, either by limiting themselves to scripted situations or by presenting \"excuses\" for poor reasoning and conversational abilities, such as mental illness or a poor grasp of English.[12][13][14]\n In 1966, Joseph Weizenbaum created a program called ELIZA, which mimicked a Rogerian psychotherapist. The program would search the user's sentence for keywords before repeating them back to the user, providing the impression of a program listening and paying attention.[15] Weizenbaum thus succeeded by designing a context where a chatbot could mimic a person despite \"knowing almost nothing of the real world\".[13] Weizenbaum's program was able to fool some people into believing that they were talking to a real person.[13]\n Kenneth Colby created PARRY in 1972, a program modeled after the behaviour of paranoid schizophrenics.[16] Psychiatrists asked to compare transcripts of conversations generated by the program to those of conversations by actual schizophrenics could only identify about 52 percent of cases correctly (a figure consistent with random guessing).[17]\n In 2001, three programmers developed Eugene Goostman, a chatbot portraying itself as a 13-year old boy from Odesa who spoke English as a second language. This background was intentionally chosen so judges would forgive mistakes by the program. In a competition, 33% of judges thought Goostman was human.[18][19][20]\n In June 2022, Google's LaMDA model received widespread coverage after claims about it having achieved sentience. Initially in an article in The Economist Google Research Fellow Blaise Ag\u00fcera y Arcas said the chatbot had demonstrated a degree of understanding of social relationships.[21] Several days later, Google engineer Blake Lemoine claimed in an interview with the Washington Post that LaMDA had achieved sentience. Lemoine had been placed on leave by Google for internal assertions to this effect. Google had investigated the claims but dismissed them.[22][23]\n OpenAI's chatbot, ChatGPT, was released in November 2022, is based on GPT-3.5 and GPT-4 large language models. Celeste Biever wrote in a Nature article that \"ChatGPT broke the Turing test\".[24] Stanford researchers reported that ChatGPT passes the test; they found that ChatGPT-4 \"passes a rigorous Turing test, diverging from average human behavior chiefly to be more cooperative\",[25] making it the first computer program to successfully do so.[26]\n \nThe question of whether it is possible for machines to think has a long history, which is firmly entrenched in the distinction between dualist and materialist views of the mind. Ren\u00e9 Descartes prefigures aspects of the Turing test in his 1637 Discourse on the Method when he writes:  [H]ow many different automata or moving machines could be made by the industry of man\u00a0... For we can easily understand a machine's being constituted so that it can utter words, and even emit some responses to action on it of a corporeal kind, which brings about a change in its organs; for instance, if touched in a particular part it may ask what we wish to say to it; if in another part it may exclaim that it is being hurt, and so on. But it never happens that it arranges its speech in various ways, in order to reply appropriately to everything that may be said in its presence, as even the lowest type of man can do.[27] Here Descartes notes that automata are capable of responding to human interactions but argues that such automata cannot respond appropriately to things said in their presence in the way that any human can. Descartes therefore prefigures the Turing test by defining the insufficiency of appropriate linguistic response as that which separates the human from the automaton. Descartes fails to consider the possibility that future automata might be able to overcome such insufficiency, and so does not propose the Turing test as such, even if he prefigures its conceptual framework and criterion.\n Denis Diderot formulates in his 1746 book Pens\u00e9es philosophiques a Turing-test criterion, though with the important implicit limiting assumption maintained, of the participants being natural living beings, rather than considering created artifacts:\n If they find a parrot who could answer to everything, I would claim it to be an intelligent being without hesitation. This does not mean he agrees with this, but that it was already a common argument of materialists at that time.\n According to dualism, the mind is non-physical (or, at the very least, has non-physical properties)[28] and, therefore, cannot be explained in purely physical terms. According to materialism, the mind can be explained physically, which leaves open the possibility of minds that are produced artificially.[29]\n In 1936, philosopher Alfred Ayer considered the standard philosophical question of other minds: how do we know that other people have the same conscious experiences that we do? In his book, Language, Truth and Logic, Ayer suggested a protocol to distinguish between a conscious man and an unconscious machine: \"The only ground I can have for asserting that an object which appears to be conscious is not really a conscious being, but only a dummy or a machine, is that it fails to satisfy one of the empirical tests by which the presence or absence of consciousness is determined\".[30] (This suggestion is very similar to the Turing test, but it is not certain that Ayer's popular philosophical classic was familiar to Turing.) In other words, a thing is not conscious if it fails the consciousness test.\n A rudimentary idea of the Turing test appears in the 1726 novel Gulliver's Travels by Jonathan Swift.[31][32] When Gulliver is brought before the king of Brobdingnag, the king thinks at first that Gulliver might be a \"a piece of clock-work (which is in that country arrived to a very great perfection) contrived by some ingenious artist\". Even when he hears Gulliver speaking, the king still doubts whether Gulliver was taught \"a set of words\" to make him \"sell at a better price\". Gulliver tells that only after \"he put several other questions to me, and still received rational answers\" the king became satisfied that Gulliver was not a machine.[33]\n Tests where a human judges whether a computer or an alien is intelligent were an established convention in science fiction by the 1940s, and it is likely that Turing would have been aware of these.[34] Stanley G. Weinbaum's \"A Martian Odyssey\" (1934) provides an example of how nuanced such tests could be.[34]\n Earlier examples of machines or automatons attempting to pass as human include the Ancient Greek myth of Pygmalion who creates a sculpture of a woman that is animated by Aphrodite, Carlo Collodi's novel The Adventures of Pinocchio, about a puppet who wants to become a real boy, and E. T. A. Hoffmann's 1816 story \"The Sandman,\" where the protagonist falls in love with an automaton. In all these examples, people are fooled by artificial beings that - up to a point - pass as human.[35]\n Researchers in the United Kingdom had been exploring \"machine intelligence\" for up to ten years prior to the founding of the field of artificial intelligence (AI) research in 1956.[36] It was a common topic among the members of the Ratio Club, an informal group of British cybernetics and electronics researchers that included Alan Turing.[37]\n Turing, in particular, had been running the notion of machine intelligence since at least 1941[38] and one of the earliest-known mentions of \"computer intelligence\" was made by him in 1947.[39] In Turing's report, \"Intelligent Machinery,\"[40] he investigated \"the question of whether or not it is possible for machinery to show intelligent behaviour\"[41] and, as part of that investigation, proposed what may be considered the forerunner to his later tests:\n It is not difficult to devise a paper machine which will play a not very bad game of chess.[42] Now get three men A, B and C as subjects for the experiment. A and C are to be rather poor chess players, B is the operator who works the paper machine. ... Two rooms are used with some arrangement for communicating moves, and a game is played between C and either A or the paper machine. C may find it quite difficult to tell which he is playing.[43] \"Computing Machinery and Intelligence\" (1950) was the first published paper by Turing to focus exclusively on machine intelligence. Turing begins the 1950 paper with the claim, \"I propose to consider the question 'Can machines think?'\"[5] As he highlights, the traditional approach to such a question is to start with definitions, defining both the terms \"machine\" and \"think\". Turing chooses not to do so; instead, he replaces the question with a new one, \"which is closely related to it and is expressed in relatively unambiguous words\".[5] In essence he proposes to change the question from \"Can machines think?\" to \"Can machines do what we (as thinking entities) can do?\"[44] The advantage of the new question, Turing argues, is that it draws \"a fairly sharp line between the physical and intellectual capacities of a man\".[45]\n To demonstrate this approach Turing proposes a test inspired by a party game, known as the \"imitation game\", in which a man and a woman go into separate rooms and guests try to tell them apart by writing a series of questions and reading the typewritten answers sent back. In this game, both the man and the woman aim to convince the guests that they are the other. (Huma Shah argues that this two-human version of the game was presented by Turing only to introduce the reader to the machine-human question-answer test.[46]) Turing described his new version of the game as follows:\n We now ask the question, \"What will happen when a machine takes the part of A in this game?\" Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? These questions replace our original, \"Can machines think?\"[45] Later in the paper, Turing suggests an \"equivalent\" alternative formulation involving a judge conversing only with a computer and a man.[47] While neither of these formulations precisely matches the version of the Turing test that is more generally known today, he proposed a third in 1952. In this version, which Turing discussed in a BBC radio broadcast, a jury asks questions of a computer and the role of the computer is to make a significant proportion of the jury believe that it is really a man.[48]\n Turing's paper considered nine putative objections, which include some of the major arguments against artificial intelligence that have been raised in the years since the paper was published (see \"Computing Machinery and Intelligence\").[6]\n John Searle's 1980 paper Minds, Brains, and Programs proposed the \"Chinese room\" thought experiment and argued that the Turing test could not be used to determine if a machine could think. Searle noted that software (such as ELIZA) could pass the Turing test simply by manipulating symbols of which they had no understanding. Without understanding, they could not be described as \"thinking\" in the same sense people did. Therefore, Searle concluded, the Turing test could not prove that machines could think.[49] Much like the Turing test itself, Searle's argument has been both widely criticised[50] and endorsed.[51]\n Arguments such as Searle's and others working on the philosophy of mind sparked off a more intense debate about the nature of intelligence, the possibility of machines with a conscious mind and the value of the Turing test that continued through the 1980s and 1990s.[52]\n The Loebner Prize provides an annual platform for practical Turing tests with the first competition held in November 1991.[53] It is underwritten by Hugh Loebner. The Cambridge Center for Behavioral Studies in Massachusetts, United States, organised the prizes up to and including the 2003 contest. As Loebner described it, one reason the competition was created is to advance the state of AI research, at least in part, because no one had taken steps to implement the Turing test despite 40 years of discussing it.[54]\n The first Loebner Prize competition in 1991 led to a renewed discussion of the viability of the Turing test and the value of pursuing it, in both the popular press[55] and academia.[56] The first contest was won by a mindless program with no identifiable intelligence that managed to fool na\u00efve interrogators into making the wrong identification. This highlighted several of the shortcomings of the Turing test (discussed below): The winner won, at least in part, because it was able to \"imitate human typing errors\";[55] the unsophisticated interrogators were easily fooled;[56] and some researchers in AI have been led to feel that the test is merely a distraction from more fruitful research.[57]\n The silver (text only) and gold (audio and visual) prizes have never been won. However, the competition has awarded the bronze medal every year for the computer system that, in the judges' opinions, demonstrates the \"most human\" conversational behaviour among that year's entries. Artificial Linguistic Internet Computer Entity (A.L.I.C.E.) has won the bronze award on three occasions in recent times (2000, 2001, 2004). Learning AI Jabberwacky won in 2005 and 2006.\n The Loebner Prize tests conversational intelligence; winners are typically chatterbot programs, or Artificial Conversational Entities (ACE)s. Early Loebner Prize rules restricted conversations: Each entry and hidden-human conversed on a single topic,[58] thus the interrogators were restricted to one line of questioning per entity interaction. The restricted conversation rule was lifted for the 1995 Loebner Prize. Interaction duration between judge and entity has varied in Loebner Prizes. In Loebner 2003, at the University of Surrey, each interrogator was allowed five minutes to interact with an entity, machine or hidden-human. Between 2004 and 2007, the interaction time allowed in Loebner Prizes was more than twenty minutes.\n CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) is one of the oldest concepts for artificial intelligence. The CAPTCHA system is commonly used online to tell humans and bots apart on the internet. It is based on the Turing test. Displaying distorted letters and numbers, it asks the user to identify the letters and numbers and type them into a field, which bots struggle to do.[18][59]\n The reCaptcha is a CAPTCHA system owned by Google. The reCaptcha v1 and v2 both used to operate by asking the user to match distorted pictures or identify distorted letters and numbers. The reCaptcha v3 is designed to not interrupt users and run automatically when pages are loaded or buttons are clicked. This \"invisible\" CAPTCHA verification happens in the background and no challenges appear, which filters out most basic bots.[60][61]\n Saul Traiger argues that there are at least three primary versions of the Turing test, two of which are offered in \"Computing Machinery and Intelligence\" and one that he describes as the \"Standard Interpretation\".[62] While there is some debate regarding whether the \"Standard Interpretation\" is that described by Turing or, instead, based on a misreading of his paper, these three versions are not regarded as equivalent,[62] and their strengths and weaknesses are distinct.[63]\n Turing's original article describes a simple party game involving three players. Player A is a man, player B is a woman and player C (who plays the role of the interrogator) is of either gender. In the imitation game, player C is unable to see either player A or player B, and can communicate with them only through written notes. By asking questions of player A and player B, player C tries to determine which of the two is the man and which is the woman. Player A's role is to trick the interrogator into making the wrong decision, while player B attempts to assist the interrogator in making the right one.[7]\n Turing then asks:\n \"What will happen when a machine takes the part of A in this game? Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman?\" These questions replace our original, \"Can machines think?\"[45] The second version appeared later in Turing's 1950 paper. Similar to the original imitation game test, the role of player A is performed by a computer. However, the role of player B is performed by a man rather than a woman.\n Let us fix our attention on one particular digital computer C. Is it true that by modifying this computer to have an adequate storage, suitably increasing its speed of action, and providing it with an appropriate programme, C can be made to play satisfactorily the part of A in the imitation game, the part of B being taken by a man?[45] In this version, both player A (the computer) and player B are trying to trick the interrogator into making an incorrect decision.\n The standard interpretation is not included in the original paper, but is both accepted and debated.\nCommon understanding has it that the purpose of the Turing test is not specifically to determine whether a computer is able to fool an interrogator into believing that it is a human, but rather whether a computer could imitate a human.[7] While there is some dispute whether this interpretation was intended by Turing, Sterrett believes that it was[64] and thus conflates the second version with this one, while others, such as Traiger, do not[62]\u00a0\u2013 this has nevertheless led to what can be viewed as the \"standard interpretation\". In this version, player A is a computer and player B a person of either sex. The role of the interrogator is not to determine which is male and which is female, but which is a computer and which is a human.[65] The fundamental issue with the standard interpretation is that the interrogator cannot differentiate which responder is human, and which is machine. There are issues about duration, but the standard interpretation generally considers this limitation as something that should be reasonable.\n Controversy has arisen over which of the alternative formulations of the test Turing intended.[64] Sterrett argues that two distinct tests can be extracted from his 1950 paper and that, pace Turing's remark, they are not equivalent. The test that employs the party game and compares frequencies of success is referred to as the \"Original Imitation Game Test\", whereas the test consisting of a human judge conversing with a human and a machine is referred to as the \"Standard Turing Test\", noting that Sterrett equates this with the \"standard interpretation\" rather than the second version of the imitation game. Sterrett agrees that the standard Turing test (STT) has the problems that its critics cite but feels that, in contrast, the original imitation game test (OIG test) so defined is immune to many of them, due to a crucial difference: Unlike the STT, it does not make similarity to human performance the criterion, even though it employs human performance in setting a criterion for machine intelligence. A man can fail the OIG test, but it is argued that it is a virtue of a test of intelligence that failure indicates a lack of resourcefulness: The OIG test requires the resourcefulness associated with intelligence and not merely \"simulation of human conversational behaviour\". The general structure of the OIG test could even be used with non-verbal versions of imitation games.[66]\n According to Huma Shah, Turing himself was concerned with whether a machine could think and was providing a simple method to examine this: through human-machine question-answer sessions.[67] Shah argues the imitation game which Turing described could be practicalized in two different ways: a) one-to-one interrogator-machine test, and b) simultaneous comparison of a machine with a human, both questioned in parallel by an interrogator.[46]\n Still other writers[68] have interpreted Turing as proposing that the imitation game itself is the test, without specifying how to take into account Turing's statement that the test that he proposed using the party version of the imitation game is based upon a criterion of comparative frequency of success in that imitation game, rather than a capacity to succeed at one round of the game.\n \nSome writers argue that the imitation game is best understood by its social aspects. In his 1948 paper, Turing refers to intelligence as an \"emotional concept,\" and notes that  The extent to which we regard something as behaving in an intelligent manner is determined as much by our own state of mind and training as by the properties of the object under consideration. If we are able to explain and predict its behaviour or if there seems to be little underlying plan, we have little temptation to imagine intelligence. With the same object therefore it is possible that one man would consider it as intelligent and another would not; the second man would have found out the rules of its behaviour.[69] Following this remark and similar ones scattered throughout Turing's publications, Diane Proudfoot[70] claims that Turing held a response-dependence approach to intelligence, according to which an intelligent (or thinking) entity is one that appears intelligent to an average interrogator. Shlomo Danziger[71] promotes a socio-technological interpretation, according to which Turing saw the imitation game not as an intelligence test but as a technological aspiration - one whose realization would likely involve a change in society's attitude toward machines. According to this reading, Turing's celebrated 50-year prediction - that by the end of the 20th century his test will be passed by some machine - actually consists of two distinguishable predictions. The first is a technological prediction: I believe that in about fifty years' time it will be possible to programme computers ... to make them play the imitation game so well that an average interrogator will not have more than 70% chance of making the right identification after five minutes of questioning.[72] The second prediction Turing makes is a sociological one: I believe that at the end of the century the use of words and general educated opinion will have altered so much that one will be able to speak of machines thinking without expecting to be contradicted.[72] Danziger claims further that for Turing, alteration of society's attitude towards machinery is a prerequisite for the existence of intelligent machines: Only when the term \"intelligent machine\" is no longer seen as an oxymoron the existence of intelligent machines would become logically possible.\n Saygin has suggested that maybe the original game is a way of proposing a less biased experimental design as it hides the participation of the computer.[73] The imitation game also includes a \"social hack\" not found in the standard interpretation, as in the game both computer and male human are required to play as pretending to be someone they are not.[74]\n A crucial piece of any laboratory test is that there should be a control. Turing never makes clear whether the interrogator in his tests is aware that one of the participants is a computer. He states only that player A is to be replaced with a machine, not that player C is to be made aware of this replacement.[45] When Colby, FD Hilf, S Weber and AD Kramer tested PARRY, they did so by assuming that the interrogators did not need to know that one or more of those being interviewed was a computer during the interrogation.[75] As Ayse Saygin, Peter Swirski,[76] and others have highlighted, this makes a big difference to the implementation and outcome of the test.[7] An experimental study looking at Gricean maxim violations using transcripts of Loebner's one-to-one (interrogator-hidden interlocutor) Prize for AI contests between 1994 and 1999, Ayse Saygin found significant differences between the responses of participants who knew and did not know about computers being involved.[77]\n The power and appeal of the Turing test derives from its simplicity. The philosophy of mind, psychology, and modern neuroscience have been unable to provide definitions of \"intelligence\" and \"thinking\" that are sufficiently precise and general to be applied to machines. Without such definitions, the central questions of the philosophy of artificial intelligence cannot be answered. The Turing test, even if imperfect, at least provides something that can actually be measured. As such, it is a pragmatic attempt to answer a difficult philosophical question.\n The format of the test allows the interrogator to give the machine a wide variety of intellectual tasks. Turing wrote that \"the question and answer method seems to be suitable for introducing almost any one of the fields of human endeavour that we wish to include\".[78] John Haugeland adds that \"understanding the words is not enough; you have to understand the topic as well\".[79]\n To pass a well-designed Turing test, the machine must use natural language, reason, have knowledge and learn. The test can be extended to include video input, as well as a \"hatch\" through which objects can be passed: this would force the machine to demonstrate skilled use of well designed vision and robotics as well. Together, these represent almost all of the major problems that artificial intelligence research would like to solve.[80]\n The Feigenbaum test is designed to take advantage of the broad range of topics available to a Turing test. It is a limited form of Turing's question-answer game which compares the machine against the abilities of experts in specific fields such as literature or chemistry.\n As a Cambridge honours graduate in mathematics, Turing might have been expected to propose a test of computer intelligence requiring expert knowledge in some highly technical field, and thus anticipating a more recent approach to the subject. Instead, as already noted, the test which he described in his seminal 1950 paper requires the computer to be able to compete successfully in a common party game, and this by performing as well as the typical man in answering a series of questions so as to pretend convincingly to be the woman contestant.\n Given the status of human sexual dimorphism as one of the most ancient of subjects, it is thus implicit in the above scenario that the questions to be answered will involve neither specialised factual knowledge nor information processing technique. The challenge for the computer, rather, will be to demonstrate empathy for the role of the female, and to demonstrate as well a characteristic aesthetic sensibility\u2014both of which qualities are on display in this snippet of dialogue which Turing has imagined:\n When Turing does introduce some specialised knowledge into one of his imagined dialogues, the subject is not maths or electronics, but poetry:\n Turing thus once again demonstrates his interest in empathy and aesthetic sensitivity as components of an artificial intelligence; and in light of an increasing awareness of the threat from an AI run amok,[81] it has been suggested[82] that this focus perhaps represents a critical intuition on Turing's part, i.e., that emotional and aesthetic intelligence will play a key role in the creation of a \"friendly AI\". It is further noted, however, that whatever inspiration Turing might be able to lend in this direction depends upon the preservation of his original vision, which is to say, further, that the promulgation of a \"standard interpretation\" of the Turing test\u2014i.e., one which focuses on a discursive intelligence only\u2014must be regarded with some caution.\n Turing did not explicitly state that the Turing test could be used as a measure of \"intelligence\", or any other human quality. He wanted to provide a clear and understandable alternative to the word \"think\", which he could then use to reply to criticisms of the possibility of \"thinking machines\" and to suggest ways that research might move forward.\n Nevertheless, the Turing test has been proposed as a measure of a machine's \"ability to think\" or its \"intelligence\". This proposal has received criticism from both philosophers and computer scientists. The interpretation makes the assumption that an interrogator can determine if a machine is \"thinking\" by comparing its behaviour with human behaviour. Every element of this assumption has been questioned: the reliability of the interrogator's judgement, the value of comparing the machine with a human, and the value of comparing only behaviour. Because of these and other considerations, some AI researchers have questioned the relevance of the test to their field.\n In practice, the test's results can easily be dominated not by the computer's intelligence, but by the attitudes, skill, or na\u00efvet\u00e9 of the questioner. Numerous experts in the field, including cognitive scientist Gary Marcus, insist that the Turing test only shows how easy it is to fool humans and is not an indication of machine intelligence.[83]\n Turing doesn't specify the precise skills and knowledge required by the interrogator in his description of the test, but he did use the term \"average interrogator\": \"[the] average interrogator would not have more than 70 per cent chance of making the right identification after five minutes of questioning\".[72]\n Chatterbot programs such as ELIZA have repeatedly fooled unsuspecting people into believing that they are communicating with human beings. In these cases, the \"interrogators\" are not even aware of the possibility that they are interacting with computers. To successfully appear human, there is no need for the machine to have any intelligence whatsoever and only a superficial resemblance to human behaviour is required.\n Early Loebner Prize competitions used \"unsophisticated\" interrogators who were easily fooled by the machines.[56] Since 2004, the Loebner Prize organisers have deployed philosophers, computer scientists, and journalists among the interrogators. Nonetheless, some of these experts have been deceived by the machines.[84]\n One interesting feature of the Turing test is the frequency of the confederate effect, when the confederate (tested) humans are misidentified by the interrogators as machines. It has been suggested that what interrogators expect as human responses is not necessarily typical of humans. As a result, some individuals can be categorised as machines. This can therefore work in favour of a competing machine. The humans are instructed to \"act themselves\", but sometimes their answers are more like what the interrogator expects a machine to say.[85] This raises the question of how to ensure that the humans are motivated to \"act human\".\n The Turing test does not directly test whether the computer behaves intelligently. It tests only whether the computer behaves like a human being. Since human behaviour and intelligent behaviour are not exactly the same thing, the test can fail to accurately measure intelligence in two ways:\n The Turing test is concerned strictly with how the subject acts\u00a0\u2013 the external behaviour of the machine. In this regard, it takes a behaviourist or functionalist approach to the study of the mind. The example of ELIZA suggests that a machine passing the test may be able to simulate human conversational behaviour by following a simple (but large) list of mechanical rules, without thinking or having a mind at all.\n John Searle has argued that external behaviour cannot be used to determine if a machine is \"actually\" thinking or merely \"simulating thinking\".[49] His Chinese room argument is intended to show that, even if the Turing test is a good operational definition of intelligence, it may not indicate that the machine has a mind, consciousness, or intentionality. (Intentionality is a philosophical term for the power of thoughts to be \"about\" something.)\n \nTuring anticipated this line of criticism in his original paper,[89] writing:  I do not wish to give the impression that I think there is no mystery about consciousness. There is, for instance, something of a paradox connected with any attempt to localise it. But I do not think these mysteries necessarily need to be solved before we can answer the question with which we are concerned in this paper.[90]  Mainstream AI researchers argue that trying to pass the Turing test is merely a distraction from more fruitful research.[57] Indeed, the Turing test is not an active focus of much academic or commercial effort\u2014as Stuart Russell and Peter Norvig write: \"AI researchers have devoted little attention to passing the Turing test\".[91] There are several reasons.\n First, there are easier ways to test their programs. Most current research in AI-related fields is aimed at modest and specific goals, such as object recognition or logistics. To test the intelligence of the programs that solve these problems, AI researchers simply give them the task directly. Stuart Russell and Peter Norvig suggest an analogy with the history of flight: Planes are tested by how well they fly, not by comparing them to birds. \"Aeronautical engineering texts,\" they write, \"do not define the goal of their field as 'making machines that fly so exactly like pigeons that they can fool other pigeons.'\"[91]\n Second, creating lifelike simulations of human beings is a difficult problem on its own that does not need to be solved to achieve the basic goals of AI research. Believable human characters may be interesting in a work of art, a game, or a sophisticated user interface, but they are not part of the science of creating intelligent machines, that is, machines that solve problems using intelligence.\n Turing did not intend for his idea to be used to test the intelligence of programs\u2014he wanted to provide a clear and understandable example to aid in the discussion of the philosophy of artificial intelligence.[92] John McCarthy argues that we should not be surprised that a philosophical idea turns out to be useless for practical applications. He observes that the philosophy of AI is \"unlikely to have any more effect on the practice of AI research than philosophy of science generally has on the practice of science\".[93][94]\n Another well known objection raised towards the Turing test concerns its exclusive focus on linguistic behaviour (i.e. it is only a \"language-based\" experiment, while all the other cognitive faculties are not tested). This drawback downsizes the role of other modality-specific \"intelligent abilities\" concerning human beings that the psychologist Howard Gardner, in his \"multiple intelligence theory\", proposes to consider (verbal-linguistic abilities are only one of those).[95]\n A critical aspect of the Turing test is that a machine must give itself away as being a machine by its utterances. An interrogator must then make the \"right identification\" by correctly identifying the machine as being just that. If, however, a machine remains silent during a conversation, then it is not possible for an interrogator to accurately identify the machine other than by means of a calculated guess.[96]\nEven taking into account a parallel/hidden human as part of the test may not help the situation as humans can often be misidentified as being a machine.[97]\n By focusing on imitating humans, rather than augmenting or extending human capabilities, the Turing Test risks directing research and implementation toward technologies that substitute for humans and thereby drive down wages and income for workers. As they lose economic power, these workers may also lose political power, making it more difficult for them to change the allocation of wealth and income. This can trap them in a bad equilibrium. Erik Brynjolfsson has called this \"The Turing Trap\"[98] and argued that there are currently excess incentives for creating machines that imitate rather than augment humans.\n Numerous other versions of the Turing test, including those expounded above, have been raised through the years.\n A modification of the Turing test wherein the objective of one or more of the roles have been reversed between machines and humans is termed a reverse Turing test. An example is implied in the work of psychoanalyst Wilfred Bion,[99] who was particularly fascinated by the \"storm\" that resulted from the encounter of one mind by another. In his 2000 book,[76] among several other original points with regard to the Turing test, literary scholar Peter Swirski discussed in detail the idea of what he termed the Swirski test\u2014essentially the reverse Turing test. He pointed out that it overcomes most if not all standard objections levelled at the standard version.\n Carrying this idea forward, R. D. Hinshelwood[100] described the mind as a \"mind recognizing apparatus\". The challenge would be for the computer to be able to determine if it were interacting with a human or another computer. This is an extension of the original question that Turing attempted to answer but would, perhaps, offer a high enough standard to define a machine that could \"think\" in a way that we typically define as characteristically human.\n CAPTCHA is a form of reverse Turing test. Before being allowed to perform some action on a website, the user is presented with alphanumerical characters in a distorted graphic image and asked to type them out. This is intended to prevent automated systems from being used to abuse the site. The rationale is that software sufficiently sophisticated to read and reproduce the distorted image accurately does not exist (or is not available to the average user), so any system able to do so is likely to be a human.\n Software that could reverse CAPTCHA with some accuracy by analysing patterns in the generating engine started being developed soon after the creation of CAPTCHA.[101]\nIn 2013, researchers at Vicarious announced that they had developed a system to solve CAPTCHA challenges from Google, Yahoo!, and PayPal up to 90% of the time.[102]\nIn 2014, Google engineers demonstrated a system that could defeat CAPTCHA challenges with 99.8% accuracy.[103]\nIn 2015, Shuman Ghosemajumder, former click fraud czar of Google, stated that there were cybercriminal sites that would defeat CAPTCHA challenges for a fee, to enable various forms of fraud.[104]\n A further variation is motivated by the concern that modern Natural Language Processing prove to be highly successful in generating text on the basis of a huge text corpus and could eventually pass the Turing test simply by manipulating words and sentences that have been used in the initial training of the model. Since the interrogator has no precise understanding of the training data, the model might simply be returning sentences that exist in similar fashion in the enormous amount of training data. For this reason, Arthur Schwaninger proposes a variation of the Turing test that can distinguish between systems that are only capable of using language and systems that understand language. He proposes a test in which the machine is confronted with philosophical questions that do not depend on any prior knowledge and yet require self-reflection to be answered appropriately.[105]\n Another variation is described as the subject-matter expert Turing test, where a machine's response cannot be distinguished from an expert in a given field. This is also known as a \"Feigenbaum test\" and was proposed by Edward Feigenbaum in a 2003 paper.[106]\n Robert French (1990) makes the case that an interrogator can distinguish human and non-human interlocutors by posing questions that reveal the low-level (i.e., unconscious) processes of human cognition, as studied by cognitive science. Such questions reveal the precise details of the human embodiment of thought and can unmask a computer unless it experiences the world as humans do.[107]\n The \"Total Turing test\"[3] variation of the Turing test, proposed by cognitive scientist Stevan Harnad,[108] adds two further requirements to the traditional Turing test. The interrogator can also test the perceptual abilities of the subject (requiring computer vision) and the subject's ability to manipulate objects (requiring robotics).[109]\n A letter published in Communications of the ACM[110] describes the concept of generating a synthetic patient population and proposes a variation of Turing test to assess the difference between synthetic and real patients. The letter states: \"In the EHR context, though a human physician can readily distinguish between synthetically generated and real live human patients, could a machine be given the intelligence to make such a determination on its own?\" and further the letter states: \"Before synthetic patient identities become a public health problem, the legitimate EHR market might benefit from applying Turing Test-like techniques to ensure greater data reliability and diagnostic value. Any new techniques must thus consider patients' heterogeneity and are likely to have greater complexity than the Allen eighth-grade-science-test is able to grade\".\n The minimum intelligent signal test was proposed by Chris McKinstry as \"the maximum abstraction of the Turing test\",[111] in which only binary responses (true/false or yes/no) are permitted, to focus only on the capacity for thought. It eliminates text chat problems like anthropomorphism bias, and does not require emulation of unintelligent human behaviour, allowing for systems that exceed human intelligence. The questions must each stand on their own, however, making it more like an IQ test than an interrogation. It is typically used to gather statistical data against which the performance of artificial intelligence programs may be measured.[112]\n The organisers of the Hutter Prize believe that compressing natural language text is a hard AI problem, equivalent to passing the Turing test. The data compression test has some advantages over most versions and variations of a Turing test, including:[citation needed]\n The main disadvantages of using data compression as a test are:\n A related approach to Hutter's prize which appeared much earlier in the late 1990s is the inclusion of compression problems in an extended Turing test.[113] or by tests which are completely derived from Kolmogorov complexity.[114]\nOther related tests in this line are presented by Hernandez-Orallo and Dowe.[115]\n Algorithmic IQ, or AIQ for short, is an attempt to convert the theoretical Universal Intelligence Measure from Legg and Hutter (based on Solomonoff's inductive inference) into a working practical test of machine intelligence.[116]\n Two major advantages of some of these tests are their applicability to nonhuman intelligences and their absence of a requirement for human testers.\n The Turing test inspired the Ebert test proposed in 2011 by film critic Roger Ebert which is a test whether a computer-based synthesised voice has sufficient skill in terms of intonations, inflections, timing and so forth, to make people laugh.[117]\n Taking advantage of large language models, in 2023 the research company AI21 Labs created an online social experiment titled \"Human or Not?\"[118][119]  It was played more than 10 million times by more than 2 million people.[120] It is the biggest Turing-style experiment to that date. The results showed that 32% of people could not distinguish between humans and machines.[121][122]\n 1990 marked the fortieth anniversary of the first publication of Turing's \"Computing Machinery and Intelligence\" paper, and saw renewed interest in the test. Two significant events occurred in that year: the first was the Turing Colloquium, which was held at the University of Sussex in April, and brought together academics and researchers from a wide variety of disciplines to discuss the Turing test in terms of its past, present, and future; the second was the formation of the annual Loebner Prize competition.\n Blay Whitby lists four major turning points in the history of the Turing test\u00a0\u2013 the publication of \"Computing Machinery and Intelligence\" in 1950, the announcement of Joseph Weizenbaum's ELIZA in 1966, Kenneth Colby's creation of PARRY, which was first described in 1972, and the Turing Colloquium in 1990.[123]\n In parallel to the 2008 Loebner Prize held at the University of Reading,[124]\nthe Society for the Study of Artificial Intelligence and the Simulation of Behaviour (AISB), hosted a one-day symposium to discuss the Turing test, organised by John Barnden, Mark Bishop, Huma Shah and Kevin Warwick.[125]\nThe speakers included the Royal Institution's Director Baroness Susan Greenfield, Selmer Bringsjord, Turing's biographer Andrew Hodges, and consciousness scientist Owen Holland. No agreement emerged for a canonical Turing test, though Bringsjord expressed that a sizeable prize would result in the Turing test being passed sooner.\n",
        "doc_number": 37
    },
    {
        "url": "https://en.wikipedia.org/wiki/Chinese_room",
        "content": "\n The Chinese room argument holds that a computer executing a program cannot have a mind, understanding, or consciousness,[a] regardless of how intelligently or human-like the program may make the computer behave. The argument was presented in a 1980 paper by the philosopher John Searle entitled \"Minds, Brains, and Programs\" and published in the journal Behavioral and Brain Sciences.[1] Before Searle, similar arguments had been presented by figures including Gottfried Wilhelm Leibniz (1714), Anatoly Dneprov (1961), Lawrence Davis (1974) and Ned Block (1978). Searle's version has been widely discussed in the years since.[2] The centerpiece of Searle's argument is a thought experiment known as the Chinese room.[3]\n The thought experiment starts by placing a computer that can perfectly converse in Chinese in one room, and a human that only knows English in another, with a door separating them. Chinese characters are written and placed on a piece of paper underneath the door, and the computer can reply fluently, slipping the reply underneath the door. The human is then given English instructions which replicate the instructions and function of the computer program to converse in Chinese. The human follows the instructions and the two rooms can perfectly communicate in Chinese, but the human still does not actually understand the characters, merely following instructions to converse. Searle states that both the computer and human are doing identical tasks, following instructions without truly understanding or \"thinking\".\n The argument is directed against the philosophical positions of functionalism and computationalism,[4] which hold that the mind may be viewed as an information-processing system operating on formal symbols, and that simulation of a given mental state is sufficient for its presence. Specifically, the argument is intended to refute a position Searle calls the strong AI hypothesis:[b] \"The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.\"[c]\n Although its proponents originally presented the argument in reaction to statements of artificial intelligence (AI) researchers, it is not an argument against the goals of mainstream AI research because it does not show a limit in the amount of intelligent behavior a machine can display.[5] The argument applies only to digital computers running programs and does not apply to machines in general.[6] While widely discussed, the argument has been subject to significant criticism and remains controversial among philosophers of mind and AI researchers.[7][8]\n Suppose that artificial intelligence research has succeeded in programming a computer to behave as if it understands Chinese. The machine accepts Chinese characters as input, carries out each instruction of the program step by step, and then produces Chinese characters as output. The machine does this so perfectly that no one can tell that they are communicating with a machine and not a hidden Chinese speaker.\n The questions at issue are these: does the machine actually understand the conversation, or is it just simulating the ability to understand the conversation? Does the machine have a mind in exactly the same sense that people do, or is it just acting as if it has a mind?\n Now suppose that Searle is in a room with an English version of the program, along with sufficient pencils, paper, erasers and filing cabinets. Chinese characters are slipped in under the door, he follows the program step-by-step, which eventually instructs him to slide other Chinese characters back out under the door. If the computer had passed the Turing test this way, it follows that Searle would do so as well, simply by running the program by hand.\n Searle asserts that there is no essential difference between the roles of the computer and himself in the experiment. Each simply follows a program, step-by-step, producing behavior that makes them appear to understand. However, Searle would not be able to understand the conversation. Therefore, he argues, it follows that the computer would not be able to understand the conversation either.\n Searle argues that, without \"understanding\" (or \"intentionality\"), we cannot describe what the machine is doing as \"thinking\" and, since it does not think, it does not have a \"mind\" in the normal sense of the word. Therefore, he concludes that the strong AI hypothesis is false: a computer running a program that simulates a mind would not have a mind in the same sense that human beings have a mind.\n Gottfried Leibniz made a similar argument in 1714 against mechanism (the idea that everything that makes up a human being could, in principle, be explained in mechanical terms. In other words, that a person, including their mind, is merely a very complex machine). Leibniz used the thought experiment of expanding the brain until it was the size of a mill.[9] Leibniz found it difficult to imagine that a \"mind\" capable of \"perception\" could be constructed using only mechanical processes.[d]\n Peter Winch made the same point in his book The Idea of a Social Science and its Relation to Philosophy (1958), where he provides an argument to show that \"a man who understands Chinese is not a man who has a firm grasp of the statistical probabilities for the occurrence of the various words in the Chinese language\" (p.\u00a0108).\n Soviet cyberneticist Anatoly Dneprov made an essentially identical argument in 1961, in the form of the short story \"The Game\". In it, a stadium of people act as switches and memory cells implementing a program to translate a sentence of Portuguese, a language that none of them know.[10] The game was organized by a \"Professor Zarubin\" to answer the question \"Can mathematical machines think?\" Speaking through Zarubin, Dneprov writes \"the only way to prove that machines can think is to turn yourself into a machine and examine your thinking process\" and he concludes, as Searle does, \"We've proven that even the most perfect simulation of machine thinking is not the thinking process itself.\"\n In 1974, Lawrence H. Davis imagined duplicating the brain using telephone lines and offices staffed by people, and in 1978 Ned Block envisioned the entire population of China involved in such a brain simulation. This thought experiment is called the China brain, also the \"Chinese Nation\" or the \"Chinese Gym\".[11]\n Searle's version appeared in his 1980 paper \"Minds, Brains, and Programs\", published in Behavioral and Brain Sciences.[1] It eventually became the journal's \"most influential target article\",[2] generating an enormous number of commentaries and responses in the ensuing decades, and Searle has continued to defend and refine the argument in many papers, popular articles and books. David Cole writes that \"the Chinese Room argument has probably been the most widely discussed philosophical argument in cognitive science to appear in the past 25 years\".[12]\n Most of the discussion consists of attempts to refute it. \"The overwhelming majority\", notes Behavioral and Brain Sciences editor Stevan Harnad,[e] \"still think that the Chinese Room Argument is dead wrong\".[13] The sheer volume of the literature that has grown up around it inspired Pat Hayes to comment that the field of cognitive science ought to be redefined as \"the ongoing research program of showing Searle's Chinese Room Argument to be false\".[14]\n Searle's argument has become \"something of a classic in cognitive science\", according to Harnad.[13] Varol Akman agrees, and has described the original paper as \"an exemplar of philosophical clarity and purity\".[15]\n Although the Chinese Room argument was originally presented in reaction to the statements of artificial intelligence researchers, philosophers have come to consider it as an important part of the philosophy of mind. It is a challenge to functionalism and the computational theory of mind,[f] and is related to such questions as the mind\u2013body problem, the problem of other minds, the symbol grounding problem, and the hard problem of consciousness.[a]\n Searle identified a philosophical position he calls \"strong AI\":\n The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.[c]\n The definition depends on the distinction between simulating a mind and actually having one. Searle writes that \"according to Strong AI, the correct simulation really is a mind. According to Weak AI, the correct simulation is a model of the mind.\"[22]\n The claim is implicit in some of the statements of early AI researchers and analysts. For example, in 1955, AI founder Herbert A. Simon declared that \"there are now in the world machines that think, that learn and create\".[23] Simon, together with Allen Newell and Cliff Shaw, after having completed the first program that could do formal reasoning (the Logic Theorist), claimed that they had \"solved the venerable mind\u2013body problem, explaining how a system composed of matter can have the properties of mind.\"[24] John Haugeland wrote that \"AI wants only the genuine article: machines with minds, in the full and literal sense. This is not science fiction, but real science, based on a theoretical conception as deep as it is daring: namely, we are, at root, computers ourselves.\"[25]\n Searle also ascribes the following claims to advocates of strong AI:\n In more recent presentations of the Chinese room argument, Searle has identified \"strong AI\" as \"computer functionalism\" (a term he attributes to Daniel Dennett).[4][30] Functionalism is a position in modern philosophy of mind that holds that we can define mental phenomena (such as beliefs, desires, and perceptions) by describing their functions in relation to each other and to the outside world. Because a computer program can accurately represent functional relationships as relationships between symbols, a computer can have mental phenomena if it runs the right program, according to functionalism.\n Stevan Harnad argues that Searle's depictions of strong AI can be reformulated as \"recognizable tenets of computationalism, a position (unlike \"strong AI\") that is actually held by many thinkers, and hence one worth refuting.\"[31] Computationalism[i] is the position in the philosophy of mind which argues that the mind can be accurately described as an information-processing system.\n Each of the following, according to Harnad, is a \"tenet\" of computationalism:[34]\n Recent philosophical discussions have revisited the implications of computationalism for artificial intelligence. Goldstein and Levinstein explore whether large language models (LLMs) like ChatGPT can possess minds, focusing on their ability to exhibit folk psychology, including beliefs, desires, and intentions. The authors argue that LLMs satisfy several philosophical theories of mental representation, such as informational, causal, and structural theories, by demonstrating robust internal representations of the world. However, they highlight that the evidence for LLMs having action dispositions necessary for belief-desire psychology remains inconclusive. Additionally, they refute common skeptical challenges, such as the \"stochastic parrots\" argument and concerns over memorization, asserting that LLMs exhibit structured internal representations that align with these philosophical criteria.[35]\n Building on this discourse, Kristina \u0160ekrst highlights how AI hallucinations offer a unique perspective on computationalism. While functionalism defines mental states through their functional relationships, the emergence of hallucinations in AI systems reveals the limitations of such states when divorced from intrinsic understanding. These hallucinations, though arising from accurate functional representations, underscore the gap between computational reliability and the ontological complexity of human mental states. By doing so, they challenge the adequacy of functional accuracy in attributing mental phenomena to AI systems within a computationalist framework.[36]\n David Chalmers suggests that while current LLMs lack features like recurrent processing and unified agency, advancements in AI could address these limitations within the next decade, potentially enabling systems to achieve consciousness. This perspective challenges Searle's original claim that purely \"syntactic\" processing cannot yield understanding or consciousness, arguing instead that such systems could have authentic mental states.[37]\n Searle holds a philosophical position he calls \"biological naturalism\": that consciousness[a] and understanding require specific biological machinery that are found in brains. He writes \"brains cause minds\"[38] and that \"actual human mental phenomena [are] dependent on actual physical\u2013chemical properties of actual human brains\".[38] Searle argues that this machinery (known in neuroscience as the \"neural correlates of consciousness\") must have some causal powers that permit the human experience of consciousness.[39] Searle's belief in the existence of these powers has been criticized.\n Searle does not disagree with the notion that machines can have consciousness and understanding, because, as he writes, \"we are precisely such machines\".[6] Searle holds that the brain is, in fact, a machine, but that the brain gives rise to consciousness and understanding using specific machinery. If neuroscience is able to isolate the mechanical process that gives rise to consciousness, then Searle grants that it may be possible to create machines that have consciousness and understanding. However, without the specific machinery required, Searle does not believe that consciousness can occur.\n Biological naturalism implies that one cannot determine if the experience of consciousness is occurring merely by examining how a system functions, because the specific machinery of the brain is essential. Thus, biological naturalism is directly opposed to both behaviorism and functionalism (including \"computer functionalism\" or \"strong AI\").[40] Biological naturalism is similar to identity theory (the position that mental states are \"identical to\" or \"composed of\" neurological events); however, Searle has specific technical objections to identity theory.[41][j] Searle's biological naturalism and strong AI are both opposed to Cartesian dualism,[40] the classical idea that the brain and mind are made of different \"substances\". Indeed, Searle accuses strong AI of dualism, writing that \"strong AI only makes sense given the dualistic assumption that, where the mind is concerned, the brain doesn't matter\".[26]\n Searle's original presentation emphasized understanding\u2014that is, mental states with intentionality\u2014and did not directly address other closely related ideas such as \"consciousness\". However, in more recent presentations, Searle has included consciousness as the real target of the argument.[4]\n Computational models of consciousness are not sufficient by themselves for consciousness. The computational model for consciousness stands to consciousness in the same way the computational model of anything stands to the domain being modelled. Nobody supposes that the computational model of rainstorms in London will leave us all wet. But they make the mistake of supposing that the computational model of consciousness is somehow conscious. It is the same mistake in both cases.[42] David Chalmers writes, \"it is fairly clear that consciousness is at the root of the matter\" of the Chinese room.[43]\n Colin McGinn argues that the Chinese room provides strong evidence that the hard problem of consciousness is fundamentally insoluble. The argument, to be clear, is not about whether a machine can be conscious, but about whether it (or anything else for that matter) can be shown to be conscious. It is plain that any other method of probing the occupant of a Chinese room has the same difficulties in principle as exchanging questions and answers in Chinese. It is simply not possible to divine whether a conscious agency or some clever simulation inhabits the room.[44]\n Searle argues that this is only true for an observer outside of the room. The whole point of the thought experiment is to put someone inside the room, where they can directly observe the operations of consciousness. Searle claims that from his vantage point within the room there is nothing he can see that could imaginably give rise to consciousness, other than himself, and clearly he does not have a mind that can speak Chinese. In Searle's words, \"the computer has nothing more than I have in the case where I understand nothing\".[45]\n Patrick Hew used the Chinese Room argument to deduce requirements from military command and control systems if they are to preserve a commander's moral agency. He drew an analogy between a commander in their command center and the person in the Chinese Room, and analyzed it under a reading of Aristotle's notions of \"compulsory\" and \"ignorance\". Information could be \"down converted\" from meaning to symbols, and manipulated symbolically, but moral agency could be undermined if there was inadequate 'up conversion' into meaning. Hew cited examples from the USS Vincennes incident.[46]\n The Chinese room argument is primarily an argument in the philosophy of mind, and both major computer scientists and artificial intelligence researchers consider it irrelevant to their fields.[5] However, several concepts developed by computer scientists are essential to understanding the argument, including symbol processing, Turing machines, Turing completeness, and the Turing test.\n Searle's arguments are not usually considered an issue for AI research. The primary mission of artificial intelligence research is only to create useful systems that act intelligently and it does not matter if the intelligence is \"merely\" a simulation. AI researchers Stuart J. Russell and Peter Norvig wrote in 2021: \"We are interested in programs that behave intelligently. Individual aspects of consciousness\u2014awareness, self-awareness, attention\u2014can be programmed and can be part of an intelligent machine. The additional project making a machine conscious in exactly the way humans are is not one that we are equipped to take on.\"[5]\n Searle does not disagree that AI research can create machines that are capable of highly intelligent behavior. The Chinese room argument leaves open the possibility that a digital machine could be built that acts more intelligently than a person, but does not have a mind or intentionality in the same way that brains do.\n Searle's \"strong AI hypothesis\" should not be confused with \"strong AI\" as defined by Ray Kurzweil and other futurists,[47][21] who use the term to describe machine intelligence that rivals or exceeds human intelligence\u2014that is, artificial general intelligence, human level AI or superintelligence. Kurzweil is referring primarily to the amount of intelligence displayed by the machine, whereas Searle's argument sets no limit on this. Searle argues that a superintelligent machine would not necessarily have a mind and consciousness.\n The Chinese room implements a version of the Turing test.[49] Alan Turing introduced the test in 1950 to help answer the question \"can machines think?\" In the standard version, a human judge engages in a natural language conversation with a human and a machine designed to generate performance indistinguishable from that of a human being. All participants are separated from one another. If the judge cannot reliably tell the machine from the human, the machine is said to have passed the test.\n Turing then considered each possible objection to the proposal \"machines can think\", and found that there are simple, obvious answers if the question is de-mystified in this way. He did not, however, intend for the test to measure for the presence of \"consciousness\" or \"understanding\". He did not believe this was relevant to the issues that he was addressing. He wrote:\n I do not wish to give the impression that I think there is no mystery about consciousness. There is, for instance, something of a paradox connected with any attempt to localise it. But I do not think these mysteries necessarily need to be solved before we can answer the question with which we are concerned in this paper.[49] To Searle, as a philosopher investigating in the nature of mind and consciousness, these are the relevant mysteries. The Chinese room is designed to show that the Turing test is insufficient to detect the presence of consciousness, even if the room can behave or function as a conscious mind would.\n Computers manipulate physical objects in order to carry out calculations and do simulations. AI researchers Allen Newell and Herbert A. Simon called this kind of machine a physical symbol system. It is also equivalent to the formal systems used in the field of mathematical logic.\n Searle emphasizes the fact that this kind of symbol manipulation is syntactic (borrowing a term from the study of grammar). The computer manipulates the symbols using a form of syntax, without any knowledge of the symbol's semantics (that is, their meaning).\n Newell and Simon had conjectured that a physical symbol system (such as a digital computer) had all the necessary machinery for \"general intelligent action\", or, as it is known today, artificial general intelligence. They framed this as a philosophical position, the physical symbol system hypothesis: \"A physical symbol system has the necessary and sufficient means for general intelligent action.\"[50][51] The Chinese room argument does not refute this, because it is framed in terms of \"intelligent action\", i.e. the external behavior of the machine, rather than the presence or absence of understanding, consciousness and mind.\n Twenty-first century AI programs (such as \"deep learning\") do mathematical operations on huge matrixes of unidentified numbers and bear little resemblance to the symbolic processing used by AI programs at the time Searle wrote his critique in 1980. Nils Nilsson describes systems like these as \"dynamic\" rather than \"symbolic\". Nilsson notes that these are essentially digitized representations of dynamic systems\u2014the individual numbers do not have a specific semantics, but are instead samples or data points from a dynamic signal, and it is the signal being approximated which would have semantics. Nilsson argues it is not reasonable to consider these signals as \"symbol processing\" in the same sense as the physical symbol systems hypothesis.[52]\n The Chinese room has a design analogous to that of a modern computer. It has a Von Neumann architecture, which consists of a program (the book of instructions), some memory (the papers and file cabinets), a machine that follows the instructions (the man), and a means to write symbols in memory (the pencil and eraser). A machine with this design is known in theoretical computer science as \"Turing complete\", because it has the necessary machinery to carry out any computation that a Turing machine can do, and therefore it is capable of doing a step-by-step simulation of any other digital machine, given enough memory and time. Turing writes, \"all digital computers are in a sense equivalent.\"[53] The widely accepted Church\u2013Turing thesis holds that any function computable by an effective procedure is computable by a Turing machine.\n The Turing completeness of the Chinese room implies that it can do whatever any other digital computer can do (albeit much, much more slowly). Thus, if the Chinese room does not or can not contain a Chinese-speaking mind, then no other digital computer can contain a mind. Some replies to Searle begin by arguing that the room, as described, cannot have a Chinese-speaking mind. Arguments of this form, according to Stevan Harnad, are \"no refutation (but rather an affirmation)\"[54] of the Chinese room argument, because these arguments actually imply that no digital computers can have a mind.[28]\n There are some critics, such as Hanoch Ben-Yami, who argue that the Chinese room cannot simulate all the abilities of a digital computer, such as being able to determine the current time.[55]\n Searle has produced a more formal version of the argument of which the Chinese Room forms a part. He presented the first version in 1984. The version given below is from 1990.[56][k] The Chinese room thought experiment is intended to prove point A3.[l]\n He begins with three axioms:\n Searle posits that these lead directly to this conclusion:\n This much of the argument is intended to show that artificial intelligence can never produce a machine with a mind by writing programs that manipulate symbols. The remainder of the argument addresses a different issue. Is the human brain running a program? In other words, is the computational theory of mind correct?[f] He begins with an axiom that is intended to express the basic modern scientific consensus about brains and minds:\n Searle claims that we can derive \"immediately\" and \"trivially\"[57] that:\n And from this he derives the further conclusions:\n Refutations of Searle's argument take many different forms (see below). Computationalists and functionalists reject A3, arguing that \"syntax\" (as Searle describes it) can have \"semantics\" if the syntax has the right functional structure. Eliminative materialists reject A2, arguing that minds don't actually have \"semantics\"\u2014that thoughts and other mental phenomena are inherently meaningless but nevertheless function as if they had meaning.\n Replies to Searle's argument may be classified according to what they claim to show:[m]\n Some of the arguments (robot and brain simulation, for example) fall into multiple categories.\n These replies attempt to answer the question: since the man in the room does not speak Chinese, where is the mind that does? These replies address the key ontological issues of mind versus body and simulation vs. reality. All of the replies that identify the mind in the room are versions of \"the system reply\".\n The basic version of the system reply argues that it is the \"whole system\" that understands Chinese.[62][n] While the man understands only English, when he is combined with the program, scratch paper, pencils and file cabinets, they form a system that can understand Chinese. \"Here, understanding is not being ascribed to the mere individual; rather it is being ascribed to this whole system of which he is a part\" Searle explains.[29]\n Searle notes that (in this simple version of the reply) the \"system\" is nothing more than a collection of ordinary physical objects; it grants the power of understanding and consciousness to \"the conjunction of that person and bits of paper\"[29] without making any effort to explain how this pile of objects has become a conscious, thinking being. Searle argues that no reasonable person should be satisfied with the reply, unless they are \"under the grip of an ideology;\"[29] In order for this reply to be remotely plausible, one must take it for granted that consciousness can be the product of an information processing \"system\", and does not require anything resembling the actual biology of the brain.\n Searle then responds by simplifying this list of physical objects: he asks what happens if the man memorizes the rules and keeps track of everything in his head? Then the whole system consists of just one object: the man himself. Searle argues that if the man does not understand Chinese then the system does not understand Chinese either because now \"the system\" and \"the man\" both describe exactly the same object.[29]\n Critics of Searle's response argue that the program has allowed the man to have two minds in one head.[who?] If we assume a \"mind\" is a form of information processing, then the theory of computation can account for two computations occurring at once, namely (1) the computation for universal programmability (which is the function instantiated by the person and note-taking materials independently from any particular program contents) and (2) the computation of the Turing machine that is described by the program (which is instantiated by everything including the specific program).[64] The theory of computation thus formally explains the open possibility that the second computation in the Chinese Room could entail a human-equivalent semantic understanding of the Chinese inputs. The focus belongs on the program's Turing machine rather than on the person's.[65] However, from Searle's perspective, this argument is circular. The question at issue is whether consciousness is a form of information processing, and this reply requires that we make that assumption.\n More sophisticated versions of the systems reply try to identify more precisely what \"the system\" is and they differ in exactly how they describe it. According to these replies,[who?] the \"mind that speaks Chinese\" could be such things as: the \"software\", a \"program\", a \"running program\", a simulation of the \"neural correlates of consciousness\", the \"functional system\", a \"simulated mind\", an \"emergent property\", or \"a virtual mind\".\n Marvin Minsky suggested a version of the system reply known as the \"virtual mind reply\".[o] The term \"virtual\" is used in computer science to describe an object that appears to exist \"in\" a computer (or computer network) only because software makes it appear to exist. The objects \"inside\" computers (including files, folders, and so on) are all \"virtual\", except for the computer's electronic components. Similarly, Minsky that a computer may contain a \"mind\" that is virtual in the same sense as virtual machines, virtual communities and virtual reality.\n To clarify the distinction between the simple systems reply given above and virtual mind reply, David Cole notes that two simulations could be running on one system at the same time: one speaking Chinese and one speaking Korean. While there is only one system, there can be multiple \"virtual minds,\" thus the \"system\" cannot be the \"mind\".[69]\n Searle responds that such a mind is at best a simulation, and writes: \"No one supposes that computer simulations of a five-alarm fire will burn the neighborhood down or that a computer simulation of a rainstorm will leave us all drenched.\"[70] Nicholas Fearn responds that, for some things, simulation is as good as the real thing. \"When we call up the pocket calculator function on a desktop computer, the image of a pocket calculator appears on the screen. We don't complain that it isn't really a calculator, because the physical attributes of the device do not matter.\"[71] The question is, is the human mind like the pocket calculator, essentially composed of information, where a perfect simulation of the thing just is the thing? Or is the mind like the rainstorm, a thing in the world that is more than just its simulation, and not realizable in full by a computer simulation? For decades, this question of simulation has led AI researchers and philosophers to consider whether the term \"synthetic intelligence\" is more appropriate than the common description of such intelligences as \"artificial.\"\n These replies provide an explanation of exactly who it is that understands Chinese. If there is something besides the man in the room that can understand Chinese, Searle cannot argue that (1) the man does not understand Chinese, therefore (2) nothing in the room understands Chinese. This, according to those who make this reply, shows that Searle's argument fails to prove that \"strong AI\" is false.[p]\n These replies, by themselves, do not provide any evidence that strong AI is true, however. They do not show that the system (or the virtual mind) understands Chinese, other than the hypothetical premise that it passes the Turing test. Searle argues that, if we are to consider Strong AI remotely plausible, the Chinese Room is an example that requires explanation, and it is difficult or impossible to explain how consciousness might \"emerge\" from the room or how the system would have consciousness. As Searle writes \"the systems reply simply begs the question by insisting that the system must understand Chinese\"[29] and thus is dodging the question or hopelessly circular.\n As far as the person in the room is concerned, the symbols are just meaningless \"squiggles.\" But if the Chinese room really \"understands\" what it is saying, then the symbols must get their meaning from somewhere. These arguments attempt to connect the symbols to the things they symbolize. These replies address Searle's concerns about intentionality, symbol grounding and syntax vs. semantics.\n Suppose that instead of a room, the program was placed into a robot that could wander around and interact with its environment. This would allow a \"causal connection\" between the symbols and things they represent.[73][q] Hans Moravec comments: \"If we could graft a robot to a reasoning program, we wouldn't need a person to provide the meaning anymore: it would come from the physical world.\"[75][r]\n Searle's reply is to suppose that, unbeknownst to the individual in the Chinese room, some of the inputs came directly from a camera mounted on a robot, and some of the outputs were used to manipulate the arms and legs of the robot. Nevertheless, the person in the room is still just following the rules, and does not know what the symbols mean. Searle writes \"he doesn't see what comes into the robot's eyes.\"[77]\n Some respond that the room, as Searle describes it, is connected to the world: through the Chinese speakers that it is \"talking\" to and through the programmers who designed the knowledge base in his file cabinet. The symbols Searle manipulates are already meaningful, they are just not meaningful to him.[78][s]\n Searle says that the symbols only have a \"derived\" meaning, like the meaning of words in books. The meaning of the symbols depends on the conscious understanding of the Chinese speakers and the programmers outside the room. The room, like a book, has no understanding of its own.[t]\n Some have argued that the meanings of the symbols would come from a vast \"background\" of commonsense knowledge encoded in the program and the filing cabinets. This would provide a \"context\" that would give the symbols their meaning.[76][u]\n Searle agrees that this background exists, but he does not agree that it can be built into programs. Hubert Dreyfus has also criticized the idea that the \"background\" can be represented symbolically.[81]\n To each of these suggestions, Searle's response is the same: no matter how much knowledge is written into the program and no matter how the program is connected to the world, he is still in the room manipulating symbols according to rules. His actions are syntactic and this can never explain to him what the symbols stand for. Searle writes \"syntax is insufficient for semantics.\"[82][v]\n However, for those who accept that Searle's actions simulate a mind, separate from his own, the important question is not what the symbols mean to Searle, what is important is what they mean to the virtual mind. While Searle is trapped in the room, the virtual mind is not: it is connected to the outside world through the Chinese speakers it speaks to, through the programmers who gave it world knowledge, and through the cameras and other sensors that roboticists can supply.\n These arguments are all versions of the systems reply that identify a particular kind of system as being important; they identify some special technology that would create conscious understanding in a machine. (The \"robot\" and \"commonsense knowledge\" replies above also specify a certain kind of system as being important.)\n Suppose that the program simulated in fine detail the action of every neuron in the brain of a Chinese speaker.[84][w] This strengthens the intuition that there would be no significant difference between the operation of the program and the operation of a live human brain.\n Searle replies that such a simulation does not reproduce the important features of the brain\u2014its causal and intentional states. He is adamant that \"human mental phenomena [are] dependent on actual physical\u2013chemical properties of actual human brains.\"[26] Moreover, he argues:\n [I]magine that instead of a monolingual man in a room shuffling symbols we have the man operate an elaborate set of water pipes with valves connecting them. When the man receives the Chinese symbols, he looks up in the program, written in English, which valves he has to turn on and off. Each water connection corresponds to a synapse in the Chinese brain, and the whole system is rigged up so that after doing all the right firings, that is after turning on all the right faucets, the Chinese answers pop out at the output end of the series of pipes.\nNow where is the understanding in this system? It takes Chinese as input, it simulates the formal structure of the synapses of the Chinese brain, and it gives Chinese as output. But the man certainly doesn't understand Chinese, and neither do the water pipes, and if we are tempted to adopt what I think is the absurd view that somehow the conjunction of man and water pipes understands, remember that in principle the man can internalize the formal structure of the water pipes and do all the \"neuron firings\" in his imagination.[86] What if we ask each citizen of China to simulate one neuron, using the telephone system to simulate the connections between axons and dendrites? In this version, it seems obvious that no individual would have any understanding of what the brain might be saying.[87][x] It is also obvious that this system would be functionally equivalent to a brain, so if consciousness is a function, this system would be conscious.\n In this, we are asked to imagine that engineers have invented a tiny computer that simulates the action of an individual neuron. What would happen if we replaced one neuron at a time? Replacing one would clearly do nothing to change conscious awareness. Replacing all of them would create a digital computer that simulates a brain. If Searle is right, then conscious awareness must disappear during the procedure (either gradually or all at once). Searle's critics argue that there would be no point during the procedure when he can claim that conscious awareness ends and mindless simulation begins.[89][y][z] (See Ship of Theseus for a similar thought experiment.)\n These arguments (and the robot or common-sense knowledge replies) identify some special technology that would help create conscious understanding in a machine. They may be interpreted in two ways: either they claim (1) this technology is required for consciousness, the Chinese room does not or cannot implement this technology, and therefore the Chinese room cannot pass the Turing test or (even if it did) it would not have conscious understanding. Or they may be claiming that (2) it is easier to see that the Chinese room has a mind if we visualize this technology as being used to create it.\n In the first case, where features like a robot body or a connectionist architecture are required, Searle claims that strong AI (as he understands it) has been abandoned.[ac] The Chinese room has all the elements of a Turing complete machine, and thus is capable of simulating any digital computation whatsoever. If Searle's room cannot pass the Turing test then there is no other digital technology that could pass the Turing test. If Searle's room could pass the Turing test, but still does not have a mind, then the Turing test is not sufficient to determine if the room has a \"mind\". Either way, it denies one or the other of the positions Searle thinks of as \"strong AI\", proving his argument.\n The brain arguments in particular deny strong AI if they assume that there is no simpler way to describe the mind than to create a program that is just as mysterious as the brain was. He writes \"I thought the whole idea of strong AI was that we don't need to know how the brain works to know how the mind works.\"[27] If computation does not provide an explanation of the human mind, then strong AI has failed, according to Searle.\n Other critics hold that the room as Searle described it does, in fact, have a mind, however they argue that it is difficult to see\u2014Searle's description is correct, but misleading. By redesigning the room more realistically they hope to make this more obvious. In this case, these arguments are being used as appeals to intuition (see next section).\n In fact, the room can just as easily be redesigned to weaken our intuitions. Ned Block's Blockhead argument[95] suggests that the program could, in theory, be rewritten into a simple lookup table of rules of the form \"if the user writes S, reply with P and goto X\". At least in principle, any program can be rewritten (or \"refactored\") into this form, even a brain simulation.[ad] In the blockhead scenario, the entire mental state is hidden in the letter X, which represents a memory address\u2014a number associated with the next rule. It is hard to visualize that an instant of one's conscious experience can be captured in a single large number, yet this is exactly what \"strong AI\" claims. On the other hand, such a lookup table would be ridiculously large (to the point of being physically impossible), and the states could therefore be overly specific.\n Searle argues that however the program is written or however the machine is connected to the world, the mind is being simulated  by a simple step-by-step digital machine (or machines). These machines are always just like the man in the room: they understand nothing and do not speak Chinese. They are merely manipulating symbols without knowing what they mean. Searle writes: \"I can have any formal program you like, but I still understand nothing.\"[96]\n The following arguments (and the intuitive interpretations of the arguments above) do not directly explain how a Chinese speaking mind could exist in Searle's room, or how the symbols he manipulates could become meaningful. However, by raising doubts about Searle's intuitions they support other positions, such as the system and robot replies. These arguments, if accepted, prevent Searle from claiming that his conclusion is obvious by undermining the intuitions that his certainty requires.\n Several critics believe that Searle's argument relies entirely on intuitions. Block writes \"Searle's argument depends for its force on intuitions that certain entities do not think.\"[97] Daniel Dennett describes the Chinese room argument as a misleading \"intuition pump\"[98] and writes \"Searle's thought experiment depends, illicitly, on your imagining too simple a case, an irrelevant case, and drawing the obvious conclusion from it.\"[98]\n Some of the arguments above also function as appeals to intuition, especially those that are intended to make it seem more plausible that the Chinese room contains a mind, which can include the robot, commonsense knowledge, brain simulation and connectionist replies. Several of the replies above also address the specific issue of complexity. The connectionist reply emphasizes that a working artificial intelligence system would have to be as complex and as interconnected as the human brain. The commonsense knowledge reply emphasizes that any program that passed a Turing test would have to be \"an extraordinarily supple, sophisticated, and multilayered system, brimming with 'world knowledge' and meta-knowledge and meta-meta-knowledge\", as Daniel Dennett explains.[80]\n Many of these critiques emphasize speed and complexity of the human brain,[ae] which processes information at 100 billion operations per second (by some estimates).[100] Several critics point out that the man in the room would probably take millions of years to respond to a simple question, and would require \"filing cabinets\" of astronomical proportions.[101] This brings the clarity of Searle's intuition into doubt.\n An especially vivid version of the speed and complexity reply is from Paul and Patricia Churchland. They propose this analogous thought experiment: \"Consider a dark room containing a man holding a bar magnet or charged object. If the man pumps the magnet up and down, then, according to Maxwell's theory of artificial luminance (AL), it will initiate a spreading circle of electromagnetic waves and will thus be luminous. But as all of us who have toyed with magnets or charged balls well know, their forces (or any other forces for that matter), even when set in motion produce no luminance at all. It is inconceivable that you might constitute real luminance just by moving forces around!\"[88] Churchland's point is that the problem is that he would have to wave the magnet up and down something like 450 trillion times per second in order to see anything.[102]\n Stevan Harnad is critical of speed and complexity replies when they stray beyond addressing our intuitions. He writes \"Some have made a cult of speed and timing, holding that, when accelerated to the right speed, the computational may make a phase transition into the mental. It should be clear that is not a counterargument but merely an ad hoc speculation (as is the view that it is all just a matter of ratcheting up to the right degree of 'complexity.')\"[103][af]\n Searle argues that his critics are also relying on intuitions, however his opponents' intuitions have no empirical basis. He writes that, in order to consider the \"system reply\" as remotely plausible, a person must be \"under the grip of an ideology\".[29] The system reply only makes sense (to Searle) if one assumes that any \"system\" can have consciousness, just by virtue of being a system with the right behavior and functional parts. This assumption, he argues, is not tenable given our experience of consciousness.\n Several replies argue that Searle's argument is irrelevant because his assumptions about the mind and consciousness are faulty. Searle believes that human beings directly experience their consciousness, intentionality and the nature of the mind every day, and that this experience of consciousness is not open to question. He writes that we must \"presuppose the reality and knowability of the mental.\"[106] The replies below question whether Searle is justified in using his own experience of consciousness to determine that it is more than mechanical symbol processing. In particular, the other minds reply argues that we cannot use our experience of consciousness to answer questions about other minds (even the mind of a computer), the epiphenoma replies question whether we can make any argument at all about something like consciousness which can not, by definition, be detected by any experiment, and the eliminative materialist reply argues that Searle's own personal consciousness does not \"exist\" in the sense that Searle thinks it does.\n The \"Other Minds Reply\" points out that Searle's argument is a version of the problem of other minds, applied to machines. There is no way we can determine if other people's subjective experience is the same as our own. We can only study their behavior (i.e., by giving them our own Turing test). Critics of Searle argue that he is holding the Chinese room to a higher standard than we would hold an ordinary person.[107][ag]\n Nils Nilsson writes \"If a program behaves as if it were multiplying, most of us would say that it is, in fact, multiplying. For all I know, Searle may only be behaving as if he were thinking deeply about these matters. But, even though I disagree with him, his simulation is pretty good, so I'm willing to credit him with real thought.\"[109]\n Turing anticipated Searle's line of argument (which he called \"The Argument from Consciousness\") in 1950 and makes the other minds reply.[110] He noted that people never consider the problem of other minds when dealing with each other. He writes that \"instead of arguing continually over this point it is usual to have the polite convention that everyone thinks.\"[111] The Turing test simply extends this \"polite convention\" to machines. He does not intend to solve the problem of other minds (for machines or people) and he does not think we need to.[ah]\n If we accept Searle's description of intentionality, consciousness, and the mind, we are forced to accept that consciousness is epiphenomenal: that it \"casts no shadow\" i.e. is undetectable in the outside world. Searle's \"causal properties\" cannot be detected by anyone outside the mind, otherwise the Chinese Room could not pass the Turing test\u2014the people outside would be able to tell there was not a Chinese speaker in the room by detecting their causal properties. Since they cannot detect causal properties, they cannot detect the existence of the mental. Thus, Searle's \"causal properties\" and consciousness itself is undetectable, and anything that cannot be detected either does not exist or does not matter.\n Mike Alder calls this the \"Newton's Flaming Laser Sword Reply\". He argues that the entire argument is frivolous, because it is non-verificationist: not only is the distinction between simulating a mind and having a mind ill-defined, but it is also irrelevant because no experiments were, or even can be, proposed to distinguish between the two.[113]\n Daniel Dennett provides this illustration: suppose that, by some mutation, a human being is born that does not have Searle's \"causal properties\" but nevertheless acts exactly like a human being. This is a philosophical zombie, as formulated in the philosophy of mind. This new animal would reproduce just as any other human and eventually there would be more of these zombies. Natural selection would favor the zombies, since their design is (we could suppose) a bit simpler. Eventually the humans would die out. So therefore, if Searle is right, it is most likely that human beings (as we see them today) are actually \"zombies\", who nevertheless insist they are conscious. It is impossible to know whether we are all zombies or not. Even if we are all zombies, we would still believe that we are not.[114]\n Several philosophers argue that consciousness, as Searle describes it, does not exist. Daniel Dennett describes consciousness as a \"user illusion\".[115]\n This position is sometimes referred to as eliminative materialism: the view that consciousness is not a concept that can \"enjoy reduction\" to a strictly mechanical description, but rather is a concept that will be simply eliminated once the way the material brain works is fully understood, in just the same way as the concept of a demon has already been eliminated from science rather than enjoying reduction to a strictly mechanical description. Other mental properties, such as original intentionality (also called \u201cmeaning\u201d, \u201ccontent\u201d, and \u201csemantic character\u201d), are also commonly regarded as special properties related to beliefs and other propositional attitudes. Eliminative materialism maintains that propositional attitudes such as beliefs and desires, among other intentional mental states that have content, do not exist. If eliminative materialism is the correct scientific account of human cognition then the assumption of the Chinese room argument that \"minds have mental contents (semantics)\" must be rejected.[116]\n Searle disagrees with this analysis and argues that \"the study of the mind starts with such facts as that humans have beliefs, while thermostats, telephones, and adding machines don't ... what we wanted to know is what distinguishes the mind from thermostats and livers.\"[77] He takes it as obvious that we can detect the presence of consciousness and dismisses these replies as being off the point.\n Margaret Boden argued in her paper \"Escaping from the Chinese Room\" that even if the person in the room does not understand the Chinese, it does not mean there is no understanding in the room. The person in the room at least understands the rule book used to provide output responses.[117]\n Searle conclusion that \"human mental phenomena [are] dependent on actual physical\u2013chemical properties of actual human brains\"[26] have been sometimes described as a form of \"Carbon chauvinism\".[118] Steven Pinker suggested that a response to that conclusion would be to make a counter thought experiment to the Chinese Room, where the incredulity goes the other way.[119] He brings as an example the short story They're Made Out of Meat which depicts an alien race composed of some electronic beings who upon finding Earth express disbelief that the meat brain of humans can experience consciousness and thought.[120]\n However, Searle himself denied being \"Carbon chauvinist\".[121] He said \"I have not tried to show that only biological based systems like our brains can think. [...] I regard this issue as up for grabs\".[122] He said that even silicon machines could theoretically have human-like consciousness and thought, if the actual physical\u2013chemical properties of silicon could be used in a way that can produce consciousness and thought, but \"until we know how the brain does it we are not in a position to try to do it artificially\".[123]\n",
        "doc_number": 38
    },
    {
        "url": "https://en.wikipedia.org/wiki/OpenAI",
        "content": "\n OpenAI is an American artificial intelligence (AI) research organization founded in December 2015 and headquartered in San Francisco, California. Its stated mission is to develop \"safe and beneficial\" artificial general intelligence (AGI), which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\".[5] As a leading organization in the ongoing AI boom,[6] OpenAI is known for the GPT family of large language models, the DALL-E series of text-to-image models, and a text-to-video model named Sora.[7][8] Its release of ChatGPT in November 2022 has been credited with catalyzing widespread interest in generative AI.\n The organization consists of the non-profit OpenAI, Inc.,[9] registered in Delaware, and its for-profit subsidiary introduced in 2019, OpenAI Global, LLC.[10] Microsoft owns roughly 49% of OpenAI's equity, having invested US$13 billion.[11] It also provides computing resources to OpenAI through its cloud platform, Microsoft Azure.[12]\n In 2023 and 2024, OpenAI faced multiple lawsuits for alleged copyright infringement against authors and media companies whose work was used to train some of OpenAI's products. In November 2023, OpenAI's board removed Sam Altman as CEO, citing a lack of confidence in him, but reinstated him five days later after negotiations resulting in a reconstructed board. Many AI safety researchers left OpenAI in 2024.[13][14]\n In December 2015, OpenAI was founded by Sam Altman, Elon Musk, Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk as the co-chairs. A total of $1 billion in capital was pledged by Sam Altman, Greg Brockman, Elon Musk, Reid Hoffman, Jessica Livingston, Peter Thiel, Amazon Web Services (AWS), Infosys, and YC Research.[15][16] The actual collected total amount of contributions was only $130 million until 2019.[10] According to an investigation led by TechCrunch, Musk was its largest donor while YC Research did not contribute anything at all.[17] The organization stated it would \"freely collaborate\" with other institutions and researchers by making its patents and research open to the public.[18][19] OpenAI was initially run from Brockman's living room.[20] It was later headquartered at the Pioneer Building in the Mission District, San Francisco.[21][22]\n According to Wired, Brockman met with Yoshua Bengio, one of the \"founding fathers\" of deep learning, and drew up a list of the \"best researchers in the field\".[23] Brockman was able to hire nine of them as the first employees in December 2015.[23] In 2016, OpenAI paid corporate-level (rather than nonprofit-level) salaries, but did not pay AI researchers salaries comparable to those of Facebook or Google.[23]\n Microsoft's Peter Lee stated that the cost of a top AI researcher exceeds the cost of a top NFL quarterback prospect.[23] OpenAI's potential and mission drew these researchers to the firm; a Google employee said he was willing to leave Google for OpenAI \"partly because of the very strong group of people and, to a very large extent, because of its mission.\"[23] Brockman stated that \"the best thing that I could imagine doing was moving humanity closer to building real AI in a safe way.\"[23] OpenAI co-founder Wojciech Zaremba stated that he turned down \"borderline crazy\" offers of two to three times his market value to join OpenAI instead.[23]\n In April 2016, OpenAI released a public beta of \"OpenAI Gym\", its platform for reinforcement learning research.[24] Nvidia gifted its first DGX-1 supercomputer to OpenAI in August 2016 to help it train larger and more complex AI models with the capability of reducing processing time from six days to two hours.[25][26] In December 2016, OpenAI released \"Universe\", a software platform for measuring and training an AI's general intelligence across the world's supply of games, websites, and other applications.[27][28][29][30]\n In 2017, OpenAI spent $7.9\u00a0million, or a quarter of its functional expenses, on cloud computing alone.[31] In comparison, DeepMind's total expenses in 2017 were $442\u00a0million. In the summer of 2018, simply training OpenAI's Dota 2 bots required renting 128,000 CPUs and 256 GPUs from Google for multiple weeks.\n In 2018, Musk resigned from his Board of Directors seat, citing \"a potential future conflict [of interest]\" with his role as CEO of Tesla due to Tesla's AI development for self-driving cars.[32] Sam Altman claims that Musk believed that OpenAI had fallen behind other players like Google and Musk proposed instead to take over OpenAI himself, which the board rejected. Musk subsequently left OpenAI.\n In February 2019, GPT-2 was announced, which gained attention for its ability to generate human-like text.[33]\n In 2019, OpenAI transitioned from non-profit to \"capped\" for-profit, with the profit being capped at 100 times any investment.[34] According to OpenAI, the capped-profit model allows OpenAI Global, LLC to legally attract investment from venture funds and, in addition, to grant employees stakes in the company.[35] Many top researchers work for Google Brain, DeepMind, or Facebook, which offer stock options that a nonprofit would be unable to.[36] Before the transition, public disclosure of the compensation of top employees at OpenAI was legally required.[37]\n The company then distributed equity to its employees and partnered with Microsoft,[38] announcing an investment package of $1 billion into the company. Since then, OpenAI systems have run on an Azure-based supercomputing platform from Microsoft.[39][40][41]\n OpenAI Global, LLC then announced its intention to commercially license its technologies.[42] It planned to spend the $1 billion \"within five years, and possibly much faster.\"[43] Altman has stated that even a billion dollars may turn out to be insufficient, and that the lab may ultimately need \"more capital than any non-profit has ever raised\" to achieve artificial general intelligence.[44]\n The transition from a nonprofit to a capped-profit company was viewed with skepticism by Oren Etzioni of the nonprofit Allen Institute for AI, who agreed that wooing top researchers to a nonprofit is difficult, but stated \"I disagree with the notion that a nonprofit can't compete\" and pointed to successful low-budget projects by OpenAI and others. \"If bigger and better funded was always better, then IBM would still be number one.\"\n The nonprofit, OpenAI, Inc., is the sole controlling shareholder of OpenAI Global, LLC, which, despite being a for-profit company, retains a formal fiduciary responsibility to OpenAI, Inc.'s nonprofit charter. A majority of OpenAI, Inc.'s board is barred from having financial stakes in OpenAI Global, LLC.[35] In addition, minority members with a stake in OpenAI Global, LLC are barred from certain votes due to conflict of interest.[36] Some researchers have argued that OpenAI Global, LLC's switch to for-profit status is inconsistent with OpenAI's claims to be \"democratizing\" AI.[45]\n In 2020, OpenAI announced GPT-3, a language model trained on large internet datasets. GPT-3 is aimed at natural language answering questions, but it can also translate between languages and coherently generate improvised text. It also announced that an associated API, named simply \"the API\", would form the heart of its first commercial product.[46]\n Eleven employees left OpenAI, mostly between December 2020 and January 2021, in order to establish Anthropic.[47]\n In 2021, OpenAI introduced DALL-E, a specialized deep learning model adept at generating complex digital images from textual descriptions, utilizing a variant of the GPT-3 architecture.[48]\n In December 2022, OpenAI received widespread media coverage after launching a free preview of ChatGPT, its new AI chatbot based on GPT-3.5. According to OpenAI, the preview received over a million signups within the first five days.[50] According to anonymous sources cited by Reuters in December 2022, OpenAI Global, LLC was projecting $200 million of revenue in 2023 and $1 billion in revenue in 2024.[51]\n In January 2023, OpenAI Global, LLC was in talks for funding that would value the company at $29 billion, double its 2021 value.[52] On January 23, 2023, Microsoft announced a new US$10 billion investment in OpenAI Global, LLC over multiple years, partially needed to use Microsoft's cloud-computing service Azure.[53][54] Rumors of this deal suggested that Microsoft may receive 75% of OpenAI's profits until it secures its investment return and a 49% stake in the company.[55] The investment is believed to be a part of Microsoft's efforts to integrate OpenAI's ChatGPT into the Bing search engine. Google announced a similar AI application (Bard), after ChatGPT was launched, fearing that ChatGPT could threaten Google's place as a go-to source for information.[56][57]\n On February 7, 2023, Microsoft announced that it was building AI technology based on the same foundation as ChatGPT into Microsoft Bing, Edge, Microsoft 365 and other products.[58]\n On March 3, 2023, Reid Hoffman resigned from his board seat, citing a desire to avoid conflicts of interest with his investments in AI companies via Greylock Partners, and his co-founding of the AI startup Inflection AI. Hoffman remained on the board of Microsoft, a major investor in OpenAI.[59]\n On March 14, 2023, OpenAI released GPT-4, both as an API (with a waitlist) and as a feature of ChatGPT Plus.[60]\n On May 22, 2023, Sam Altman, Greg Brockman and Ilya Sutskever posted recommendations for the governance of superintelligence.[61] They consider that superintelligence could happen within the next 10 years, allowing a \"dramatically more prosperous future\" and that \"given the possibility of existential risk, we can't just be reactive\". They propose creating an international watchdog organization similar to IAEA to oversee AI systems above a certain capability threshold, suggesting that relatively weak AI systems on the other side should not be overly regulated. They also call for more technical safety research for superintelligences, and ask for more coordination, for example through governments launching a joint project which \"many current efforts become part of\".[61][62]\n In July 2023, OpenAI launched the superalignment project, aiming to find within 4 years how to align future superintelligences by automating alignment research using AI.[63]\n In August 2023, it was announced that OpenAI had acquired the New York-based start-up Global Illumination, a company that deploys AI to develop digital infrastructure and creative tools.[64]\n On September 21, 2023, Microsoft had begun rebranding all variants of its Copilot to Microsoft Copilot, including the former Bing Chat and the Microsoft 365 Copilot.[65] This strategy was followed in December 2023 by adding the MS-Copilot to many installations of Windows 11 and Windows 10 as well as a standalone Microsoft Copilot app released for Android[66] and one released for iOS thereafter.[67]\n In October 2023, Sam Altman and Peng Xiao, CEO of the Emirati AI firm G42, announced Open AI would let G42 deploy Open AI technology.[68]\n On November 6, 2023, OpenAI launched GPTs, allowing individuals to create customized versions of ChatGPT for specific purposes, further expanding the possibilities of AI applications across various industries.[69] On November 14, 2023, OpenAI announced they temporarily suspended new sign-ups for ChatGPT Plus due to high demand.[70] Access for newer subscribers re-opened a month later on December 13.[71]\n On January 18, 2024, OpenAI announced a partnership with Arizona State University that would give it complete access to ChatGPT Enterprise. It is OpenAI's first partnership with an educational institution.[72]\n In February 2024, the U.S. Securities and Exchange Commission was reportedly investigating OpenAI over whether internal company communications made by Altman were used to mislead investors; and an investigation of Altman's statements, opened by the Southern New York U.S. Attorney's Office the previous November, was ongoing.[73][74]\n On February 15, 2024, OpenAI announced a text-to-video model named Sora, which it plans to release to the public at an unspecified date.[75] It is currently available for red teams for managing critical harms and risks.[76]\n On February 29, 2024, OpenAI and CEO Sam Altman were sued by Elon Musk, who accused them of prioritizing profits over public good, contrary to OpenAI's original mission[10] of developing AI for humanity's benefit.[77] The lawsuit cited OpenAI's policy shift after partnering with Microsoft, questioning its open-source commitment and stirring the AI ethics-vs.-profit debate.[78] In a blog post, OpenAI stated that \"Elon understood the mission did not imply open-sourcing AGI.\"[79] In a staff memo, they also denied being a de facto Microsoft subsidiary.[80]\n On March 11, 2024, court filing, OpenAI said it was \"doing just fine without Elon Musk\" after he left the company in 2018. They also responded to Musk's lawsuit, calling the billionaire's claims \"incoherent\", \"frivolous\", \"extraordinary\" and \"a fiction\".[81] On June 11, 2024, Musk unexpectedly withdrew the lawsuit.[82] On August 5, 2024, Musk reopened the lawsuit against Altman and others, alleging that Altman claimed that OpenAI was going to be founded as a non-profit organization.[83][84]\n On May 15, 2024, Ilya Sutskever resigned from OpenAI and was replaced with Jakub Pachocki to be the Chief Scientist.[85] Hours later, Jan Leike, the other co-leader of the superalignment team, announced his departure, citing an erosion of safety and trust in OpenAI's leadership.[86] Their departures along with several researchers leaving the group, led OpenAI to absorb the team's work into other research areas, and officially shut down the superalignment group.[87] According to sources interviewed by Fortune, OpenAI's promise of allocating 20% of its computing capabilities to the superalignment project had not been fulfilled.[88]\n On May 19, 2024, Reddit and OpenAI announced a partnership to integrate Reddit's content into OpenAI products, including ChatGPT. This collaboration allows OpenAI to access Reddit's Data API, providing real-time, structured content to enhance AI tools and user engagement with Reddit communities. Additionally, Reddit plans to develop new AI-powered features for users and moderators using OpenAI's platform. The partnership aligns with Reddit's commitment to privacy, adhering to its Public Content Policy and existing Data API Terms, which restrict commercial use without approval. OpenAI will also serve as a Reddit advertising partner.[89]\n On May 22, 2024, OpenAI entered into an agreement with News Corp to integrate news content from The Wall Street Journal, the New York Post, The Times, and The Sunday Times into its AI platform. Meanwhile, other publications like The New York Times chose to sue OpenAI and Microsoft for copyright infringement over the use of their content to train AI models.[90]\n On May 29, 2024, Axios reported that OpenAI had signed deals with Vox Media and The Atlantic to share content to enhance the accuracy of AI models like ChatGPT by incorporating reliable news sources, addressing concerns about AI misinformation.[91] Concerns were expressed about the decision by journalists, including those working for the publications, as well as the publications' unions. The Vox Union stated, \"As both journalists and workers, we have serious concerns about this partnership, which we believe could adversely impact members of our union, not to mention the well-documented ethical and environmental concerns surrounding the use of generative AI.\"[92]\n A group of nine current and former OpenAI employees has accused the company of prioritizing profits over safety, using restrictive agreements to silence concerns, and moving too quickly with inadequate risk management. They call for greater transparency, whistleblower protections, and legislative regulation of AI development.[93]\n On June 10, 2024, it was announced at WWDC 2024 that OpenAI had partnered with Apple Inc. to bring ChatGPT features to Apple Intelligence and iPhone.[94]\n On June 13, 2024, OpenAI announced that Paul Nakasone, the former head of the NSA was joining the company's board of directors. Nakasone also joined the company's security subcommittee.[95]\n On June 24, 2024, OpenAI acquired Multi, a startup running a collaboration platform based on Zoom.[96]\n In July 2024, Reuters reported that OpenAI was working on a project code-named \"Strawberry\" (previously known as Q*) aiming to enhance AI reasoning capabilities. The project reportedly seeks to enable AI to plan ahead, navigate the internet autonomously, and conduct \"deep research\".[97][98] The project was officially released on September 12 and named o1.[99]\n On August 5, TechCrunch reported that OpenAI's cofounder John Schulman has left the company to join rival AI startup Anthropic. Schulman cited a desire to focus more deeply on AI alignment research as his reason for the move. Additionally, OpenAI's president and co-founder, Greg Brockman, took an extended leave until November.[100]\n In September 2024, OpenAI's global affairs chief, Anna Makanju, expressed support for the UK's approach to AI regulation during her testimony to the House of Lords Communications and Digital Committee, stating that the company favors \"smart regulation\" and sees the UK's AI white paper as a positive step towards responsible AI development.[101]\n On September 25, OpenAI's Chief Technology Officer (CTO) Mira Murati announced her departure from the company to \"create the time and space to do my own exploration\".[102] It had previously been reported Murati was among those who expressed concerns to the Board about Altman.[103]\n In October 2024, OpenAI raised $6.6 billion from investors, potentially valuing the company at $157 billion. The funding attracted returning venture capital firms like Thrive Capital and Khosla Ventures, along with major backer Microsoft and new investors Nvidia and Softbank.[104][105]  OpenAI's CFO, Sarah Friar, informed employees that a tender offer for share buybacks would follow the funding, although specifics were yet to be determined. Thrive Capital invested around $1.2 billion, with the option for an additional $1 billion if revenue goals were met. Apple, despite initial interest, did not participate in this funding round.[106]\n Also in October 2024, The Intercept revealed that OpenAI's tools were considered \"essential\" for AFRICOM's mission and included in an \"Exception to Fair Opportunity\" contractual agreement between the Department of Defense and Microsoft.[107]\n In November 2024, OpenAI acquired the long-standing domain Chat.com and redirected it to ChatGPT's main site.[108][109] Moreover, Greg Brockman rejoined OpenAI after a three-month leave from his role as president. An OpenAI spokesperson confirmed his return, highlighting that Brockman would collaborate with Altman on tackling key technical challenges. His return followed a wave of high-profile departures, including Mira Murati and Ilya Sutskever, who had since launched their own AI ventures.[110]\n In December 2024, OpenAI launched several significant features as part of its \"12 Days of OpenAI\" event, which started on December 5. It announced Sora, a text-to-video model intended to create realistic videos from text prompts, and available to ChatGPT Plus and Pro users.[111][112] Additionally, OpenAI unveiled the o1 model, which is designed to be capable of advanced reasoning through its chain-of-thought processing, enabling it to engage in explicit reasoning before generating responses.[113][114] This model is intended to tackle complex tasks with improved accuracy and transparency. Another major release was ChatGPT Pro, a subscription service priced at $200 per month that provides users with unlimited access to the o1 model and enhanced voice features.[115] The event also saw the expansion of the Canvas feature, allowing all users to utilize side-by-side digital editing capabilities.\n \n Sources:[10][121]\n Some scientists, such as Stephen Hawking and Stuart Russell, have articulated concerns that if advanced AI gains the ability to redesign itself at an ever-increasing rate, an unstoppable \"intelligence explosion\" could lead to human extinction. Co-founder Musk characterizes AI as humanity's \"biggest existential threat\".[123]\n Musk and Altman have stated they are partly motivated by concerns about AI safety and the existential risk from artificial general intelligence.[124][125] OpenAI states that \"it's hard to fathom how much human-level AI could benefit society,\" and that it is equally difficult to comprehend \"how much it could damage society if built or used incorrectly\".[19] Research on safety cannot safely be postponed: \"because of AI's surprising history, it's hard to predict when human-level AI might come within reach.\"[126] OpenAI states that AI \"should be an extension of individual human wills and, in the spirit of liberty, as broadly and evenly distributed as possible.\"[19] Co-chair Sam Altman expects the decades-long project to surpass human intelligence.[127]\n Vishal Sikka, former CEO of Infosys, stated that an \"openness\", where the endeavor would \"produce results generally in the greater interest of humanity\", was a fundamental requirement for his support; and that OpenAI \"aligns very nicely with our long-held values\" and their \"endeavor to do purposeful work\".[128] Cade Metz of Wired suggested that corporations such as Amazon might be motivated by a desire to use open-source software and data to level the playing field against corporations such as Google and Facebook, which own enormous supplies of proprietary data. Altman stated that Y Combinator companies would share their data with OpenAI.[127]\n In the early years before his 2018 departure, Musk posed the question: \"What is the best thing we can do to ensure the future is good? We could sit on the sidelines or we can encourage regulatory oversight, or we could participate with the right structure with people who care deeply about developing AI in a way that is safe and is beneficial to humanity.\" He acknowledged that \"there is always some risk that in actually trying to advance (friendly) AI we may create the thing we are concerned about\"; but nonetheless, that the best defense was \"to empower as many people as possible to have AI. If everyone has AI powers, then there's not any one person or a small set of individuals who can have AI superpower.\"[116]\n Musk and Altman's counterintuitive strategy\u2014that of trying to reduce the potential harm of AI by giving everyone access to it\u2014is controversial among those concerned with existential risk from AI. Philosopher Nick Bostrom said, \"If you have a button that could do bad things to the world, you don't want to give it to everyone.\"[125] During a 2016 conversation about technological singularity, Altman said, \"We don't plan to release all of our source code\" and mentioned a plan to \"allow wide swaths of the world to elect representatives to a new governance board\". Greg Brockman stated, \"Our goal right now\u00a0... is to do the best thing there is to do. It's a little vague.\"[129]\n Conversely, OpenAI's initial decision to withhold GPT-2 around 2019, due to a wish to \"err on the side of caution\" in the presence of potential misuse, was criticized by advocates of openness. Delip Rao, an expert in text generation, stated, \"I don't think [OpenAI] spent enough time proving [GPT-2] was actually dangerous.\" Other critics argued that open publication was necessary to replicate the research and to create countermeasures.[130]\n More recently, in 2022, OpenAI published its approach to the alignment problem, anticipating that aligning AGI to human values would likely be harder than aligning current AI systems: \"Unaligned AGI could pose substantial risks to humanity[,] and solving the AGI alignment problem could be so difficult that it will require all of humanity to work together\". They stated that they intended to explore how to better use human feedback to train AI systems, and how to safely use AI to incrementally automate alignment research.[131]\n In 2024, following the temporary removal of Sam Altman and his return, many employees gradually left OpenAI, including most of the original leadership team and a significant number of AI safety researchers.[132][133] OpenAI also planned a restructuring to operate as a for-profit company. This restructuring could grant Altman a stake in the company.[134]\n At its beginning, OpenAI's research included many projects focused on reinforcement learning (RL).[135] OpenAI has been viewed as an important competitor to DeepMind.[136]\n Announced in 2016, Gym is an open-source Python library designed to facilitate the development of reinforcement learning algorithms. It aimed to standardize how environments are defined in AI research, making published research more easily reproducible[24][137] while providing users with a simple interface for interacting with these environments. In 2022, new developments of Gym have been moved to the library Gymnasium.[138][139]\n Released in 2018, Gym Retro is a platform for reinforcement learning (RL) research on video games[140] using RL algorithms and study generalization. Prior RL research focused mainly on optimizing agents to solve single tasks. Gym Retro gives the ability to generalize between games with similar concepts but different appearances.\n Released in 2017, RoboSumo is a virtual world where humanoid metalearning robot agents initially lack knowledge of how to even walk, but are given the goals of learning to move and to push the opposing agent out of the ring.[141] Through this adversarial learning process, the agents learn how to adapt to changing conditions. When an agent is then removed from this virtual environment and placed in a new virtual environment with high winds, the agent braces to remain upright, suggesting it had learned how to balance in a generalized way.[141][142] OpenAI's Igor Mordatch argued that competition between agents could create an intelligence \"arms race\" that could increase an agent's ability to function even outside the context of the competition.[141]\n OpenAI Five is a team of five OpenAI-curated bots used in the competitive five-on-five video game Dota 2, that learn to play against human players at a high skill level entirely through trial-and-error algorithms. Before becoming a team of five, the first public demonstration occurred at The International 2017, the annual premiere championship tournament for the game, where Dendi, a professional Ukrainian player, lost against a bot in a live one-on-one matchup.[143][144] After the match, CTO Greg Brockman explained that the bot had learned by playing against itself for two weeks of real time, and that the learning software was a step in the direction of creating software that can handle complex tasks like a surgeon.[145][146] The system uses a form of reinforcement learning, as the bots learn over time by playing against themselves hundreds of times a day for months, and are rewarded for actions such as killing an enemy and taking map objectives.[147][148][149]\n By June 2018, the ability of the bots expanded to play together as a full team of five, and they were able to defeat teams of amateur and semi-professional players.[150][147][151][152] At The International 2018, OpenAI Five played in two exhibition matches against professional players, but ended up losing both games.[153][154][155] In April 2019, OpenAI Five defeated OG, the reigning world champions of the game at the time, 2:0 in a live exhibition match in San Francisco.[156][157] The bots' final public appearance came later that month, where they played in 42,729 total games in a four-day open online competition, winning 99.4% of those games.[158]\n OpenAI Five's mechanisms in Dota 2's bot player shows the challenges of AI systems in multiplayer online battle arena (MOBA) games and how OpenAI Five has demonstrated the use of deep reinforcement learning (DRL) agents to achieve superhuman competence in Dota 2 matches.[159]\n Developed in 2018, Dactyl uses machine learning to train a Shadow Hand, a human-like robot hand, to manipulate physical objects.[160] It learns entirely in simulation using the same RL algorithms and training code as OpenAI Five. OpenAI tackled the object orientation problem by using domain randomization, a simulation approach which exposes the learner to a variety of experiences rather than trying to fit to reality. The set-up for Dactyl, aside from having motion tracking cameras, also has RGB cameras to allow the robot to manipulate an arbitrary object by seeing it. In 2018, OpenAI showed that the system was able to manipulate a cube and an octagonal prism.[161]\n In 2019, OpenAI demonstrated that Dactyl could solve a Rubik's Cube. The robot was able to solve the puzzle 60% of the time. Objects like the Rubik's Cube introduce complex physics that is harder to model. OpenAI did this by improving the robustness of Dactyl to perturbations by using Automatic Domain Randomization (ADR), a simulation approach of generating progressively more difficult environments. ADR differs from manual domain randomization by not needing a human to specify randomization ranges.[162]\n In June 2020, OpenAI announced a multi-purpose API which it said was \"for accessing new AI models developed by OpenAI\" to let developers call on it for \"any English language AI task\".[163][164]\n The company has popularized generative pretrained transformers (GPT).[165]\n The original paper on generative pre-training of a transformer-based language model was written by Alec Radford and his colleagues, and published in preprint on OpenAI's website on June 11, 2018.[166] It showed how a generative model of language could acquire world knowledge and process long-range dependencies by pre-training on a diverse corpus with long stretches of contiguous text.\n Generative Pre-trained Transformer 2 (\"GPT-2\") is an unsupervised transformer language model and the successor to OpenAI's original GPT model (\"GPT-1\"). GPT-2 was announced in February 2019, with only limited demonstrative versions initially released to the public. The full version of GPT-2 was not immediately released due to concern about potential misuse, including applications for writing fake news.[167] Some experts expressed skepticism that GPT-2 posed a significant threat.\n In response to GPT-2, the Allen Institute for Artificial Intelligence responded with a tool to detect \"neural fake news\".[168] Other researchers, such as Jeremy Howard, warned of \"the technology to totally fill Twitter, email, and the web up with reasonable-sounding, context-appropriate prose, which would drown out all other speech and be impossible to filter\".[169] In November 2019, OpenAI released the complete version of the GPT-2 language model.[170] Several websites host interactive demonstrations of different instances of GPT-2 and other transformer models.[171][172][173]\n GPT-2's authors argue unsupervised language models to be general-purpose learners, illustrated by GPT-2 achieving state-of-the-art accuracy and perplexity on 7 of 8 zero-shot tasks (i.e. the model was not further trained on any task-specific input-output examples).\n The corpus it was trained on, called WebText, contains slightly 40 gigabytes of text from URLs shared in Reddit submissions with at least 3 upvotes. It avoids certain issues encoding vocabulary with word tokens by using byte pair encoding. This permits representing any string of characters by encoding both individual characters and multiple-character tokens.[174]\n First described in May 2020, Generative Pre-trained[a] Transformer 3 (GPT-3) is an unsupervised transformer language model and the successor to GPT-2.[175][176][177] OpenAI stated that the full version of GPT-3 contained 175\u00a0billion parameters,[177] two orders of magnitude larger than the 1.5\u00a0billion[178] in the full version of GPT-2 (although GPT-3 models with as few as 125 million parameters were also trained).[179]\n OpenAI stated that GPT-3 succeeded at certain \"meta-learning\" tasks and could generalize the purpose of a single input-output pair. The GPT-3 release paper gave examples of translation and cross-linguistic transfer learning between English and Romanian, and between English and German.[177]\n GPT-3 dramatically improved benchmark results  over GPT-2. OpenAI cautioned that such scaling-up of language models could be approaching or encountering the fundamental capability limitations of predictive language models.[180] Pre-training GPT-3 required several thousand petaflop/s-days[b] of compute, compared to tens of petaflop/s-days for the full GPT-2 model.[177] Like its predecessor,[167] the GPT-3 trained model was not immediately released to the public for concerns of possible abuse, although OpenAI planned to allow access through a paid cloud API after a two-month free private beta that began in June 2020.[163][182]\n On September 23, 2020, GPT-3 was licensed exclusively to Microsoft.[183][184]\n Announced in mid-2021, Codex is a descendant of GPT-3 that has additionally been trained on code from 54 million GitHub repositories,[185][186] and is the AI powering the code autocompletion tool GitHub Copilot.[186] In August 2021, an API was released in private beta.[187] According to OpenAI, the model can create working code in over a dozen programming languages, most effectively in Python.[185]\n Several issues with glitches, design flaws and security vulnerabilities were cited.[188][189]\n GitHub Copilot has been accused of emitting copyrighted code, with no author attribution or license.[190]\n OpenAI announced that they would discontinue support for Codex API on March 23, 2023.[191]\n On March 14, 2023, OpenAI announced the release of Generative Pre-trained Transformer 4 (GPT-4), capable of accepting text or image inputs.[192] They announced that the updated technology passed a simulated law school bar exam with a score around the top 10% of test takers. (By contrast, GPT-3.5 scored around the bottom 10%.) They said that GPT-4 could also read, analyze or generate up to 25,000 words of text, and write code in all major programming languages.[193]\n Observers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous GPT-3.5-based iteration, with the caveat that GPT-4 retained some of the problems with earlier revisions.[194] GPT-4 is also capable of taking images as input on ChatGPT.[195] OpenAI has declined to reveal various technical details and statistics about GPT-4, such as the precise size of the model.[196]\n On May 13, 2024, OpenAI announced and released GPT-4o, which can process and generate text, images and audio.[197] GPT-4o achieved state-of-the-art results in voice, multilingual, and vision benchmarks, setting new records in audio speech recognition and translation.[198][199] It scored 88.7% on the Massive Multitask Language Understanding (MMLU) benchmark compared to 86.5% by GPT-4.[200]\n On July 18, 2024, OpenAI released GPT-4o mini, a smaller version of GPT-4o replacing GPT-3.5 Turbo on the ChatGPT interface. Its API costs $0.15 per million input tokens and $0.60 per million output tokens, compared to $5 and $15 respectively for GPT-4o. OpenAI expects it to be particularly useful for enterprises, startups and developers seeking to automate services with AI agents.[201]\n On September 12, 2024, OpenAI released the o1-preview and o1-mini models, which have been designed to take more time to think about their responses, leading to higher accuracy. These models are particularly effective in science, coding, and reasoning tasks, and were made available to ChatGPT Plus and Team members.[202][203] In December 2024, o1-preview was replaced by o1.[204]\n On December 20, 2024, OpenAI unveiled o3, the successor of the o1 reasoning model. OpenAI also unveiled o3-mini, a lighter and faster version of OpenAI o3. As of December 21, 2024, this model is not available for public use. According to OpenAI, they are testing o3 and o3-mini.[205][206] Until January 10, 2025, safety and security researchers have the opportunity to apply for early access to these models.[207] The model is called o3 rather than o2 to avoid confusion with British telecommunications services provider O2.[208]\n Revealed in 2021, CLIP (Contrastive Language\u2013Image Pre-training) is a model that is trained to analyze the semantic similarity between text and images. It can notably be used for image classification.[209]\n Revealed in 2021, DALL-E is a Transformer model that creates images from textual descriptions.[210] DALL-E uses a 12-billion-parameter version of GPT-3 to interpret natural language inputs (such as \"a green leather purse shaped like a pentagon\" or \"an isometric view of a sad capybara\") and generate corresponding images. It can create images of realistic objects (\"a stained-glass window with an image of a blue strawberry\") as well as objects that do not exist in reality (\"a cube with the texture of a porcupine\"). As of March 2021, no API or code is available.\n In April 2022, OpenAI announced DALL-E 2, an updated version of the model with more realistic results.[211] In December 2022, OpenAI published on GitHub software for Point-E, a new rudimentary system for converting a text description into a 3-dimensional model.[212]\n In September 2023, OpenAI announced DALL-E 3, a more powerful model better able to generate images from complex descriptions without manual prompt engineering and render complex details like hands and text.[213] It was released to the public as a ChatGPT Plus feature in October.[214]\n Sora is a text-to-video model that can generate videos based on short descriptive prompts[215] as well as extend existing videos forwards or backwards in time.[216] It can generate videos with resolution up to 1920x1080 or 1080x1920. The maximal length of generated videos is unknown.\n Sora's development team named it after the Japanese word for \"sky\", to signify its \"limitless creative potential\".[215] Sora's technology is an adaptation of the technology behind the DALL\u00b7E 3 text-to-image model.[217] OpenAI trained the system using publicly-available videos as well as copyrighted videos licensed for that purpose, but did not reveal the number or the exact sources of the videos.[215]\n OpenAI demonstrated some Sora-created high-definition videos to the public on February 15, 2024, stating that it could generate videos up to one minute long. It also shared a technical report highlighting the methods used to train the model, and the model's capabilities.[217] It acknowledged some of its shortcomings, including struggles simulating complex physics.[218] Will Douglas Heaven of the MIT Technology Review called the demonstration videos \"impressive\", but noted that they must have been cherry-picked and might not represent Sora's typical output.[217]\n Despite skepticism from some academic leaders following Sora's public demo, notable entertainment-industry figures have shown significant interest in the technology's potential. In an interview, actor/filmmaker Tyler Perry expressed his astonishment at the technology's ability to generate realistic video from text descriptions, citing its potential to revolutionize storytelling and content creation. He said that his excitement about Sora's possibilities was so strong that he had decided to pause plans for expanding his Atlanta-based movie studio.[219]\n \n Released in 2022, Whisper is a general-purpose speech recognition model.[220] It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.[221]\n Released in 2019, MuseNet is a deep neural net trained to predict subsequent musical notes in MIDI music files. It can generate songs with 10 instruments in 15 styles. According to The Verge, a song generated by MuseNet tends to start reasonably but then fall into chaos the longer it plays.[222][223] In pop culture, initial applications of this tool were used as early as 2020 for the internet psychological thriller Ben Drowned to create music for the titular character.[224][225]\n Released in 2020, Jukebox is an open-sourced algorithm to generate music with vocals. After training on 1.2\u00a0million samples, the system accepts a genre, artist, and a snippet of lyrics and outputs song samples. OpenAI stated the songs \"show local musical coherence [and] follow traditional chord patterns\" but acknowledged that the songs lack \"familiar larger musical structures such as choruses that repeat\" and that \"there is a significant gap\" between Jukebox and human-generated music. The Verge stated \"It's technologically impressive, even if the results sound like mushy versions of songs that might feel familiar\", while Business Insider stated \"surprisingly, some of the resulting songs are catchy and sound legitimate\".[226][227][228]\n In 2018, OpenAI launched the Debate Game, which teaches machines to debate toy problems in front of a human judge. The purpose is to research whether such an approach may assist in auditing AI decisions and in developing explainable AI.[229][230]\n Released in 2020, Microscope[231] is a collection of visualizations of every significant layer and neuron of eight neural network models which are often studied in interpretability.[232] Microscope was created to analyze the features that form inside these neural networks easily. The models included are AlexNet, VGG-19, different versions of Inception, and different versions of CLIP Resnet.[233]\n Launched in November 2022, ChatGPT is an artificial intelligence tool built on top of GPT-3 that provides a conversational interface that allows users to ask questions in natural language. The system then responds with an answer within seconds. ChatGPT reached 1 million users 5 days after its launch.[234][235]\n As of 2023, ChatGPT Plus is a GPT-4 backed version of ChatGPT[236] available for a US$20 per month subscription fee[237] (the original version is backed by GPT-3.5).[238] OpenAI also makes GPT-4 available to a select group of applicants through their GPT-4 API waitlist;[239] after being accepted, an additional fee of US$0.03 per 1000 tokens in the initial text provided to the model (\"prompt\"), and US$0.06 per 1000 tokens that the model generates (\"completion\"), is charged for access to the version of the model with an 8192-token context window; for the 32768-token context window, the prices are doubled.[240]\n In May 2023, OpenAI launched a user interface for ChatGPT for the App Store on iOS and later in July 2023 for the Play Store on Android.[241] The app supports chat history syncing and voice input (using Whisper, OpenAI's speech recognition model).[242][241][243] In September 2023, OpenAI announced that ChatGPT \"can now see, hear, and speak\". ChatGPT Plus users can upload images, while mobile app users can talk to the chatbot.[244][245]\n In October 2023, OpenAI's latest image generation model, DALL-E 3, was integrated into ChatGPT Plus and ChatGPT Enterprise. The integration uses ChatGPT to write prompts for DALL-E guided by conversation with users.[246][247]\n OpenAI's GPT Store, initially slated for a 2023 launch, is now deferred to an undisclosed date in early 2024, attributed likely to the leadership changes in November following the initial announcement.[248]\n Concerns about the energy consumption of generative AI, including ChatGPT, are rising. In September 2024, Microsoft entered a deal with Constellation Energy to reopen the Three Mile Island nuclear plant to supply power to its AI-driven data centers.[249]\n In December 2024, OpenAI launched a new feature allowing users to call ChatGPT for up to 15 minutes per month for free.[250][251]\n SearchGPT, a prototype search engine developed by OpenAI, was unveiled on July 25, 2024, with an initial limited release to 10,000 test users. It combines traditional search engine features with generative AI capabilities.[252][253]\n Stargate is a potential artificial intelligence supercomputer in development by Microsoft and OpenAI.[254] Stargate is designed as part of a greater data center project, which could represent an investment of as much as $100 billion by Microsoft.[255]\n Stargate is reported to be part of a series of AI-related construction projects planned in the next few years by the companies Microsoft and OpenAI.[255] The supercomputers will be constructed in five phases.[254] The fourth phase should consist in a smaller OpenAI supercomputer, planned to launch around 2026.[254] Stargate is the fifth and final phase of the program, and will take five and six years to complete and is slated to launch around 2028.[255]\n The artificial intelligence of Stargate is slated to be contained on millions of special server chips.[255] The supercomputer's data center will be built in the US across 700 acres of land.[255]  It has a planned power consumption of 5 gigawatts, for which it could rely on nuclear energy.[255] The name \"Stargate\" is a homage to the 1994 sci-fi film Stargate.[255]\n On November 17, 2023, Sam Altman was removed as CEO when its board of directors (composed of Helen Toner, Ilya Sutskever, Adam D'Angelo and Tasha McCauley) cited a lack of confidence in him. Chief Technology Officer Mira Murati took over as interim CEO. Greg Brockman, the president of OpenAI, was also removed as chairman of the board[256][257] and resigned from the company's presidency shortly thereafter.[258] Three senior OpenAI researchers subsequently resigned: director of research and GPT-4 lead Jakub Pachocki, head of AI risk Aleksander Madry, and researcher Szymon Sidor.[259][260]\n On November 18, 2023, there were reportedly talks of Altman returning as CEO amid pressure placed upon the board by investors such as Microsoft and Thrive Capital, who objected to Altman's departure.[261] Although Altman himself spoke in favor of returning to OpenAI, he has since stated that he considered starting a new company and bringing former OpenAI employees with him if talks to reinstate him didn't work out.[262] The board members agreed \"in principle\" to resign if Altman returned.[263] On November 19, 2023, negotiations with Altman to return failed and Murati was replaced by Emmett Shear as interim CEO.[264] The board initially contacted Anthropic CEO Dario Amodei (a former OpenAI executive) about replacing Altman, and proposed a merger of the two companies, but both offers were declined.[265]\n On November 20, 2023, Microsoft CEO Satya Nadella announced Altman and Brockman would be joining Microsoft to lead a new advanced AI research team, but added that they were still committed to OpenAI despite recent events.[266] Before the partnership with Microsoft was finalized, Altman gave the board another opportunity to negotiate with him.[267] About 738 of OpenAI's 770 employees, including Murati and Sutskever, signed an open letter stating they would quit their jobs and join Microsoft if the board did not rehire Altman and then resign.[268][269] This prompted OpenAI investors to consider legal action against the board as well.[270] In response, OpenAI management sent an internal memo to employees stating that negotiations with Altman and the board had resumed and would take some time.[271]\n \nOn November 21, 2023, after continued negotiations, Altman and Brockman returned to the company in their prior roles along with a reconstructed board made up of new members Bret Taylor (as chairman) and Lawrence Summers, with D'Angelo remaining.[272] On November 22, 2023, emerging reports suggested that Sam Altman's dismissal from OpenAI may have been linked to his alleged mishandling of a significant breakthrough in the organization's secretive project codenamed Q*. According to sources within OpenAI, Q* is aimed at developing AI capabilities in logical and mathematical reasoning, and reportedly involves performing math on the level of grade-school students.[273][274][275] Concerns about Altman's response to this development, specifically regarding the discovery's potential safety implications, were reportedly raised with the company's board shortly before Altman's firing.[276] On November 29, 2023, OpenAI announced that an anonymous Microsoft employee had joined the board as a non-voting member to observe the company's operations;[277] Microsoft resigned from the board in July 2024.[278]\n In January 2023, OpenAI has been criticized for outsourcing the annotation of data sets to Sama, a company based in San Francisco that employed workers in Kenya. These annotations were used to train an AI model to detect toxicity, which could then be used to moderate toxic content, notably from ChatGPT's training data and outputs. However, these pieces of text usually contained detailed descriptions of various types of violence, including sexual violence. The investigation uncovered that OpenAI began sending snippets of data to Sama as early as November 2021. The four Sama employees interviewed by Time described themselves as mentally scarred. OpenAI paid Sama $12.50 per hour of work, and Sama was redistributing the equivalent of between $1.32 and $2.00 per hour post-tax to its annotators. Sama's spokesperson said that the $12.50 was also covering other implicit costs, among which were infrastructure expenses, quality assurance and management.[279]\n In March 2023, the company was also criticized for disclosing particularly few technical details about products like GPT-4, contradicting its initial commitment to openness and making it harder for independent researchers to replicate its work and develop safeguards. OpenAI cited competitiveness and safety concerns to justify this strategic turn. OpenAI's former chief scientist Ilya Sutskever argued in 2023 that open-sourcing increasingly capable models was increasingly risky, and that the safety reasons for not open-sourcing the most potent AI models would become \"obvious\" in a few years.[280]\n On May 17, 2024, a Vox article reported that OpenAI was asking departing employees to sign a lifelong non-disparagement agreement forbidding them from criticizing OpenAI or acknowledging the existence of the agreement. Daniel Kokotajlo, a former employee, publicly stated that he forfeited his vested equity in OpenAI in order to leave without signing the agreement.[281][282] Sam Altman stated that he was unaware of the equity cancellation provision, and that OpenAI never enforced it to cancel any employee's vested equity.[283] Vox published leaked documents and emails challenging this claim.[284] On May 23, 2024, OpenAI sent a memo releasing former employees from the agreement.[285]\n OpenAI was sued for copyright infringement by authors Sarah Silverman, Matthew Butterick, Paul Tremblay and Mona Awad in July 2023.[286][287][288] In September 2023, 17 authors, including George R. R. Martin, John Grisham, Jodi Picoult and Jonathan Franzen, joined the Authors Guild in filing a class action lawsuit against OpenAI, alleging that the company's technology was illegally using their copyrighted work.[289][290] The New York Times also sued the company in late December 2023.[287][291] In May 2024 it was revealed that OpenAI had destroyed its Books1 and Books2 training datasets, which were used in the training of GPT-3, and which the Authors Guild believed to have contained over 100,000 copyrighted books.[292]\n In 2021, OpenAI developed a speech recognition tool called Whisper. OpenAI used it to transcribe more than one million hours of YouTube videos into text for training GPT-4. The automated transcription of YouTube videos raised concerns within OpenAI employees regarding potential violations of YouTube's terms of service, which prohibit the use of videos for applications independent of the platform, as well as any type of automated access to its videos. Despite these concerns, the project proceeded with notable involvement from OpenAI's president, Greg Brockman. The resulting dataset proved instrumental in training GPT-4.[293]\n In February 2024, The Intercept as well as Raw Story and Alternate Media Inc. filed lawsuit against OpenAI on copyright litigation ground.[294][295] The lawsuit is said to have charted a new legal strategy for digital-only publishers to sue OpenAI.[296]\n On April 30, 2024, eight newspapers filed a lawsuit in the Southern District of New York against OpenAI and Microsoft, claiming illegal harvesting of their copyrighted articles. The suing publications included The Mercury News, The Denver Post, The Orange County Register, St. Paul Pioneer Press, Chicago Tribune, Orlando Sentinel, Sun Sentinel, and New York Daily News.[297]\n In April 2023, the EU's European Data Protection Board (EDPB) formed a dedicated task force on ChatGPT \"to foster cooperation and to exchange information on possible enforcement actions conducted by data protection authorities\" based on the \"enforcement action undertaken by the Italian data protection authority against Open AI about the Chat GPT service\".[298]\n In late April 2024 NOYB filed a complaint with the Austrian Datenschutzbeh\u00f6rde against OpenAI for violating the European General Data Protection Regulation. A text created with ChatGPT gave a false date of birth for a living person without giving the individual the option to see the personal data used in the process. A request to correct the mistake was denied. Additionally, neither the recipients of ChatGPT's work nor the sources used, could be made available, OpenAI claimed.[299]\n OpenAI was criticized for lifting its ban on using ChatGPT for \"military and warfare\". Up until January 10, 2024, its \"usage policies\" included a ban on \"activity that has high risk of physical harm, including,\" specifically, \"weapons development\" and \"military and warfare.\" Its new policies prohibit \"[using] our service to harm yourself or others\" and to \"develop or use weapons\".[300][301] As one of the industry collaborators, OpenAI provides LLM to the Artificial Intelligence Cyber Challenge (AIxCC) sponsored by Defense Advanced Research Projects Agency (DARPA) and Advanced Research Projects Agency for Health to protect software critical to Americans.[302] In October 2024, The Intercept revealed that OpenAI's tools are considered \"essential\" for AFRICOM's mission and included in an \"Exception to Fair Opportunity\" contractural agreement between the United States Department of Defense and Microsoft.[107] In December 2024, OpenAI said it would partner with defense-tech company Anduril to build drone defense technologies for the United States and its allies.[303]\n In June 2023, a lawsuit claimed that OpenAI scraped 300 billion words online without consent and without registering as a data broker. It was filed in San Francisco, California, by sixteen anonymous plaintiffs.[304] They also claimed that OpenAI and its partner as well as customer Microsoft continued to unlawfully collect and use personal data from millions of consumers worldwide to train artificial intelligence models.[305]\n On May 22, 2024, OpenAI entered into an agreement with News Corp to integrate news content from The Wall Street Journal, the New York Post, The Times, and The Sunday Times into its AI platform. Meanwhile, other publications like The New York Times chose to sue OpenAI and Microsoft for copyright infringement over the use of their content to train AI models.[306] In November 2024, a coalition of Canadian news outlets, including the Toronto Star, Metroland Media, Postmedia, The Globe and Mail, The Canadian Press and CBC, sued OpenAI for using their news articles to train its software without permission.[307]\n",
        "doc_number": 39
    },
    {
        "url": "https://en.wikipedia.org/wiki/DeepMind",
        "content": "\n DeepMind Technologies Limited,[1] trading as Google DeepMind or simply DeepMind, is a British-American artificial intelligence research laboratory which serves as a subsidiary of Alphabet Inc.. Founded in the UK in 2010, it was acquired by Google in 2014[8] and merged with Google AI's Google Brain division to become Google DeepMind in April 2023. The company is based in London, with research centres in Canada,[9] France,[10] Germany, and the United States.\n DeepMind introduced neural Turing machines (neural networks that can access external memory like a conventional Turing machine),[11] resulting in a computer that loosely resembles short-term memory in the human brain.[12][13]\n DeepMind has created neural network models to play video games and board games. It made headlines in 2016 after its AlphaGo program beat a human professional Go player Lee Sedol, a world champion, in a five-game match, which was the subject of a documentary film.[14] A more general program, AlphaZero, beat the most powerful programs playing go, chess and shogi (Japanese chess) after a few days of play against itself using reinforcement learning.[15]\n In 2020, DeepMind made significant advances in the problem of protein folding with AlphaFold.[16] In July 2022, it was announced that over 200 million predicted protein structures, representing virtually all known proteins, would be released on the AlphaFold database.[17][18] AlphaFold's database of predictions achieved state of the art records on benchmark tests for protein folding algorithms, although each individual prediction still requires confirmation by experimental tests. AlphaFold3 was released in May 2024, making structural predictions for the interaction of proteins with various molecules. It achieved new standards on various benchmarks, raising the state of the art accuracies from 28 and 52 percent to 65 and 76 percent.\n The start-up was founded by Demis Hassabis, Shane Legg and Mustafa Suleyman in November 2010.[2] Hassabis and Legg first met at the Gatsby Computational Neuroscience Unit at University College London (UCL).[19]\n Demis Hassabis has said that the start-up began working on artificial intelligence technology by teaching it how to play old games from the seventies and eighties, which are relatively primitive compared to the ones that are available today. Some of those games included Breakout, Pong, and Space Invaders. AI was introduced to one game at a time, without any prior knowledge of its rules. After spending some time on learning the game, AI would eventually become an expert in it. \"The cognitive processes which the AI goes through are said to be very like those of a human who had never seen the game would use to understand and attempt to master it.\"[20] The goal of the founders is to create a general-purpose AI that can be useful and effective for almost anything.\n Major venture capital firms Horizons Ventures and Founders Fund invested in the company,[21] as well as entrepreneurs Scott Banister,[22] Peter Thiel,[23] and Elon Musk.[24] Jaan Tallinn was an early investor and an adviser to the company.[25] On 26 January 2014, Google confirmed its acquisition of DeepMind for a price reportedly ranging between $400 million and $650 million.[26][27][28] and that it had agreed to take over DeepMind Technologies. The sale to Google took place after Facebook reportedly ended negotiations with DeepMind Technologies in 2013.[29] The company was afterwards renamed Google DeepMind and kept that name for about two years.[30]\n In 2014, DeepMind received the \"Company of the Year\" award from Cambridge Computer Laboratory.[31]\n In September 2015, DeepMind and the Royal Free NHS Trust signed their initial information sharing agreement to co-develop a clinical task management app, Streams.[32]\n After Google's acquisition the company established an artificial intelligence ethics board.[33] The ethics board for AI research remains a mystery, with both Google and DeepMind declining to reveal who sits on the board.[34] DeepMind has opened a new unit called DeepMind Ethics and Society and focused on the ethical and societal questions raised by artificial intelligence featuring prominent philosopher Nick Bostrom as advisor.[35] In October 2017, DeepMind launched a new research team to investigate AI ethics.[36][37]\n In December 2019, co-founder Suleyman announced he would be leaving DeepMind to join Google, working in a policy role.[38] In March 2024, Microsoft appointed him as the EVP and CEO of its newly created consumer AI unit, Microsoft AI.[39]\n In April 2023, DeepMind merged with Google AI's Google Brain division to form Google DeepMind, as part of the company's continued efforts to accelerate work on AI in response to OpenAI's ChatGPT.[40] This marked the end of a years-long struggle from DeepMind executives to secure greater autonomy from Google.[41]\n Google Research released a paper in 2016 regarding AI safety and avoiding undesirable behaviour during the AI learning process.[42] In 2017 DeepMind released GridWorld, an open-source testbed for evaluating whether an algorithm learns to disable its kill switch or otherwise exhibits certain undesirable behaviours.[43][44]\n In July 2018, researchers from DeepMind trained one of its systems to play the computer game Quake III Arena.[45]\n As of 2020, DeepMind has published over a thousand papers, including thirteen papers that were accepted by Nature or Science.[citation needed] DeepMind received media attention during the AlphaGo period; according to a LexisNexis search, 1842 published news stories mentioned DeepMind in 2016, declining to 1363 in 2019.[46]\n Unlike earlier AIs, such as IBM's Deep Blue or Watson, which were developed for a pre-defined purpose and only function within that scope, DeepMind's initial algorithms were intended to be general. They used reinforcement learning, an algorithm that learns from experience using only raw pixels as data input. Their initial approach used deep Q-learning with a convolutional neural network.[30][47] They tested the system on video games, notably early arcade games, such as Space Invaders or Breakout.[47][48] Without altering the code, the same AI was able to play certain games more efficiently than any human ever could.[48]\n In 2013, DeepMind published research on an AI system that surpassed human abilities in games such as Pong, Breakout and Enduro, while surpassing state of the art performance on Seaquest, Beamrider, and Q*bert.[49][50] This work reportedly led to the company's acquisition by Google.[51] DeepMind's AI had been applied to video games made in the 1970s and 1980s; work was ongoing for more complex 3D games such as Quake, which first appeared in the 1990s.[48]\n In 2020, DeepMind published Agent57,[52][53] an AI Agent which surpasses human level performance on all 57 games of the Atari 2600 suite.[54] In July 2022, DeepMind announced the development of DeepNash, a model-free multi-agent reinforcement learning system capable of playing the board game Stratego at the level of a human expert.[55]\n In October 2015, a computer Go program called AlphaGo, developed by DeepMind, beat the European Go champion Fan Hui, a 2 dan (out of 9 dan possible) professional, five to zero.[56] This was the first time an artificial intelligence (AI) defeated a professional Go player.[57] Previously, computers were only known to have played Go at \"amateur\" level.[56][58] Go is considered much more difficult for computers to win compared to other games like chess, due to the much larger number of possibilities, making it prohibitively difficult for traditional AI methods such as brute-force.[56][58]\n In March 2016 it beat Lee Sedol, one of the highest ranked players in the world, with a score of 4 to 1 in a five-game match. In the 2017 Future of Go Summit, AlphaGo won a three-game match with Ke Jie, who had been the world's highest-ranked player for two years.[59][60] In 2017, an improved version, AlphaGo Zero, defeated AlphaGo in a hundred out of a hundred games. Later that year, AlphaZero, a modified version of AlphaGo Zero, gained superhuman abilities at chess and shogi. In 2019, DeepMind released a new model named MuZero that mastered the domains of Go, chess, shogi, and Atari 2600 games without human data, domain knowledge, or known rules.[61][62]\n AlphaGo technology was developed based on deep reinforcement learning, making it different from the AI technologies then on the market. The data fed into the AlphaGo algorithm consisted of various moves based on historical tournament data. The number of moves was increased gradually until over 30 million of them were processed. The aim was to have the system mimic the human player, as represented by the input data, and eventually become better. It played against itself and learned from the outcomes; thus, it learned to improve itself over the time and increased its winning rate as a result.[63]\n AlphaGo used two deep neural networks: a policy network to evaluate move probabilities and a value network to assess positions. The policy network trained via supervised learning, and was subsequently refined by policy-gradient reinforcement learning. The value network learned to predict winners of games played by the policy network against itself. After training, these networks employed a lookahead Monte Carlo tree search, using the policy network to identify candidate high-probability moves, while the value network (in conjunction with Monte Carlo rollouts using a fast rollout policy) evaluated tree positions.[64]\n In contrast, AlphaGo Zero was trained without being fed data of human-played games. Instead it generated its own data, playing millions of games against itself. It used a single neural network, rather than separate policy and value networks. Its simplified tree search relied upon this neural network to evaluate positions and sample moves. A new reinforcement learning algorithm incorporated lookahead search inside the training loop.[64] AlphaGo Zero employed around 15 people and millions in computing resources.[65] Ultimately, it needed much less computing power than AlphaGo, running on four specialized AI processors (Google TPUs), instead of AlphaGo's 48.[66] It also required less training time, being able to beat its predecessor after just three days, compared with months required for the original AlphaGo.[67] Similarly, AlphaZero also learned via self-play.\n Researchers applied MuZero to solve the real world challenge of video compression with a set number of bits with respect to Internet traffic on sites such as YouTube, Twitch, and Google Meet. The goal of MuZero is to optimally compress the video so the quality of the video is maintained with a reduction in data. The final result using MuZero was a 6.28% average reduction in bitrate.[68][69]\n In 2016, Hassabis discussed the game StarCraft as a future challenge, since it requires strategic thinking and handling imperfect information.[70]\n In January 2019, DeepMind introduced AlphaStar, a program playing the real-time strategy game StarCraft II. AlphaStar used reinforcement learning based on replays from human players, and then played against itself to enhance its skills. At the time of the presentation, AlphaStar had knowledge equivalent to 200 years of playing time. It won 10 consecutive matches against two professional players, although it had the unfair advantage of being able to see the entire field, unlike a human player who has to move the camera manually. A preliminary version in which that advantage was fixed lost a subsequent match.[71]\n In July 2019, AlphaStar began playing against random humans on the public 1v1 European multiplayer ladder. Unlike the first iteration of AlphaStar, which played only Protoss v. Protoss, this one played as all of the game's races, and had earlier unfair advantages fixed.[72][73] By October 2019, AlphaStar had reached Grandmaster level on the StarCraft II ladder on all three StarCraft races, becoming the first AI to reach the top league of a widely popular esport without any game restrictions.[74]\n In 2016, DeepMind turned its artificial intelligence to protein folding, a long-standing problem in molecular biology. In December 2018, DeepMind's AlphaFold won the 13th Critical Assessment of Techniques for Protein Structure Prediction (CASP) by successfully predicting the most accurate structure for 25 out of 43 proteins. \"This is a lighthouse project, our first major investment in terms of people and resources into a fundamental, very important, real-world scientific problem,\" Hassabis said to The Guardian.[75] In 2020, in the 14th CASP, AlphaFold's predictions achieved an accuracy score regarded as comparable with lab techniques. Dr Andriy Kryshtafovych, one of the panel of scientific adjudicators, described the achievement as \"truly remarkable\", and said the problem of predicting how proteins fold had been \"largely solved\".[76][77][78]\n In July 2021, the open-source RoseTTAFold and AlphaFold2 were released to allow scientists to run their own versions of the tools. A week later DeepMind announced that AlphaFold had completed its prediction of nearly all human proteins as well as the entire proteomes of 20 other widely studied organisms.[79] The structures were released on the AlphaFold Protein Structure Database. In July 2022, it was announced that the predictions of over 200 million proteins, representing virtually all known proteins, would be released on the AlphaFold database.[17][18]\n The most recent update, AlphaFold3, was released in May 2024, predicting the interactions of proteins with DNA, RNA, and various other molecules. In a particular benchmark test on the problem of DNA interactions, AlphaFold3's attained an accuracy of 65%, significantly improving the previous state of the art of 28%.[80]\n In October 2024, Hassabis and John Jumper received half of the 2024 Nobel Prize in Chemistry jointly for protein structure prediction, citing AlphaFold2 achievement.[81]\n In 2016, DeepMind introduced WaveNet, a text-to-speech system. It was originally too computationally intensive for use in consumer products, but in late 2017 it became ready for use in consumer applications such as Google Assistant.[82][83] In 2018 Google launched a commercial text-to-speech product, Cloud Text-to-Speech, based on WaveNet.[84][85] In 2018, DeepMind introduced a more efficient model called WaveRNN co-developed with Google AI.[86][87] In 2020 WaveNetEQ, a packet loss concealment method based on a WaveRNN architecture, was presented.[88] In 2019, Google started to roll WaveRNN with WavenetEQ out to Google Duo users.[89]\n Released in May 2022, Gato is a polyvalent multimodal model. It was trained on 604 tasks, such as image captioning, dialogue, or stacking blocks. On 450 of these tasks, Gato outperformed human experts at least half of the time, according to DeepMind.[90] Unlike models like MuZero, Gato does not need to be retrained to switch from one task to the other.\n Sparrow is an artificial intelligence-powered chatbot developed by DeepMind to build safer machine learning systems by using a mix of human feedback and Google search suggestions.[91]\n Chinchilla is a language model developed by DeepMind.[92]\n DeepMind posted a blog post on 28 April 2022 on a single visual language model (VLM) named Flamingo that can accurately describe a picture of something with just a few training images.[93][94]\n In 2022, DeepMind unveiled AlphaCode, an AI-powered coding engine that creates computer programs at a rate comparable to that of an average programmer, with the company testing the system against coding challenges created by Codeforces utilized in human competitive programming competitions.[95] AlphaCode earned a rank equivalent to 54% of the median score on Codeforces after being trained on GitHub data and Codeforce problems and solutions. The program was required to come up with a unique solution and stopped from duplicating answers.\n Gemini is a multimodal large language model which was released on 6 December 2023.[96] It is the successor of Google's LaMDA and PaLM 2 language models and sought to challenge OpenAI's GPT-4.[97] Gemini comes in 3 sizes: Nano, Pro, and Ultra.[98] Gemini is also the name of the chatbot that integrates Gemini (and which was previously called Bard).[99]\n On 12 December 2024, Google released Gemini 2.0 Flash, the first model in the Gemini 2.0 series. It notably features expanded multimodality, with the ability to also generate images and audio,[100] and is part of Google's broader plans to integrate advanced AI into autonomous agents.[101]\n Gemma is a family of lightweight, open source, large language models which was released on 21 February 2024. It's available in two distinct sizes: a 7 billion parameter model optimized for GPU and TPU usage, and a 2 billion parameter model designed for CPU and on-device applications. Gemma models were trained on up to 6 trillion tokens of text, employing similar architectures, datasets, and training methodologies as the Gemini model family.[102]\n In March 2024, DeepMind introduced Scalable Instructable Multiword Agent, or SIMA, an AI agent capable of understanding and following natural language instructions to complete tasks across various 3D virtual environments. Trained on nine video games from eight studios and four research environments, SIMA demonstrated adaptability to new tasks and settings without requiring access to game source code or APIs. The agent comprises pre-trained computer vision and language models fine-tuned on gaming data, with language being crucial for understanding and completing given tasks as instructed. DeepMind's research aimed to develop more helpful AI agents by translating advanced AI capabilities into real-world actions through a language interface.[103][104]\n In 2024, Google Deepmind published the results of an experiment where they trained two large language models to help identify and present areas of overlap among a few thousand group members they had recruited online using techiques like sortition to get a representative sample of participants. The project is named in honor of J\u00fcrgen Habermas.[105][106] In one experiment, the participants rated the summaries by the AI higher than the human moderator 56% of the time.[106]\n In May 2024, a multimodal video generation model called Veo was announced at Google I/O 2024. Google claimed that it could generate 1080p videos beyond a minute long.[8] In December 2024, Google released Veo2, available via VideoFX. It supports 4K resolution video generation, and has an improved understanding of physics.[107]\n In March 2023, DeepMind introduced \"Genie\" (Generative Interactive Environments), an AI model that can generate game-like, action-controllable virtual worlds based on textual descriptions, images, or sketches. Built as an autoregressive latent diffusion model, Genie enables frame-by-frame interactivity without requiring labeled action data for training. Its successor, Genie 2, released in December 2024, expanded these capabilities to generate diverse and interactive 3D environments.[108]\n Released in June 2023, RoboCat is an AI model that can control robotic arms. The model can adapt to new models of robotic arms, and to new types of tasks.[109][110]\n DeepMind researchers have applied machine learning models to the sport of football, often referred to as soccer in North America, modelling the behaviour of football players, including the goalkeeper, defenders, and strikers during different scenarios such as penalty kicks. The researchers used heat maps and cluster analysis to organize players based on their tendency to behave a certain way during the game when confronted with a decision on how to score or prevent the other team from scoring. \n The researchers mention that machine learning models could be used to democratize the football industry by automatically selecting interesting video clips of the game that serve as highlights. This can be done by searching videos for certain events, which is possible because video analysis is an established field of machine learning. This is also possible because of extensive sports analytics based on data including annotated passes or shots, sensors that capture data about the players movements many times over the course of a game, and game theory models.[111][112]\n Google has unveiled a new archaeology document program, named Ithaca after the Greek island in Homer's Odyssey.[113] This deep neural network helps researchers restore the empty text of damaged Greek documents, and to identify their date and geographical origin.[114] The work builds on another text analysis network that DeepMind released in 2019, named Pythia.[114] Ithaca achieves 62% accuracy in restoring damaged texts and 71% location accuracy, and has a dating precision of 30 years.[114] The authors claimed that the use of Ithaca by \"expert historians\" raised the accuracy of their work from 25 to 72 percent.[113] However, Eleanor Dickey noted that this test was actually only made of students, saying that it wasn't clear how helpful Ithaca would be to \"genuinely qualified editors\".[114]\n The team is working on extending the model to other ancient languages, including Demotic, Akkadian, Hebrew, and Mayan.[113]\n In November 2023, Google DeepMind announced an Open Source Graph Network for Materials Exploration (GNoME). The tool proposes millions of materials previously unknown to chemistry, including several hundred thousand stable crystalline structures, of which 736 had been experimentally produced by the Massachusetts Institute of Technology, at the time of the release.[115][116] However, according to Anthony Cheetham, GNoME did not make \"a useful, practical contribution to the experimental materials scientists.\"[117] A review article by Cheetham and Ram Seshadri were unable to identify any \"strikingly novel\" materials found by GNoME, with most being minor variants of already-known materials.[117][118]\n In October 2022, DeepMind released AlphaTensor, which used reinforcement learning techniques similar to those in AlphaGo, to find novel algorithms for matrix multiplication.[119][120] In the special case of multiplying two 4\u00d74 matrices with integer entries, where only the evenness or oddness of the entries is recorded, AlphaTensor found an algorithm requiring only 47 distinct multiplications; the previous optimum, known since 1969, was the more general Strassen algorithm, using 49 multiplications.[121] Computer scientist Josh Alman described AlphaTensor as \"a proof of concept for something that could become a breakthrough,\" while Vassilevska Williams called it \"a little overhyped\"[121] despite also acknowledging its basis in reinforcement learning as \"something completely different\" from previous approaches.[120]\n AlphaGeometry is a neuro-symbolic AI that was able to solve 25 out of 30 geometry problems of the International Mathematical Olympiad, a performance comparable to that of a gold medalist.[122]\n Traditional geometry programs are symbolic engines that rely exclusively on human-coded rules to generate rigorous proofs, which makes them lack flexibility in unusual situations. AlphaGeometry combines such a symbolic engine with a specialized large language model trained on synthetic data of geometrical proofs. When the symbolic engine doesn't manage to find a formal and rigorous proof on its own, it solicits the large language model, which suggests a geometrical construct to move forward. However, it is unclear how applicable this method is to other domains of mathematics or reasoning, because symbolic engines rely on domain-specific rules and because of the need for synthetic data.[122]\n AlphaProof is an AI model, which couples a pre-trained language model with the AlphaZero reinforcement learning algorithm. AlphaZero has previously taught itself how to master games. The pre-trained language model used in this combination is the fine-tuning of a Gemini model to automatically translate natural language problem statements into formal statements, creating a large library of formal problems of varying difficulty. For this purpose, mathematical statements are defined in the formal language Lean. At the 2024 International Mathematical Olympiad, AlphaProof together with an adapted version of AlphaGeometry have reached the same level of solving problems in the combined categories as a silver medalist in that competition for the first time.[123][124]\n In June 2023, Deepmind announced that AlphaDev, which searches for improved computer science algorithms using reinforcement learning, discovered a more efficient way of coding a sorting algorithm and a hashing algorithm. The new sorting algorithm was 70% faster for shorter sequences and 1.7% faster for sequences exceeding 250,000 elements, and the new hashing algorithm was 30% faster in some cases. The sorting algorithm was accepted into the C++ Standard Library sorting algorithms, and was the first change to those algorithms in more than a decade and the first update to involve an algorithm discovered using AI.[125] The hashing algorithm was released to an opensource library.[126] Google estimates that these two algorithms are used trillions of times every day.[127]\n AlphaChip is an reinforcement learning-based neural architecture that guides the task of chip placement. DeepMind claimed that the time needed to create chip layouts fell from weeks to hours. Its chip designs were used in every Tensor Processing Unit (TPU) iteration since 2020.[128][129]\n Google has stated that DeepMind algorithms have greatly increased the efficiency of cooling its data centers by automatically balancing the cost of hardware failures against the cost of cooling.[130] In addition, DeepMind (alongside other Alphabet AI researchers) assists Google Play's personalized app recommendations.[84] DeepMind has also collaborated with the Android team at Google for the creation of two new features which were made available to people with devices running Android Pie, the ninth installment of Google's mobile operating system. These features, Adaptive Battery and Adaptive Brightness, use machine learning to conserve energy and make devices running the operating system easier to use. It is the first time DeepMind has used these techniques on such a small scale, with typical machine learning applications requiring orders of magnitude more computing power.[131]\n In July 2016, a collaboration between DeepMind and Moorfields Eye Hospital was announced to develop AI applications for healthcare.[132] DeepMind would be applied to the analysis of anonymised eye scans, searching for early signs of diseases leading to blindness.\n In August 2016, a research programme with University College London Hospital was announced with the aim of developing an algorithm that can automatically differentiate between healthy and cancerous tissues in head and neck areas.[133]\n There are also projects with the Royal Free London NHS Foundation Trust and Imperial College Healthcare NHS Trust to develop new clinical mobile apps linked to electronic patient records.[134] Staff at the Royal Free Hospital were reported as saying in December 2017 that access to patient data through the app had saved a 'huge amount of time' and made a 'phenomenal' difference to the management of patients with acute kidney injury. Test result data is sent to staff's mobile phones and alerts them to changes in the patient's condition. It also enables staff to see if someone else has responded, and to show patients their results in visual form.[135][unreliable source?]\n In November 2017, DeepMind announced a research partnership with the Cancer Research UK Centre at Imperial College London with the goal of improving breast cancer detection by applying machine learning to mammography.[136] Additionally, in February 2018, DeepMind announced it was working with the U.S. Department of Veterans Affairs in an attempt to use machine learning to predict the onset of acute kidney injury in patients, and also more broadly the general deterioration of patients during a hospital stay so that doctors and nurses can more quickly treat patients in need.[137]\n DeepMind developed an app called Streams, which sends alerts to doctors about patients at risk of acute kidney injury.[138] On 13 November 2018, DeepMind announced that its health division and the Streams app would be absorbed into Google Health.[139] Privacy advocates said the announcement betrayed patient trust and appeared to contradict previous statements by DeepMind that patient data would not be connected to Google accounts or services.[140][141] A spokesman for DeepMind said that patient data would still be kept separate from Google services or projects.[142]\n In April 2016, New Scientist obtained a copy of a data sharing agreement between DeepMind and the Royal Free London NHS Foundation Trust. The latter operates three London hospitals where an estimated 1.6 million patients are treated annually. The agreement shows DeepMind Health had access to admissions, discharge and transfer data, accident and emergency, pathology and radiology, and critical care at these hospitals. This included personal details such as whether patients had been diagnosed with HIV, suffered from depression or had ever undergone an abortion in order to conduct research to seek better outcomes in various health conditions.[143][144]\n A complaint was filed to the Information Commissioner's Office (ICO), arguing that the data should be pseudonymised and encrypted.[145] In May 2016, New Scientist published a further article claiming that the project had failed to secure approval from the Confidentiality Advisory Group of the Medicines and Healthcare products Regulatory Agency.[146]\n In 2017, the ICO concluded a year-long investigation that focused on how the Royal Free NHS Foundation Trust tested the app, Streams, in late 2015 and 2016.[147] The ICO found that the Royal Free failed to comply with the Data Protection Act when it provided patient details to DeepMind, and found several shortcomings in how the data was handled, including that patients were not adequately informed that their data would be used as part of the test. DeepMind published its thoughts[148] on the investigation in July 2017, saying \"we need to do better\" and highlighting several activities and initiatives they had initiated for transparency, oversight and engagement. This included developing a patient and public involvement strategy[149] and being transparent in its partnerships.\n In May 2017, Sky News published a leaked letter from the National Data Guardian, Dame Fiona Caldicott, revealing that in her \"considered opinion\" the data-sharing agreement between DeepMind and the Royal Free took place on an \"inappropriate legal basis\".[150] The Information Commissioner's Office ruled in July 2017 that the Royal Free hospital failed to comply with the Data Protection Act when it handed over personal data of 1.6 million patients to DeepMind.[151]\n In October 2017, DeepMind announced a new research unit, DeepMind Ethics & Society.[152] Their goal is to fund external research of the following themes: privacy, transparency, and fairness; economic impacts; governance and accountability; managing AI risk; AI morality and values; and how AI can address the world's challenges. As a result, the team hopes to further understand the ethical implications of AI and aid society to seeing AI can be beneficial.[153]\n This new subdivision of DeepMind is a completely separate unit from the partnership of leading companies using AI, academia, civil society organizations and nonprofits of the name Partnership on Artificial Intelligence to Benefit People and Society of which DeepMind is also a part.[154] The DeepMind Ethics and Society board is also distinct from the mooted AI Ethics Board that Google originally agreed to form when acquiring DeepMind.[155]\n DeepMind sponsors three chairs of machine learning:\n",
        "doc_number": 40
    },
    {
        "url": "https://en.wikipedia.org/wiki/Google_AI",
        "content": "\n Google AI is a division of Google dedicated to artificial intelligence.[1] It was announced at Google I/O 2017 by CEO Sundar Pichai.[2]\n This division has expanded its reach with research facilities in various parts of the world such as Zurich, Paris, Israel, and Beijing.[3] In 2023, Google AI was part of the reorganization initiative that elevated its head, Jeff Dean, to the position of chief scientist at Google.[4] This reorganization involved the merging of Google Brain and DeepMind, a UK-based company that Google acquired in 2014 that operated separately from the company's core research.[5]\n In March 2019 Google announced the creation of an Advanced Technology External Advisory Council (ATEAC) comprising eight members: Alessandro Acquisti, Bubacarr Bah, De Kai, Dyan Gibbens, Joanna Bryson, Kay Coles James, Luciano Floridi and William Joseph Burns.  Following objections from a large number of Google staff to the appointment of Kay Coles James, the Council was abandoned within one month of its establishment.[6]\n",
        "doc_number": 41
    },
    {
        "url": "https://en.wikipedia.org/wiki/Amazon_Machine_Learning",
        "content": "\n Amazon Web Services, Inc. (AWS) is a subsidiary of Amazon that provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered, pay-as-you-go basis. Clients will often use this in combination with autoscaling (a process that allows a client to use more computing in times of high application usage, and then scale down to reduce costs when there is less traffic). These cloud computing web services provide various services related to networking, compute, storage, middleware, IoT and other processing capacity, as well as software tools via AWS server farms.  This frees clients from managing, scaling, and patching hardware and operating systems. \nOne of the foundational services is Amazon Elastic Compute Cloud (EC2), which allows users to have at their disposal a virtual cluster of computers, with extremely high availability, which can be interacted with over the internet via REST APIs, a CLI or the AWS console.  AWS's virtual computers emulate most of the attributes of a real computer, including hardware central processing units (CPUs) and graphics processing units (GPUs) for processing; local/RAM memory; hard-disk (HDD)/SSD storage; a choice of operating systems; networking; and pre-loaded application software such as web servers, databases, and customer relationship management (CRM).\n AWS services are delivered to customers via a network of AWS server farms located throughout the world. Fees are based on a combination of usage (known as a \"Pay-as-you-go\" model), hardware, operating system, software, and networking features chosen by the subscriber requiring various degrees of availability, redundancy, security, and service options. Subscribers can pay for a single virtual AWS computer, a dedicated physical computer, or clusters of either.[7] Amazon provides select portions of security for subscribers (e.g. physical security of the data centers) while other aspects of security are the responsibility of the subscriber (e.g. account management, vulnerability scanning, patching). AWS operates from many global geographical regions including seven in North America.[8]\n Amazon markets AWS to subscribers as a way of obtaining large-scale computing capacity more quickly and cheaply than building an actual physical server farm.[9] All services are billed based on usage, but each service measures usage in varying ways. As of 2023 Q1, AWS has 31% market share for cloud infrastructure while the next two competitors Microsoft Azure and Google Cloud have 25%, and 11% respectively, according to Synergy Research Group.[10][11]\n As of 2021,[update] AWS comprises over 200[12] products and services including computing, storage, networking, database, analytics, application services, deployment, management, machine learning,[13] mobile, developer tools, RobOps and tools for the Internet of Things. The most popular include Amazon Elastic Compute Cloud (EC2), Amazon Simple Storage Service (Amazon S3), Amazon Connect, and AWS Lambda (a serverless function that can perform arbitrary code written in any language that can be configured to be triggered by hundreds of events, including HTTP calls).[14]\n Services expose functionality through APIs for clients to use in their applications.  These APIs are accessed over HTTP, using the REST architectural style and SOAP protocol for older APIs and exclusively JSON for newer ones. Clients can interact with these APIs in various ways, including from the AWS console (a website), by using SDKs written in various languages (such as Python, Java, and JavaScript), or by making direct REST calls.\n The genesis of AWS came in the early 2000s. After building Merchant.com, Amazon's e-commerce-as-a-service platform that offers third-party retailers a way to build their own web-stores, Amazon pursued service-oriented architecture as a means to scale its engineering operations,[15][16][17][18][19][20][21] led by then CTO Allan Vermeulen.[22]\n Around the same time frame, Amazon was frustrated with the speed of its software engineering, and sought to implement various recommendations put forth by Matt Round, an engineering leader at the time, including maximization of autonomy for engineering teams, adoption of REST, standardization of infrastructure, removal of gate-keeping decision-makers (bureaucracy), and continuous deployment. He also called for increasing the percentage of the time engineers spent building the software rather than doing other tasks.[23] Amazon created \"a shared IT platform\" so its engineering organizations, which were spending 70% of their time on \"undifferentiated heavy-lifting\" such as IT and infrastructure problems, could focus on customer-facing innovation instead.[24][25] Besides, in dealing with unusual peak traffic patterns, especially during the holiday season, by migrating services to commodity Linux hardware and relying on open source software, Amazon's Infrastructure team, led by Tom Killalea,[26] Amazon's first CISO,[27] had already run its data centers and associated services in a \"fast, reliable, cheap\" way.[26]\n In July 2002 Amazon.com Web Services, managed by Colin Bryar,[28] launched its first web services, opening up the Amazon.com platform to all developers.[29] Over one hundred applications were built on top of it by 2004.[30]  This unexpected developer interest took Amazon by surprise and convinced them that developers were \"hungry for more\".[25]\n By the summer of 2003, Andy Jassy had taken over Bryar's portfolio[31] at Rick Dalzell's behest, after Vermeulen, who was Bezos' first pick, declined the offer.[22] Jassy subsequently mapped out the vision for an \"Internet OS\"[15][17][19][32] made up of foundational infrastructure primitives that alleviated key impediments to shipping software applications faster.[15][16][17][19][21] By fall 2003,[15][17] databases, storage, and compute were identified as the first set of infrastructure pieces that Amazon should launch.[15][17][25]\n Jeff Barr, an early AWS employee, credits Vermeulen, Jassy, Bezos himself, and a few others for coming up with the idea that would evolve into EC2, S3, and RDS;[33] Jassy recalls the idea was the result of brainstorming for about a week with \"ten of the best technology minds and ten of the best product management minds\" on about ten different internet applications and the most primitive building blocks required to build them.[19] Werner Vogels cites Amazon's desire to make the process of \"invent, launch, reinvent, relaunch, start over, rinse, repeat\" as fast as it could was leading them to break down organizational structures with \"two-pizza teams\"[c] and application structures with distributed systems;[d] and that these changes ultimately paved way for the formation of AWS[21] and its mission \"to expose all of the atomic-level pieces of the Amazon.com platform\".[36] According to Brewster Kahle, co-founder of Alexa Internet, which was acquired by Amazon in 1999, his start-up's compute infrastructure helped Amazon solve its big data problems and later informed the innovations that underpinned AWS.[37]\n Jassy assembled a founding team of 57 employees from a mix of engineering and business backgrounds to kick-start these initiatives,[19][18] with a majority of the hires coming from outside the company;[19] Jeff Lawson, Twilio CEO,[38] Adam Selipsky, Tableau CEO,[39][40] and Mikhail Seregine,[41] co-founder at Outschool among them.\n In late 2003, the concept for compute,[e] which would later launch as EC2, was reformulated when Chris Pinkham and Benjamin Black presented a paper internally describing a vision for Amazon's retail computing infrastructure that was completely standardized, completely automated, and would rely extensively on web services for services such as storage and would draw on internal work already underway. Near the end of their paper, they mentioned the possibility of selling access to virtual servers as a service, proposing the company could generate revenue from the new infrastructure investment.[43][unreliable source?] Thereafter Pinkham, Willem van Biljon, and lead developer Christopher Brown developed the Amazon EC2 service, with a team in Cape Town, South Africa.[44]\n In November 2004, AWS launched its first infrastructure service for public usage: Simple Queue Service (SQS).[45]\n On March 14, 2006, AWS launched Amazon S3 cloud storage[46] followed by EC2 in August 2006.[47][48] Pi Corporation, a startup Paul Maritz co-founded, was the first beta-user of EC2 outside of Amazon,[19] while Microsoft was among EC2's first enterprise customers.[49] Later that year, SmugMug, one of the early AWS adopters, attributed savings of around US$400,000 in storage costs to S3.[50] According to Vogels, S3 was built with 8 microservices when it launched in 2006, but had over 300 microservices by 2022.[51]\n In September 2007, AWS announced its annual Start-up Challenge, a contest with prizes worth $100,000 for entrepreneurs and software developers based in the US using AWS services such as S3 and EC2 to build their businesses.[52] The first edition saw participation from Justin.tv,[53] which Amazon would later acquire in 2014.[54] Ooyala, an online media company,[55] was the eventual winner.[53]\n Additional AWS services from this period include SimpleDB, Mechanical Turk, Elastic Block Store, Elastic Beanstalk, Relational Database Service, DynamoDB, CloudWatch, Simple Workflow, CloudFront, and Availability Zones.\n In November 2010, it was reported that all of Amazon.com's retail sites had migrated to AWS.[56] Prior to 2012, AWS was considered a part of Amazon.com and so its revenue was not delineated in Amazon financial statements. In that year industry watchers for the first time estimated AWS revenue to be over $1.5 billion.[57]\n On November 27, 2012, AWS hosted its first major annual conference, re:Invent with a focus on AWS's partners and ecosystem,[58] with over 150 sessions.[59] The three-day event was held in Las Vegas because of its relatively cheaper connectivity with locations across the United States and the rest of the world.[60] Andy Jassy and Werner Vogels presented keynotes, with Jeff Bezos joining Vogels for a fireside chat.[61] AWS opened early registrations at  US$1,099 per head for their customers[59] from over 190 countries.[62] On stage with Andy Jassy at the event which saw around 6000 attendees, Reed Hastings, CEO at Netflix, announced plans to migrate 100% of Netflix's infrastructure to AWS.[61]\n To support industry-wide training and skills standardization, AWS began offering a certification program for computer engineers, on April 30, 2013, to highlight expertise in cloud computing.[63] Later that year, in October, AWS launched Activate, a program for start-ups worldwide to leverage AWS credits, third-party integrations, and free access to AWS experts to help build their business.[64]\n In 2014, AWS launched its partner network, AWS Partner Network (APN), which is focused on helping AWS-based companies grow and scale the success of their business with close collaboration and best practices.[65][66]\n In January 2015, Amazon Web Services acquired Annapurna Labs, an Israel-based microelectronics company for a reported US$350\u2013370M.[67][68]\n In April 2015, Amazon.com reported AWS was profitable, with sales of $1.57 billion in the first quarter of the year and $265 million of operating income. Founder Jeff Bezos described it as a fast-growing $5 billion business; analysts described it as \"surprisingly more profitable than forecast\".[69] In October, Amazon.com said in its Q3 earnings report that AWS's operating income was $521 million, with operating margins at 25 percent. AWS's 2015 Q3 revenue was $2.1 billion, a 78% increase from 2014's Q3 revenue of $1.17 billion.[70] 2015 Q4 revenue for the AWS segment increased 69.5% y/y to $2.4 billion with a 28.5% operating margin, giving AWS a $9.6 billion run rate. In 2015, Gartner estimated that AWS customers are deploying 10x more infrastructure on AWS than the combined adoption of the next 14 providers.[71]\n In 2016 Q1, revenue was $2.57 billion with net income of $604 million, a 64% increase over 2015 Q1 that resulted in AWS being more profitable than Amazon's North American retail business for the first time.[72] Jassy was thereafter promoted to CEO of the division.[73] Around the same time, Amazon experienced a 42% rise in stock value as a result of increased earnings, of which AWS contributed 56% to corporate profits.[74]\n AWS had $17.46 billion in annual revenue in 2017.[75] By the end of 2020, the number had grown to $46 billion.[76] Reflecting the success of AWS, Jassy's annual compensation in 2017 hit nearly $36 million.[77]\n In January 2018, Amazon launched an autoscaling service on AWS.[78][79]\n In November 2018, AWS announced customized ARM cores for use in its servers.[80] Also in November 2018, AWS is developing ground stations to communicate with customers' satellites.[81]\n In 2019, AWS reported 37% yearly growth and accounted for 12% of Amazon's revenue (up from 11% in 2018).[82]\n In April 2021, AWS reported 32% yearly growth and accounted for 32% of $41.8 billion cloud market in Q1 2021.[83]\n In January 2022, AWS joined the MACH Alliance, a non-profit enterprise technology advocacy group.[84]\n In June 2022, it was reported that in 2019 Capital One had not secured their AWS resources properly, and was subject to a data breach by a former AWS employee. The employee was convicted of hacking into the company's cloud servers to steal customer data and use computer power to mine cryptocurrency. The ex-employee was able to download the personal information of more than 100 million Capital One customers.[85]\n In June 2022, AWS announced they had launched the AWS Snowcone, a small computing device, to the International Space Station on the Axiom Mission 1.[86]\n In September 2023, AWS announced it would become AI startup Anthropic's primary cloud provider. Amazon has committed to investing up to $4 billion in Anthropic and will have a minority ownership position in the company.[87] AWS also announced the GA of Amazon Bedrock, a fully managed service that makes foundation models (FMs) from leading AI companies available through a single application programming interface (API)[88]\n In April 2024, AWS announced a new service called Deadline Cloud, which lets customers set up, deploy and scale up graphics and visual effects rendering pipelines on AWS cloud infrastructure.[89]\n In December 2024, AWS announced Amazon Nova, its own family of foundation models. These models, offered through Amazon Bedrock, are designed for various tasks including content generation, video understanding, and building agentic applications. They are available in six different sizes.[90]\n Notable customers include NASA,[91] and the Obama presidential campaign of 2012.[92]\n In October 2013, AWS was awarded a $600M contract with the CIA.[93]\n In 2019, it was reported that more than 80% of Germany's listed DAX companies use AWS.[94]\n In August 2019, the U.S. Navy said it moved 72,000 users from six commands to an AWS cloud system as a first step toward pushing all of its data and analytics onto the cloud.[95]\n In 2021, DISH Network announced it will develop and launch its 5G network on AWS.[96]\n In October 2021, it was reported that spy agencies and government departments in the UK such as GCHQ, MI5, MI6, and the Ministry of Defence, have contracted AWS to host their classified materials.[97]\n In 2022 Amazon shared a $9 billion contract from the United States Department of Defense for cloud computing with Google, Microsoft, and Oracle.[98]\n Multiple financial services firms have shifted to AWS in some form.[99][100][101]\n As of March\u00a02024,[update] AWS has distinct operations in 33 geographical \"regions\":[8] eight in North America, one in South America, eight in Europe, three in the Middle East, one in Africa, and twelve in Asia Pacific.\n Most AWS regions are enabled by default for AWS accounts. Regions introduced after 20 March 2019 are considered to be opt-in regions, requiring a user to explicitly enable them in order for the region to be usable in the account. For opt-in regions, Identity and Access Management (IAM) resources such as users and roles are only propagated to the regions that are enabled.[112]\n Each region is wholly contained within a single country and all of its data and services stay within the designated region.[7] Each region has multiple \"Availability Zones\",[113] which consist of one or more discrete data centers, each with redundant power, networking, and connectivity, housed in separate facilities. Availability Zones do not automatically provide additional scalability or redundancy within a region, since they are intentionally isolated from each other to prevent outages from spreading between zones. Several services can operate across Availability Zones (e.g., S3, DynamoDB) while others can be configured to replicate across zones to spread demand and avoid downtime from failures.\n As of December\u00a02014,[update] Amazon Web Services operated an estimated 1.4 million servers across 11 regions and 28 availability zones.[114] The global network of AWS Edge locations consists of over 300 points of presence worldwide, including locations in North America, Europe, Asia, Australia, Africa, and South America.[115]\n As of March\u00a02024,[update] AWS has announced the planned launch of six additional regions in Malaysia, Mexico, New Zealand, Thailand, Saudi Arabia, and the European Union.[8] In mid March 2023, Amazon Web Services signed a cooperation agreement with the New Zealand Government to build large data centers in New Zealand.[116]\n In 2014, AWS claimed its aim was to achieve 100% renewable energy usage in the future.[117]  In the United States, AWS's partnerships with renewable energy providers include Community Energy of Virginia, to support the US East region;[118] Pattern Development, in January 2015, to construct and operate Amazon Wind Farm Fowler Ridge;[119] Iberdrola Renewables, LLC, in July 2015, to construct and operate Amazon Wind Farm US East; EDP Renewables North America, in November 2015, to construct and operate Amazon Wind Farm US Central;[120] and Tesla Motors, to apply battery storage technology to address power needs in the US West (Northern California) region.[118]\n AWS also has \"pop-up lofts\" in different locations around the world.[121] These market AWS to entrepreneurs and startups in different tech industries in a physical location. Visitors can work or relax inside the loft, or learn more about what they can do with AWS. In June 2014, AWS opened their first temporary pop-up loft in San Francisco.[122] In May 2015 they expanded to New York City,[123][124] and in September 2015 expanded to Berlin.[125] AWS opened its fourth location, in Tel Aviv from March 1, 2016, to March 22, 2016.[126] A pop-up loft was open in London from September 10 to October 29, 2015.[127] The pop-up lofts in New York[128] and San Francisco[129] are indefinitely closed due to the COVID-19 pandemic while Tokyo has remained open in a limited capacity.[130]\n In 2017, AWS launched AWS re/Start in the United Kingdom to help young adults and military veterans retrain in technology-related skills.  In partnership with the Prince's Trust and the Ministry of Defence (MoD), AWS will help to provide re-training opportunities for young people from disadvantaged backgrounds and former military personnel.  AWS is working alongside a number of partner companies including Cloudreach, Sage Group, EDF Energy, and Tesco Bank.[131]\n In April 2022, AWS announced the organization has committed more than $30 million over three years to early-stage start-ups led by Black, Latino, LGBTQIA+, and Women founders as part of its AWS impact Accelerator. The Initiative offers qualifying start-ups up to $225,000 in cash, credits, extensive training, mentoring, technical guidance and includes up to $100,000 in AWS service credits.[132]\n In 2016, Greenpeace assessed major tech companies\u2014including cloud services providers like AWS, Microsoft, Oracle, Google, IBM, Salesforce and Rackspace\u2014based on their level of \"clean energy\" usage. Greenpeace evaluated companies on their mix of renewable-energy sources; transparency; renewable-energy commitment and policies; energy efficiency and greenhouse-gas mitigation; renewable-energy procurement; and advocacy. The group gave AWS an overall \"C\" grade. Greenpeace credited AWS for its advances toward greener computing in recent years and its plans to launch multiple wind and solar farms across the United States. The organization stated that Amazon is opaque about its carbon footprint.[133]\n In January 2021, AWS joined an industry pledge to achieve climate neutrality of data centers by 2030, the Climate Neutral Data Centre Pact.[134] As of 2023, Amazon as a whole is the largest corporate purchaser of renewable energy in the world, a position it has held since 2020, and has a global portfolio of over 20 GW of renewable energy capacity.[135] In 2022, 90% of all Amazon operations, including data centers, were powered by renewables.[136]\n US Department of Homeland Security has employed the software ATLAS, which runs on Amazon Cloud. It scanned more than 16.5 million records of naturalized Americans and flagged approximately 124,000 of them for manual analysis and review by USCIS officers regarding denaturalization.[137][138] Some of the scanned data came from the Terrorist Screening Database and the National Crime Information Center. The algorithm and the criteria for the algorithm were secret. Amazon faced protests from its own employees and activists for the anti-migrant collaboration with authorities.[139]\n The contract for Project Nimbus drew rebuke and condemnation from the companies' shareholders as well as their employees, over concerns that the project would lead to abuses of Palestinians' human rights in the context of the ongoing occupation and the Israeli\u2013Palestinian conflict.[140][141][142][143] Specifically, they voice concern over how the technology will enable further surveillance of Palestinians and unlawful data collection on them as well as facilitate the expansion of Israel's illegal settlements on Palestinian land.[142] A government procurement document featuring 'obligatory customers' of Nimbus, including \"two of Israel\u2019s leading state-owned weapons manufacturers\" Israel Aerospace Industries and Rafael Advanced Defense Systems, was published in 2021 with periodic updates since (up to Oct 2023).[144]\n Like other cloud computing solutions, applications hosted on Amazon Web Services (AWS) are subject to the fallacies of distributed computing, a series of misconceptions that can lead to significant issues in software development and deployment. [145]\n Some AWS customers have complained about receiving unexpectedly large bills, commonly referred to as \"surprise bills.\" This can occur due to various reasons, including but not limited to misconfigurations, security breaches, complex pricing\u2014especially when multiple AWS services are used together\u2014and unexpected data transfer charges.[146][147][148]\n",
        "doc_number": 42
    },
    {
        "url": "https://en.wikipedia.org/wiki/IBM_Watson",
        "content": "IBM Watson is a computer system capable of answering questions posed in natural language.[1] It was developed as a part of IBM's DeepQA project by a research team, led by principal investigator David Ferrucci.[2] Watson was named after IBM's founder and first CEO, industrialist Thomas J. Watson.[3][4]\n The computer system was initially developed to answer questions on the popular quiz show Jeopardy![5] and in 2011, the Watson computer system competed on Jeopardy! against champions Brad Rutter and Ken Jennings,[3][6] winning the first-place prize of US$1 million.[7]\n In February 2013, IBM announced that Watson's first commercial application would be for utilization management decisions in lung cancer treatment, at Memorial Sloan Kettering Cancer Center, New York City, in conjunction with WellPoint (now Elevance Health).[8]\n Watson was created as a question answering (QA) computing system that IBM built to apply advanced natural language processing, information retrieval, knowledge representation, automated reasoning, and machine learning technologies to the field of open domain question answering.[1]\n IBM stated that Watson uses \"more than 100 different techniques to analyze natural language, identify sources, find and generate hypotheses, find and score evidence, and merge and rank hypotheses.\"[10]\n In recent years, Watson's capabilities have been extended and the way in which Watson works has been changed to take advantage of new deployment models (Watson on IBM Cloud), evolved machine learning capabilities, and optimized hardware available to developers and researchers. [citation needed]\n Watson uses IBM's DeepQA software and the Apache UIMA (Unstructured Information Management Architecture) framework implementation. The system was written in various languages, including Java, C++, and Prolog, and runs on the SUSE Linux Enterprise Server 11 operating system using the Apache Hadoop framework to provide distributed computing.[11][12][13]\n The system is workload-optimized, integrating massively parallel POWER7 processors and built on IBM's DeepQA technology,[14] which it uses to generate hypotheses, gather massive evidence, and analyze data.[1] Watson employs a cluster of ninety IBM Power 750 servers, each of which uses a 3.5\u00a0GHz POWER7 eight-core processor, with four threads per core. In total, the system uses 2,880 POWER7 processor threads and 16 terabytes of RAM.[14]\n According to John Rennie, Watson can process 500 gigabytes (the equivalent of a million books) per second.[15] IBM master inventor and senior consultant Tony Pearson estimated Watson's hardware cost at about three million dollars.[16] Its Linpack performance stands at 80 TeraFLOPs, which is about half as fast as the cut-off line for the Top 500 Supercomputers list.[17] According to Rennie, all content was stored in Watson's RAM for the Jeopardy game because data stored on hard drives would be too slow to compete with human Jeopardy champions.[15]\n The sources of information for Watson include encyclopedias, dictionaries, thesauri, newswire articles and literary works. Watson also used databases, taxonomies and ontologies including DBPedia, WordNet and Yago.[18] The IBM team provided Watson with millions of documents, including dictionaries, encyclopedias and other reference material, that it could use to build its knowledge.[19]\n Watson parses questions into different keywords and sentence fragments in order to find statistically related phrases.[19] Watson's main innovation was not in the creation of a new algorithm for this operation, but rather its ability to quickly execute hundreds of proven language analysis algorithms simultaneously.[19][20] The more algorithms that find the same answer independently, the more likely Watson is to be correct. Once Watson has a small number of potential solutions, it is able to check against its database to ascertain whether the solution makes sense or not.[19]\n Watson's basic working principle is to parse keywords in a clue while searching for related terms as responses. This gives Watson some advantages and disadvantages compared with human Jeopardy! players.[21] Watson has deficiencies in understanding the context of the clues. Watson can read, analyze, and learn from natural language, which gives it the ability to make human-like decisions.[22] As a result, human players usually generate responses faster than Watson, especially to short clues.[19] Watson's programming prevents it from using the popular tactic of buzzing before it is sure of its response.[19] However, Watson has consistently better reaction time on the buzzer once it has generated a response, and is immune to human players' psychological tactics, such as jumping between categories on every clue.[19][23]\n In a sequence of 20 mock games of Jeopardy!, human participants were able to use the six to seven seconds that Watson needed to hear the clue and decide whether to signal for responding.[19] During that time, Watson also has to evaluate the response and determine whether it is sufficiently confident in the result to signal.[19] Part of the system used to win the Jeopardy! contest was the electronic circuitry that receives the \"ready\" signal and then examines whether Watson's confidence level was great enough to activate the buzzer. Given the speed of this circuitry compared to the speed of human reaction times, Watson's reaction time was faster than the human contestants except when the human anticipated (instead of reacted to) the ready signal.[24] After signaling, Watson speaks with an electronic voice and gives the responses in Jeopardy!'s question format.[19] Watson's voice was synthesized from recordings that actor Jeff Woodman made for an IBM text-to-speech program in 2004.[25]\n The Jeopardy! staff used different means to notify Watson and the human players when to buzz,[24] which was critical in many rounds.[23] The humans were notified by a light, which took them tenths of a second to perceive.[26][27] Watson was notified by an electronic signal and could activate the buzzer within about eight milliseconds.[28] The humans tried to compensate for the perception delay by anticipating the light,[29] but the variation in the anticipation time was generally too great to fall within Watson's response time.[23] Watson did not attempt to anticipate the notification signal.[27][29]\n Since Deep Blue's victory over Garry Kasparov in chess in 1997, IBM had been on the hunt for a new challenge. In 2004, IBM Research manager Charles Lickel, over dinner with coworkers, noticed that the restaurant they were in had fallen silent. He soon discovered the cause of this evening's hiatus: Ken Jennings, who was then in the middle of his successful 74-game run on Jeopardy!. Nearly the entire restaurant had piled toward the televisions, mid-meal, to watch Jeopardy!. Intrigued by the quiz show as a possible challenge for IBM, Lickel passed the idea on, and in 2005, IBM Research executive Paul Horn supported Lickel, pushing for someone in his department to take up the challenge of playing Jeopardy! with an IBM system. Though he initially had trouble finding any research staff willing to take on what looked to be a much more complex challenge than the wordless game of chess, eventually David Ferrucci took him up on the offer.[30] In competitions managed by the United States government, Watson's predecessor, a system named Piquant, was usually able to respond correctly to only about 35% of clues and often required several minutes to respond.[31][32][33] To compete successfully on Jeopardy!, Watson would need to respond in no more than a few seconds, and at that time, the problems posed by the game show were deemed to be impossible to solve.[19]\n In initial tests run during 2006 by David Ferrucci, the senior manager of IBM's Semantic Analysis and Integration department, Watson was given 500 clues from past Jeopardy! programs. While the best real-life competitors buzzed in half the time and responded correctly to as many as 95% of clues, Watson's first pass could get only about 15% correct. During 2007, the IBM team was given three to five years and a staff of 15 people to solve the problems.[19] John E. Kelly III succeeded Paul Horn as head of IBM Research in 2007.[34] InformationWeek described Kelly as \"the father of Watson\" and credited him for encouraging the system to compete against humans on Jeopardy!.[35] By 2008, the developers had advanced Watson such that it could compete with Jeopardy! champions.[19] By February 2010, Watson could beat human Jeopardy! contestants on a regular basis.[36]\n During the game, Watson had access to 200 million pages of structured and unstructured content consuming four terabytes of disk storage[11] including the full text of the 2011 edition of Wikipedia,[37] but was not connected to the Internet.[38][19] For each clue, Watson's three most probable responses were displayed on the television screen. Watson consistently outperformed its human opponents on the game's signaling device, but had trouble in a few categories, notably those having short clues containing only a few words.[citation needed]\n Although the system is primarily an IBM effort, Watson's development involved faculty and graduate students from Rensselaer Polytechnic Institute, Carnegie Mellon University, University of Massachusetts Amherst, the University of Southern California's Information Sciences Institute, the University of Texas at Austin, the Massachusetts Institute of Technology, and the University of Trento,[9] as well as students from New York Medical College.[39] Among the team of IBM programmers who worked on Watson was 2001 Who Wants to Be a Millionaire? top prize winner Ed Toutant, who himself had appeared on Jeopardy! in 1989 (winning one game).[40]\n In 2008, IBM representatives communicated with Jeopardy! executive producer Harry Friedman about the possibility of having Watson compete against Ken Jennings and Brad Rutter, two of the most successful contestants on the show, and the program's producers agreed.[19][41] Watson's differences with human players had generated conflicts between IBM and Jeopardy! staff during the planning of the competition.[21] IBM repeatedly expressed concerns that the show's writers would exploit Watson's cognitive deficiencies when writing the clues, thereby turning the game into a Turing test. To alleviate that claim, a third party randomly picked the clues from previously written shows that were never broadcast.[21] Jeopardy! staff also showed concerns over Watson's reaction time on the buzzer. Originally Watson signaled electronically, but show staff requested that it press a button physically, as the human contestants would.[42] Even with a robotic \"finger\" pressing the buzzer, Watson remained faster than its human competitors. Ken Jennings noted, \"If you're trying to win on the show, the buzzer is all\", and that Watson \"can knock out a microsecond-precise buzz every single time with little or no variation. Human reflexes can't compete with computer circuits in this regard.\"[23][29][43] Stephen Baker, a journalist who recorded Watson's development in his book Final Jeopardy, reported that the conflict between IBM and Jeopardy! became so serious in May 2010 that the competition was almost cancelled.[21] As part of the preparation, IBM constructed a mock set in a conference room at one of its technology sites to model the one used on Jeopardy!. Human players, including former Jeopardy! contestants, also participated in mock games against Watson with Todd Alan Crain of The Onion playing host.[19] About 100 test matches were conducted with Watson winning 65% of the games.[44]\n To provide a physical presence in the televised games, Watson was represented by an \"avatar\" of a globe, inspired by the IBM \"smarter planet\" symbol. Jennings described the computer's avatar as a \"glowing blue ball crisscrossed by 'threads' of thought\u201442 threads, to be precise\",[45] and stated that the number of thought threads in the avatar was an in-joke referencing the significance of the number 42 in Douglas Adams' Hitchhiker's Guide to the Galaxy.[45] Joshua Davis, the artist who designed the avatar for the project, explained to Stephen Baker that there are 36 trigger-able states that Watson was able to use throughout the game to show its confidence in responding to a clue correctly; he had hoped to be able to find forty-two, to add another level to the Hitchhiker's Guide reference, but he was unable to pinpoint enough game states.[46]\n A practice match was recorded on January 13, 2011, and the official matches were recorded on January 14, 2011. All participants maintained secrecy about the outcome until the match was broadcast in February.[47]\n In a practice match before the press on January 13, 2011, Watson won a 15-question round against Ken Jennings and Brad Rutter with a score of $4,400 to Jennings's $3,400 and Rutter's $1,200, though Jennings and Watson were tied before the final $1,000 question. None of the three players responded incorrectly to a clue.[48]\n The first round was broadcast February 14, 2011, and the second round, on February 15, 2011. The right to choose the first category had been determined by a draw won by Rutter.[49] Watson, represented by a computer monitor display and artificial voice, responded correctly to the second clue and then selected the fourth clue of the first category, a deliberate strategy to find the Daily Double as quickly as possible.[50] Watson's guess at the Daily Double location was correct. At the end of the first round, Watson was tied with Rutter at $5,000; Jennings had $2,000.[49]\n Watson's performance was characterized by some quirks. In one instance, Watson repeated a reworded version of an incorrect response offered by Jennings. (Jennings said \"What are the '20s?\" in reference to the 1920s. Then Watson said \"What is 1920s?\") Because Watson could not recognize other contestants' responses, it did not know that Jennings had already given the same response. In another instance, Watson was initially given credit for a response of \"What is a leg?\" after Jennings incorrectly responded \"What is: he only had one hand?\" to a clue about George Eyser (the correct response was, \"What is: he's missing a leg?\"). Because Watson, unlike a human, could not have been responding to Jennings's mistake, it was decided that this response was incorrect. The broadcast version of the episode was edited to omit Trebek's original acceptance of Watson's response.[51] Watson also demonstrated complex wagering strategies on the Daily Doubles, with one bet at $6,435 and another at $1,246.[52] Gerald Tesauro, one of the IBM researchers who worked on Watson, explained that Watson's wagers were based on its confidence level for the category and a complex regression model called the Game State Evaluator.[53]\n Watson took a commanding lead in Double Jeopardy!, correctly responding to both Daily Doubles. Watson responded to the second Daily Double correctly with a 32% confidence score.[52]\n However, during the Final Jeopardy! round, Watson was the only contestant to miss the clue in the category U.S. Cities (\"Its largest airport was named for a World War II hero; its second largest, for a World War II battle\"). Rutter and Jennings gave the correct response of Chicago, but Watson's response was \"What is Toronto?????\" with five question marks appended indicating a lack of confidence.[52][54][55] Ferrucci offered reasons why Watson would appear to have guessed a Canadian city: categories only weakly suggest the type of response desired, the phrase \"U.S. city\" did not appear in the question, there are cities named Toronto in the U.S., and Toronto in Ontario has an American League baseball team.[56] Chris Welty, who also worked on Watson, suggested that it may not have been able to correctly parse the second part of the clue, \"its second largest, for a World War II battle\" (which was not a standalone clause despite it following a semicolon, and required context to understand that it was referring to a second-largest airport).[57] Eric Nyberg, a professor at Carnegie Mellon University and a member of the development team, stated that the error occurred because Watson does not possess the comparative knowledge to discard that potential response as not viable.[55] Although not displayed to the audience as with non-Final Jeopardy! questions, Watson's second choice was Chicago. Both Toronto and Chicago were well below Watson's confidence threshold, at 14% and 11% respectively. Watson wagered only $947 on the question.[58]\n The game ended with Jennings with $4,800, Rutter with $10,400, and Watson with $35,734.[52]\n During the introduction, Trebek (a Canadian native) joked that he had learned Toronto was a U.S. city, and Watson's error in the first match prompted an IBM engineer to wear a Toronto Blue Jays jacket to the recording of the second match.[59]\n In the first round, Jennings was finally able to choose a Daily Double clue,[60] while Watson responded to one Daily Double clue incorrectly for the first time in the Double Jeopardy! Round.[61] After the first round, Watson placed second for the first time in the competition after Rutter and Jennings were briefly successful in increasing their dollar values before Watson could respond.[61][62] Nonetheless, the final result ended with a victory for Watson with a score of $77,147, besting Jennings who scored $24,000 and Rutter who scored $21,600.[63]\n The prizes for the competition were $1 million for first place (Watson), $300,000 for second place (Jennings), and $200,000 for third place (Rutter). As promised, IBM donated 100% of Watson's winnings to charity, with 50% of those winnings going to World Vision and 50% going to World Community Grid.[64] Similarly, Jennings and Rutter donated 50% of their winnings to their respective charities.[65]\n \nIn acknowledgement of IBM and Watson's achievements, Jennings made an additional remark in his Final Jeopardy! response: \"I for one welcome our new computer overlords\", paraphrasing a joke from The Simpsons.[66][67] Jennings later wrote an article for Slate, in which he stated:  IBM has bragged to the media that Watson's question-answering skills are good for more than annoying Alex Trebek. The company sees a future in which fields like medical diagnosis, business analytics, and tech support are automated by question-answering software like Watson. Just as factory jobs were eliminated in the 20th century by new assembly-line robots, Brad and I were the first knowledge-industry workers put out of work by the new generation of 'thinking' machines. 'Quiz show contestant' may be the first job made redundant by Watson, but I'm sure it won't be the last.[45] Philosopher John Searle argues that Watson\u2014despite impressive capabilities\u2014cannot actually think.[68] Drawing on his Chinese room thought experiment, Searle claims that Watson, like other computational machines, is capable only of manipulating symbols, but has no ability to understand the meaning of those symbols; however, Searle's experiment has its detractors.[69]\n On February 28, 2011, Watson played an untelevised exhibition match of Jeopardy! against members of the United States House of Representatives. In the first round, Rush D. Holt, Jr. (D-NJ, a former Jeopardy! contestant), who was challenging the computer with Bill Cassidy (R-LA, later Senator from Louisiana), led with Watson in second place. However, combining the scores between all matches, the final score was $40,300 for Watson and $30,000 for the congressional players combined.[70]\n IBM's Christopher Padilla said of the match, \"The technology behind Watson represents a major advancement in computing. In the data-intensive environment of government, this type of technology can help organizations make better decisions and improve how government helps its citizens.\"[70]\n According to IBM, \"The goal is to have computers start to interact in natural human terms across a range of applications and processes, understanding the questions that humans ask and providing answers that humans can understand and justify.\"[36] It has been suggested by Robert C. Weber, IBM's general counsel, that Watson may be used for legal research.[71] The company also intends to use Watson in other information-intensive fields, such as telecommunications, financial services, and government.[72]\n Watson is based on commercially available IBM Power 750 servers that have been marketed since February 2010.[19]\n Commentator Rick Merritt said that \"there's another really important reason why it is strategic for IBM to be seen very broadly by the American public as a company that can tackle tough computer problems. A big slice of [IBM's profit] comes from selling to the U.S. government some of the biggest, most expensive systems in the world.\"[73]\n In 2013, it was reported that three companies were working with IBM to create apps embedded with Watson technology. Fluid is developing an app for retailers, one called \"The North Face\", which is designed to provide advice to online shoppers. Welltok is developing an app designed to give people advice on ways to engage in activities to improve their health. MD Buyline is developing an app for the purpose of advising medical institutions on equipment procurement decisions.[74][75]\n In November 2013, IBM announced it would make Watson's API available to software application providers, enabling them to build apps and services that are embedded in Watson's capabilities. To build out its base of partners who create applications on the Watson platform, IBM consults with a network of venture capital firms, which advise IBM on which of their portfolio companies may be a logical fit for what IBM calls the Watson Ecosystem. Thus far, roughly 800 organizations and individuals have signed up with IBM, with interest in creating applications that could use the Watson platform.[76]\n On January 30, 2013, it was announced that Rensselaer Polytechnic Institute would receive a successor version of Watson, which would be housed at the institute's technology park and be available to researchers and students.[77] By summer 2013, Rensselaer had become the first university to receive a Watson computer.[78]\n On February 6, 2014, it was reported that IBM plans to invest $100 million in a 10-year initiative to use Watson and other IBM technologies to help countries in Africa address development problems, beginning with healthcare and education.[79]\n On June 3, 2014, three new Watson Ecosystem partners were chosen from more than 400 business concepts submitted by teams spanning 18 industries from 43 countries. \"These bright and enterprising organizations have discovered innovative ways to apply Watson that can deliver demonstrable business benefits\", said Steve Gold, vice president, IBM Watson Group. The winners were Majestyk Apps with their adaptive educational platform, FANG (Friendly Anthropomorphic Networked Genome);[80][81] Red Ant with their retail sales trainer;[82] and GenieMD[83] with their medical recommendation service.[84]\n On July 9, 2014, Genesys Telecommunications Laboratories announced plans to integrate Watson to improve their customer experience platform, citing the sheer volume of customer data to analyze.[85]\n Watson has been integrated with databases including Bon App\u00e9tit magazine to perform a recipe generating platform.[86]\n Watson is being used by Decibel, a music discovery startup, in its app MusicGeek which uses the supercomputer to provide music recommendations to its users. The use of Watson has also been found in the hospitality industry. Go Moment uses Watson for its Rev1 app, which gives hotel staff a way to quickly respond to questions from guests.[87] Arria NLG has built an app that helps energy companies stay within regulatory guidelines, making it easier for managers to make sense of thousands of pages of legal and technical jargon.\n OmniEarth, Inc. uses Watson computer vision services to analyze satellite and aerial imagery, along with other municipal data, to infer water usage on a property-by-property basis, helping districts in California improve water conservation efforts.[88]\n In September 2016, Cond\u00e9 Nast started using Watson to help build and strategize social influencer campaigns for brands. Using software built by IBM and Influential, Cond\u00e9 Nast's clients will be able to know which influencer's demographics, personality traits and more best align with a marketer and the audience it is targeting.[89]\n In February 2017, Rare Carat, a New York City-based startup and e-commerce platform for buying diamonds and diamond rings, introduced an IBM Watson-powered chatbot called \"Rocky\" to assist novice diamond buyers through the daunting process of purchasing a diamond. As part of the IBM Global Entrepreneur Program, Rare Carat received the assistance of IBM in the development of the Rocky Chat Bot.[90][91][92]\nIn May 2017, IBM partnered with the Pebble Beach Company to use Watson as a concierge.[93]  Watson technology was added to an app developed by Pebble Beach and was used to guide visitors around the resort.  The mobile app was designed by IBM iX and hosted on the IBM Cloud.  It uses Watson's Conversation applications programming interface.\n In November 2017, in Mexico City, the Experience Voices of Another Time was opened at the National Museum of Anthropology using IBM Watson as an alternative to visiting a museum.[94]\n In healthcare, Watson has been used to analyze medical data and assist doctors in making diagnoses and treatment decisions, including in areas such as oncology and radiology.[95]  Watson's natural language, hypothesis generation, and evidence-based learning capabilities are being investigated to see how Watson may contribute to clinical decision support systems.[96] To aid physicians in the treatment of their patients, once a physician has posed a query to the system describing symptoms and other related factors, Watson first parses the input to identify the most important pieces of information; then mines patient data to find facts relevant to the patient's medical and hereditary history; then examines available data sources to form and test hypotheses;[96] and finally provides a list of individualized, confidence-scored recommendations.[97] The sources of data that Watson uses for analysis can include treatment guidelines, electronic medical record data, notes from healthcare providers, research materials, clinical studies, journal articles and patient information.[96] Despite being developed and marketed as a \"diagnosis and treatment advisor\", Watson has never been actually involved in the medical diagnosis process, only in assisting with identifying treatment options for patients who have already been diagnosed.[98] An analysis of 1,000 difficult patient cases reviewed by cancer researcher Ned Sharpless found that Watson made the same recommendations as human doctors in 99% of cases.[99]\n In February 2011, it was announced that IBM would be partnering with Nuance Communications for a research project to develop a commercial product during the next 18 to 24 months, designed to exploit Watson's clinical decision support capabilities. Physicians at Columbia University would help to identify critical issues in the practice of medicine where the system's technology may be able to contribute, and physicians at the University of Maryland would work to identify the best way that a technology like Watson could interact with medical practitioners to provide the maximum assistance.[100]\n In September 2011, IBM and WellPoint (now Anthem) announced a partnership to utilize Watson to help suggest treatment options to physicians.[101] Then, in February 2013, IBM and WellPoint gave Watson its first commercial application, for utilization management decisions in lung cancer treatment at Memorial Sloan\u2013Kettering Cancer Center.[8]\n IBM announced a partnership with Cleveland Clinic in October 2012. The company has sent Watson to the Cleveland Clinic Lerner College of Medicine of Case Western Reserve University, where it will increase its health expertise and assist medical professionals in treating patients. The medical facility will utilize Watson's ability to store and process large quantities of information to help speed up and increase the accuracy of the treatment process. \"Cleveland Clinic's collaboration with IBM is exciting because it offers us the opportunity to teach Watson to 'think' in ways that have the potential to make it a powerful tool in medicine\", said C. Martin Harris, MD, chief information officer of Cleveland Clinic.[102]\n In 2013, IBM and MD Anderson Cancer Center began a pilot program to further the center's \"mission to eradicate cancer\".[103][104]  However, after spending $62 million, the project did not meet its goals and it has been stopped.[105]\n On February 8, 2013, IBM announced that oncologists at the Maine Center for Cancer Medicine and Westmed Medical Group in New York have started to test Watson in an effort to recommend treatment for lung cancer.[106]\n On July 29, 2016, IBM and Manipal Hospitals[107][108][109] (a leading hospital chain in India) announced the launch of IBM Watson for Oncology, for cancer patients. This product provides information and insights to physicians and cancer patients to help them identify personalized, evidence-based cancer care options. Manipal Hospitals is the second hospital[110] in the world to adopt this technology and first in the world to offer it to patients online as an expert second opinion through their website.[107][111] Manipal discontinued this contract in December 2018. [citation needed]\n On January 7, 2017, IBM and Fukoku Mutual Life Insurance entered into a contract for IBM to deliver analysis to compensation payouts via its IBM Watson Explorer AI, which resulted in the loss of 34 jobs. The company said it would speed up compensation payout analysis via analyzing claims and medical records, and increase productivity by 30%. The company also said it would save \u00a5140m in running costs.[112]\n Several startups in the healthcare space have been effectively using seven business model archetypes to take solutions based on IBM Watson to the marketplace. These archetypes depends on the value generate for the target user (e.g. patient focus vs. healthcare provider and payer focus) and value capturing mechanisms (e.g. providing information or connecting stakeholders).[113]\n By 2022, IBM Watson Health was generating about a billion dollars in annual gross revenue,[114] but was facing a lack of profitability and increased competition.   One expert assessed to CNN that \"IBM was clearly not gaining much traction in the healthcare market\". A 2021 post from the Association for Computing Machinery (ACM) titled \"What Happened To Watson Health?\" described the portfolio management challenges of IBM Watson Health given the number of acquisitions involved in the Watson Health division creation in 2015, as well as technical limitations that existed at the time regarding where the Watson AI framework could be deployed.[115] In February 2021, the Wall Street Journal reported that Watson Health was exploring a sale.[116]  \nOn January 21, 2022, IBM announced the sell-off of its Watson Health unit to Francisco Partners.[117]\n On January 9, 2014, IBM announced it was creating a business unit around Watson, led by senior vice president Michael Rhodin.[118] IBM Watson Group will have headquarters in New York City's Silicon Alley and will employ 2,000 people. IBM has invested $1 billion to get the division going. Watson Group will develop three new cloud-delivered services: Watson Discovery Advisor, Watson Engagement Advisor, and Watson Explorer. Watson Discovery Advisor will focus on research and development projects in pharmaceutical industry, publishing, and biotechnology, Watson Engagement Advisor will focus on self-service applications using insights on the basis of natural language questions posed by business users, and Watson Explorer will focus on helping enterprise users uncover and share data-driven insights based on federated search more easily.[118] The company is also launching a $100 million venture fund to spur application development for \"cognitive\" applications. According to IBM, the cloud-delivered enterprise-ready Watson has seen its speed increase 24 times over\u2014a 2,300 percent improvement in performance and its physical size shrank by 90 percent\u2014from the size of a master bedroom to three stacked pizza boxes.[118] IBM CEO Virginia Rometty said she wants Watson to generate $10 billion in annual revenue within ten years.[119] In 2017, IBM and MIT established a new joint research venture in artificial intelligence. IBM invested $240 million to create the MIT\u2013IBM Watson AI Lab in partnership with MIT, which brings together researchers in academia and industry to advance AI research, with projects ranging from computer vision and NLP to devising new ways to ensure that AI systems are fair, reliable and secure.[120] In March 2018, IBM's CEO Ginni Rometty proposed \"Watson's Law,\" the \"use of and application of business, smart cities, consumer applications and life in general.\"[121]\n Watson helped a team of chefs create five new poutines for the 2015 La Poutine Week food festival in Toronto and Montreal. It analyzed the demographics and popular cuisines of the cities and drew from a database of tens of thousands of recipes to create fusion pairings for each city.[122] IBM and Bon App\u00e9tit magazine co-created an AI cooking app known as Chef Watson.[123]\n Watson is being used via IBM partner program as a chatbot to provide the conversation for children's toys.[124]\n In 2015, the engineering firm ENGEO created an online service via the IBM partner program named GoFetchCode. GoFetchCode applies Watson's natural language processing and question-answering capabilities to the International Code Council's model building codes.[125]\n IBM Watson is being used for several projects relating to education, and has entered partnerships with Pearson Education, Blackboard, Sesame Workshop and Apple.[126][127]\n In its partnership with Pearson, Watson is being made available inside electronic text books to provide natural language, one-on-one tutoring to students on the reading material.[128]\n As an individual using the free Watson APIs available to the public, Ashok Goel, a professor at Georgia Tech, used Watson to create a virtual teaching assistant to assist students in his class.[129] Initially, Goel did not reveal the nature of \"Jill\", which was created with the help of a few students and IBM.  Jill answered questions where it had a 97% certainty of an accurate answer, with the remainder being answered by human assistants.[130]\n The research group of Sabri Pllana developed an assistant for learning parallel programming using the IBM Watson.[131] A survey with a number of novice parallel programmers at the Linnaeus University indicated that such assistants will be welcomed by students that learn parallel programming.\n In August 2016, IBM announced it would be using Watson for weather forecasting.[132] Specifically, the company announced they would use Watson to analyze data from over 200,000 Weather Underground personal weather stations, as well as data from other sources, as a part of Project Deep Thunder.[133]\n IBM Watson together with Marchesa designed a dress that changed the colour of the fabric depending on the mood of the audience. The dress lit up in different colours based on the sentiment of Tweets about the dress. Tweets were passed through a Watson tone analyzer and then sent back to a small computer inside the waist of the dress.[134]\n On February 5\u20136, 2017, tax preparation company H&R Block began nationwide use of a Watson-based program.[135]\n In September 2017, IBM announced that with its acquisition of The Weather Company's advertising sales division, and a partnership with advertising neural network Cognitiv, Watson will provide AI-powered advertising solutions.[136][137][138]\n",
        "doc_number": 43
    },
    {
        "url": "https://en.wikipedia.org/wiki/Data_mining",
        "content": "Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems.[1] Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use.[1][2][3][4] Data mining is the analysis step of the \"knowledge discovery in databases\" process, or KDD.[5] Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.[1]\n The term \"data mining\" is a misnomer because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself.[6] It also is a buzzword[7] and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. Often the more general terms (large scale) data analysis and analytics\u2014or, when referring to actual methods, artificial intelligence and machine learning\u2014are more appropriate.\n The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, although they do belong to the overall KDD process as additional steps.\n The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data. In contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.[8]\n The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.\n In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term \"data mining\" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.[9][10] Lovell indicates that the practice \"masquerades under a variety of aliases, ranging from \"experimentation\" (positive) to \"fishing\" or \"snooping\" (negative).\n The term data mining appeared around 1990 in the database community, with generally positive connotations. For a short time in 1980s, the phrase \"database mining\"\u2122, was used, but since it was trademarked by HNC, a San Diego\u2013based company, to pitch their Database Mining Workstation;[11] researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term \"knowledge discovery in databases\" for the first workshop on the same topic (KDD-1989) and this term became more popular in the AI and machine learning communities. However, the term data mining became more popular in the business and press communities.[12] Currently, the terms data mining and knowledge discovery are used interchangeably.\n The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s).[13] The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct \"hands-on\" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns.[14] in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.\n The knowledge discovery in databases (KDD) process is commonly defined with the stages:\n It exists, however, in many variations on this theme, such as the Cross-industry standard process for data mining (CRISP-DM) which defines six phases:\n or a simplified process such as (1) Pre-processing, (2) Data Mining, and (3) Results Validation.\n Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners.[15][16][17][18]\n The only other data mining standard named in these polls was SEMMA. However, 3\u20134 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models,[19] and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.[20]\n Before data mining algorithms can be used, a target data set must be assembled. As data mining can only uncover patterns actually present in the data, the target data set must be large enough to contain these patterns while remaining concise enough to be mined within an acceptable time limit. A common source for data is a data mart or data warehouse. Pre-processing is essential to analyze the multivariate data sets before data mining. The target set is then cleaned. Data cleaning removes the observations containing noise and those with missing data.\n Data mining involves six common classes of tasks:[5]\n Data mining can unintentionally be misused, producing results that appear to be significant but which do not actually predict future behavior and cannot be reproduced on a new sample of data, therefore bearing little use. This is sometimes caused by investigating too many hypotheses and not performing proper statistical hypothesis testing. A simple version of this problem in machine learning is known as overfitting, but the same problem can arise at different phases of the process and thus a train/test split\u2014when applicable at all\u2014may not be sufficient to prevent this from happening.[21]\n The final step of knowledge discovery from data is to verify that the patterns produced by the data mining algorithms occur in the wider data set. Not all patterns found by the algorithms are necessarily valid. It is common for data mining algorithms to find patterns in the training set which are not present in the general data set. This is called overfitting. To overcome this, the evaluation uses a test set of data on which the data mining algorithm was not trained. The learned patterns are applied to this test set, and the resulting output is compared to the desired output. For example, a data mining algorithm trying to distinguish \"spam\" from \"legitimate\" e-mails would be trained on a training set of sample e-mails. Once trained, the learned patterns would be applied to the test set of e-mails on which it had not been trained. The accuracy of the patterns can then be measured from how many e-mails they correctly classify. Several statistical methods may be used to evaluate the algorithm, such as ROC curves.\n If the learned patterns do not meet the desired standards, it is necessary to re-evaluate and change the pre-processing and data mining steps. If the learned patterns do meet the desired standards, then the final step is to interpret the learned patterns and turn them into knowledge.\n The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD).[22][23] Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings,[24] and since 1999 it has published a biannual academic journal titled \"SIGKDD Explorations\".[25]\n Computer science conferences on data mining include:\n Data mining topics are also present in many data management/database conferences such as the ICDE Conference, SIGMOD Conference and International Conference on Very Large Data Bases.\n There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.\n For exchanging the extracted models\u2014in particular for use in predictive analytics\u2014the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.[26]\n Data mining is used wherever there is digital data available. Notable examples of data mining can be found throughout business, medicine, science, finance, construction, and surveillance.\n While the term \"data mining\" itself may have no ethical implications, it is often associated with the mining of information in relation to user behavior (ethical and otherwise).[27]\n The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics.[28] In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.[29][30]\n Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent).[31] This is not data mining per se, but a result of the preparation of data before\u2014and for the purposes of\u2014the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.[32]\n It is recommended[according to whom?] to be aware of the following before data are collected:[31]\n Data may also be modified so as to become anonymous, so that individuals may not readily be identified.[31] However, even \"anonymized\" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.[33]\n The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,\nemotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling\nprescription information to data mining companies who in turn provided the data\nto pharmaceutical companies.[34]\n Europe has rather strong privacy laws, and efforts are underway to further strengthen the rights of the consumers. However, the U.S.\u2013E.U. Safe Harbor Principles, developed between 1998 and 2000, currently effectively expose European users to privacy exploitation by U.S. companies. As a consequence of Edward Snowden's global surveillance disclosure, there has been increased discussion to revoke this agreement, as in particular the data will be fully exposed to the National Security Agency, and attempts to reach an agreement with the United States have failed.[35]\n In the United Kingdom in particular there have been cases of corporations using data mining as a way to target certain groups of customers forcing them to pay unfairly high prices. These groups tend to be people of lower socio-economic status who are not savvy to the ways they can be exploited in digital market places.[36]\n In the United States, privacy concerns have been addressed by the US Congress via the passage of regulatory controls such as the Health Insurance Portability and Accountability Act (HIPAA). The HIPAA requires individuals to give their \"informed consent\" regarding information they provide and its intended present and future uses. According to an article in Biotech Business Week, \"'[i]n practice, HIPAA may not offer any greater protection than the longstanding regulations in the research arena,' says the AAHC. More importantly, the rule's goal of protection through informed consent is approach a level of incomprehensibility to average individuals.\"[37] This underscores the necessity for data anonymity in data aggregation and mining practices.\n U.S. information privacy legislation such as HIPAA and the Family Educational Rights and Privacy Act (FERPA) applies only to the specific areas that each such law addresses. The use of data mining by the majority of businesses in the U.S. is not controlled by any legislation.\n Under European copyright database laws, the mining of in-copyright works (such as by web mining) without the permission of the copyright owner is not legal. Where a database is pure data in Europe, it may be that there is no copyright\u2014but database rights may exist, so data mining becomes subject to intellectual property owners' rights that are protected by the Database Directive. On the recommendation of the Hargreaves review, this led to the UK government to amend its copyright law in 2014 to allow content mining as a limitation and exception.[38] The UK was the second country in the world to do so after Japan, which introduced an exception in 2009 for data mining. However, due to the restriction of the Information Society Directive (2001), the UK exception only allows content mining for non-commercial purposes. UK copyright law also does not allow this provision to be overridden by contractual terms and conditions.\nSince 2020 also Switzerland has been regulating data mining by allowing it in the research field under certain conditions laid down by art. 24d of the Swiss Copyright Act. This new article entered into force on 1 April 2020.[39]\n The European Commission facilitated stakeholder discussion on text and data mining in 2013, under the title of Licences for Europe.[40] The focus on the solution to this legal issue, such as licensing rather than limitations and exceptions, led to representatives of universities, researchers, libraries, civil society groups and open access publishers to leave the stakeholder dialogue in May 2013.[41]\n US copyright law, and in particular its provision for fair use, upholds the legality of content mining in America, and other fair use countries such as Israel, Taiwan and South Korea. As content mining is transformative, that is it does not supplant the original work, it is viewed as being lawful under fair use. For example, as part of the Google Book settlement the presiding judge on the case ruled that Google's digitization project of in-copyright books was lawful, in part because of the transformative uses that the digitization project displayed\u2014one being text and data mining.[42]\n The following applications are available under free/open-source licenses. Public access to application source code is also available.\n The following applications are available under proprietary licenses.\n For more information about extracting information out of data (as opposed to analyzing data), see:\n",
        "doc_number": 44
    },
    {
        "url": "https://en.wikipedia.org/wiki/Big_data",
        "content": "\n Big data primarily refers to data sets that are too large or complex to be dealt with by traditional data-processing software. Data with many entries (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate.[2]\n Big data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity.[3] The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data.[4] Without sufficient investment in expertise for big data veracity, the volume and variety of data can produce costs and risks that exceed an organization's capacity to create and capture value from big data.[5]\n Current usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. \"There is little doubt that the quantities of data now available are indeed large, but that's not the most relevant characteristic of this new data ecosystem.\"[6]\nAnalysis of data sets can find new correlations to \"spot business trends, prevent diseases, combat crime and so on\".[7] Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics,[8] connectomics, complex physics simulations, biology, and environmental research.[9]\n The size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing) equipment, software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks.[10][11] The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s;[12] as of 2012[update], every day 2.5 exabytes (2.17\u00d7260 bytes) of data are generated.[13] Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data.[14] According to IDC, global spending on big data and business analytics (BDA) solutions is estimated to reach $215.7 billion in 2021.[15][16] While Statista report, the global big data market is forecasted to grow to $103 billion by 2027.[17] In 2011 McKinsey & Company reported, if US healthcare were to use big data creatively and effectively to drive efficiency and quality, the sector could create more than $300 billion in value every year.[18] In the developed economies of Europe, government administrators could save more than \u20ac100 billion ($149 billion) in operational efficiency improvements alone by using big data.[18] And users of services enabled by personal-location data could capture $600 billion in consumer surplus.[18] One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.[19]\n Relational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require \"massively parallel software running on tens, hundreds, or even thousands of servers\".[20] What qualifies as \"big data\" varies depending on the capabilities of those analyzing it and their tools. Furthermore, expanding capabilities make big data a moving target. \"For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.\"[21]\n The term big data has been in use since the 1990s, with some giving credit to John Mashey for popularizing the term.[22][23]  Big data usually includes data sets with sizes beyond the ability of commonly used software tools to capture, curate, manage, and process data within a tolerable elapsed time.[24][page\u00a0needed]  Big data philosophy encompasses unstructured, semi-structured and structured data; however, the main focus is on unstructured data.[25] Big data \"size\" is a constantly moving target; as of 2012[update] ranging from a few dozen terabytes to many zettabytes of data.[26]  Big data requires a set of techniques and technologies with new forms of integration to reveal insights from data-sets that are diverse, complex, and of a massive scale.[27]\n \"Volume\", \"variety\", \"velocity\", and various other \"Vs\" are added by some organizations to describe it, a revision challenged by some industry authorities.[28] The Vs of big data were often referred to as the \"three Vs\", \"four Vs\", and \"five Vs\". They represented the qualities of big data in volume, variety, velocity, veracity, and value.[4] Variability is often included as an additional quality of big data.\n A 2018 definition states \"Big data is where parallel computing tools are needed to handle data\", and notes, \"This represents a distinct and clearly defined change in the computer science used, via parallel programming theories, and losses of some of the guarantees and capabilities made by Codd's relational model.\"[29]\n In a comparative study of big datasets, Kitchin and McArdle found that none of the commonly considered characteristics of big data appear consistently across all of the analyzed cases.[30] For this reason, other studies identified the redefinition of power dynamics in knowledge discovery as the defining trait.[31] Instead of focusing on the intrinsic characteristics of big data, this alternative perspective pushes forward a relational understanding of the object claiming that what matters is the way in which data is collected, stored, made available and analyzed.\n The growing maturity of the concept more starkly delineates the difference between \"big data\" and \"business intelligence\":[32]\n Big data can be described by the following characteristics:\n Other possible characteristics of big data are:[41]\n Big data repositories have existed in many forms, often built by corporations with a special need. Commercial vendors historically offered parallel database management systems for big data beginning in the 1990s. For many years, WinterCorp published the largest database report.[42][promotional source?]\n Teradata Corporation in 1984 marketed the parallel processing DBC 1012 system. Teradata systems were the first to store and analyze 1 terabyte of data in 1992. Hard disk drives were 2.5 GB in 1991 so the definition of big data continuously evolves. Teradata installed the first petabyte class RDBMS based system in 2007. As of 2017[update], there are a few dozen petabyte class Teradata relational databases installed, the largest of which exceeds 50 PB. Systems up until 2008 were 100% structured relational data. Since then, Teradata has added semi structured data types including XML, JSON, and Avro.\n In 2000, Seisint Inc. (now LexisNexis Risk Solutions) developed a C++-based distributed platform for data processing and querying known as the HPCC Systems platform. This system automatically partitions, distributes, stores and delivers structured, semi-structured, and unstructured data across multiple commodity servers. Users can write data processing pipelines and queries in a declarative dataflow programming language called ECL. Data analysts working in ECL are not required to define data schemas upfront and can rather focus on the particular problem at hand, reshaping data in the best possible manner as they develop the solution. In 2004, LexisNexis acquired Seisint Inc.[43] and their high-speed parallel processing platform and successfully used this platform to integrate the data systems of Choicepoint Inc. when they acquired that company in 2008.[44] In 2011, the HPCC systems platform was open-sourced under the Apache v2.0 License.\n CERN and other physics experiments have collected big data sets for many decades, usually analyzed via high-throughput computing rather than the map-reduce architectures usually meant by the current \"big data\" movement.\n In 2004, Google published a paper on a process called MapReduce that uses a similar architecture. The MapReduce concept provides a parallel processing model, and an associated implementation was released to process huge amounts of data. With MapReduce, queries are split and distributed across parallel nodes and processed in parallel (the \"map\" step). The results are then gathered and delivered (the \"reduce\" step). The framework was very successful,[45] so others wanted to replicate the algorithm. Therefore, an implementation of the MapReduce framework was adopted by an Apache open-source project named \"Hadoop\".[46] Apache Spark was developed in 2012 in response to limitations in the MapReduce paradigm, as it adds in-memory processing and the ability to set up many operations (not just map followed by reducing).\n MIKE2.0 is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled \"Big Data Solution Offering\".[47] The methodology addresses handling big data in terms of useful permutations of data sources, complexity in interrelationships, and difficulty in deleting (or modifying) individual records.[48]\n Studies in 2012 showed that a multiple-layer architecture was one option to address the issues that big data presents. A distributed parallel architecture distributes data across multiple servers; these parallel execution environments can dramatically improve data processing speeds. This type of architecture inserts data into a parallel DBMS, which implements the use of MapReduce and Hadoop frameworks. This type of framework looks to make the processing power transparent to the end-user by using a front-end application server.[49]\n The data lake allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management. This enables quick segregation of data into the data lake, thereby reducing the overhead time.[50][51]\n A 2011 McKinsey Global Institute report characterizes the main components and ecosystem of big data as follows:[52]\n Multidimensional big data can also be represented as OLAP data cubes or, mathematically, tensors. Array database systems have set out to provide storage and high-level query support on this data type.\nAdditional technologies being applied to big data include efficient tensor-based computation,[53] such as multilinear subspace learning,[54] massively parallel-processing (MPP) databases, search-based applications, data mining,[55] distributed file systems, distributed cache (e.g., burst buffer and Memcached), distributed databases, cloud and HPC-based infrastructure (applications, storage and computing resources),[56] and the Internet.[citation needed] Although, many approaches and technologies have been developed, it still remains difficult to carry out machine learning with big data.[57]\n Some MPP relational databases have the ability to store and manage petabytes of data. Implicit is the ability to load, monitor, back up, and optimize the use of the large data tables in the RDBMS.[58][promotional source?]\n DARPA's Topological Data Analysis program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called \"Ayasdi\".[59][third-party source needed]\n The practitioners of big data analytics processes are generally hostile to slower shared storage,[60] preferring direct-attached storage (DAS) in its various forms from solid state drive (SSD) to high capacity SATA disk buried inside parallel processing nodes. The perception of shared storage architectures\u2014storage area network (SAN) and network-attached storage (NAS)\u2014 is that they are relatively slow, complex, and expensive. These qualities are not consistent with big data analytics systems that thrive on system performance, commodity infrastructure, and low cost.\n Real or near-real-time information delivery is one of the defining characteristics of big data analytics. Latency is therefore avoided whenever and wherever possible. Data in direct-attached memory or disk is good\u2014data on memory or disk at the other end of an FC SAN connection is not. The cost of an SAN at the scale needed for analytics applications is much higher than other storage techniques.\n Big data has increased the demand of information management specialists so much so that Software AG, Oracle Corporation, IBM, Microsoft, SAP, EMC, HP, and Dell have spent more than $15\u00a0billion on software firms specializing in data management and analytics. In 2010, this industry was worth more than $100\u00a0billion and was growing at almost 10\u00a0percent a year, about twice as fast as the software business as a whole.[7]\n Developed economies increasingly use data-intensive technologies. There are 4.6\u00a0billion mobile-phone subscriptions worldwide, and between 1\u00a0billion and 2\u00a0billion people accessing the internet.[7] Between 1990 and 2005, more than 1\u00a0billion people worldwide entered the middle class, which means more people became more literate, which in turn led to information growth. The world's effective capacity to exchange information through telecommunication networks was 281 petabytes in 1986, 471 petabytes in 1993, 2.2 exabytes in 2000, 65 exabytes in 2007[12] and predictions put the amount of internet traffic at 667 exabytes annually by 2014.[7] According to one estimate, one-third of the globally stored information is in the form of alphanumeric text and still image data,[61] which is the format most useful for most big data applications. This also shows the potential of yet unused data (i.e. in the form of video and audio content).\n While many vendors offer off-the-shelf products for big data, experts promote the development of in-house custom-tailored systems if the company has sufficient technical capabilities.[62]\n The use and adoption of big data within governmental processes allows efficiencies in terms of cost, productivity, and innovation,[63] but comes with flaws. Data analysis often requires multiple parts of government (central and local) to work in collaboration and create new and innovative processes to deliver the desired outcome. A common government organization that makes use of big data is the National Security Administration (NSA), which monitors the activities of the Internet constantly in search for potential patterns of suspicious or illegal activities their system may pick up.\n Civil registration and vital statistics (CRVS) collects all certificates status from birth to death. CRVS is a source of big data for governments.\n Research on the effective usage of information and communication technologies for development (also known as \"ICT4D\") suggests that big data technology can make important contributions but also present unique challenges to international development.[64][65] Advancements in big data analysis offer cost-effective opportunities to improve decision-making in critical development areas such as health care, employment, economic productivity, crime, security, and natural disaster and resource management.[66][page\u00a0needed][67][68] Additionally, user-generated data offers new opportunities to give the unheard a voice.[69] However, longstanding challenges for developing regions such as inadequate technological infrastructure and economic and human resource scarcity exacerbate existing concerns with big data such as privacy, imperfect methodology, and interoperability issues.[66][page\u00a0needed]  The challenge of \"big data for development\"[66][page\u00a0needed] is currently evolving toward the application of this data through machine learning, known as \"artificial intelligence for development (AI4D).[70]\n A major practical application of big data for development has been \"fighting poverty with data\".[71] In 2015, Blumenstock and colleagues estimated predicted poverty and wealth from mobile phone metadata[72] and in 2016 Jean and colleagues combined satellite imagery and machine learning to predict poverty.[73] Using digital trace data to study the labor market and the digital economy in Latin America, Hilbert and colleagues [74][75] argue that digital trace data has several benefits such as:\n At the same time, working with digital trace data instead of traditional survey data does not eliminate the traditional challenges involved when working in the field of international quantitative analysis. Priorities change, but the basic discussions remain the same. Among the main challenges are:\n Big Data is being rapidly adopted in Finance to 1) speed up processing and 2) deliver better, more informed inferences, both internally and to the clients of the financial institutions.[77] The financial applications of Big Data range from investing decisions and trading (processing volumes of available price data, limit order books, economic data and more, all at the same time), portfolio management (optimizing over an increasingly large array of financial instruments, potentially selected from different asset classes), risk management (credit rating based on extended information), and any other aspect where the data inputs are large.[78] Big Data has also been a typical concept within the field of alternative financial service. Some of the major areas involve crowd-funding platforms and crypto currency exchanges.[79]\n Big data analytics has been used in healthcare in providing personalized medicine and prescriptive analytics, clinical risk intervention and predictive analytics, waste and care variability reduction, automated external and internal reporting of patient data, standardized medical terms and patient registries.[80][81][82][83] Some areas of improvement are more aspirational than actually implemented. The level of data generated within healthcare systems is not trivial. With the added adoption of mHealth, eHealth and wearable technologies the volume of data will continue to increase. This includes electronic health record data, imaging data, patient generated data, sensor data, and other forms of difficult to process data. There is now an even greater need for such environments to pay greater attention to data and information quality.[84] \"Big data very often means 'dirty data' and the fraction of data inaccuracies increases with data volume growth.\" Human inspection at the big data scale is impossible and there is a desperate need in health service for intelligent tools for accuracy and believability control and handling of information missed.[85] While extensive information in healthcare is now electronic, it fits under the big data umbrella as most is unstructured and difficult to use.[86] The use of big data in healthcare has raised significant ethical challenges ranging from risks for individual rights, privacy and autonomy, to transparency and trust.[87]\n Big data in health research is particularly promising in terms of exploratory biomedical research, as data-driven analysis can move forward more quickly than hypothesis-driven research.[88] Then, trends seen in data analysis can be tested in traditional, hypothesis-driven follow up biological research and eventually clinical research.\n A related application sub-area, that heavily relies on big data, within the healthcare field is that of computer-aided diagnosis in medicine.[89][page\u00a0needed]  For instance, for epilepsy monitoring it is customary to create 5 to 10 GB of data daily.[90] Similarly, a single uncompressed image of breast tomosynthesis averages 450 MB of data.[91]  These are just a few of the many examples where computer-aided diagnosis uses big data. For this reason, big data has been recognized as one of the seven key challenges that computer-aided diagnosis systems need to overcome in order to reach the next level of performance.[92]\n A McKinsey Global Institute study found a shortage of 1.5 million highly trained data professionals and managers[52] and a number of universities[93][better\u00a0source\u00a0needed] including University of Tennessee and UC Berkeley, have created masters programs to meet this demand. Private boot camps have also developed programs to meet that demand, including paid programs like The Data Incubator or General Assembly.[94] In the specific field of marketing, one of the problems stressed by Wedel and Kannan[95] is that marketing has several sub domains (e.g., advertising, promotions,\nproduct development, branding) that all use different types of data.\n To understand how the media uses big data, it is first necessary to provide some context into the mechanism used for media process. It has been suggested by Nick Couldry and Joseph Turow that practitioners in media and advertising approach big data as many actionable points of information about millions of individuals. The industry appears to be moving away from the traditional approach of using specific media environments such as newspapers, magazines, or television shows and instead taps into consumers with technologies that reach targeted people at optimal times in optimal locations. The ultimate aim is to serve or convey, a message or content that is (statistically speaking) in line with the consumer's mindset. For example, publishing environments are increasingly tailoring messages (advertisements) and content (articles) to appeal to consumers that have been exclusively gleaned through various data-mining activities.[96]\n Channel 4, the British public-service television broadcaster, is a leader in the field of big data and data analysis.[98]\n Health insurance providers are collecting data on social \"determinants of health\" such as food and TV consumption, marital status, clothing size, and purchasing habits, from which they make predictions on health costs, in order to spot health issues in their clients. It is controversial whether these predictions are currently being used for pricing.[99]\n Big data and the IoT work in conjunction. Data extracted from IoT devices provides a mapping of device inter-connectivity. Such mappings have been used by the media industry, companies, and governments to more accurately target their audience and increase media efficiency. The IoT is also increasingly adopted as a means of gathering sensory data, and this sensory data has been used in medical,[100] manufacturing[101] and transportation[102] contexts.\n Kevin Ashton, the digital innovation expert who is credited with coining the term,[103] defines the Internet of things in this quote: \"If we had computers that knew everything there was to know about things\u2014using data they gathered without any help from us\u2014we would be able to track and count everything, and greatly reduce waste, loss, and cost. We would know when things needed replacing, repairing, or recalling, and whether they were fresh or past their best.\"\n Especially since 2015, big data has come to prominence within business operations as a tool to help employees work more efficiently and streamline the collection and distribution of information technology (IT). The use of big data to resolve IT and data collection issues within an enterprise is called IT operations analytics (ITOA).[104] By applying big data principles into the concepts of machine intelligence and deep computing, IT departments can predict potential issues and prevent them.[104] ITOA businesses offer platforms for systems management that bring data silos together and generate insights from the whole of the system rather than from isolated pockets of data.\n Compared to survey-based data collection, big data has low cost per data point, applies analysis techniques via machine learning and data mining, and includes diverse and new data sources, e.g., registers, social media, apps, and other forms digital data. Since 2018, survey scientists have started to examine how big data and survey science can complement each other to allow researchers and practitioners to improve the production of statistics and its quality. There have been three Big Data Meets Survey Science (BigSurv) conferences in 2018, 2020 (virtual), 2023, and as of 2023[update] one conference forthcoming in 2025,[105] a special issue in the Social Science Computer Review,[106] a special issue in Journal of the Royal Statistical Society,[107] and a special issue in EP J Data Science,[108] and a book called Big Data Meets Social Sciences[109] edited by Craig Hill and five other Fellows of the American Statistical Association. In 2021, the founding members of BigSurv received the Warren J. Mitofsky Innovators Award from the American Association for Public Opinion Research.[110]\n Big data is notable in marketing due to the constant \"datafication\"[111] of everyday consumers of the internet, in which all forms of data are tracked. The datafication of consumers can be defined as  quantifying many of or all human behaviors for the purpose of marketing.[111] The increasingly digital world of rapid datafication makes this idea relevant to marketing because the amount of data constantly grows exponentially. It is predicted to increase from 44 to 163 zettabytes within the span of five years.[112] The size of big data can often be difficult to navigate for marketers.[113] As a result, adopters of big data may find themselves at a disadvantage. Algorithmic findings can be difficult to achieve with such large datasets.[114] Big data in marketing is a highly lucrative tool that can be used for large corporations, its value being as a result of the possibility of predicting significant trends, interests, or statistical outcomes in a consumer-based manner.[115]\n There are three significant factors in the use of big data in marketing:\n Examples of uses of big data in public services:\n Big data can be used to improve training and understanding competitors, using sport sensors. It is also possible to predict winners in a match using big data analytics.[159]\nFuture performance of players could be predicted as well.[160] Thus, players' value and salary is determined by data collected throughout the season.[161]\n In Formula One races, race cars with hundreds of sensors generate terabytes of data. These sensors collect data points from tire pressure to fuel burn efficiency.[162]\nBased on the data, engineers and data analysts decide whether adjustments should be made in order to win a race. Besides, using big data, race teams try to predict the time they will finish the race beforehand, based on simulations using data collected over the season.[163]\n During the COVID-19 pandemic, big data was raised as a way to minimise the impact of the disease. Significant applications of big data included minimising the spread of the virus, case identification and development of medical treatment.[169]\n Governments used big data to track infected people to minimise spread. Early adopters included China, Taiwan, South Korea, and Israel.[170][171][172]\n Encrypted search and cluster formation in big data were demonstrated in March 2014 at the American Society of Engineering Education. Gautam Siwach engaged at Tackling the challenges of Big Data by MIT Computer Science and Artificial Intelligence Laboratory and Amir Esmailpour at the UNH Research Group investigated the key features of big data as the formation of clusters and their interconnections. They focused on the security of big data and the orientation of the term towards the presence of different types of data in an encrypted form at cloud interface by providing the raw definitions and real-time examples within the technology. Moreover, they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data.[173]\n In March 2012, The White House announced a national \"Big Data Initiative\" that consisted of six federal departments and agencies committing more than $200\u00a0million to big data research projects.[174]\n The initiative included a National Science Foundation \"Expeditions in Computing\" grant of $10 million over five years to the AMPLab[175] at the University of California, Berkeley.[176] The AMPLab also received funds from DARPA, and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion[177] to fighting cancer.[178]\n The White House Big Data Initiative also included a commitment by the Department of Energy to provide $25 million in funding over five years to establish the Scalable Data Management, Analysis and Visualization (SDAV) Institute,[179] led by the Energy Department's Lawrence Berkeley National Laboratory. The SDAV Institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the department's supercomputers.\n The U.S. state of Massachusetts announced the Massachusetts Big Data Initiative in May 2012, which provides funding from the state government and private companies to a variety of research institutions.[180] The Massachusetts Institute of Technology hosts the Intel Science and Technology Center for Big Data in the MIT Computer Science and Artificial Intelligence Laboratory, combining government, corporate, and institutional funding and research efforts.[181]\n The European Commission is funding the two-year-long Big Data Public Private Forum through their Seventh Framework Program to engage companies, academics and other stakeholders in discussing big data issues. The project aims to define a strategy in terms of research and innovation to guide supporting actions from the European Commission in the successful implementation of the big data economy. Outcomes of this project will be used as input for Horizon 2020, their next framework program.[182]\n The British government announced in March 2014 the founding of the Alan Turing Institute, named after the computer pioneer and code-breaker, which will focus on new ways to collect and analyze large data sets.[183]\n At the University of Waterloo Stratford Campus Canadian Open Data Experience (CODE) Inspiration Day, participants demonstrated how using data visualization can increase the understanding and appeal of big data sets and communicate their story to the world.[184]\n Computational social sciences\u00a0\u2013 Anyone can use application programming interfaces (APIs) provided by big data holders, such as Google and Twitter, to do research in the social and behavioral sciences.[185] Often these APIs are provided for free.[185] Tobias Preis et al. used Google Trends data to demonstrate that Internet users from countries with a higher per capita gross domestic products (GDPs) are more likely to search for information about the future than information about the past. The findings suggest there may be a link between online behaviors and real-world economic indicators.[186][187][188] The authors of the study examined Google queries logs made by ratio of the volume of searches for the coming year (2011) to the volume of searches for the previous year (2009), which they call the \"future orientation index\".[189] They compared the future orientation index to the per capita GDP of each country, and found a strong tendency for countries where Google users inquire more about the future to have a higher GDP.\n Tobias Preis and his colleagues Helen Susannah Moat and H. Eugene Stanley introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends.[190] Their analysis of Google search volume for 98 terms of varying financial relevance, published in Scientific Reports,[191] suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets.[192][193][194][195][196][197][198]\n Big data sets come with algorithmic challenges that previously did not exist. Hence, there is seen by some to be a need to fundamentally change the processing ways.[199]\n A research question that is asked about big data sets is whether it is necessary to look at the full data to draw certain conclusions about the properties of the data or if is a sample is good enough. The name big data itself contains a term related to size and this is an important characteristic of big data. But sampling enables the selection of right data points from within the larger data set to estimate the characteristics of the whole population. In manufacturing different types of sensory data such as acoustics, vibration, pressure, current, voltage, and controller data are available at short time intervals. To predict downtime it may not be necessary to look at all the data but a sample may be sufficient. Big data can be broken down by various data point categories such as demographic, psychographic, behavioral, and transactional data. With large sets of data points, marketers are able to create and use more customized segments of consumers for more strategic targeting.\n Critiques of the big data paradigm come in two flavors: those that question the implications of the approach itself, and those that question the way it is currently done.[200] One approach to this criticism is the field of critical data studies.\n \"A crucial problem is that we do not know much about the underlying empirical micro-processes that lead to the emergence of the[se] typical network characteristics of Big Data.\"[24][page\u00a0needed]  In their critique, Snijders, Matzat, and Reips point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro-processes. Mark Graham has leveled broad critiques at Chris Anderson's assertion that big data will spell the end of theory:[201] focusing in particular on the notion that big data must always be contextualized in their social, economic, and political contexts.[202] Even as companies invest eight- and nine-figure sums to derive insight from information streaming in from suppliers and customers, less than 40% of employees have sufficiently mature processes and skills to do so. To overcome this insight deficit, big data, no matter how comprehensive or well analyzed, must be complemented by \"big judgment\", according to an article in the Harvard Business Review.[203]\n Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably \"informed by the world as it was in the past, or, at best, as it currently is\".[66][page\u00a0needed] Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past.[204] If the system's dynamics of the future change (if it is not a stationary process), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory.[204] As a response to this critique Alemany Oliver and Vayre suggest to use \"abductive reasoning as a first step in the research process in order to bring context to consumers' digital traces and make new theories emerge\".[205]  Additionally, it has been suggested to combine big data approaches with computer simulations, such as agent-based models[66][page\u00a0needed] and complex systems. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms.[206][207] Finally, the use of multivariate methods that probe for the latent structure of the data, such as factor analysis and cluster analysis, have proven useful as analytic approaches that go well beyond the bi-variate approaches (e.g. contingency tables) typically employed with smaller data sets.\n In health and biology, conventional scientific approaches are based on experimentation. For these approaches, the limiting factor is the relevant data that can confirm or refute the initial hypothesis.[208]  A new postulate is accepted now in biosciences: the information provided by the data in huge volumes (omics) without prior hypothesis is complementary and sometimes necessary to conventional approaches based on experimentation.[209][210] In the massive approaches it is the formulation of a relevant hypothesis to explain the data that is the limiting factor.[211] The search logic is reversed and the limits of induction (\"Glory of Science and Philosophy scandal\", C. D. Broad, 1926) are to be considered.[citation needed]\n Privacy advocates are concerned about the threat to privacy represented by increasing storage and integration of personally identifiable information; expert panels have released various policy recommendations to conform practice to expectations of privacy.[212] The misuse of big data in several cases by media, companies, and even the government has allowed for abolition of trust in almost every fundamental institution holding up society.[213]\n Barocas and Nissenbaum argue that one way of protecting individual users is by being informed about the types of information being collected, with whom it is shared, under what constraints and for what purposes.[214]\n The \"V\" model of big data is concerning as it centers around computational scalability and lacks in a loss around the perceptibility and understandability of information. This led to the framework of cognitive big data, which characterizes big data applications according to:[215]\n Large data sets have been analyzed by computing machines for well over a century, including the US census analytics performed by IBM's punch-card machines which computed statistics including means and variances of populations across the whole continent. In more recent decades, science experiments such as CERN have produced data on similar scales to current commercial \"big data\". However, science experiments have tended to analyze their data using specialized custom-built high-performance computing (super-computing) clusters and grids, rather than clouds of cheap commodity computers as in the current commercial wave, implying a difference in both culture and technology stack.\n Ulf-Dietrich Reips and Uwe Matzat wrote in 2014 that big data had become a \"fad\" in scientific research.[185] Researcher Danah Boyd has raised concerns about the use of big data in science neglecting principles such as choosing a representative sample by being too concerned about handling the huge amounts of data.[216] This approach may lead to results that have a bias in one way or another.[217] Integration across heterogeneous data resources\u2014some that might be considered big data and others not\u2014presents formidable logistical as well as analytical challenges, but many researchers argue that such integrations are likely to represent the most promising new frontiers in science.[218]\nIn the provocative article \"Critical Questions for Big Data\",[219] the authors title big data a part of mythology: \"large data sets offer a higher form of intelligence and knowledge [...], with the aura of truth, objectivity, and accuracy\". Users of big data are often \"lost in the sheer volume of numbers\", and \"working with Big Data is still subjective, and what it quantifies does not necessarily have a closer claim on objective truth\".[219] Recent developments in BI domain, such as pro-active reporting especially target improvements in the usability of big data, through automated filtering of non-useful data and correlations.[220] Big structures are full of spurious correlations[221] either because of non-causal coincidences (law of truly large numbers), solely nature of big randomness[222] (Ramsey theory), or existence of non-included factors so the hope, of early experimenters to make large databases of numbers \"speak for themselves\" and revolutionize scientific method, is questioned.[223] Catherine Tucker has pointed to \"hype\" around big data, writing \"By itself, big data is unlikely to be valuable.\" The article explains: \"The many contexts where data is cheap relative to the cost of retaining talent to process it, suggests that processing skills are more important than data itself in creating value for a firm.\"[224]\n Big data analysis is often shallow compared to analysis of smaller data sets.[225] In many big data projects, there is no large data analysis happening, but the challenge is the extract, transform, load part of data pre-processing.[225]\n Big data is a buzzword and a \"vague term\",[226][227] but at the same time an \"obsession\"[227] with entrepreneurs, consultants, scientists, and the media. Big data showcases such as Google Flu Trends failed to deliver good predictions in recent years, overstating the flu outbreaks by a factor of two. Similarly, Academy awards and election predictions solely based on Twitter were more often off than on target.\nBig data often poses the same challenges as small data; adding more data does not solve problems of bias, but may emphasize other problems. In particular data sources such as Twitter are not representative of the overall population, and results drawn from such sources may then lead to wrong conclusions. Google Translate\u2014which is based on big data statistical analysis of text\u2014does a good job at translating web pages. However, results from specialized domains may be dramatically skewed.\nOn the other hand, big data may also introduce new problems, such as the multiple comparisons problem: simultaneously testing a large set of hypotheses is likely to produce many false results that mistakenly appear significant.\nIoannidis argued that \"most published research findings are false\"[228] due to essentially the same effect: when many scientific teams and researchers each perform many experiments (i.e. process a big amount of scientific data; although not with big data technology), the likelihood of a \"significant\" result being false grows fast \u2013 even more so, when only positive results are published.\nFurthermore, big data analytics results are only as good as the model on which they are predicated. In an example, big data took part in attempting to predict the results of the 2016 U.S. presidential election[229] with varying degrees of success.\n Big data has been used in policing and surveillance by institutions like law enforcement and corporations.[230] Due to the less visible nature of data-based surveillance as compared to traditional methods of policing, objections to big data policing are less likely to arise. According to Sarah Brayne's Big Data Surveillance: The Case of Policing,[231] big data policing can reproduce existing societal inequalities in three ways:\n If these potential problems are not corrected or regulated, the effects of big data policing may continue to shape societal hierarchies. Conscientious usage of big data policing could prevent individual level biases from becoming institutional biases, Brayne also notes.\n",
        "doc_number": 45
    },
    {
        "url": "https://en.wikipedia.org/wiki/Cloud_computing",
        "content": "\n \"Cloud computing is a paradigm for enabling network access to a scalable and elastic pool of shareable physical or virtual resources with self-service provisioning and administration on-demand,\" according to ISO.[1]\n In 2011, the National Institute of Standards and Technology (NIST) identified five \"essential characteristics\" for cloud systems.[2] Below are the exact definitions according to NIST:[2]\n By 2023, the International Organization for Standardization (ISO) had expanded and refined the list.[3]\n The history of cloud computing extends back to the 1960s, with the initial concepts of time-sharing becoming popularized via remote job entry (RJE). The \"data center\" model, where users submitted jobs to operators to run on mainframes, was predominantly used during this era. This was a time of exploration and experimentation with ways to make large-scale computing power available to more users through time-sharing, optimizing the infrastructure, platform, and applications, and increasing efficiency for end users.[4]\n The \"cloud\" metaphor for virtualized services dates to 1994, when it was used by General Magic for the universe of \"places\" that mobile agents in the Telescript environment could \"go\". The metaphor is credited to David Hoffman, a General Magic communications specialist, based on its long-standing use in networking and telecom.[5] The expression cloud computing became more widely known in 1996 when Compaq Computer Corporation drew up a business plan for future computing and the Internet. The company's ambition was to supercharge sales with \"cloud computing-enabled applications\". The business plan foresaw that online consumer file storage would likely be commercially successful. As a result, Compaq decided to sell server hardware to internet service providers.[6]\n In the 2000s, the application of cloud computing began to take shape with the establishment of Amazon Web Services (AWS) in 2002, which allowed developers to build applications independently. In 2006 Amazon Simple Storage Service, known as Amazon S3, and the Amazon Elastic Compute Cloud (EC2) were released. In 2008 NASA's development of the first open-source software for deploying private and hybrid clouds.[7][8]\n The following decade saw the launch of various cloud services. In 2010, Microsoft launched Microsoft Azure, and Rackspace Hosting and NASA initiated an open-source cloud-software project, OpenStack. IBM introduced the IBM SmartCloud framework in 2011, and Oracle announced the Oracle Cloud in 2012. In December 2019, Amazon launched AWS Outposts, a service that extends AWS infrastructure, services, APIs, and tools to customer data centers, co-location spaces, or on-premises facilities.[9][10]\n Cloud computing can enable shorter time to market by providing pre-configured tools, scalable resources, and managed services, allowing users to focus on their core business value instead of maintaining infrastructure. Cloud platforms can enable organizations and individuals to reduce upfront capital expenditures on physical infrastructure by shifting to an operational expenditure model, where costs scale with usage. Cloud platforms also offer managed services and tools, such as artificial intelligence, data analytics, and machine learning, which might otherwise require significant in-house expertise and infrastructure investment.[11][12][13]\n While cloud computing can offer cost advantages through effective resource optimization, organizations often face challenges such as unused resources, inefficient configurations, and hidden costs without proper oversight and governance. Many cloud platforms provide cost management tools, such as AWS Cost Explorer and Azure Cost Management, and frameworks like FinOps have emerged to standardize financial operations in the cloud. Cloud computing also facilitates collaboration, remote work, and global service delivery by enabling secure access to data and applications from any location with an internet connection.[11][12][13]\n Cloud providers offer various redundancy options for core services, such as managed storage and managed databases, though redundancy configurations often vary by service tier. Advanced redundancy strategies, such as cross-region replication or failover systems, typically require explicit configuration and may incur additional costs or licensing fees.[11][12][13]\n Cloud environments operate under a shared responsibility model, where providers are typically responsible for infrastructure security, physical hardware, and software updates, while customers are accountable for data encryption, identity and access management (IAM), and application-level security. These responsibilities vary depending on the cloud service model\u2014Infrastructure as a Service (IaaS), Platform as a Service (PaaS), or Software as a Service (SaaS)\u2014with customers typically having more control and responsibility in IaaS environments and progressively less in PaaS and SaaS models, often trading control for convenience and managed services.[11][12][13]\n The decision to adopt cloud computing or maintain on-premises infrastructure depends on factors such as scalability, cost structure, latency requirements, regulatory constraints, and infrastructure customization.[14][15][16][17]\n Organizations with variable or unpredictable workloads, limited capital for upfront investments, or a focus on rapid scalability benefit from cloud adoption. Startups, SaaS companies, and e-commerce platforms often prefer the pay-as-you-go operational expenditure (OpEx) model of cloud infrastructure. Additionally, companies prioritizing global accessibility, remote workforce enablement, disaster recovery, and leveraging advanced services such as AI/ML and analytics are well-suited for the cloud. In recent years, some cloud providers have started offering specialized services for high-performance computing and low-latency applications, addressing some use cases previously exclusive to on-premises setups.[14][15][16][17]\n On the other hand, organizations with strict regulatory requirements, highly predictable workloads, or reliance on deeply integrated legacy systems may find cloud infrastructure less suitable. Businesses in industries like defense, government, or those handling highly sensitive data often favor on-premises setups for greater control and data sovereignty. Additionally, companies with ultra-low latency requirements, such as high-frequency trading (HFT) firms, rely on custom hardware (e.g., FPGAs) and physical proximity to exchanges, which most cloud providers cannot fully replicate despite recent advancements. Similarly, tech giants like Google, Meta, and Amazon build their own data centers due to economies of scale, predictable workloads, and the ability to customize hardware and network infrastructure for optimal efficiency. However, these companies also use cloud services selectively for certain workloads and applications where it aligns with their operational needs.[14][15][16][17]\n In practice, many organizations are increasingly adopting hybrid cloud architectures, combining on-premises infrastructure with cloud services. This approach allows businesses to balance scalability, cost-effectiveness, and control, offering the benefits of both deployment models while mitigating their respective limitations.[14][15][16][17]\n One of the main challenges of cloud computing, in comparison to more traditional on-premises computing, is data security and privacy. Cloud users entrust their sensitive data to third-party providers, who may not have adequate measures to protect it from unauthorized access, breaches, or leaks. Cloud users also face compliance risks if they have to adhere to certain regulations or standards regarding data protection, such as GDPR or HIPAA.[18]\n Another challenge of cloud computing is reduced visibility and control. Cloud users may not have full insight into how their cloud resources are managed, configured, or optimized by their providers. They may also have limited ability to customize or modify their cloud services according to their specific needs or preferences.[18] Complete understanding of all technology may be impossible, especially given the scale, complexity, and deliberate opacity of contemporary systems; however, there is a need for understanding complex technologies and their interconnections to have power and agency within them.[19] The metaphor of the cloud can be seen as problematic as cloud computing retains the aura of something noumenal and numinous; it is something experienced without precisely understanding what it is or how it works.[20]\n Additionally, cloud migration is a significant challenge. This process involves transferring data, applications, or workloads from one cloud environment to another, or from on-premises infrastructure to the cloud. Cloud migration can be complicated, time-consuming, and expensive, particularly when there are compatibility issues between different cloud platforms or architectures. If not carefully planned and executed, cloud migration can lead to downtime, reduced performance, or even data loss.[21]\n According to the 2024 State of the Cloud Report by Flexera, approximately 50% of respondents identified the following top challenges when migrating workloads to public clouds:[22]\n Applications hosted in the cloud are susceptible to the fallacies of distributed computing, a series of misconceptions that can lead to significant issues in software development and deployment.[23]\n In a report by Gartner, a survey of 200 IT leaders revealed that 69% experienced budget overruns in their organizations' cloud expenditures during 2023. Conversely, 31% of IT leaders whose organizations stayed within budget attributed their success to accurate forecasting and budgeting, proactive monitoring of spending, and effective optimization.[24]\n The 2024 Flexera State of Cloud Report identifies the top cloud challenges as managing cloud spend, followed by security concerns and lack of expertise. Public cloud expenditures exceeded budgeted amounts by an average of 15%. The report also reveals that cost savings is the top cloud initiative for 60% of respondents. Furthermore, 65% measure cloud progress through cost savings, while 42% prioritize shorter time-to-market, indicating that cloud's promise of accelerated deployment is often overshadowed by cost concerns.[22]\n Typically, cloud providers' Service Level Agreements (SLAs) do not encompass all forms of service interruptions. Exclusions typically include planned maintenance, downtime resulting from external factors such as network issues, human errors, like misconfigurations, natural disasters, force majeure events, or security breaches. Typically, customers bear the responsibility of monitoring SLA compliance and must file claims for any unmet SLAs within a designated timeframe. Customers should be aware of how deviations from SLAs are calculated, as these parameters may vary by service. These requirements can place a considerable burden on customers. Additionally, SLA percentages and conditions can differ across various services within the same provider, with some services lacking any SLA altogether. In cases of service interruptions due to hardware failures in the cloud provider, the company typically does not offer monetary compensation. Instead, eligible users may receive credits as outlined in the corresponding SLA.[25][26][27][28]\n Cloud computing abstractions aim to simplify resource management, but leaky abstractions can expose underlying complexities. These variations in abstraction quality depend on the cloud vendor, service and architecture. Mitigating leaky abstractions requires users to understand the implementation details and limitations of the cloud services they utilize.[29][30][31]\n Service lock-in within the same vendor occurs when a customer becomes dependent on specific services within a cloud vendor, making it challenging to switch to alternative services within the same vendor when their needs change.[32][33]\n Cloud computing poses privacy concerns because the service provider can access the data that is in the cloud at any time.  It could accidentally or deliberately alter or delete information.[34] Many cloud providers can share information with third parties if necessary for purposes of law and order without a warrant. That is permitted in their privacy policies, which users must agree to before they start using cloud services. Solutions to privacy include policy and legislation as well as end-users' choices for how data is stored.[34] Users can encrypt data that is processed or stored within the cloud to prevent unauthorized access.[34] Identity management systems can also provide practical solutions to privacy concerns in cloud computing. These systems distinguish between authorized and unauthorized users and determine the amount of data that is accessible to each entity.[35] The systems work by creating and describing identities, recording activities, and getting rid of unused identities.\n According to the Cloud Security Alliance, the top three threats in the cloud are Insecure Interfaces and APIs, Data Loss & Leakage, and Hardware Failure\u2014which accounted for 29%, 25% and 10% of all cloud security outages respectively. Together, these form shared technology vulnerabilities. In a cloud provider platform being shared by different users, there may be a possibility that information belonging to different customers resides on the same data server. Additionally, Eugene Schultz, chief technology officer at Emagined Security, said that hackers are spending substantial time and effort looking for ways to penetrate the cloud. \"There are some real Achilles' heels in the cloud infrastructure that are making big holes for the bad guys to get into\". Because data from hundreds or thousands of companies can be stored on large cloud servers, hackers can theoretically gain control of huge stores of information through a single attack\u2014a process he called \"hyperjacking\". Some examples of this include the Dropbox security breach, and iCloud 2014 leak.[36] Dropbox had been breached in October 2014, having over seven million of its users passwords stolen by hackers in an effort to get monetary value from it by Bitcoins (BTC). By having these passwords, they are able to read private data as well as have this data be indexed by search engines (making the information public).[36]\n There is the problem of legal ownership of the data (If a user stores some data in the cloud, can the cloud provider profit from it?). Many Terms of Service agreements are silent on the question of ownership.[37] Physical control of the computer equipment (private cloud) is more secure than having the equipment off-site and under someone else's control (public cloud). This delivers great incentive to public cloud computing service providers to prioritize building and maintaining strong management of secure services.[38] Some small businesses that do not have expertise in IT security could find that it is more secure for them to use a public cloud. There is the risk that end users do not understand the issues involved when signing on to a cloud service (persons sometimes do not read the many pages of the terms of service agreement, and just click \"Accept\" without reading). This is important now that cloud computing is common and required for some services to work, for example for an intelligent personal assistant (Apple's Siri or Google Assistant). Fundamentally, private cloud is seen as more secure with higher levels of control for the owner, however public cloud is seen to be more flexible and requires less time and money investment from the user.[39]\n The attacks that can be made on cloud computing systems include man-in-the middle attacks, phishing attacks, authentication attacks, and malware attacks. One of the largest threats is considered to be malware attacks, such as Trojan horses. Recent research conducted in 2022 has revealed that the Trojan horse injection method is a serious problem with harmful impacts on cloud computing systems.[40]\n The National Institute of Standards and Technology recognized three cloud service models in 2011: Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS).[2] The International Organization for Standardization (ISO) later identified additional models in 2023, including \"Network as a Service\", \"Communications as a Service\", \"Compute as a Service\", and \"Data Storage as a Service\".[3]\n Infrastructure as a service (IaaS) refers to online services that provide high-level APIs used to abstract various low-level details of underlying network infrastructure like physical computing resources, location, data partitioning, scaling, security, backup, etc. A hypervisor runs the virtual machines as guests. Pools of hypervisors within the cloud operational system can support large numbers of virtual machines and the ability to scale services up and down according to customers' varying requirements. Linux containers run in isolated partitions of a single Linux kernel running directly on the physical hardware. Linux cgroups and namespaces are the underlying Linux kernel technologies used to isolate, secure and manage the containers. The use of containers offers higher performance than virtualization because there is no hypervisor overhead. IaaS clouds often offer additional resources such as a virtual-machine disk-image library, raw block storage, file or object storage, firewalls, load balancers, IP addresses, virtual local area networks (VLANs), and software bundles.[41]\n The NIST's definition of cloud computing describes IaaS as \"where the consumer is able to deploy and run arbitrary software, which can include operating systems and applications. The consumer does not manage or control the underlying cloud infrastructure but has control over operating systems, storage, and deployed applications; and possibly limited control of select networking components (e.g., host firewalls).\"[2]\n IaaS-cloud providers supply these resources on-demand from their large pools of equipment installed in data centers. For wide-area connectivity, customers can use either the Internet or carrier clouds (dedicated virtual private networks). To deploy their applications, cloud users install operating-system images and their application software on the cloud infrastructure. In this model, the cloud user patches and maintains the operating systems and the application software. Cloud providers typically bill IaaS services on a utility computing basis: cost reflects the number of resources allocated and consumed.[42]\n The NIST's definition of cloud computing defines Platform as a Service as:[2]\n The capability provided to the consumer is to deploy onto the cloud infrastructure consumer-created or acquired applications created using programming languages, libraries, services, and tools supported by the provider. The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, or storage, but has control over the deployed applications and possibly configuration settings for the application-hosting environment. PaaS vendors offer a development environment to application developers. The provider typically develops toolkit and standards for development and channels for distribution and payment. In the PaaS models, cloud providers deliver a computing platform, typically including an operating system, programming-language execution environment, database, and the web server. Application developers develop and run their software on a cloud platform instead of directly buying and managing the underlying hardware and software layers. With some PaaS, the underlying computer and storage resources scale automatically to match application demand so that the cloud user does not have to allocate resources manually.[43][need quotation to verify]\n Some integration and data management providers also use specialized applications of PaaS as delivery models for data. Examples include iPaaS (Integration Platform as a Service) and dPaaS (Data Platform as a Service). iPaaS enables customers to develop, execute and govern integration flows.[44] Under the iPaaS integration model, customers drive the development and deployment of integrations without installing or managing any hardware or middleware.[45] dPaaS delivers integration\u2014and data-management\u2014products as a fully managed service.[46] Under the dPaaS model, the PaaS provider, not the customer, manages the development and execution of programs by building data applications for the customer. dPaaS users access data through data-visualization tools.[47]\n The NIST's definition of cloud computing defines Software as a Service as:[2]\n The capability provided to the consumer is to use the provider's applications running on a cloud infrastructure. The applications are accessible from various client devices through either a thin client interface, such as a web browser (e.g., web-based email), or a program interface. The consumer does not manage or control the underlying cloud infrastructure including network, servers, operating systems, storage, or even individual application capabilities, with the possible exception of limited user-specific application configuration settings. In the software as a service (SaaS) model, users gain access to application software and databases. Cloud providers manage the infrastructure and platforms that run the applications. SaaS is sometimes referred to as \"on-demand software\" and is usually priced on a pay-per-use basis or using a subscription fee.[48] In the SaaS model, cloud providers install and operate application software in the cloud and cloud users access the software from cloud clients. Cloud users do not manage the cloud infrastructure and platform where the application runs. This eliminates the need to install and run the application on the cloud user's own computers, which simplifies maintenance and support. Cloud applications differ from other applications in their scalability\u2014which can be achieved by cloning tasks onto multiple virtual machines at run-time to meet changing work demand.[49] Load balancers distribute the work over the set of virtual machines. This process is transparent to the cloud user, who sees only a single access-point. To accommodate a large number of cloud users, cloud applications can be multitenant, meaning that any machine may serve more than one cloud-user organization.\n The pricing model for SaaS applications is typically a monthly or yearly flat fee per user,[50] so prices become scalable and adjustable if users are added or removed at any point. It may also be free.[51] Proponents claim that SaaS gives a business the potential to reduce IT operational costs by outsourcing hardware and software maintenance and support to the cloud provider. This enables the business to reallocate IT operations costs away from hardware/software spending and from personnel expenses, towards meeting other goals. In addition, with applications hosted centrally, updates can be released without the need for users to install new software. One drawback of SaaS comes with storing the users' data on the cloud provider's server. As a result,[citation needed] there could be unauthorized access to the data.[52] Examples of applications offered as SaaS are games and productivity software like Google Docs and Office Online. SaaS applications may be integrated with cloud storage or File hosting services, which is the case with Google Docs being integrated with Google Drive, and Office Online being integrated with OneDrive.[53]\n Serverless computing allows customers to use various cloud capabilities without the need to provision, deploy, or manage hardware or software resources, apart from providing their application code or data. ISO/IEC 22123-2:2023 classifies serverless alongside Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS) under the broader category of cloud service categories. Notably, while ISO refers to these classifications as cloud service categories, the National Institute of Standards and Technology (NIST) refers to them as service models.[2][3]\n \"A cloud deployment model represents the way in which cloud computing can be organized based on the control and sharing of physical or virtual resources.\"[3] Cloud deployment models define the fundamental patterns of interaction between cloud customers and cloud providers. They do not detail implementation specifics or the configuration of resources.[3]\n Private cloud is cloud infrastructure operated solely for a single organization, whether managed internally or by a third party, and hosted either internally or externally.[2] Undertaking a private cloud project requires significant engagement to virtualize the business environment, and requires the organization to reevaluate decisions about existing resources. It can improve business, but every step in the project raises security issues that must be addressed to prevent serious vulnerabilities. Self-run data centers[54] are generally capital intensive. They have a significant physical footprint, requiring allocations of space, hardware, and environmental controls. These assets have to be refreshed periodically, resulting in additional capital expenditures. They have attracted criticism because users \"still have to buy, build, and manage them\" and thus do not benefit from less hands-on management,[55] essentially \"[lacking] the economic model that makes cloud computing such an intriguing concept\".[56][57]\n Cloud services are considered \"public\" when they are delivered over the public Internet, and they may be offered as a paid subscription, or free of charge.[58] Architecturally, there are few differences between public- and private-cloud services, but security concerns increase substantially when services (applications, storage, and other resources) are shared by multiple customers. Most public-cloud providers offer direct-connection services that allow customers to securely link their legacy data centers to their cloud-resident applications.[59][60]\n Several factors like the functionality of the solutions, cost, integrational and organizational aspects as well as safety & security are influencing the decision of enterprises and organizations to choose a public cloud or on-premises solution.[61]\n Hybrid cloud is a composition of a public cloud and a private environment, such as a private cloud or on-premises resources,[62][63] that remain distinct entities but are bound together, offering the benefits of multiple deployment models. Hybrid cloud can also mean the ability to connect collocation, managed and/or dedicated services with cloud resources.[2] Gartner defines a hybrid cloud service as a cloud computing service that is composed of some combination of private, public and community cloud services, from different service providers.[64] A hybrid cloud service crosses isolation and provider boundaries so that it cannot be simply put in one category of private, public, or community cloud service. It allows one to extend either the capacity or the capability of a cloud service, by aggregation, integration or customization with another cloud service.\n Varied use cases for hybrid cloud composition exist. For example, an organization may store sensitive client data in house on a private cloud application, but interconnect that application to a business intelligence application provided on a public cloud as a software service.[65] This example of hybrid cloud extends the capabilities of the enterprise to deliver a specific business service through the addition of externally available public cloud services. Hybrid cloud adoption depends on a number of factors such as data security and compliance requirements, level of control needed over data, and the applications an organization uses.[66]\n Another example of hybrid cloud is one where IT organizations use public cloud computing resources to meet temporary capacity needs that can not be met by the private cloud.[67] This capability enables hybrid clouds to employ cloud bursting for scaling across clouds.[2] Cloud bursting is an application deployment model in which an application runs in a private cloud or data center and \"bursts\" to a public cloud when the demand for computing capacity increases. A primary advantage of cloud bursting and a hybrid cloud model is that an organization pays for extra compute resources only when they are needed.[68] Cloud bursting enables data centers to create an in-house IT infrastructure that supports average workloads, and use cloud resources from public or private clouds, during spikes in processing demands.[69]\n Community cloud shares infrastructure between several organizations from a specific community with common concerns (security, compliance, jurisdiction, etc.), whether it is managed internally or by a third-party, and hosted internally or externally, the costs are distributed among fewer users compared to a public cloud (but more than a private cloud). As a result, only a portion of the potential cost savings of cloud computing is achieved.\n[2]\n According to ISO/IEC 22123-1: \"multi-cloud is a cloud deployment model in which a customer uses public cloud services provided by two or more cloud service providers\". \u00a0[70] Poly cloud refers to the use of multiple public clouds for the purpose of leveraging specific services that each provider offers. It differs from Multi cloud in that it is not designed to increase flexibility or mitigate against failures but is rather used to allow an organization to achieve more than could be done with a single provider.[71]\n According to International Data Corporation (IDC), global spending on cloud computing services has reached $706 billion and is expected to reach $1.3 trillion by 2025.[72] Gartner estimated that global public cloud services end-user spending would reach $600 billion by 2023.[73] According to a McKinsey & Company report, cloud cost-optimization levers and value-oriented business use cases foresee more than $1 trillion in run-rate EBITDA across Fortune 500 companies as up for grabs in 2030.[74] In 2022, more than $1.3 trillion in enterprise IT spending was at stake from the shift to the cloud, growing to almost $1.8 trillion in 2025, according to Gartner.[75]\n The European Commission's 2012 Communication identified several issues which were impeding the development of the cloud computing market:[76]:\u200aSection 3\u200a\n The Communication set out a series of \"digital agenda actions\" which the Commission proposed to undertake in order to support the development of a fair and effective market for cloud computing services.[76]:\u200aPages 6\u201314\u200a\n The goal of cloud computing is to allow users to take benefit from all of these technologies, without the need for deep knowledge about or expertise with each one of them. The cloud aims to cut costs and helps the users focus on their core business instead of being impeded by IT obstacles.[77] The main enabling technology for cloud computing is virtualization. Virtualization software separates a physical computing device into one or more \"virtual\" devices, each of which can be easily used and managed to perform computing tasks. With operating system\u2013level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. Virtualization provides the agility required to speed up IT operations and reduces cost by increasing infrastructure utilization. Autonomic computing automates the process through which the user can provision resources on-demand. By minimizing user involvement, automation speeds up the process, reduces labor costs and reduces the possibility of human errors.[77]\n Cloud computing uses concepts from utility computing to provide metrics for the services used. Cloud computing attempts to address QoS (quality of service) and reliability problems of other grid computing models.[77]\n Cloud computing shares characteristics with:\n  Media related to Cloud computing at Wikimedia Commons\n",
        "doc_number": 46
    },
    {
        "url": "https://en.wikipedia.org/wiki/Internet_of_things",
        "content": "\n Internet of things (IoT) describes devices with sensors, processing ability, software and other technologies that connect and exchange data with other devices and systems over the Internet or other communication networks.[1][2][3][4][5] The Internet of things encompasses electronics, communication, and computer science engineering. \"Internet of things\" has been considered a misnomer because devices do not need to be connected to the public internet; they only need to be connected to a network[6] and be individually addressable.[7][8]\n The field has evolved due to the convergence of multiple technologies, including ubiquitous computing, commodity sensors, and increasingly powerful embedded systems, as well as machine learning.[9] Older fields of embedded systems, wireless sensor networks, control systems, automation (including home and building automation), independently and collectively enable the Internet of things.[10]  In the consumer market, IoT technology is most synonymous with \"smart home\" products, including devices and appliances (lighting fixtures, thermostats, home security systems, cameras, and other home appliances) that support one or more common ecosystems and can be controlled via devices associated with that ecosystem, such as smartphones and smart speakers. IoT is also used in healthcare systems.[11]\n There are a number of concerns about the risks in the growth of IoT technologies and products, especially in the areas of privacy and security, and consequently there have been industry and government moves to address these concerns, including the development of international and local standards, guidelines, and regulatory frameworks.[12] Because of their interconnected nature, IoT devices are vulnerable to security breaches and privacy concerns. At the same time, the way these devices communicate wirelessly creates regulatory ambiguities, complicating jurisdictional boundaries of the data transfer.[13]\n Around 1972, for its remote site use, Stanford Artificial Intelligence Laboratory developed a computer controlled vending machine, adapted from a machine rented from Canteen Vending, which sold for cash or, though a computer terminal (Teletype Model 33 KSR),[14] on credit.[15] Products included, at least, beer, yogurt, and milk.[15][14] It was called the Prancing Pony, after the name of the room, named after an inn in Tolkien's Lord of the Rings,[15][16] as each room at  Stanford Artificial Intelligence Laboratory was named after a place in Middle Earth.[17] A successor version still operates in the Computer Science Department at Stanford, with both hardware and software having been updated.[15]\n In 1982,[18] an early concept of a network connected smart device was built as an Internet interface for sensors installed in the Carnegie Mellon University Computer Science Department's departmental Coca-Cola vending machine, supplied by graduate student volunteers, provided a temperature model and an inventory status,[19][20] inspired by the computer controlled vending machine in the Prancing Pony room at Stanford Artificial Intelligence Laboratory.[21] First accessible only on the CMU campus, it became the first ARPANET-connected appliance,[22][23]\n Mark Weiser's 1991 paper on ubiquitous computing, \"The Computer of the 21st Century\", as well as academic venues such as UbiComp and PerCom produced the contemporary vision of the IoT.[24][25] In 1994, Reza Raji described the concept in IEEE Spectrum as \"[moving] small packets of data to a large set of nodes, so as to integrate and automate everything from home appliances to entire factories\".[26] Between 1993 and 1997, several companies proposed solutions like Microsoft's at Work or Novell's NEST. The field gained momentum when Bill Joy envisioned device-to-device communication as a part of his \"Six Webs\" framework, presented at the World Economic Forum at Davos in 1999.[27]\n The concept of the \"Internet of things\" and the term itself, first appeared in a speech by Peter T. Lewis, to the Congressional Black Caucus Foundation 15th Annual Legislative Weekend in Washington, D.C., published in September 1985. According to Lewis, \"The Internet of Things, or IoT, is the integration of people, processes and technology with connectable devices and sensors to enable remote monitoring, status, manipulation and evaluation of trends of such devices.\"[28]\n The term \"Internet of things\" was coined independently by Kevin Ashton of Procter & Gamble, later of MIT's Auto-ID Center, in 1999,[29] though he prefers the phrase \"Internet for things\".[30] At that point, he viewed radio-frequency identification (RFID) as essential to the Internet of things,[31] which would allow computers to manage all individual things.[32][33][34] The main theme of the Internet of things is to embed short-range mobile transceivers in various gadgets and daily necessities to enable new forms of communication between people and things, and between things themselves.[35]\n In 2004 Cornelius \"Pete\" Peterson, CEO of NetSilicon, predicted that, \"The next era of information technology will be dominated by [IoT] devices, and networked devices will ultimately gain in popularity and significance to the extent that they will far exceed the number of networked computers and workstations.\" Peterson believed that medical devices and industrial controls would become dominant applications of the technology.[36]\n Defining the Internet of things as \"simply the point in time when more 'things or objects' were connected to the Internet than people\", Cisco Systems estimated that the IoT was \"born\" between 2008 and 2009, with the things/people ratio growing from 0.08 in 2003 to 1.84 in 2010.[37]\n The extensive set of applications for IoT devices[38] is often divided into consumer, commercial, industrial, and infrastructure spaces.[39][40]\n A growing portion of IoT devices is created for consumer use, including connected vehicles, home automation, wearable technology, connected health, and appliances with remote monitoring capabilities.[41]\n IoT devices are a part of the larger concept of home automation, which can include lighting, heating and air conditioning, media and security systems and camera systems.[42][43] Long-term benefits could include energy savings by automatically ensuring lights and electronics are turned off or by making the residents in the home aware of usage.[44]\n A smart home or automated home could be based on a platform or hubs that control smart devices and appliances.[45] For instance, using Apple's HomeKit, manufacturers can have their home products and accessories controlled by an application in iOS devices such as the iPhone and the Apple Watch.[46][47] This could be a dedicated app or iOS native applications such as Siri.[48] This can be demonstrated in the case of Lenovo's Smart Home Essentials, which is a line of smart home devices that are controlled through Apple's Home app or Siri without the need for a Wi-Fi bridge.[48] There are also dedicated smart home hubs that are offered as standalone platforms to connect different smart home products. These include the Amazon Echo, Google Home, Apple's HomePod, and Samsung's SmartThings Hub.[49] In addition to the commercial systems, there are many non-proprietary, open source ecosystems, including Home Assistant, OpenHAB and Domoticz.[50]\n One key application of a smart home is to assist the elderly and disabled. These home systems use assistive technology to accommodate an owner's specific disabilities.[51] Voice control can assist users with sight and mobility limitations while alert systems can be connected directly to cochlear implants worn by hearing-impaired users.[52] They can also be equipped with additional safety features, including sensors that monitor for medical emergencies such as falls or seizures.[53] Smart home technology applied in this way can provide users with more freedom and a higher quality of life.[51]\n The term \"Enterprise IoT\" refers to devices used in business and corporate settings. \n The Internet of Medical Things (IoMT) is an application of the IoT for medical and health-related purposes, data collection and analysis for research, and monitoring.[54][55][56][57][58] The IoMT has been referenced as \"Smart Healthcare\",[59] as the technology for creating a digitized healthcare system, connecting available medical resources and healthcare services.[60][61]\n IoT devices can be used to enable remote health monitoring and emergency notification systems. These health monitoring devices can range from blood pressure and heart rate monitors to advanced devices capable of monitoring specialized implants, such as pacemakers, Fitbit electronic wristbands, or advanced hearing aids.[62] Some hospitals have begun implementing \"smart beds\" that can detect when they are occupied and when a patient is attempting to get up. It can also adjust itself to ensure appropriate pressure and support are applied to the patient without the manual interaction of nurses.[54] A 2015 Goldman Sachs report indicated that healthcare IoT devices \"can save the United States more than $300 billion in annual healthcare expenditures by increasing revenue and decreasing cost.\"[63] Moreover, the use of mobile devices to support medical follow-up led to the creation of 'm-health', used analyzed health statistics.\"[64]\n Specialized sensors can also be equipped within living spaces to monitor the health and general well-being of senior citizens, while also ensuring that proper treatment is being administered and assisting people to regain lost mobility via therapy as well.[65] These sensors create a network of intelligent sensors that are able to collect, process, transfer, and analyze valuable information in different environments, such as connecting in-home monitoring devices to hospital-based systems.[59] Other consumer devices to encourage healthy living, such as connected scales or wearable heart monitors, are also a possibility with the IoT.[66] End-to-end health monitoring IoT platforms are also available for antenatal and chronic patients, helping one manage health vitals and recurring medication requirements.[67]\n Advances in plastic and fabric electronics fabrication methods have enabled ultra-low cost, use-and-throw\u00a0IoMT sensors. These sensors, along with the required RFID electronics, can be fabricated on paper or e-textiles for wireless powered disposable sensing devices.[68] Applications have been established for point-of-care medical diagnostics, where portability and low system-complexity is essential.[69]\n As of 2018[update] IoMT was not only being applied in the clinical laboratory industry,[56] but also in the healthcare and health insurance industries. IoMT in the healthcare industry is now permitting doctors, patients, and others, such as guardians of patients, nurses, families, and similar, to be part of a system, where patient records are saved in a database, allowing doctors and the rest of the medical staff to have access to patient information.[70]\n IoT devices within healthcare are structured in a multi-layered architecture.[71] Initially, data is collected from the devices and sensors within the IoT. Subsequently, data is processed and stored within the institution's network.[72] At this point, data becomes accessible for further internal and external procedures.[71] Simultaneously, data is protected via various cybersecurity measures, such as the principle of least privilege (PoLP),[73] data encryption for example, with the Advanced Encryption Standard (AES),[72] intrusion detection systems (IDS), and intrusion prevention systems (IPS).[74] Lastly, data can be accessed by users on workstations or portable devices through applications like patient management software (PMS).[71]\n IoMT in the insurance industry provides access to better and new types of dynamic information. This includes sensor-based solutions such as biosensors, wearables, connected health devices, and mobile apps to track customer behavior. This can lead to more accurate underwriting and new pricing models.[75]\n The application of the IoT in healthcare plays a fundamental role in managing chronic diseases and in disease prevention and control. Remote monitoring is made possible through the connection of powerful wireless solutions. The connectivity enables health practitioners to capture patient's data and apply complex algorithms in health data analysis.[76]\n The IoT can assist in the integration of communications, control, and information processing across various transportation systems. Application of the IoT extends to all aspects of transportation systems (i.e., the vehicle,[77] the infrastructure, and the driver or user). Dynamic interaction between these components of a transport system enables inter- and intra-vehicular communication,[78] smart traffic control, smart parking, electronic toll collection systems, logistics and fleet management, vehicle control, safety, and road assistance.[62][79]\n In vehicular communication systems, vehicle-to-everything communication (V2X), consists of three main components: vehicle-to-vehicle communication (V2V), vehicle-to-infrastructure communication (V2I) and vehicle to pedestrian communications (V2P). V2X is the first step to autonomous driving and connected road infrastructure.[80]\n IoT devices can be used to monitor and control the mechanical, electrical and electronic systems used in various types of buildings (e.g., public and private, industrial, institutions, or residential)[62] in home automation and building automation systems. In this context, three main areas are being covered in literature:[81]\n Also known as IIoT, industrial IoT devices acquire and analyze data from connected equipment, operational technology (OT), locations, and people. Combined with operational technology (OT) monitoring devices, IIoT helps regulate and monitor industrial systems.[82] Also, the same implementation can be carried out for automated record updates of asset placement in industrial storage units as the size of the assets can vary from a small screw to the whole motor spare part, and misplacement of such assets can cause a loss of manpower time and money.\n The IoT can connect various manufacturing devices equipped with sensing, identification, processing, communication, actuation, and networking capabilities.[83] Network control and management of manufacturing equipment, asset and situation management, or manufacturing process control allow IoT to be used for industrial applications and smart manufacturing.[84] IoT intelligent systems enable rapid manufacturing and optimization of new products and rapid response to product demands.[62]\n Digital control systems to automate process controls, operator tools and service information systems to optimize plant safety and security are within the purview of the IIoT.[85] IoT can also be applied to asset management via predictive maintenance, statistical evaluation, and measurements to maximize reliability.[86] Industrial management systems can be integrated with smart grids, enabling energy optimization. Measurements, automated controls, plant optimization, health and safety management, and other functions are provided by networked sensors.[62]\n In addition to general manufacturing, IoT is also used for processes in the industrialization of construction.[87]\n There are numerous IoT applications in farming[88] such as collecting data on temperature, rainfall, humidity, wind speed, pest infestation, and soil content. This data can be used to automate farming techniques, take informed decisions to improve quality and quantity, minimize risk and waste, and reduce the effort required to manage crops. For example, farmers can now monitor soil temperature and moisture from afar and even apply IoT-acquired data to precision fertilization programs.[89] The overall goal is that data from sensors, coupled with the farmer's knowledge and intuition about his or her farm, can help increase farm productivity, and also help reduce costs.\n In August 2018, Toyota Tsusho began a partnership with Microsoft to create fish farming tools using the Microsoft Azure application suite for IoT technologies related to water management. Developed in part by researchers from Kindai University, the water pump mechanisms use artificial intelligence to count the number of fish on a conveyor belt, analyze the number of fish, and deduce the effectiveness of water flow from the data the fish provide.[90] The FarmBeats project[91] from Microsoft Research that uses TV white space to connect farms is also a part of the Azure Marketplace now.[92]\n IoT devices are in use to monitor the environments and systems of boats and yachts.[93] Many pleasure boats are left unattended for days in summer, and months in winter so such devices provide valuable early alerts of boat flooding, fire, and deep discharge of batteries. The use of global Internet data networks such as Sigfox, combined with long-life batteries, and microelectronics allows the engine rooms, bilge, and batteries to be constantly monitored and reported to connected Android & Apple applications for example.\n Monitoring and controlling operations of sustainable urban and rural infrastructures like bridges, railway tracks and on- and offshore wind farms is a key application of the IoT.[85] The IoT infrastructure can be used for monitoring any events or changes in structural conditions that can compromise safety and increase risk. The IoT can benefit the construction industry by cost-saving, time reduction, better quality workday, paperless workflow and increase in productivity. It can help in taking faster decisions and saving money in Real-Time Data Analytics. It can also be used for scheduling repair and maintenance activities efficiently, by coordinating tasks between different service providers and users of these facilities.[62] IoT devices can also be used to control critical infrastructure like bridges to provide access to ships. The usage of IoT devices for monitoring and operating infrastructure is likely to improve incident management and emergency response coordination, and quality of service, up-times and reduce costs of operation in all infrastructure-related areas.[94] Even areas such as waste management can benefit.[95]\n There are several planned or ongoing large-scale deployments of the IoT, to enable better management of cities and systems. For example, Songdo, South Korea, the first of its kind fully equipped and wired smart city, is gradually being built[when?], with approximately 70 percent of the business district completed as of June\u00a02018[update]. Much of the city is planned to be wired and automated, with little or no human intervention.[96]\n In 2014 another application was undergoing a project in Santander, Spain. For this deployment, two approaches have been adopted. This city of 180,000 inhabitants has already seen 18,000 downloads of its city smartphone app. The app is connected to 10,000 sensors that enable services like parking search, and environmental monitoring. City context information is used in this deployment so as to benefit merchants through a spark deals mechanism based on city behavior that aims at maximizing the impact of each notification.[97]\n Other examples of large-scale deployments underway include the Sino-Singapore Guangzhou Knowledge City;[98] work on improving air and water quality, reducing noise pollution, and increasing transportation efficiency in San Jose, California;[99] and smart traffic management in western Singapore.[100] Using its RPMA (Random Phase Multiple Access) technology, San Diego\u2013based Ingenu has built a nationwide public network[101] for low-bandwidth data transmissions using the same unlicensed 2.4 gigahertz spectrum as Wi-Fi. Ingenu's \"Machine Network\" covers more than a third of the US population across 35 major cities including San Diego and Dallas.[102] French company, Sigfox, commenced building an Ultra Narrowband wireless data network in the San Francisco Bay Area in 2014, the first business to achieve such a deployment in the U.S.[103][104] It subsequently announced it would set up a total of 4000 base stations to cover a total of 30 cities in the U.S. by the end of 2016, making it the largest IoT network coverage provider in the country thus far.[105][106] Cisco also participates in smart cities projects. Cisco has deployed technologies for Smart Wi-Fi, Smart Safety & Security, Smart Lighting, Smart Parking, Smart Transports, Smart Bus Stops, Smart Kiosks, Remote Expert for Government Services (REGS) and Smart Education in the five km area in the city of Vijaywada, India.[107][108]\n Another example of a large deployment is the one completed by New York Waterways in New York City to connect all the city's vessels and be able to monitor them live 24/7. The network was designed and engineered by Fluidmesh Networks, a Chicago-based company developing wireless networks for critical applications. The NYWW network is currently providing coverage on the Hudson River, East River, and Upper New York Bay. With the wireless network in place, NY Waterway is able to take control of its fleet and passengers in a way that was not previously possible. New applications can include security, energy and fleet management, digital signage, public Wi-Fi, paperless ticketing and others.[109]\n Significant numbers of energy-consuming devices (e.g. lamps, household appliances, motors, pumps, etc.) already integrate Internet connectivity, which can allow them to communicate with utilities not only to balance power generation but also helps optimize the energy consumption as a whole.[62] These devices allow for remote control by users, or central management via a cloud-based interface, and enable functions like scheduling (e.g., remotely powering on or off heating systems, controlling ovens, changing lighting conditions etc.).[62] The smart grid is a utility-side IoT application; systems gather and act on energy and power-related information to improve the efficiency of the production and distribution of electricity.[110] Using advanced metering infrastructure (AMI) Internet-connected devices, electric utilities not only collect data from end-users, but also manage distribution automation devices like transformers.[62]\n Environmental monitoring applications of the IoT typically use sensors to assist in environmental protection[111] by monitoring air or water quality,[112] atmospheric or soil conditions,[113] and can even include areas like monitoring the movements of wildlife and their habitats.[114] Development of resource-constrained devices connected to the Internet also means that other applications like earthquake or tsunami early-warning systems can also be used by emergency services to provide more effective aid. IoT devices in this application typically span a large geographic area and can also be mobile.[62] It has been argued that the standardization that IoT brings to wireless sensing will revolutionize this area.[115]\n Another example of integrating the IoT is Living Lab which integrates and combines research and innovation processes, establishing within a public-private-people-partnership.[116] Between 2006 and January 2024, there were over 440 Living Labs (though not all are currently active)[117] that use the IoT to collaborate and share knowledge between stakeholders to co-create innovative and technological products. For companies to implement and develop IoT services[118] for smart cities, they need to have incentives. The governments play key roles in smart city projects as changes in policies will help cities to implement the IoT which provides effectiveness, efficiency, and accuracy of the resources that are being used. For instance, the government provides tax incentives and cheap rent, improves public transports, and offers an environment where start-up companies, creative industries, and multinationals may co-create, share a common infrastructure and labor markets, and take advantage of locally embedded technologies, production process, and transaction costs.[116]\n The Internet of Military Things (IoMT) is the application of IoT technologies in the military domain for the purposes of reconnaissance, surveillance, and other combat-related objectives. It is heavily influenced by the future prospects of warfare in an urban environment and involves the use of sensors, munitions, vehicles, robots, human-wearable biometrics, and other smart technology that is relevant on the battlefield.[119]\n One of the examples of IOT devices used in the military is Xaver 1000 system. The Xaver 1000 was developed by Israel's Camero Tech, which is the latest in the company's line of \"through wall imaging systems\". The Xaver line uses millimeter wave (MMW) radar, or radar in the range of 30-300 gigahertz. It is equipped with an AI-based life target tracking system as well as its own 3D 'sense-through-the-wall' technology.[120]\n The Internet of Battlefield Things (IoBT) is a project initiated and executed by the U.S. Army Research Laboratory (ARL) that focuses on the basic science related to the IoT that enhance the capabilities of Army soldiers.[121] In 2017, ARL launched the Internet of Battlefield Things Collaborative Research Alliance (IoBT-CRA), establishing a working collaboration between industry, university, and Army researchers to advance the theoretical foundations of IoT technologies and their applications to Army operations.[122][123]\n The Ocean of Things project is a DARPA-led program designed to establish an Internet of things across large ocean areas for the purposes of collecting, monitoring, and analyzing environmental and vessel activity data. The project entails the deployment of about 50,000 floats that house a passive sensor suite that autonomously detect and track military and commercial vessels as part of a cloud-based network.[124]\n There are several applications of smart or active packaging in which a QR code or NFC tag is affixed on a product or its packaging. The tag itself is passive, however, it contains a unique identifier (typically a URL) which enables a user to access digital content about the product via a smartphone.[125] Strictly speaking, such passive items are not part of the Internet of things, but they can be seen as enablers of digital interactions.[126] The term \"Internet of Packaging\" has been coined to describe applications in which unique identifiers are used, to automate supply chains, and are scanned on large scale by consumers to access digital content.[127] Authentication of the unique identifiers, and thereby of the product itself, is possible via a copy-sensitive digital watermark or copy detection pattern for scanning when scanning a QR code,[128] while NFC tags can encrypt communication.[129]\n The IoT's major significant trend in recent years[when?] is the growth of devices connected and controlled via the Internet.[130] The wide range of applications for IoT technology mean that the specifics can be very different from one device to the next but there are basic characteristics shared by most.\n The IoT creates opportunities for more direct integration of the physical world into computer-based systems, resulting in efficiency improvements, economic benefits, and reduced human exertions.[131][132][133][134]\n IoT Analytics reported there were 16.6 billion IoT devices connected in 2023. In 2020, the same firm projected there would be 30 billion devices connected by 2025. As of October, 2024, there are around 17 billion.[135][136][137]\n Ambient intelligence and autonomous control are not part of the original concept of the Internet of things. Ambient intelligence and autonomous control do not necessarily require Internet structures, either. However, there is a shift in research (by companies such as Intel) to integrate the concepts of the IoT and autonomous control, with initial outcomes towards this direction considering objects as the driving force for autonomous IoT.[138] An approach in this context is deep reinforcement learning where most of IoT systems provide a dynamic and interactive environment.[139] Training an agent (i.e., IoT device) to behave smartly in such an environment cannot be addressed by conventional machine learning algorithms such as supervised learning. By reinforcement learning approach, a learning agent can sense the environment's state (e.g., sensing home temperature), perform actions (e.g., turn HVAC on or off) and learn through the maximizing accumulated rewards it receives in long term.\n IoT intelligence can be offered at three levels: IoT devices, Edge/Fog nodes, and cloud computing.[140] The need for intelligent control and decision at each level depends on the time sensitiveness of the IoT application. For example, an autonomous vehicle's camera needs to make real-time obstacle detection to avoid an accident. This fast decision making would not be possible through transferring data from the vehicle to cloud instances and return the predictions back to the vehicle. Instead, all the operation should be performed locally in the vehicle. Integrating advanced machine learning algorithms including deep learning into IoT devices is an active research area to make smart objects closer to reality. Moreover, it is possible to get the most value out of IoT deployments through analyzing IoT data, extracting hidden information, and predicting control decisions. A wide variety of machine learning techniques have been used in IoT domain ranging from traditional methods such as regression, support vector machine, and random forest to advanced ones such as convolutional neural networks, LSTM, and variational autoencoder.[141][140]\n In the future, the Internet of things may be a non-deterministic and open network in which auto-organized or intelligent entities (web services, SOA components) and virtual objects (avatars) will be interoperable and able to act independently (pursuing their own objectives or shared ones) depending on the context, circumstances or environments. Autonomous behavior through the collection and reasoning of context information as well as the object's ability to detect changes in the environment (faults affecting sensors) and introduce suitable mitigation measures constitutes a major research trend,[142] clearly needed to provide credibility to the IoT technology. Modern IoT products and solutions in the marketplace use a variety of different technologies to support such context-aware automation, but more sophisticated forms of intelligence are requested to permit sensor units and intelligent cyber-physical systems to be deployed in real environments.[143]\n IoT system architecture, in its simplistic view, consists of three tiers: Tier 1: Devices, Tier 2: the Edge Gateway, and Tier 3: the Cloud.[144] Devices include networked things, such as the sensors and actuators found in IoT equipment, particularly those that use protocols such as Modbus, Bluetooth, Zigbee, or proprietary protocols, to connect to an Edge Gateway.[144] The Edge Gateway layer consists of sensor data aggregation systems called Edge Gateways that provide functionality, such as pre-processing of the data, securing connectivity to cloud, using systems such as WebSockets, the event hub, and, even in some cases, edge analytics or fog computing.[144] Edge Gateway layer is also required to give a common view of the devices to the upper layers to facilitate in easier management. The final tier includes the cloud application built for IoT using the microservices architecture, which are usually polyglot and inherently secure in nature using HTTPS/OAuth. It includes various database systems that store sensor data, such as time series databases or asset stores using backend data storage systems (e.g. Cassandra, PostgreSQL).[144] The cloud tier in most cloud-based IoT system features event queuing and messaging system that handles communication that transpires in all tiers.[145] Some experts classified the three-tiers in the IoT system as edge, platform, and enterprise and these are connected by proximity network, access network, and service network, respectively.[146]\n Building on the Internet of things, the web of things is an architecture for the application layer of the Internet of things looking at the convergence of data from IoT devices into Web applications to create innovative use-cases. In order to program and control the flow of information in the Internet of things, a predicted architectural direction is being called BPM Everywhere which is a blending of traditional process management with process mining and special capabilities to automate the control of large numbers of coordinated devices.[citation needed]\n The Internet of things requires huge scalability in the network space to handle the surge of devices.[147] IETF 6LoWPAN can be used to connect devices to IP networks. With billions of devices[148] being added to the Internet space, IPv6 will play a major role in handling the network layer scalability. IETF's Constrained Application Protocol, ZeroMQ, and MQTT can provide lightweight data transport. In practice many groups of IoT devices are hidden behind gateway nodes and may not have unique addresses. Also the vision of everything-interconnected is not needed for most applications as it is mainly the data which need interconnecting at a higher layer.[citation needed]\n Fog computing is a viable alternative to prevent such a large burst of data flow through the Internet.[149] The edge devices' computation power to analyze and process data is extremely limited. Limited processing power is a key attribute of IoT devices as their purpose is to supply data about physical objects while remaining autonomous. Heavy processing requirements use more battery power harming IoT's ability to operate. Scalability is easy because IoT devices simply supply data through the Internet to a server with sufficient processing power.[150]\n Decentralized Internet of things, or decentralized IoT, is a modified IoT which utilizes fog computing to handle and balance requests of connected IoT devices in order to reduce loading on the cloud servers and improve responsiveness for latency-sensitive IoT applications like vital signs monitoring of patients, vehicle-to-vehicle communication of autonomous driving, and critical failure detection of industrial devices.[151] Performance is improved, especially for huge IoT systems with millions of nodes.[152]\n Conventional IoT is connected via a mesh network and led by a major head node (centralized controller).[153] The head node decides how a data is created, stored, and transmitted.[154] In contrast, decentralized IoT attempts to divide IoT systems into smaller divisions.[155] The head node authorizes partial decision-making power to lower level sub-nodes under mutual agreed policy.[156]\n Some approached to decentralized IoT attempts to address the limited bandwidth and hashing capacity of battery powered or wireless IoT devices via blockchain.[157][158][159]\n In semi-open or closed loops (i.e., value chains, whenever a global finality can be settled) the IoT will often be considered and studied as a complex system[160] due to the huge number of different links, interactions between autonomous actors, and its capacity to integrate new actors. At the overall stage (full open loop) it will likely be seen as a chaotic environment (since systems always have finality). \nAs a practical approach, not all elements on the Internet of things run in a global, public space. Subsystems are often implemented to mitigate the risks of privacy, control and reliability. For example, domestic robotics (domotics) running inside a smart home might only share data within and be available via a local network.[161] Managing and controlling a high dynamic ad hoc IoT things/devices network is a tough task with the traditional networks architecture, Software Defined Networking (SDN) provides the agile dynamic solution that can cope with the special requirements of the diversity of innovative IoT applications.[162][163]\n The exact scale of the Internet of things is unknown, with quotes of billions or trillions often quoted at the beginning of IoT articles. In 2015 there were 83 million smart devices in people's homes. This number is expected to grow to 193 million devices by 2020.[43][164]\n The figure of online capable devices grew 31% from 2016 to 2017 to reach 8.4 billion.[165]\n In the Internet of things, the precise geographic location of a thing\u2014and also the precise geographic dimensions of a thing\u2014can be critical.[166] Therefore, facts about a thing, such as its location in time and space, have been less critical to track because the person processing the information can decide whether or not that information was important to the action being taken, and if so, add the missing information (or decide to not take the action). (Note that some things on the Internet of things will be sensors, and sensor location is usually important.[167]) The GeoWeb and Digital Earth are applications that become possible when things can become organized and connected by location. However, the challenges that remain include the constraints of variable spatial scales, the need to handle massive amounts of data, and an indexing for fast search and neighbour operations. On the Internet of things, if things are able to take actions on their own initiative, this human-centric mediation role is eliminated. Thus, the time-space context that we as humans take for granted must be given a central role in this information ecosystem. Just as standards play a key role on the Internet and the Web, geo-spatial standards will play a key role on the Internet of things.[168][169]\n Many IoT devices have the potential to take a piece of this market. Jean-Louis Gass\u00e9e (Apple initial alumni team, and BeOS co-founder) has addressed this topic in an article on Monday Note,[170] where he predicts that the most likely problem will be what he calls the \"basket of remotes\" problem, where we'll have hundreds of applications to interface with hundreds of devices that don't share protocols for speaking with one another.[170] For improved user interaction, some technology leaders are joining forces to create standards for communication between devices to solve this problem. Others are turning to the concept of predictive interaction of devices, \"where collected data is used to predict and trigger actions on the specific devices\" while making them work together.[171]\n Social Internet of things (SIoT) is a new kind of IoT that focuses the importance of social interaction and relationship between IoT devices.[172] SIoT is a pattern of how cross-domain IoT devices enabling application to application communication and collaboration without human intervention in order to serve their owners with autonomous services,[173] and this only can be realized when gained low-level architecture support from both IoT software and hardware engineering.[174]\n IoT defines a device with an identity like a citizen in a community and connect them to the Internet to provide services to its users.[175] SIoT defines a social network for IoT devices only to interact with each other for different goals that to serve human.[176]\n SIoT is different from the original IoT in terms of the collaboration characteristics. IoT is passive, it was set to serve for dedicated purposes with existing IoT devices in predetermined system. SIoT is active, it was programmed and managed by AI to serve for unplanned purposes with mix and match of potential IoT devices from different systems that benefit its users.[177]\n IoT devices built-in with sociability will broadcast their abilities or functionalities, and at the same time discovers, shares information, monitors, navigates and groups with other IoT devices in the same or nearby network realizing SIoT [178] and facilitating useful service compositions in order to help its users proactively in every day's life especially during emergency.[179]\n There are many technologies that enable the IoT. Crucial to the field is the network used to communicate between devices of an IoT installation, a role that several wireless or wired technologies may fulfill:[186][187][188]\n The original idea of the Auto-ID Center is based on RFID-tags and distinct identification through the Electronic Product Code. This has evolved into objects having an IP address or URI.[189] An alternative view, from the world of the Semantic Web[190] focuses instead on making all things (not just those electronic, smart, or RFID-enabled) addressable by the existing naming protocols, such as URI. The objects themselves do not converse, but they may now be referred to by other agents, such as powerful centralised servers acting for their human owners.[191] Integration with the Internet implies that devices will use an IP address as a distinct identifier. Due to the limited address space of IPv4 (which allows for 4.3 billion different addresses), objects in the IoT will have to use the next generation of the Internet protocol (IPv6) to scale to the extremely large address space required.[192][193][194]\nInternet-of-things devices additionally will benefit from the stateless address auto-configuration present in IPv6,[195] as it reduces the configuration overhead on the hosts,[193] and the IETF 6LoWPAN header compression. To a large extent, the future of the Internet of things will not be possible without the support of IPv6; and consequently, the global adoption of IPv6 in the coming years will be critical for the successful development of the IoT in the future.[194]\n Different technologies have different roles in a protocol stack. Below is a simplified[notes 1] presentation of the roles of several popular communication technologies in IoT applications:\n This is a list of technical standards for the IoT, most of which are open standards, and the standards organizations that aspire to successfully setting them.[210][211]\n The GS1 digital link standard,[215] first released in August 2018, allows the use QR Codes, GS1 Datamatrix, RFID and NFC to enable various types of business-to-business, as well as business-to-consumers interactions.\n Some scholars and activists argue that the IoT can be used to create new models of civic engagement if device networks can be open to user control and inter-operable platforms. Philip N. Howard, a professor and author, writes that political life in both democracies and authoritarian regimes will be shaped by the way the IoT will be used for civic engagement. For that to happen, he argues that any connected device should be able to divulge a list of the \"ultimate beneficiaries\" of its sensor data and that individual citizens should be able to add new organisations to the beneficiary list. In addition, he argues that civil society groups need to start developing their IoT strategy for making use of data and engaging with the public.[217]\n One of the key drivers of the IoT is data. The success of the idea of connecting devices to make them more efficient is dependent upon access to and storage & processing of data. For this purpose, companies working on the IoT collect data from multiple sources and store it in their cloud network for further processing. This leaves the door wide open for privacy and security dangers and single point vulnerability of multiple systems.[218] The other issues pertain to consumer choice and ownership of data[219] and how it is used. Though still in their infancy, regulations and governance regarding these issues of privacy, security, and data ownership continue to develop.[220][221][222] IoT regulation depends on the country. Some examples of legislation that is relevant to privacy and data collection are: the US Privacy Act of 1974, OECD Guidelines on the Protection of Privacy and Transborder Flows of Personal Data of 1980, and the EU Directive 95/46/EC of 1995.[223]\n Current regulatory environment:\n A report published by the Federal Trade Commission (FTC) in January 2015 made the following three recommendations:[224]\n However, the FTC stopped at just making recommendations for now. According to an FTC analysis, the existing framework, consisting of the FTC Act, the Fair Credit Reporting Act, and the Children's Online Privacy Protection Act, along with developing consumer education and business guidance, participation in multi-stakeholder efforts and advocacy to other agencies at the federal, state and local level, is sufficient to protect consumer rights.[226]\n A resolution passed by the Senate in March 2015, is already being considered by the Congress.[227] This resolution recognized the need for formulating a National Policy on IoT and the matter of privacy, security and spectrum. Furthermore, to provide an impetus to the IoT ecosystem, in March 2016, a bipartisan group of four Senators proposed a bill, The Developing Innovation and Growing the Internet of Things (DIGIT) Act, to direct the Federal Communications Commission to assess the need for more spectrum to connect IoT devices.\n Approved on 28 September 2018, California Senate Bill No. 327[228] goes into effect on 1 January 2020. The bill requires \"a manufacturer of a connected device, as those terms are defined, to equip the device with a reasonable security feature or features that are appropriate to the nature and function of the device, appropriate to the information it may collect, contain, or transmit, and designed to protect the device and any information contained therein from unauthorized access, destruction, use, modification, or disclosure,\"\n Several standards for the IoT industry are actually being established relating to automobiles because most concerns arising from use of connected cars apply to healthcare devices as well. In fact, the National Highway Traffic Safety Administration (NHTSA) is preparing cybersecurity guidelines and a database of best practices to make automotive computer systems more secure.[229]\n A recent report from the World Bank examines the challenges and opportunities in government adoption of IoT.[230] These include \u2013\n In early December 2021, the U.K. government introduced the Product Security and Telecommunications Infrastructure bill (PST), an effort to legislate IoT distributors, manufacturers, and importers to meet certain cybersecurity standards. The bill also seeks to improve the security credentials of consumer IoT devices.[231]\n The IoT suffers from platform fragmentation, lack of interoperability and common technical standards[232][233][234][235][236][237][238][excessive citations] a situation where the variety of IoT devices, in terms of both hardware variations and differences in the software running on them, makes the task of developing applications that work consistently between different inconsistent technology ecosystems hard.[1] For example, wireless connectivity for IoT devices can be done using Bluetooth, Wi-Fi, Wi-Fi HaLow, Zigbee, Z-Wave, LoRa, NB-IoT, Cat M1 as well as completely custom proprietary radios \u2013 each with its own advantages and disadvantages; and unique support ecosystem.[239]\n The IoT's amorphous computing nature is also a problem for security, since patches to bugs found in the core operating system often do not reach users of older and lower-price devices.[240][241][242] One set of researchers says that the failure of vendors to support older devices with patches and updates leaves more than 87% of active Android devices vulnerable.[243][244]\n Philip N. Howard, a professor and author, writes that the Internet of things offers immense potential for empowering citizens, making government transparent, and broadening information access. Howard cautions, however, that privacy threats are enormous, as is the potential for social control and political manipulation.[245]\n Concerns about privacy have led many to consider the possibility that big data infrastructures such as the Internet of things and data mining are inherently incompatible with privacy.[246] Key challenges of increased digitalization in the water, transport or energy sector are related to privacy and cybersecurity which necessitate an adequate response from research and policymakers alike.[247]\n Writer Adam Greenfield claims that IoT technologies are not only an invasion of public space but are also being used to perpetuate normative behavior, citing an instance of billboards with hidden cameras that tracked the demographics of passersby who stopped to read the advertisement.\n The Internet of Things Council compared the increased prevalence of digital surveillance due to the Internet of things to the concept of the panopticon described by Jeremy Bentham in the 18th century.[248] The assertion is supported by the works of French philosophers Michel Foucault and Gilles Deleuze. In Discipline and Punish: The Birth of the Prison, Foucault asserts that the panopticon was a central element of the discipline society developed during the Industrial Era.[249] Foucault also argued that the discipline systems established in factories and school reflected Bentham's vision of panopticism.[249] In his 1992 paper \"Postscripts on the Societies of Control\", Deleuze wrote that the discipline society had transitioned into a control society, with the computer replacing the panopticon as an instrument of discipline and control while still maintaining the qualities similar to that of panopticism.[250]\n Peter-Paul Verbeek, a professor of philosophy of technology at the University of Twente, Netherlands, writes that technology already influences our moral decision making, which in turn affects human agency, privacy and autonomy. He cautions against viewing technology merely as a human tool and advocates instead to consider it as an active agent.[251]\n Justin Brookman, of the Center for Democracy and Technology, expressed concern regarding the impact of the IoT on consumer privacy, saying that \"There are some people in the commercial space who say, 'Oh, big data \u2013 well, let's collect everything, keep it around forever, we'll pay for somebody to think about security later.' The question is whether we want to have some sort of policy framework in place to limit that.\"[252]\n Tim O'Reilly believes that the way companies sell the IoT devices on consumers are misplaced, disputing the notion that the IoT is about gaining efficiency from putting all kinds of devices online and postulating that the \"IoT is really about human augmentation. The applications are profoundly different when you have sensors and data driving the decision-making.\"[253]\n Editorials at WIRED have also expressed concern, one stating \"What you're about to lose is your privacy. Actually, it's worse than that. You aren't just going to lose your privacy, you're going to have to watch the very concept of privacy be rewritten under your nose.\"[254]\n The American Civil Liberties Union (ACLU) expressed concern regarding the ability of IoT to erode people's control over their own lives. The ACLU wrote that \"There's simply no way to forecast how these immense powers \u2013 disproportionately accumulating in the hands of corporations seeking financial advantage and governments craving ever more control \u2013 will be used. Chances are big data and the Internet of Things will make it harder for us to control our own lives, as we grow increasingly transparent to powerful corporations and government institutions that are becoming more opaque to us.\"[255]\n In response to rising concerns about privacy and smart technology, in 2007 the British Government stated it would follow formal Privacy by Design principles when implementing their smart metering program. The program would lead to replacement of traditional power meters with smart power meters, which could track and manage energy usage more accurately.[256] However the British Computer Society is doubtful these principles were ever actually implemented.[257] In 2009 the Dutch Parliament rejected a similar smart metering program, basing their decision on privacy concerns. The Dutch program later revised and passed in 2011.[257]\n A challenge for producers of IoT applications is to clean, process and interpret the vast amount of data which is gathered by the sensors. There is a solution proposed for the analytics of the information referred to as Wireless Sensor Networks.[258] These networks share data among sensor nodes that are sent to a distributed system for the analytics of the sensory data.[259]\n Another challenge is the storage of this bulk data. Depending on the application, there could be high data acquisition requirements, which in turn lead to high storage requirements. In 2013, the Internet was estimated to be responsible for consuming 5% of the total energy produced,[258] and a \"daunting challenge to power\" IoT devices to collect and even store data still remains.[260]\n Data silos, although a common challenge of legacy systems, still commonly occur with the implementation of IoT devices, particularly within manufacturing. As there are a lot of benefits to be gained from IoT and IIoT devices, the means in which the data is stored can present serious challenges without the principles of autonomy, transparency, and interoperability being considered.[261] The challenges do not occur by the device itself, but the means in which databases and data warehouses are set-up. These challenges were commonly identified in manufactures and enterprises which have begun upon digital transformation, and are part of the digital foundation, indicating that in order to receive the optimal benefits from IoT devices and for decision making, enterprises will have to first re-align their data storing methods. These challenges were identified by Keller (2021) when investigating the IT and application landscape of I4.0 implementation within German M&E manufactures.[261]\n Security is the biggest concern in adopting Internet of things technology,[262] with concerns that rapid development is happening without appropriate consideration of the profound security challenges involved[263] and the regulatory changes that might be necessary.[264][265] The rapid development of the Internet of Things (IoT) has allowed billions of devices to connect to the network. Due to too many connected devices and the limitation of communication security technology, various security issues gradually appear in the IoT.[266]\n Most of the technical security concerns are similar to those of conventional servers, workstations and smartphones.[267] These concerns include using weak authentication, forgetting to change default credentials, unencrypted messages sent between devices, SQL injections, man-in-the-middle attacks, and poor handling of security updates.[268][269] However, many IoT devices have severe operational limitations on the computational power available to them. These constraints often make them unable to directly use basic security measures such as implementing firewalls or using strong cryptosystems to encrypt their communications with other devices[270] - and the low price and consumer focus of many devices makes a robust security patching system uncommon.[271]\n Rather than conventional security vulnerabilities, fault injection attacks are on the rise and targeting IoT devices. A fault injection attack is a physical attack on a device to purposefully introduce faults in the system to change the intended behavior. Faults might happen unintentionally by environmental noises and electromagnetic fields. There are ideas stemmed from control-flow integrity (CFI) to prevent fault injection attacks and system recovery to a healthy state before the fault.[272]\n Internet of things devices also have access to new areas of data, and can often control physical devices,[273] so that even by 2014 it was possible to say that many Internet-connected appliances could already \"spy on people in their own homes\" including televisions, kitchen appliances,[274] cameras, and thermostats.[275] Computer-controlled devices in automobiles such as brakes, engine, locks, hood and trunk releases, horn, heat, and dashboard have been shown to be vulnerable to attackers who have access to the on-board network. In some cases, vehicle computer systems are Internet-connected, allowing them to be exploited remotely.[276] By 2008 security researchers had shown the ability to remotely control pacemakers without authority. Later hackers demonstrated remote control of insulin pumps[277] and implantable cardioverter defibrillators.[278]\n Poorly secured Internet-accessible IoT devices can also be subverted to attack others. In 2016, a distributed denial of service attack powered by Internet of things devices running the Mirai malware took down a DNS provider and major web sites.[279] The Mirai Botnet had infected roughly 65,000 IoT devices within the first 20 hours.[280] Eventually the infections increased to around 200,000 to 300,000 infections.[280] Brazil, Colombia and Vietnam made up of 41.5% of the infections.[280] The Mirai Botnet had singled out specific IoT devices that consisted of DVRs, IP cameras, routers and printers.[280] Top vendors that contained the most infected devices were identified as Dahua, Huawei, ZTE, Cisco, ZyXEL and MikroTik.[280] In May 2017, Junade Ali, a computer scientist at Cloudflare noted that native DDoS vulnerabilities exist in IoT devices due to a poor implementation of the Publish\u2013subscribe pattern.[281][282] These sorts of attacks have caused security experts to view IoT as a real threat to Internet services.[283]\n The U.S. National Intelligence Council in an unclassified report maintains that it would be hard to deny \"access to networks of sensors and remotely-controlled objects by enemies of the United States, criminals, and mischief makers... An open market for aggregated sensor data could serve the interests of commerce and security no less than it helps criminals and spies identify vulnerable targets. Thus, massively parallel sensor fusion may undermine social cohesion, if it proves to be fundamentally incompatible with Fourth-Amendment guarantees against unreasonable search.\"[284] In general, the intelligence community views the Internet of things as a rich source of data.[285]\n On 31 January 2019, The Washington Post wrote an article regarding the security and ethical challenges that can occur with IoT doorbells and cameras: \"Last month, Ring got caught allowing its team in Ukraine to view and annotate certain user videos; the company says it only looks at publicly shared videos and those from Ring owners who provide consent. Just last week, a California family's Nest camera let a hacker take over and broadcast fake audio warnings about a missile attack, not to mention peer in on them, when they used a weak password.\"[286]\n There have been a range of responses to concerns over security. The Internet of Things Security Foundation (IoTSF) was launched on 23 September 2015 with a mission to secure the Internet of things by promoting knowledge and best practice. Its founding board is made from technology providers and telecommunications companies. In addition, large IT companies are continually developing innovative solutions to ensure the security of IoT devices. In 2017, Mozilla launched Project Things, which allows to route IoT devices through a safe Web of Things gateway.[287] As per the estimates from KBV Research,[288] the overall IoT security market[289] would grow at 27.9% rate during 2016\u20132022 as a result of growing infrastructural concerns and diversified usage of Internet of things.[290][291]\n Governmental regulation is argued by some to be necessary to secure IoT devices and the wider Internet \u2013 as market incentives to secure IoT devices is insufficient.[292][264][265] It was found that due to the nature of most of the IoT development boards, they generate predictable and weak keys which make it easy to be utilized by man-in-the-middle attack. However, various hardening approaches were proposed by many researchers to resolve the issue of SSH weak implementation and weak keys.[293]\n IoT security within the field of manufacturing presents different challenges, and varying perspectives. Within the EU and Germany, data protection is constantly referenced throughout manufacturing and digital policy particularly that of I4.0. However, the attitude towards data security differs from the enterprise perspective whereas there is an emphasis on less data protection in the form of GDPR as the data being collected from IoT devices in the manufacturing sector does not display personal details.[261] Yet, research has indicated that manufacturing experts are concerned about \"data security for protecting machine technology from international competitors with the ever-greater push for interconnectivity\".[261]\n IoT systems are typically controlled by event-driven smart apps that take as input either sensed data, user inputs, or other external triggers (from the Internet) and command one or more actuators towards providing different forms of automation.[294] Examples of sensors include smoke detectors, motion sensors, and contact sensors. Examples of actuators include smart locks, smart power outlets, and door controls. Popular control platforms on which third-party developers can build smart apps that interact wirelessly with these sensors and actuators include Samsung's SmartThings,[295] Apple's HomeKit,[296] and Amazon's Alexa,[297] among others.\n A problem specific to IoT systems is that buggy apps, unforeseen bad app interactions, or device/communication failures, can cause unsafe and dangerous physical states, e.g., \"unlock the entrance door when no one is at home\" or \"turn off the heater when the temperature is below 0 degrees Celsius and people are sleeping at night\".[294] Detecting flaws that lead to such states, requires a holistic view of installed apps, component devices, their configurations, and more importantly, how they interact. Recently, researchers from the University of California Riverside have proposed IotSan, a novel practical system that uses model checking as a building block to reveal \"interaction-level\" flaws by identifying events that can lead the system to unsafe states.[294] They have evaluated IotSan on the Samsung SmartThings platform. From 76 manually configured systems, IotSan detects 147 vulnerabilities (i.e., violations of safe physical states/properties).\n Given widespread recognition of the evolving nature of the design and management of the Internet of things, sustainable and secure deployment of IoT solutions must design for \"anarchic scalability\".[298] Application of the concept of anarchic scalability can be extended to physical systems (i.e. controlled real-world objects), by virtue of those systems being designed to account for uncertain management futures. This hard anarchic scalability thus provides a pathway forward to fully realize the potential of Internet-of-things solutions by selectively constraining physical systems to allow for all management regimes without risking physical failure.[298]\n Brown University computer scientist Michael Littman has argued that successful execution of the Internet of things requires consideration of the interface's usability as well as the technology itself. These interfaces need to be not only more user-friendly but also better integrated: \"If users need to learn different interfaces for their vacuums, their locks, their sprinklers, their lights, and their coffeemakers, it's tough to say that their lives have been made any easier.\"[299]\n A concern regarding Internet-of-things technologies pertains to the environmental impacts of the manufacture, use, and eventual disposal of all these semiconductor-rich devices.[300] Modern electronics are replete with a wide variety of heavy metals and rare-earth metals, as well as highly toxic synthetic chemicals. This makes them extremely difficult to properly recycle. Electronic components are often incinerated or placed in regular landfills. Furthermore, the human and environmental cost of mining the rare-earth metals that are integral to modern electronic components continues to grow. This leads to societal questions concerning the environmental impacts of IoT devices over their lifetime.[301]\n The Electronic Frontier Foundation has raised concerns that companies can use the technologies necessary to support connected devices to intentionally disable or \"brick\" their customers' devices via a remote software update or by disabling a service necessary to the operation of the device. In one example, home automation devices sold with the promise of a \"Lifetime Subscription\" were rendered useless after Nest Labs acquired Revolv and made the decision to shut down the central servers the Revolv devices had used to operate.[302] As Nest is a company owned by Alphabet (Google's parent company), the EFF argues this sets a \"terrible precedent for a company with ambitions to sell self-driving cars, medical devices, and other high-end gadgets that may be essential to a person's livelihood or physical safety.\"[303]\n Owners should be free to point their devices to a different server or collaborate on improved software. But such action violates the United States DMCA section 1201, which only has an exemption for \"local use\". This forces tinkerers who want to keep using their own equipment into a legal grey area. EFF thinks buyers should refuse electronics and software that prioritize the manufacturer's wishes above their own.[303]\n Examples of post-sale manipulations include Google Nest Revolv, disabled privacy settings on Android, Sony disabling Linux on PlayStation 3, and enforced EULA on Wii U.[303]\n Kevin Lonergan at Information Age, a business technology magazine, has referred to the terms surrounding the IoT as a \"terminology zoo\".[304] The lack of clear terminology is not \"useful from a practical point of view\" and a \"source of confusion for the end user\".[304] A company operating in the IoT space could be working in anything related to sensor technology, networking, embedded systems, or analytics.[304] According to Lonergan, the term IoT was coined before smart phones, tablets, and devices as we know them today existed, and there is a long list of terms with varying degrees of overlap and technological convergence: Internet of things, Internet of everything (IoE), Internet of goods (supply chain), industrial Internet, pervasive computing, pervasive sensing, ubiquitous computing, cyber-physical systems (CPS), wireless sensor networks (WSN), smart objects, digital twin, cyberobjects or avatars,[160] cooperating objects, machine to machine (M2M), ambient intelligence (AmI), Operational technology (OT), and information technology (IT).[304] Regarding IIoT, an industrial sub-field of IoT, the Industrial Internet Consortium's Vocabulary Task Group has created a \"common and reusable vocabulary of terms\"[305] to ensure \"consistent terminology\"[305][306] across publications issued by the Industrial Internet Consortium. IoT One has created an IoT Terms Database including a New Term Alert[307] to be notified when a new term is published. As of March\u00a02020[update], this database aggregates 807 IoT-related terms, while keeping material \"transparent and comprehensive\".[308][309]\n Despite a shared belief in the potential of the IoT, industry leaders and consumers are facing barriers to adopt IoT technology more widely. Mike Farley argued in Forbes that while IoT solutions appeal to early adopters, they either lack interoperability or a clear use case for end-users.[310] A study by Ericsson regarding the adoption of IoT among Danish companies suggests that many struggle \"to pinpoint exactly where the value of IoT lies for them\".[311]\n As for IoT, especially in regards to consumer IoT, information about a user's daily routine is collected so that the \"things\" around the user can cooperate to provide better services that fulfill personal preference.[312] When the collected information which describes a user in detail travels through multiple hops in a network, due to a diverse integration of services, devices and network, the information stored on a device is vulnerable to privacy violation by compromising nodes existing in an IoT network.[313]\n For example, on 21 October 2016, a multiple distributed denial of service (DDoS) attacks systems operated by domain name system provider Dyn, which caused the inaccessibility of several websites, such as GitHub, Twitter, and others. This attack is executed through a botnet consisting of a large number of IoT devices including IP cameras, gateways, and even baby monitors.[314]\n Fundamentally there are 4 security objectives that the IoT system requires: (1) data confidentiality: unauthorised parties cannot have access to the transmitted and stored data; (2) data integrity: intentional and unintentional corruption of transmitted and stored data must be detected; (3) non-repudiation: the sender cannot deny having sent a given message; (4) data availability: the transmitted and stored data should be available to authorised parties even with the denial-of-service (DOS) attacks.[315]\n Information privacy regulations also require organisations to practice \"reasonable security\". California's SB-327 Information privacy: connected devices \"would require a manufacturer of a connected device, as those terms are defined, to equip the device with a reasonable security feature or features that are appropriate to the nature and function of the device, appropriate to the information it may collect, contain, or transmit, and designed to protect the device and any information contained therein from unauthorised access, destruction, use, modification, or disclosure, as specified\".[316] As each organisation's environment is unique, it can prove challenging to demonstrate what \"reasonable security\" is and what potential risks could be involved for the business. Oregon's HB2395 also \"requires [a] person that manufactures, sells or offers to sell connected device] manufacturer to equip connected device with reasonable security features that protect connected device and information that connected device collects, contains, stores or transmits] stores from access, destruction, modification, use or disclosure that consumer does not authorise.\"[317]\n According to antivirus provider Kaspersky, there were 639 million data breaches of IoT devices in 2020 and 1.5 billion breaches in the first six months of 2021.[231]\n A study issued by Ericsson regarding the adoption of Internet of things among Danish companies identified a \"clash between IoT and companies' traditional governance structures, as IoT still presents both uncertainties and a lack of historical precedence.\"[311] Among the respondents interviewed, 60 percent stated that they \"do not believe they have the organizational capabilities, and three of four do not believe they have the processes needed, to capture the IoT opportunity.\"[311] This has led to a need to understand organizational culture in order to facilitate organizational design processes and to test new innovation management practices. A lack of digital leadership in the age of digital transformation has also stifled innovation and IoT adoption to a degree that many companies, in the face of uncertainty, \"were waiting for the market dynamics to play out\",[311] or further action in regards to IoT \"was pending competitor moves, customer pull, or regulatory requirements\".[311] Some of these companies risk being \"kodaked\" \u2013 \"Kodak was a market leader until digital disruption eclipsed film photography with digital photos\" \u2013 failing to \"see the disruptive forces affecting their industry\"[318] and \"to truly embrace the new business models the disruptive change opens up\".[318] Scott Anthony has written in Harvard Business Review that Kodak \"created a digital camera, invested in the technology, and even understood that photos would be shared online\"[318] but ultimately failed to realize that \"online photo sharing was the new business, not just a way to expand the printing business.\"[318]\n According to 2018 study, 70\u201375% of IoT deployments were stuck in the pilot or prototype stage, unable to reach scale due in part to a lack of business planning.[319][page\u00a0needed][320]\n Even though scientists, engineers, and managers across the world are continuously working to create and exploit the benefits of IoT products, there are some flaws in the governance, management and implementation of such projects. Despite tremendous forward momentum in the field of information and other underlying technologies, IoT still remains a complex area and the problem of how IoT projects are managed still needs to be addressed.\u00a0IoT projects must be run differently than simple and traditional IT, manufacturing or construction projects. Because IoT projects have longer project timelines, a lack of skilled resources and several security/legal issues, there is a need for new and specifically designed project processes. The following management techniques should improve the success rate of IoT projects:[321]\n",
        "doc_number": 47
    },
    {
        "url": "https://en.wikipedia.org/wiki/Edge_computing",
        "content": "Edge computing is a distributed computing model that brings computation and data storage closer to the sources of data. More broadly, it refers to any design that pushes computation physically closer to a user, so as to reduce the latency compared to when an application runs on a centralized data centre.[1]\n The term began being used in the 1990s to describe content delivery networks\u2014these were used to deliver website and video content from servers located near users.[2] In the early 2000s, these systems expanded their scope to hosting other applications,[3] leading to early edge computing services.[4] These services could do things like find dealers, manage shopping carts, gather real-time data, and place ads.\n The Internet of Things (IoT), where devices are connected to the internet, is often linked with edge computing. However, it's important to understand that edge computing and IoT are not the same thing.[5]\n Edge computing involves running computer programs that deliver quick responses close to where requests are made. Karim Arabi, during an IEEE DAC 2014 keynote[6] and later at an MIT MTL Seminar in 2015, described edge computing as computing that occurs outside the cloud, at the network's edge, particularly for applications needing immediate data processing.[7]\n Edge computing is often equated with fog computing, particularly in smaller setups.[8] However, in larger deployments, such as smart cities, fog computing serves as a distinct layer between edge computing and cloud computing, with each layer having its own responsibilities.[9][10]\n \"The State of the Edge\" report explains that edge computing focuses on servers located close to the end-users.[11] Alex Reznik, Chair of the ETSI MEC ISG standards committee, defines 'edge' loosely as anything that's not a traditional data center.[12]\n In cloud gaming, edge nodes, known as \"gamelets,\" are typically within one or two network hops from the client, ensuring quick response times for real-time games.[13]\n Edge computing might use virtualization technology to simplify deploying and managing various applications on edge servers.[14]\n The world's data is expected to grow 61 percent to 175 zettabytes by 2025.[15] According to research firm Gartner, around 10 percent of enterprise-generated data is created and processed outside a traditional centralized data center or cloud. By 2025, the firm predicts that this figure will reach 75 percent.[16] The increase in IoT devices at the edge of the network is producing a massive amount of data \u2014 storing and using all that data in cloud data centers pushes network bandwidth requirements to the limit.[17] Despite the improvements in network technology, data centers cannot guarantee acceptable transfer rates and response times, which often is a critical requirement for many applications.[18] Furthermore, devices at the edge constantly consume data coming from the cloud, forcing companies to decentralize data storage and service provisioning, leveraging physical proximity to the end user.\n In a similar way, the aim of edge computing is to move the computation away from data centers towards the edge of the network, exploiting smart objects, mobile phones, or network gateways to perform tasks and provide services on behalf of the cloud.[19] By moving services to the edge, it is possible to provide content caching, service delivery, persistent data storage, and IoT management resulting in better response times and transfer rates. At the same time, distributing the logic to different network nodes introduces new issues and challenges.[20]\n The distributed nature of this paradigm introduces a shift in security schemes used in cloud computing. In edge computing, data may travel between different distributed nodes connected through the Internet and thus requires special encryption mechanisms independent of the cloud. Edge nodes may also be resource-constrained devices, limiting the choice in terms of security methods. Moreover, a shift from centralized top-down infrastructure to a decentralized trust model is required.[21]\nOn the other hand, by keeping and processing data at the edge, it is possible to increase privacy by minimizing the transmission of sensitive information to the cloud. Furthermore, the ownership of collected data shifts from service providers to end-users.[22]\n Scalability in a distributed network must face different issues. First, it must take into account the heterogeneity of the devices, having different performance and energy constraints, the highly dynamic condition, and the reliability of the connections compared to more robust infrastructure of cloud data centers. Moreover, security requirements may introduce further latency in the communication between nodes, which may slow down the scaling process.[18]\n The state-of-the-art scheduling technique can increase the effective utilization of edge resources and scales the edge server by assigning minimum edge resources to each offloaded task.[23]\n Management of failovers is crucial in order to keep a service alive. If a single node goes down and is unreachable, users should still be able to access a service without interruptions. Moreover, edge computing systems must provide actions to recover from a failure and alert the user about the incident. To this aim, each device must maintain the network topology of the entire distributed system, so that detection of errors and recovery become easily applicable. Other factors that may influence this aspect are the connection technologies in use, which may provide different levels of reliability, and the accuracy of the data produced at the edge that could be unreliable due to particular environment conditions.[18] As an example, an edge computing device, such as a voice assistant, may continue to provide service to local users even during cloud service or internet outages.[22]\n Edge computing brings analytical computational resources close to the end users and therefore can increase the responsiveness and throughput of applications. A well-designed edge platform would significantly outperform a traditional cloud-based system. Some applications rely on short response times, making edge computing a significantly more feasible option than cloud computing. Examples range from IoT to autonomous driving,[24] anything health or human / public safety relevant,[25] or involving human perception such as facial recognition, which typically takes a human between 370-620\u00a0ms to perform.[26] Edge computing is more likely to be able to mimic the same perception speed as humans, which is useful in applications such as augmented reality where the headset should preferably recognize who a person is at the same time as the wearer does.\n Due to the nearness of the analytical resources to the end users, sophisticated analytical tools and Artificial Intelligence tools can run on the edge of the system. This placement at the edge helps to increase operational efficiency and is responsible for many advantages to the system.\n Additionally, the usage of edge computing as an intermediate stage between client devices and the wider internet results in efficiency savings that can be demonstrated in the following example: A client device requires computationally intensive processing on video files to be performed on external servers. By using servers located on a local edge network to perform those computations, the video files only need to be transmitted in the local network. Avoiding transmission over the internet results in significant bandwidth savings and therefore increases efficiency.[26]  Another example is voice recognition. If the recognition is performed locally, it is possible to send the recognized text to the cloud rather than audio recordings, significantly reducing the amount of required bandwidth.[22]\n Edge application services reduce the volumes of data that must be moved, the consequent traffic, and the distance that data must travel. That provides lower latency and reduces transmission costs. Computation offloading for real-time applications, such as facial recognition algorithms, showed considerable improvements in response times, as demonstrated in early research.[27] Further research showed that using resource-rich machines called cloudlets or micro data centers near mobile users, which offer services typically found in the cloud, provided improvements in execution time when some of the tasks are offloaded to the edge node.[28] On the other hand, offloading every task may result in a slowdown due to transfer times between device and nodes, so depending on the workload, an optimal configuration can be defined.\n IoT-based power grid system enables communication of electricity and data to monitor and control the power grid,[29] which makes energy management more efficient.\n Other notable applications include connected cars, autonomous cars,[30] smart cities,[31] Industry 4.0, home automation[32] and satellite systems.[33] The nascent field of edge artificial intelligence (edge AI) implements the artificial intelligence in an edge computing environment, on the device or close to where data is collected.[34]\n",
        "doc_number": 48
    },
    {
        "url": "https://en.wikipedia.org/wiki/Quantum_computing",
        "content": "\n A quantum computer is a computer that exploits quantum mechanical phenomena. On small scales, physical matter exhibits properties of both particles and waves, and quantum computing leverages this behavior using specialized hardware. Classical physics cannot explain the operation of these quantum devices, and a scalable quantum computer could perform some calculations exponentially faster[a] than any modern \"classical\" computer. Theoretically a large-scale quantum computer could break some widely used encryption schemes and aid physicists in performing physical simulations; however, the current state of the art is largely experimental and impractical, with several obstacles to useful applications.\n The basic unit of information in quantum computing, the qubit (or \"quantum bit\"), serves the same function as the bit in classical computing. However, unlike a classical bit, which can be in one of two states (a binary), a qubit can exist in a superposition of its two \"basis\" states, a state that is in an abstract sense \"between\" the two basis states. When measuring a qubit, the result is a probabilistic output of a classical bit. If a quantum computer manipulates the qubit in a particular way, wave interference effects can amplify the desired measurement results. The design of quantum algorithms involves creating procedures that allow a quantum computer to perform calculations efficiently and quickly.\n Quantum computers are not yet practical for real work. Physically engineering high-quality qubits has proven challenging. If a physical qubit is not sufficiently isolated from its environment, it suffers from quantum decoherence, introducing noise into calculations. National governments have invested heavily in experimental research that aims to develop scalable qubits with longer coherence times and lower error rates. Example implementations include superconductors (which isolate an electrical current by eliminating electrical resistance) and ion traps (which confine a single atomic particle using electromagnetic fields).\n In principle, a classical computer can solve the same computational problems as a quantum computer, given enough time. Quantum advantage comes in the form of time complexity rather than computability, and quantum complexity theory shows that some quantum algorithms are exponentially more efficient than the best-known classical algorithms. A large-scale quantum computer could in theory solve computational problems unsolvable by a classical computer in any reasonable amount of time. This concept of extra ability has been called \"quantum supremacy\". While such claims have drawn significant attention to the discipline, near-term practical use cases remain limited.\n For many years, the fields of quantum mechanics and computer science formed distinct academic communities.[1] Modern quantum theory developed in the 1920s to explain perplexing physical phenomena observed at atomic scales,[2][3] and digital computers emerged in the following decades to replace human computers for tedious calculations.[4] Both disciplines had practical applications during World War II; computers played a major role in wartime cryptography,[5] and quantum physics was essential for nuclear physics used in the Manhattan Project.[6]\n As physicists applied quantum mechanical models to computational problems and swapped digital bits for qubits, the fields of quantum mechanics and computer science began to converge. In 1980, Paul Benioff introduced the quantum Turing machine, which uses quantum theory to describe a simplified computer.[7]\nWhen digital computers became faster, physicists faced an exponential increase in overhead when simulating quantum dynamics,[8] prompting Yuri Manin and Richard Feynman to independently suggest that hardware based on quantum phenomena might be more efficient for computer simulation.[9][10][11]\nIn a 1984 paper, Charles Bennett and Gilles Brassard applied quantum theory to cryptography protocols and demonstrated that quantum key distribution could enhance information security.[12][13]\n Quantum algorithms then emerged for solving oracle problems, such as Deutsch's algorithm in 1985,[14] the Bernstein\u2013Vazirani algorithm in 1993,[15] and Simon's algorithm in 1994.[16]\nThese algorithms did not solve practical problems, but demonstrated mathematically that one could gain more information by querying a black box with a quantum state in superposition, sometimes referred to as quantum parallelism.[17]\n Peter Shor built on these results with his 1994 algorithm for breaking the widely used RSA and Diffie\u2013Hellman encryption protocols,[18] which drew significant attention to the field of quantum computing. In 1996, Grover's algorithm established a quantum speedup for the widely applicable unstructured search problem.[19][20] The same year, Seth Lloyd proved that quantum computers could simulate quantum systems without the exponential overhead present in classical simulations,[21] validating Feynman's 1982 conjecture.[22]\n Over the years, experimentalists have constructed small-scale quantum computers using trapped ions and superconductors.[23]\nIn 1998, a two-qubit quantum computer demonstrated the feasibility of the technology,[24][25] and subsequent experiments have increased the number of qubits and reduced error rates.[23]\n In 2019, Google AI and NASA announced that they had achieved quantum supremacy with a 54-qubit machine, performing a computation that is impossible for any classical computer.[26][27][28] However, the validity of this claim is still being actively researched.[29][30]\n Computer engineers typically describe a modern computer's operation in terms of classical electrodynamics.\nWithin these \"classical\" computers, some components (such as semiconductors and random number generators) may rely on quantum behavior, but these components are not isolated from their environment, so any quantum information quickly decoheres.\nWhile programmers may depend on probability theory when designing a randomized algorithm, quantum mechanical notions like superposition and interference are largely irrelevant for program analysis.\n Quantum programs, in contrast, rely on precise control of coherent quantum systems. Physicists describe these systems mathematically using linear algebra. Complex numbers model probability amplitudes, vectors model quantum states, and matrices model the operations that can be performed on these states. Programming a quantum computer is then a matter of composing operations in such a way that the resulting program computes a useful result in theory and is implementable in practice.\n As physicist Charlie Bennett describes the relationship between quantum and classical computers,[31]\n A classical computer is a quantum computer\u00a0... so we shouldn't be asking about \"where do quantum speedups come from?\" We should say, \"well, all computers are quantum.\u00a0... Where do classical slowdowns come from?\" Just as the bit is the basic concept of classical information theory, the qubit is the fundamental unit of quantum information. The same term qubit is used to refer to an abstract mathematical model and to any physical system that is represented by that model. A classical bit, by definition, exists in either of two physical states, which can be denoted 0 and 1. A qubit is also described by a state, and two states often written \n\n\n\n\n|\n\n0\n\u27e9\n\n\n{\\displaystyle |0\\rangle }\n\n and \n\n\n\n\n|\n\n1\n\u27e9\n\n\n{\\displaystyle |1\\rangle }\n\n serve as the quantum counterparts of the classical states 0 and 1. However, the quantum states \n\n\n\n\n|\n\n0\n\u27e9\n\n\n{\\displaystyle |0\\rangle }\n\n and \n\n\n\n\n|\n\n1\n\u27e9\n\n\n{\\displaystyle |1\\rangle }\n\n belong to a vector space, meaning that they can be multiplied by constants and added together, and the result is again a valid quantum state. Such a combination is known as a superposition of \n\n\n\n\n|\n\n0\n\u27e9\n\n\n{\\displaystyle |0\\rangle }\n\n and \n\n\n\n\n|\n\n1\n\u27e9\n\n\n{\\displaystyle |1\\rangle }\n\n.[32][33]\n A two-dimensional vector mathematically represents a qubit state. Physicists typically use Dirac notation for quantum mechanical linear algebra, writing \n\n\n\n\n|\n\n\u03c8\n\u27e9\n\n\n{\\displaystyle |\\psi \\rangle }\n\n 'ket psi' for a vector labeled \n\n\n\n\u03c8\n\n\n{\\displaystyle \\psi }\n\n . Because a qubit is a two-state system, any qubit state takes the form \n\n\n\n\u03b1\n\n|\n\n0\n\u27e9\n+\n\u03b2\n\n|\n\n1\n\u27e9\n\n\n{\\displaystyle \\alpha |0\\rangle +\\beta |1\\rangle }\n\n , where \n\n\n\n\n|\n\n0\n\u27e9\n\n\n{\\displaystyle |0\\rangle }\n\n and \n\n\n\n\n|\n\n1\n\u27e9\n\n\n{\\displaystyle |1\\rangle }\n\n are the standard basis states,[b] and \n\n\n\n\u03b1\n\n\n{\\displaystyle \\alpha }\n\n and \n\n\n\n\u03b2\n\n\n{\\displaystyle \\beta }\n\n are the probability amplitudes, which are in general complex numbers.[33] If either \n\n\n\n\u03b1\n\n\n{\\displaystyle \\alpha }\n\n or \n\n\n\n\u03b2\n\n\n{\\displaystyle \\beta }\n\n is zero, the qubit is effectively a classical bit; when both are nonzero, the qubit is in superposition. Such a quantum state vector acts similarly to a (classical) probability vector, with one key difference: unlike probabilities, probability amplitudes are not necessarily positive numbers.[35] Negative amplitudes allow for destructive wave interference.\n When a qubit is measured in the standard basis, the result is a classical bit. The Born rule describes the norm-squared correspondence between amplitudes and probabilities\u2014when measuring a qubit \n\n\n\n\u03b1\n\n|\n\n0\n\u27e9\n+\n\u03b2\n\n|\n\n1\n\u27e9\n\n\n{\\displaystyle \\alpha |0\\rangle +\\beta |1\\rangle }\n\n, the state collapses to \n\n\n\n\n|\n\n0\n\u27e9\n\n\n{\\displaystyle |0\\rangle }\n\n with probability \n\n\n\n\n|\n\n\u03b1\n\n\n|\n\n\n2\n\n\n\n\n{\\displaystyle |\\alpha |^{2}}\n\n, or to \n\n\n\n\n|\n\n1\n\u27e9\n\n\n{\\displaystyle |1\\rangle }\n\n with probability \n\n\n\n\n|\n\n\u03b2\n\n\n|\n\n\n2\n\n\n\n\n{\\displaystyle |\\beta |^{2}}\n\n.\nAny valid qubit state has coefficients \n\n\n\n\u03b1\n\n\n{\\displaystyle \\alpha }\n\n and \n\n\n\n\u03b2\n\n\n{\\displaystyle \\beta }\n\n such that \n\n\n\n\n|\n\n\u03b1\n\n\n|\n\n\n2\n\n\n+\n\n|\n\n\u03b2\n\n\n|\n\n\n2\n\n\n=\n1\n\n\n{\\displaystyle |\\alpha |^{2}+|\\beta |^{2}=1}\n\n.\nAs an example, measuring the qubit \n\n\n\n1\n\n/\n\n\n\n2\n\n\n\n|\n\n0\n\u27e9\n+\n1\n\n/\n\n\n\n2\n\n\n\n|\n\n1\n\u27e9\n\n\n{\\displaystyle 1/{\\sqrt {2}}|0\\rangle +1/{\\sqrt {2}}|1\\rangle }\n\n would produce either \n\n\n\n\n|\n\n0\n\u27e9\n\n\n{\\displaystyle |0\\rangle }\n\n or \n\n\n\n\n|\n\n1\n\u27e9\n\n\n{\\displaystyle |1\\rangle }\n\n with equal probability.\n Each additional qubit doubles the dimension of the state space.[34]\nAs an example, the vector \u20601/\u221a2\u2060|00\u27e9 + \u20601/\u221a2\u2060|01\u27e9 represents a two-qubit state, a tensor product of the qubit |0\u27e9 with the qubit \u20601/\u221a2\u2060|0\u27e9 + \u20601/\u221a2\u2060|1\u27e9.\nThis vector inhabits a four-dimensional vector space spanned by the basis vectors |00\u27e9, |01\u27e9, |10\u27e9, and |11\u27e9.\nThe Bell state \u20601/\u221a2\u2060|00\u27e9 + \u20601/\u221a2\u2060|11\u27e9 is impossible to decompose into the tensor product of two individual qubits\u2014the two qubits are entangled because neither qubit has a state vector of its own.\nIn general, the vector space for an n-qubit system is 2n-dimensional, and this makes it challenging for a classical computer to simulate a quantum one: representing a 100-qubit system requires storing 2100 classical values.\n The state of this one-qubit quantum memory can be manipulated by applying quantum logic gates, analogous to how classical memory can be manipulated with classical logic gates. One important gate for both classical and quantum computation is the NOT gate, which can be represented by a matrix\n\n\n\n\nX\n:=\n\n\n(\n\n\n\n0\n\n\n1\n\n\n\n\n1\n\n\n0\n\n\n\n)\n\n\n.\n\n\n{\\displaystyle X:={\\begin{pmatrix}0&1\\\\1&0\\end{pmatrix}}.}\n\n\nMathematically, the application of such a logic gate to a quantum state vector is modelled with matrix multiplication. Thus\n The mathematics of single qubit gates can be extended to operate on multi-qubit quantum memories in two important ways. One way is simply to select a qubit and apply that gate to the target qubit while leaving the remainder of the memory unaffected. Another way is to apply the gate to its target only if another part of the memory is in a desired state. These two choices can be illustrated using another example. The possible states of a two-qubit quantum memory are\n\n\n\n\n\n|\n\n00\n\u27e9\n:=\n\n\n(\n\n\n\n1\n\n\n\n\n0\n\n\n\n\n0\n\n\n\n\n0\n\n\n\n)\n\n\n;\n\n\n|\n\n01\n\u27e9\n:=\n\n\n(\n\n\n\n0\n\n\n\n\n1\n\n\n\n\n0\n\n\n\n\n0\n\n\n\n)\n\n\n;\n\n\n|\n\n10\n\u27e9\n:=\n\n\n(\n\n\n\n0\n\n\n\n\n0\n\n\n\n\n1\n\n\n\n\n0\n\n\n\n)\n\n\n;\n\n\n|\n\n11\n\u27e9\n:=\n\n\n(\n\n\n\n0\n\n\n\n\n0\n\n\n\n\n0\n\n\n\n\n1\n\n\n\n)\n\n\n.\n\n\n{\\displaystyle |00\\rangle :={\\begin{pmatrix}1\\\\0\\\\0\\\\0\\end{pmatrix}};\\quad |01\\rangle :={\\begin{pmatrix}0\\\\1\\\\0\\\\0\\end{pmatrix}};\\quad |10\\rangle :={\\begin{pmatrix}0\\\\0\\\\1\\\\0\\end{pmatrix}};\\quad |11\\rangle :={\\begin{pmatrix}0\\\\0\\\\0\\\\1\\end{pmatrix}}.}\n\n\nThe controlled NOT (CNOT) gate can then be represented using the following matrix:\n\n\n\n\nCNOT\n:=\n\n\n(\n\n\n\n1\n\n\n0\n\n\n0\n\n\n0\n\n\n\n\n0\n\n\n1\n\n\n0\n\n\n0\n\n\n\n\n0\n\n\n0\n\n\n0\n\n\n1\n\n\n\n\n0\n\n\n0\n\n\n1\n\n\n0\n\n\n\n)\n\n\n.\n\n\n{\\displaystyle \\operatorname {CNOT} :={\\begin{pmatrix}1&0&0&0\\\\0&1&0&0\\\\0&0&0&1\\\\0&0&1&0\\end{pmatrix}}.}\n\n\nAs a mathematical consequence of this definition, \n\n\n\nCNOT\n\u2061\n\n|\n\n00\n\u27e9\n=\n\n|\n\n00\n\u27e9\n\n\n{\\textstyle \\operatorname {CNOT} |00\\rangle =|00\\rangle }\n\n, \n\n\n\nCNOT\n\u2061\n\n|\n\n01\n\u27e9\n=\n\n|\n\n01\n\u27e9\n\n\n{\\textstyle \\operatorname {CNOT} |01\\rangle =|01\\rangle }\n\n, \n\n\n\nCNOT\n\u2061\n\n|\n\n10\n\u27e9\n=\n\n|\n\n11\n\u27e9\n\n\n{\\textstyle \\operatorname {CNOT} |10\\rangle =|11\\rangle }\n\n, and \n\n\n\nCNOT\n\u2061\n\n|\n\n11\n\u27e9\n=\n\n|\n\n10\n\u27e9\n\n\n{\\textstyle \\operatorname {CNOT} |11\\rangle =|10\\rangle }\n\n. In other words, the CNOT applies a NOT gate (\n\n\n\nX\n\n\n{\\textstyle X}\n\n from before) to the second qubit if and only if the first qubit is in the state \n\n\n\n\n|\n\n1\n\u27e9\n\n\n{\\textstyle |1\\rangle }\n\n. If the first qubit is \n\n\n\n\n|\n\n0\n\u27e9\n\n\n{\\textstyle |0\\rangle }\n\n, nothing is done to either qubit.\n In summary, quantum computation can be described as a network of quantum logic gates and measurements. However, any measurement can be deferred to the end of quantum computation, though this deferment may come at a computational cost, so most quantum circuits depict a network consisting only of quantum logic gates and no measurements.\n Quantum parallelism is the heuristic that quantum computers can be thought of as evaluating a function for multiple input values simultaneously. This can be achieved by preparing a quantum system in a superposition of input states and applying a unitary transformation that encodes the function to be evaluated. The resulting state encodes the function's output values for all input values in the superposition, allowing for the computation of multiple outputs simultaneously. This property is key to the speedup of many quantum algorithms. However, \"parallelism\" in this sense is insufficient to speed up a computation, because the measurement at the end of the computation gives only one value. To be useful, a quantum algorithm must also incorporate some other conceptual ingredient.[36][37]\n There are a number of models of computation for quantum computing, distinguished by the basic elements in which the computation is decomposed.\n A quantum gate array decomposes computation into a sequence of few-qubit quantum gates. A quantum computation can be described as a network of quantum logic gates and measurements. However, any measurement can be deferred to the end of quantum computation, though this deferment may come at a computational cost, so most quantum circuits depict a network consisting only of quantum logic gates and no measurements.\n Any quantum computation (which is, in the above formalism, any unitary matrix of size \n\n\n\n\n2\n\nn\n\n\n\u00d7\n\n2\n\nn\n\n\n\n\n{\\displaystyle 2^{n}\\times 2^{n}}\n\n over \n\n\n\nn\n\n\n{\\displaystyle n}\n\n qubits) can be represented as a network of quantum logic gates from a fairly small family of gates. A choice of gate family that enables this construction is known as a universal gate set, since a computer that can run such circuits is a universal quantum computer. One common such set includes all single-qubit gates as well as the CNOT gate from above. This means any quantum computation can be performed by executing a sequence of single-qubit gates together with CNOT gates. Though this gate set is infinite, it can be replaced with a finite gate set by appealing to the Solovay-Kitaev theorem. Implementation of Boolean functions using the few-qubit quantum gates is presented here.[38]\n A measurement-based quantum computer decomposes computation into a sequence of Bell state measurements and single-qubit quantum gates applied to a highly entangled initial state (a cluster state), using a technique called quantum gate teleportation.\n An adiabatic quantum computer, based on quantum annealing, decomposes computation into a slow continuous transformation of an initial Hamiltonian into a final Hamiltonian, whose ground states contain the solution.[39]\n Neuromorphic quantum computing (abbreviated as \u2018n.quantum computing\u2019) is an unconventional computing type of computing that uses neuromorphic computing to perform quantum operations. It was suggested that quantum algorithms, which are algorithms that run on a realistic model of quantum computation, can be computed equally efficiently with neuromorphic quantum computing. Both, traditional quantum computing and neuromorphic quantum computing are physics-based unconventional computing approaches to computations and do not follow the von Neumann architecture. They both construct a system (a circuit) that represents the physical problem at hand and then leverage their respective physics properties of the system to seek the \u201cminimum\u201d. Neuromorphic quantum computing and quantum computing share similar physical properties during computation.\n A topological quantum computer decomposes computation into the braiding of anyons in a 2D lattice.[40]\n A quantum Turing machine is the quantum analog of a Turing machine.[7] All of these models of computation\u2014quantum circuits,[41] one-way quantum computation,[42] adiabatic quantum computation,[43] and topological quantum computation[44]\u2014have been shown to be equivalent to the quantum Turing machine; given a perfect implementation of one such quantum computer, it can simulate all the others with no more than polynomial overhead. This equivalence need not hold for practical quantum computers, since the overhead of simulation may be too large to be practical.\n The threshold theorem shows how increasing the number of qubits can mitigate errors,[45] yet fully fault-tolerant quantum computing remains \"a rather distant dream\".[46] According to some researchers, noisy intermediate-scale quantum (NISQ) machines may have specialized uses in the near future, but noise in quantum gates limits their reliability.[46]\nScientists at Harvard University successfully created \"quantum circuits\" that correct errors more efficiently than alternative methods, which may potentially remove a major obstacle to practical quantum computers.[47][48] The Harvard research team was supported by MIT, QuEra Computing, Caltech, and Princeton University and funded by DARPA's Optimization with Noisy Intermediate-Scale Quantum devices (ONISQ) program.[49][50]\n Quantum computing has significant potential applications in the fields of cryptography and cybersecurity. Quantum cryptography, which relies on the principles of quantum mechanics, offers the possibility of secure communication channels that are resistant to eavesdropping. Quantum key distribution (QKD) protocols, such as BB84, enable the secure exchange of cryptographic keys between parties, ensuring the confidentiality and integrity of communication. Moreover, quantum random number generators (QRNGs) can produce high-quality random numbers, which are essential for secure encryption.\n However, quantum computing also poses challenges to traditional cryptographic systems. Shor's algorithm, a quantum algorithm for integer factorization, could potentially break widely used public-key cryptography schemes like RSA, which rely on the difficulty of factoring large numbers. Post-quantum cryptography, which involves the development of cryptographic algorithms that are resistant to attacks by both classical and quantum computers, is an active area of research aimed at addressing this concern.\n Ongoing research in quantum cryptography and post-quantum cryptography is crucial for ensuring the security of communication and data in the face of evolving quantum computing capabilities. Advances in these fields, such as the development of new QKD protocols, the improvement of QRNGs, and the standardization of post-quantum cryptographic algorithms, will play a key role in maintaining the integrity and confidentiality of information in the quantum era.[51]\n Quantum cryptography enables new ways to transmit data securely; for example, quantum key distribution uses entangled quantum states to establish secure cryptographic keys.[52] When a sender and receiver exchange quantum states, they can guarantee that an adversary does not intercept the message, as any unauthorized eavesdropper would disturb the delicate quantum system and introduce a detectable change.[53] With appropriate cryptographic protocols, the sender and receiver can thus establish shared private information resistant to eavesdropping.[12][54]\n Modern fiber-optic cables can transmit quantum information over relatively short distances. Ongoing experimental research aims to develop more reliable hardware (such as quantum repeaters), hoping to scale this technology to long-distance quantum networks with end-to-end entanglement. Theoretically, this could enable novel technological applications, such as distributed quantum computing and enhanced quantum sensing.[55][56]\n Progress in finding quantum algorithms typically focuses on this quantum circuit model, though exceptions like the quantum adiabatic algorithm exist. Quantum algorithms can be roughly categorized by the type of speedup achieved over corresponding classical algorithms.[57]\n Quantum algorithms that offer more than a polynomial speedup over the best-known classical algorithm include Shor's algorithm for factoring and the related quantum algorithms for computing discrete logarithms, solving Pell's equation, and more generally solving the hidden subgroup problem for abelian finite groups.[57] These algorithms depend on the primitive of the quantum Fourier transform. No mathematical proof has been found that shows that an equally fast classical algorithm cannot be discovered, but evidence suggests that this is unlikely.[58] Certain oracle problems like Simon's problem and the Bernstein\u2013Vazirani problem do give provable speedups, though this is in the quantum query model, which is a restricted model where lower bounds are much easier to prove and doesn't necessarily translate to speedups for practical problems.\n Other problems, including the simulation of quantum physical processes from chemistry and solid-state physics, the approximation of certain Jones polynomials, and the quantum algorithm for linear systems of equations have quantum algorithms appearing to give super-polynomial speedups and are BQP-complete. Because these problems are BQP-complete, an equally fast classical algorithm for them would imply that no quantum algorithm gives a super-polynomial speedup, which is believed to be unlikely.[59]\n Some quantum algorithms, like Grover's algorithm and amplitude amplification, give polynomial speedups over corresponding classical algorithms.[57] Though these algorithms give comparably modest quadratic speedup, they are widely applicable and thus give speedups for a wide range of problems.[20]\n Since chemistry and nanotechnology rely on understanding quantum systems, and such systems are impossible to simulate in an efficient manner classically, quantum simulation may be an important application of quantum computing.[60] Quantum simulation could also be used to simulate the behavior of atoms and particles at unusual conditions such as the reactions inside a collider.[61] In June 2023, IBM computer scientists reported that a quantum computer produced better results for a physics problem than a conventional supercomputer.[62][63]\n About 2% of the annual global energy output is used for nitrogen fixation to produce ammonia for the Haber process in the agricultural fertilizer industry (even though naturally occurring organisms also produce ammonia). Quantum simulations might be used to understand this process and increase the energy efficiency of production.[64] It is expected that an early use of quantum computing will be modeling that improves the efficiency of the Haber\u2013Bosch process[65] by the mid-2020s[66] although some have predicted it will take longer.[67]\n A notable application of quantum computation is for attacks on cryptographic systems that are currently in use. Integer factorization, which underpins the security of public key cryptographic systems, is believed to be computationally infeasible with an ordinary computer for large integers if they are the product of few prime numbers (e.g., products of two 300-digit primes).[68] By comparison, a quantum computer could solve this problem exponentially faster using Shor's algorithm to find its factors.[69] This ability would allow a quantum computer to break many of the cryptographic systems in use today, in the sense that there would be a polynomial time (in the number of digits of the integer) algorithm for solving the problem. In particular, most of the popular public key ciphers are based on the difficulty of factoring integers or the discrete logarithm problem, both of which can be solved by Shor's algorithm. In particular, the RSA, Diffie\u2013Hellman, and elliptic curve Diffie\u2013Hellman algorithms could be broken. These are used to protect secure Web pages, encrypted email, and many other types of data. Breaking these would have significant ramifications for electronic privacy and security.\n Identifying cryptographic systems that may be secure against quantum algorithms is an actively researched topic under the field of post-quantum cryptography.[70][71] Some public-key algorithms are based on problems other than the integer factorization and discrete logarithm problems to which Shor's algorithm applies, like the McEliece cryptosystem based on a problem in coding theory.[70][72] Lattice-based cryptosystems are also not known to be broken by quantum computers, and finding a polynomial time algorithm for solving the dihedral hidden subgroup problem, which would break many lattice based cryptosystems, is a well-studied open problem.[73] It has been proven that applying Grover's algorithm to break a symmetric (secret key) algorithm by brute force requires time equal to roughly 2n/2 invocations of the underlying cryptographic algorithm, compared with roughly 2n in the classical case,[74] meaning that symmetric key lengths are effectively halved: AES-256 would have the same security against an attack using Grover's algorithm that AES-128 has against classical brute-force search (see Key size).\n The most well-known example of a problem that allows for a polynomial quantum speedup is unstructured search, which involves finding a marked item out of a list of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n items in a database. This can be solved by Grover's algorithm using \n\n\n\nO\n(\n\n\nn\n\n\n)\n\n\n{\\displaystyle O({\\sqrt {n}})}\n\n queries to the database, quadratically fewer than the \n\n\n\n\u03a9\n(\nn\n)\n\n\n{\\displaystyle \\Omega (n)}\n\n queries required for classical algorithms. In this case, the advantage is not only provable but also optimal: it has been shown that Grover's algorithm gives the maximal possible probability of finding the desired element for any number of oracle lookups. Many examples of provable quantum speedups for query problems are based on Grover's algorithm, including Brassard, H\u00f8yer, and Tapp's algorithm for finding collisions in two-to-one functions,[75] and Farhi, Goldstone, and Gutmann's algorithm for evaluating NAND trees.[76]\n Problems that can be efficiently addressed with Grover's algorithm have the following properties:[77][78]\n For problems with all these properties, the running time of Grover's algorithm on a quantum computer scales as the square root of the number of inputs (or elements in the database), as opposed to the linear scaling of classical algorithms. A general class of problems to which Grover's algorithm can be applied[79] is a Boolean satisfiability problem, where the database through which the algorithm iterates is that of all possible answers. An example and possible application of this is a password cracker that attempts to guess a password. Breaking symmetric ciphers with this algorithm is of interest to government agencies.[80]\n Quantum annealing relies on the adiabatic theorem to undertake calculations. A system is placed in the ground state for a simple Hamiltonian, which slowly evolves to a more complicated Hamiltonian whose ground state represents the solution to the problem in question. The adiabatic theorem states that if the evolution is slow enough the system will stay in its ground state at all times through the process. Adiabatic optimization may be helpful for solving computational biology problems.[81]\n Since quantum computers can produce outputs that classical computers cannot produce efficiently, and since quantum computation is fundamentally linear algebraic, some express hope in developing quantum algorithms that can speed up machine learning tasks.[46][82]\n For example, the HHL Algorithm, named after its discoverers Harrow, Hassidim, and Lloyd, is believed to provide speedup over classical counterparts.[46][83] Some research groups have recently explored the use of quantum annealing hardware for training Boltzmann machines and deep neural networks.[84][85][86]\n \nDeep generative chemistry models emerge as powerful tools to expedite drug discovery. However, the immense size and complexity of the structural space of all possible drug-like molecules pose significant obstacles, which could be overcome in the future by quantum computers. Quantum computers are naturally good for solving complex quantum many-body problems[21] and thus may be instrumental in applications involving quantum chemistry. Therefore, one can expect that quantum-enhanced generative models[87] including quantum GANs[88] may eventually be developed into ultimate generative chemistry algorithms.\n As of 2023,[update] classical computers outperform quantum computers for all real-world applications. While current quantum computers may speed up solutions to particular mathematical problems, they give no computational advantage for practical tasks. Scientists and engineers are exploring multiple technologies for quantum computing hardware and hope to develop scalable quantum architectures, but serious obstacles remain.[89][90]\n There are a number of technical challenges in building a large-scale quantum computer.[91] Physicist David DiVincenzo has listed these requirements for a practical quantum computer:[92]\n Sourcing parts for quantum computers is also very difficult. Superconducting quantum computers, like those constructed by Google and IBM, need helium-3, a nuclear research byproduct, and special superconducting cables made only by the Japanese company Coax Co.[93]\n The control of multi-qubit systems requires the generation and coordination of a large number of electrical signals with tight and deterministic timing resolution. This has led to the development of quantum controllers that enable interfacing with the qubits. Scaling these systems to support a growing number of qubits is an additional challenge.[94]\n One of the greatest challenges involved with constructing quantum computers is controlling or removing quantum decoherence. This usually means isolating the system from its environment as interactions with the external world cause the system to decohere. However, other sources of decoherence also exist. Examples include the quantum gates, and the lattice vibrations and background thermonuclear spin of the physical system used to implement the qubits. Decoherence is irreversible, as it is effectively non-unitary, and is usually something that should be highly controlled, if not avoided. Decoherence times for candidate systems in particular, the transverse relaxation time T2 (for NMR and MRI technology, also called the dephasing time), typically range between nanoseconds and seconds at low temperature.[95] Currently, some quantum computers require their qubits to be cooled to 20 millikelvin (usually using a dilution refrigerator[96]) in order to prevent significant decoherence.[97] A 2020 study argues that ionizing radiation such as cosmic rays can nevertheless cause certain systems to decohere within milliseconds.[98]\n As a result, time-consuming tasks may render some quantum algorithms inoperable, as attempting to maintain the state of qubits for a long enough duration will eventually corrupt the superpositions.[99]\n These issues are more difficult for optical approaches as the timescales are orders of magnitude shorter and an often-cited approach to overcoming them is optical pulse shaping. Error rates are typically proportional to the ratio of operating time to decoherence time; hence any operation must be completed much more quickly than the decoherence time.\n As described by the threshold theorem, if the error rate is small enough, it is thought to be possible to use quantum error correction to suppress errors and decoherence. This allows the total calculation time to be longer than the decoherence time if the error correction scheme can correct errors faster than decoherence introduces them. An often-cited figure for the required error rate in each gate for fault-tolerant computation is 10\u22123, assuming the noise is depolarizing.\n Meeting this scalability condition is possible for a wide range of systems. However, the use of error correction brings with it the cost of a greatly increased number of required qubits. The number required to factor integers using Shor's algorithm is still polynomial, and thought to be between L and L2, where L is the number of binary digits in the number to be factored; error correction algorithms would inflate this figure by an additional factor of L. For a 1000-bit number, this implies a need for about 104 bits without error correction.[100] With error correction, the figure would rise to about 107 bits. Computation time is about L2 or about 107 steps and at 1\u00a0MHz, about 10 seconds. However, the encoding and error-correction overheads increase the size of a real fault-tolerant quantum computer by several orders of magnitude. Careful estimates[101][102] show that at least 3\u00a0million physical qubits would factor 2,048-bit integer in 5 months on a fully error-corrected trapped-ion quantum computer. In terms of the number of physical qubits, to date, this remains the lowest estimate[103] for practically useful integer factorization problem sizing 1,024-bit or larger.\n Another approach to the stability-decoherence problem is to create a topological quantum computer with anyons, quasi-particles used as threads, and relying on braid theory to form stable logic gates.[104][105]\n Physicist John Preskill coined the term quantum supremacy to describe the engineering feat of demonstrating that a programmable quantum device can solve a problem beyond the capabilities of state-of-the-art classical computers.[106][107][108] The problem need not be useful, so some view the quantum supremacy test only as a potential future benchmark.[109]\n In October 2019, Google AI Quantum, with the help of NASA, became the first to claim to have achieved quantum supremacy by performing calculations on the Sycamore quantum computer more than 3,000,000 times faster than they could be done on Summit, generally considered the world's fastest computer.[27][110][111] This claim has been subsequently challenged: IBM has stated that Summit can perform samples much faster than claimed,[112][113] and researchers have since developed better algorithms for the sampling problem used to claim quantum supremacy, giving substantial reductions to the gap between Sycamore and classical supercomputers[114][115][116] and even beating it.[117][118][119]\n In December 2020, a group at USTC implemented a type of Boson sampling on 76 photons with a photonic quantum computer, Jiuzhang, to demonstrate quantum supremacy.[120][121][122] The authors claim that a classical contemporary supercomputer would require a computational time of 600 million years to generate the number of samples their quantum processor can generate in 20 seconds.[123]\n Claims of quantum supremacy have generated hype around quantum computing,[124] but they are based on contrived benchmark tasks that do not directly imply useful real-world applications.[89][125]\n In January 2024, a study published in Physical Review Letters provided direct verification of quantum supremacy experiments by computing exact amplitudes for experimentally generated bitstrings using a new-generation Sunway supercomputer, demonstrating a significant leap in simulation capability built on a multiple-amplitude tensor network contraction algorithm. This development underscores the evolving landscape of quantum computing, highlighting both the progress and the complexities involved in validating quantum supremacy claims.[126]\n Despite high hopes for quantum computing, significant progress in hardware, and optimism about future applications, a 2023 Nature spotlight article summarized current quantum computers as being \"For now, [good for] absolutely nothing\".[89] The article elaborated that quantum computers are yet to be more useful or efficient than conventional computers in any case, though it also argued that in the long term such computers are likely to be useful. A 2023 Communications of the ACM article[90] found that current quantum computing algorithms are \"insufficient for practical quantum advantage without significant improvements across the software/hardware stack\". It argues that the most promising candidates for achieving speedup with quantum computers are \"small-data problems\", for example in chemistry and materials science. However, the article also concludes that a large range of the potential applications it considered, such as machine learning, \"will not achieve quantum advantage with current quantum algorithms in the foreseeable future\", and it identified I/O constraints that make speedup unlikely for \"big data problems, unstructured linear systems, and database search based on Grover's algorithm\".\n This state of affairs can be traced to several current and long-term considerations.\n In particular, building computers with large numbers of qubits may be futile if those qubits are not connected well enough and cannot maintain sufficiently high degree of entanglement for a long time. When trying to outperform conventional computers, quantum computing researchers often look for new tasks that can be solved on quantum computers, but this leaves the possibility that efficient non-quantum techniques will be developed in response, as seen for Quantum supremacy demonstrations. Therefore, it is desirable to prove lower bounds on the complexity of best possible non-quantum algorithms (which may be unknown) and show that some quantum algorithms asymptomatically improve upon those bounds.\n Some researchers have expressed skepticism that scalable quantum computers could ever be built, typically because of the issue of maintaining coherence at large scales, but also for other reasons.\n Bill Unruh doubted the practicality of quantum computers in a paper published in 1994.[129] Paul Davies argued that a 400-qubit computer would even come into conflict with the cosmological information bound implied by the holographic principle.[130] Skeptics like Gil Kalai doubt that quantum supremacy will ever be achieved.[131][132][133] Physicist Mikhail Dyakonov has expressed skepticism of quantum computing as follows:\n A practical quantum computer must use a physical system as a programmable quantum register.[137] Researchers are exploring several technologies as candidates for reliable qubit implementations.[138] Superconductors and trapped ions are some of the most developed proposals, but experimentalists are considering other hardware possibilities as well.[139]\n The first quantum logic gates were implemented with trapped ions and prototype general purpose machines with up to 20 qubits have been realized. However, the technology behind these devices combines complex vacuum equipment, lasers, microwave and radio frequency equipment making full scale processors difficult to integrate with standard computing equipment. Moreover, the trapped ion system itself has engineering challenges to overcome.[140]\n The largest commercial systems are based on superconductor devices and have scaled to 2000 qubits. However, the error rates for larger machines have been on the order of 5%. Technologically these devices are all cryogenic and scaling to large numbers of qubits requires wafer-scale integration, a serious engineering challenge by itself.[141]\n Research efforts to create stabler qubits for quantum computing include topological quantum computer approaches. For example, Microsoft is working on a computer based on the quantum properties of two-dimensional quasiparticles called anyons.[142][143][144]\n With focus on business management's point of view, the potential applications of quantum computing into four major categories are cybersecurity, data analytics and artificial intelligence, optimization and simulation, and data management and searching.[145]\n Investment in quantum computing research has increased in the public and private sectors.[146][147]\nAs one consulting firm summarized,[148]\n ...\u00a0investment dollars are pouring in, and quantum-computing start-ups are proliferating.\u00a0... While quantum computing promises to help businesses solve problems that are beyond the reach and speed of conventional high-performance computers, use cases are largely experimental and hypothetical at this early stage. Any computational problem solvable by a classical computer is also solvable by a quantum computer.[149] Intuitively, this is because it is believed that all physical phenomena, including the operation of classical computers, can be described using quantum mechanics, which underlies the operation of quantum computers.\n Conversely, any problem solvable by a quantum computer is also solvable by a classical computer. It is possible to simulate both quantum and classical computers manually with just some paper and a pen, if given enough time. More formally, any quantum computer can be simulated by a Turing machine. In other words, quantum computers provide no additional power over classical computers in terms of computability. This means that quantum computers cannot solve undecidable problems like the halting problem, and the existence of quantum computers does not disprove the Church\u2013Turing thesis.[150]\n While quantum computers cannot solve any problems that classical computers cannot already solve, it is suspected that they can solve certain problems faster than classical computers. For instance, it is known that quantum computers can efficiently factor integers, while this is not believed to be the case for classical computers.\n The class of problems that can be efficiently solved by a quantum computer with bounded error is called BQP, for \"bounded error, quantum, polynomial time\". More formally, BQP is the class of problems that can be solved by a polynomial-time quantum Turing machine with an error probability of at most 1/3. As a class of probabilistic problems, BQP is the quantum counterpart to BPP (\"bounded error, probabilistic, polynomial time\"), the class of problems that can be solved by polynomial-time probabilistic Turing machines with bounded error.[151] It is known that \n\n\n\n\n\nB\nP\nP\n\u2286\nB\nQ\nP\n\n\n\n\n{\\displaystyle {\\mathsf {BPP\\subseteq BQP}}}\n\n and is widely suspected that \n\n\n\n\n\nB\nQ\nP\n\u228a\nB\nP\nP\n\n\n\n\n{\\displaystyle {\\mathsf {BQP\\subsetneq BPP}}}\n\n, which intuitively would mean that quantum computers are more powerful than classical computers in terms of time complexity.[152]\n The exact relationship of BQP to P, NP, and PSPACE is not known. However, it is known that \n\n\n\n\n\nP\n\u2286\nB\nQ\nP\n\u2286\nP\nS\nP\nA\nC\nE\n\n\n\n\n{\\displaystyle {\\mathsf {P\\subseteq BQP\\subseteq PSPACE}}}\n\n; that is, all problems that can be efficiently solved by a deterministic classical computer can also be efficiently solved by a quantum computer, and all problems that can be efficiently solved by a quantum computer can also be solved by a deterministic classical computer with polynomial space resources. It is further suspected that BQP is a strict superset of P, meaning there are problems that are efficiently solvable by quantum computers that are not efficiently solvable by deterministic classical computers. For instance, integer factorization and the discrete logarithm problem are known to be in BQP and are suspected to be outside of P. On the relationship of BQP to NP, little is known beyond the fact that some NP problems that are believed not to be in P are also in BQP (integer factorization and the discrete logarithm problem are both in NP, for example). It is suspected that \n\n\n\n\n\nN\nP\n\u2288\nB\nQ\nP\n\n\n\n\n{\\displaystyle {\\mathsf {NP\\nsubseteq BQP}}}\n\n; that is, it is believed that there are efficiently checkable problems that are not efficiently solvable by a quantum computer. As a direct consequence of this belief, it is also suspected that BQP is disjoint from the class of NP-complete problems (if an NP-complete problem were in BQP, then it would follow from NP-hardness that all problems in NP are in BQP).[153]\n",
        "doc_number": 49
    },
    {
        "url": "https://en.wikipedia.org/wiki/List_of_emerging_technologies",
        "content": "\n This is a list of emerging technologies, which are in-development technical innovations that have significant potential in their applications. The criteria for this list is that the technology must:\n Listing here is not a prediction that the technology will become widely adopted, only a recognition of significant potential to become widely adopted or highly useful if ongoing work continues, is successful, and the work is not overtaken by other technologies.\n \n (T-RAM, CBRAM, SONOS, RRAM, racetrack memory, NRAM, phase-change memory, FJG RAM, millipede memory, Skyrmion, programmable metallization cell, ferroelectric RAM, magnetoresistive RAM, nvSRAM)\n (SMR, HAMR, BPM, MAMR, TDMR, CPP/GMR, PMR, hard disk drive)\n General:\n Ethics:\n \n Apple\u2019s first set of AI features on iOS 18 will run natively on iPhone: Report indianexpress.com April 16, 2024 Archived from the original source\n",
        "doc_number": 50
    },
    {
        "url": "https://en.wikipedia.org/wiki/Technological_singularity",
        "content": "\n The technological singularity\u2014or simply the singularity[1]\u2014is a hypothetical future point in time at which technological growth becomes uncontrollable and irreversible, resulting in unforeseeable consequences for human civilization.[2][3] According to the most popular version of the singularity hypothesis, I. J. Good's intelligence explosion model of 1965, an upgradable intelligent agent could eventually enter a positive feedback loop of self-improvement cycles, each successive; and more intelligent generation appearing more and more rapidly, causing a rapid increase (\"explosion\") in intelligence which would ultimately result in a powerful superintelligence, qualitatively far surpassing all human intelligence.[4]\n The Hungarian-American mathematician John von Neumann (1903-1957) became the first known person to use the concept of a \"singularity\" in the technological context.[5][6]\n Alan Turing, often regarded as the father of modern computer science, laid a crucial foundation for the contemporary discourse on the technological singularity. His pivotal 1950 paper, \"Computing Machinery and Intelligence,\" introduces the idea of a machine's ability to exhibit intelligent behavior equivalent to or indistinguishable from that of a human.[7]\n Stanislaw Ulam reported in 1958 an earlier discussion with von Neumann \"centered on the accelerating progress of technology and changes in human life, which gives the appearance of approaching some essential singularity in the history of the race beyond which human affairs, as we know them, could not continue\".[8] Subsequent authors have echoed this viewpoint.[3][9]\n The concept and the term \"singularity\" were popularized by Vernor Vinge\u00a0\u2013 first in 1983 (in an article that claimed that once humans create intelligences greater than their own, there will be a technological and social transition similar in some sense to \"the knotted space-time at the center of a black hole\",[10]) and later in his 1993 essay The Coming Technological Singularity,[4][9] (in which he wrote that it would signal the end of the human era, as the new superintelligence would continue to upgrade itself and would advance technologically at an incomprehensible rate). He wrote that he would be surprised if it occurred before 2005 or after 2030.[4] Another significant contributor to wider circulation of the notion was Ray Kurzweil's 2005 book The Singularity Is Near, predicting singularity by 2045.[9]\n Some scientists, including Stephen Hawking, have expressed concern that artificial superintelligence (ASI) could result in human extinction.[11][12] The consequences of a technological singularity and its potential benefit or harm to the human race have been intensely debated.\n Prominent technologists and academics dispute the plausibility of a technological singularity and the associated artificial intelligence explosion, including Paul Allen,[13] Jeff Hawkins,[14] John Holland, Jaron Lanier, Steven Pinker,[14] Theodore Modis,[15] and Gordon Moore.[14] One claim made was that the artificial intelligence growth is likely to run into decreasing returns instead of accelerating ones, as was observed in previously developed human technologies.\n Although technological progress has been accelerating in most areas, it has been limited by the basic intelligence of the human brain, which has not, according to Paul R. Ehrlich, changed significantly for millennia.[16] However, with the increasing power of computers and other technologies, it might eventually be possible to build a machine that is significantly more intelligent than humans.[17]\n If a superhuman intelligence were to be invented\u2014either through the amplification of human intelligence or through artificial intelligence\u2014it would, in theory, vastly improve over human problem-solving and inventive skills. Such an AI is referred to as Seed AI[18][19] because if an AI were created with engineering capabilities that matched or surpassed those of its human creators, it would have the potential to autonomously improve its own software and hardware to design an even more capable machine, which could repeat the process in turn. This recursive self-improvement could accelerate, potentially allowing enormous qualitative change before any upper limits imposed by the laws of physics or theoretical computation set in. It is speculated that over many iterations, such an AI would far surpass human cognitive abilities.\n I. J. Good speculated that superhuman intelligence might bring about an intelligence explosion:[20][21]\n Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an 'intelligence explosion', and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control. One version of intelligence explosion is where computing power approaches infinity in a finite amount of time. In this version, once AIs are performing the research to improve themselves, speed doubles e.g. after 2 years, then 1 year, then 6 months, then 3 months, then 1.5 months, etc., where the infinite sum of the doubling periods is 4 years. Unless prevented by physical limits of computation and time quantization, this process would achieve infinite computing power in 4 years, properly earning the name \"singularity\" for the final state. This form of intelligence explosion is described in Yudkowsky (1996).[22]\n A superintelligence, hyperintelligence, or superhuman intelligence is a hypothetical agent that possesses intelligence far surpassing that of the brightest and most gifted human minds.[23] \"Superintelligence\" may also refer to the form or degree of intelligence possessed by such an agent. John von Neumann, Vernor Vinge and Ray Kurzweil define the concept in terms of the technological creation of super intelligence, arguing that it is difficult or impossible for present-day humans to predict what human beings' lives would be like in a post-singularity world.[4][24]\n The related concept \"speed superintelligence\" describes an AI that can function like a human mind, only much faster.[25] For example, with a million-fold increase in the speed of information processing relative to that of humans, a subjective year would pass in 30 physical seconds.[26] Such a difference in information processing speed could drive the singularity.[27]\n Technology forecasters and researchers disagree regarding when, or whether, human intelligence will likely be surpassed. Some argue that advances in artificial intelligence (AI) will probably result in general reasoning systems that bypass human cognitive limitations. Others believe that humans will evolve or directly modify their biology so as to achieve radically greater intelligence.[28][29] A number of futures studies focus on scenarios that combine these possibilities, suggesting that humans are likely to interface with computers, or upload their minds to computers, in a way that enables substantial intelligence amplification. The book The Age of Em by Robin Hanson describes a hypothetical future scenario in which human brains are scanned and digitized, creating \"uploads\" or digital versions of human consciousness. In this future, the development of these uploads may precede or coincide with the emergence of superintelligent artificial intelligence.[30]\n Some writers use \"the singularity\" in a broader way to refer to any radical changes in society brought about by new technology (such as molecular nanotechnology),[31][32][33] although Vinge and other writers specifically state that without superintelligence, such changes would not qualify as a true singularity.[4]\n There have been numerous dates predicted for the attainment of singularity.\n In 1965, Good wrote that it was more probable than not that an ultra-intelligent machine would be built within the twentieth century.[20]\n That computing capabilities for human-level AI would be available in supercomputers before 2010 was predicted in 1988 by Moravec, assuming that the current rate of improvement continued.[34]\n The attainment of greater-than-human intelligence between 2005 and 2030 was predicted by Vinge in 1993.[4]\n A singularity in 2021 was predicted by Yudkowsky in 1996.[22]\n Human-level AI around 2029 and the singularity in 2045 was predicted by Kurzweil in 2005.[35][36] He reaffirmed these predictions in 2024 in The Singularity is Nearer.[37]\n Human-level AI by 2040, and intelligence far beyond human by 2050 was predicted in 1998 by Moravec, revising his earlier prediction.[38]\n A confidence of 50% that human-level AI would be developed by 2040\u20132050 was the outcome of four polls of AI researchers, conducted in 2012 and 2013 by Bostrom and M\u00fcller.[39][40]\n Prominent technologists and academics dispute the plausibility of a technological singularity, including Paul Allen,[13] Jeff Hawkins,[14] John Holland, Jaron Lanier, Steven Pinker,[14] Theodore Modis,[15] and Gordon Moore,[14] whose law is often cited in support of the concept.[41]\n Most proposed methods for creating superhuman or transhuman minds fall into one of two categories: intelligence amplification of human brains and artificial intelligence. The many speculated ways to augment human intelligence include bioengineering, genetic engineering, nootropic drugs, AI assistants, direct brain\u2013computer interfaces and mind uploading. These multiple possible paths to an intelligence explosion, all of which will presumably be pursued, makes a singularity more likely.[26]\n Robin Hanson expressed skepticism of human intelligence augmentation, writing that once the \"low-hanging fruit\" of easy methods for increasing human intelligence have been exhausted, further improvements will become increasingly difficult.[42] Despite all of the speculated ways for amplifying human intelligence, non-human artificial intelligence (specifically seed AI) is the most popular option among the hypotheses that would advance the singularity.[citation needed]\n The possibility of an intelligence explosion depends on three factors.[43] The first accelerating factor is the new intelligence enhancements made possible by each previous improvement. Contrariwise, as the intelligences become more advanced, further advances will become more and more complicated, possibly outweighing the advantage of increased intelligence. Each improvement should generate at least one more improvement, on average, for movement towards singularity to continue. Finally, the laws of physics may eventually prevent further improvement.\n There are two logically independent, but mutually reinforcing, causes of intelligence improvements: increases in the speed of computation, and improvements to the algorithms used.[9] The former is predicted by Moore's Law and the forecasted improvements in hardware,[44] and is comparatively similar to previous technological advances. But Schulman and Sandberg[45] argue that software will present more complex challenges than simply operating on hardware capable of running at human intelligence levels or beyond.\n A 2017 email survey of authors with publications at the 2015 NeurIPS and ICML machine learning conferences asked about the chance that \"the intelligence explosion argument is broadly correct\". Of the respondents, 12% said it was \"quite likely\", 17% said it was \"likely\", 21% said it was \"about even\", 24% said it was \"unlikely\" and 26% said it was \"quite unlikely\".[46]\n Both for human and artificial intelligence, hardware improvements increase the rate of future hardware improvements. An analogy to Moore's Law suggests that if the first doubling of speed took 18 months, the second would take 18 subjective months; or 9 external months, whereafter, four months, two months, and so on towards a speed singularity.[47][22] Some upper limit on speed may eventually be reached. Jeff Hawkins has stated that a self-improving computer system would inevitably run into upper limits on computing power: \"in the end there are limits to how big and fast computers can run. We would end up in the same place; we'd just get there a bit faster. There would be no singularity.\"[14]\n It is difficult to directly compare silicon-based hardware with neurons. But Berglas (2008) notes that computer speech recognition is approaching human capabilities, and that this capability seems to require 0.01% of the volume of the brain. This analogy suggests that modern computer hardware is within a few orders of magnitude of being as powerful as the human brain, as well as taking up a lot less space.\n The exponential growth in computing technology suggested by Moore's law is commonly cited as a reason to expect a singularity in the relatively near future, and a number of authors have proposed generalizations of Moore's law. Computer scientist and futurist Hans Moravec proposed in a 1998 book[48] that the exponential growth curve could be extended back through earlier computing technologies prior to the integrated circuit.\n Ray Kurzweil postulates a law of accelerating returns in which the speed of technological change (and more generally, all evolutionary processes)[49] increases exponentially, generalizing Moore's law in the same manner as Moravec's proposal, and also including material technology (especially as applied to nanotechnology), medical technology and others.[50] Between 1986 and 2007, machines' application-specific capacity to compute information per capita roughly doubled every 14 months; the per capita capacity of the world's general-purpose computers has doubled every 18 months; the global telecommunication capacity per capita doubled every 34 months; and the world's storage capacity per capita doubled every 40 months.[51] On the other hand, it has been argued that the global acceleration pattern having the 21st century singularity as its parameter should be characterized as hyperbolic rather than exponential.[52]\n Kurzweil reserves the term \"singularity\" for a rapid increase in artificial intelligence (as opposed to other technologies), writing for example that \"The Singularity will allow us to transcend these limitations of our biological bodies and brains ... There will be no distinction, post-Singularity, between human and machine\".[53] He also defines his predicted date of the singularity (2045) in terms of when he expects computer-based intelligences to significantly exceed the sum total of human brainpower, writing that advances in computing before that date \"will not represent the Singularity\" because they do \"not yet correspond to a profound expansion of our intelligence.\"[54]\n \nSome singularity proponents argue its inevitability through extrapolation of past trends, especially those pertaining to shortening gaps between improvements to technology. In one of the first uses of the term \"singularity\" in the context of technological progress, Stanislaw Ulam tells of a conversation with John von Neumann about accelerating change:  One conversation centered on the ever accelerating progress of technology and changes in the mode of human life, which gives the appearance of approaching some essential singularity in the history of the race beyond which human affairs, as we know them, could not continue.[8] Kurzweil claims that technological progress follows a pattern of exponential growth, following what he calls the \"law of accelerating returns\". Whenever technology approaches a barrier, Kurzweil writes, new technologies will surmount it. He predicts paradigm shifts will become increasingly common, leading to \"technological change so rapid and profound it represents a rupture in the fabric of human history\".[55] Kurzweil believes that the singularity will occur by approximately 2045.[50] His predictions differ from Vinge's in that he predicts a gradual ascent to the singularity, rather than Vinge's rapidly self-improving superhuman intelligence.\n Oft-cited dangers include those commonly associated with molecular nanotechnology and genetic engineering. These threats are major issues for both singularity advocates and critics, and were the subject of Bill Joy's April 2000 Wired magazine article \"Why The Future Doesn't Need Us\".[9][56]\n Some intelligence technologies, like \"seed AI\",[18][19] may also have the potential to not just make themselves faster, but also more efficient, by modifying their source code. These improvements would make further improvements possible, which would make further improvements possible, and so on.\n The mechanism for a recursively self-improving set of algorithms differs from an increase in raw computation speed in two ways. First, it does not require external influence: machines designing faster hardware would still require humans to create the improved hardware, or to program factories appropriately.[citation needed] An AI rewriting its own source code could do so while contained in an AI box.\n Second, as with Vernor Vinge's conception of the singularity, it is much harder to predict the outcome. While speed increases seem to be only a quantitative difference from human intelligence, actual algorithm improvements would be qualitatively different. Eliezer Yudkowsky compares it to the changes that human intelligence brought: humans changed the world thousands of times more rapidly than evolution had done, and in totally different ways. Similarly, the evolution of life was a massive departure and acceleration from the previous geological rates of change, and improved intelligence could cause change to be as different again.[57]\n There are substantial dangers associated with an intelligence explosion singularity originating from a recursively self-improving set of algorithms. First, the goal structure of the AI might self-modify, potentially causing the AI to optimise for something other than what was originally intended.[58][59]\n Secondly, AIs could compete for the same scarce resources humankind uses to survive.[60][61] While not actively malicious, AIs would promote the goals of their programming, not necessarily broader human goals, and thus might crowd out humans.[62][63][64]\n Carl Shulman and Anders Sandberg suggest that algorithm improvements may be the limiting factor for a singularity; while hardware efficiency tends to improve at a steady pace, software innovations are more unpredictable and may be bottlenecked by serial, cumulative research. They suggest that in the case of a software-limited singularity, intelligence explosion would actually become more likely than with a hardware-limited singularity, because in the software-limited case, once human-level AI is developed, it could run serially on very fast hardware, and the abundance of cheap hardware would make AI research less constrained.[65] An abundance of accumulated hardware that can be unleashed once the software figures out how to use it has been called \"computing overhang\".[66]\n Some critics, like philosopher Hubert Dreyfus[67] and philosopher John Searle,[68] assert that computers or machines cannot achieve human intelligence. Others, like physicist Stephen Hawking,[69] object that whether machines can achieve a true intelligence or merely something similar to intelligence is irrelevant if the net result is the same.\n Psychologist Steven Pinker stated in 2008: \"There is not the slightest reason to believe in a coming singularity. The fact that you can visualize a future in your imagination is not evidence that it is likely or even possible. Look at domed cities, jet-pack commuting, underwater cities, mile-high buildings, and nuclear-powered automobiles\u2014all staples of futuristic fantasies when I was a child that have never arrived. Sheer processing power is not a pixie dust that magically solves all your problems.\"[14]\n Martin Ford[70] postulates a \"technology paradox\" in that before the singularity could occur most routine jobs in the economy would be automated, since this would require a level of technology inferior to that of the singularity. This would cause massive unemployment and plummeting consumer demand, which in turn would destroy the incentive to invest in the technologies that would be required to bring about the Singularity. Job displacement is increasingly no longer limited to those types of work traditionally considered to be \"routine\".[71]\n Theodore Modis[72] and Jonathan Huebner[73] argue that the rate of technological innovation has not only ceased to rise, but is actually now declining. Evidence for this decline is that the rise in computer clock rates is slowing, even while Moore's prediction of exponentially increasing circuit density continues to hold. This is due to excessive heat build-up from the chip, which cannot be dissipated quickly enough to prevent the chip from melting when operating at higher speeds. Advances in speed may be possible in the future by virtue of more power-efficient CPU designs and multi-cell processors.[74]\n Theodore Modis holds the singularity cannot happen.[75][15][76] He claims the \"technological singularity\" and especially Kurzweil lack scientific rigor; Kurzweil is alleged to mistake the logistic function (S-function) for an exponential function, and to see a \"knee\" in an exponential function where there can in fact be no such thing.[77] In a 2021 article, Modis pointed out that no milestones\u00a0\u2013 breaks in historical perspective comparable in importance to the Internet, DNA, the transistor, or nuclear energy\u00a0\u2013 had been observed in the previous twenty years while five of them would have been expected according to the exponential trend advocated by the proponents of the technological singularity.[78]\n AI researcher J\u00fcrgen Schmidhuber stated that the frequency of subjectively \"notable events\" appears to be approaching a 21st-century singularity, but cautioned readers to take such plots of subjective events with a grain of salt: perhaps differences in memory of recent and distant events could create an illusion of accelerating change where none exists.[79]\n Microsoft co-founder Paul Allen argued the opposite of accelerating returns, the complexity brake;[13] the more progress science makes towards understanding intelligence, the more difficult it becomes to make additional progress. A study of the number of patents shows that human creativity does not show accelerating returns, but in fact, as suggested by Joseph Tainter in his The Collapse of Complex Societies,[80] a law of diminishing returns. The number of patents per thousand peaked in the period from 1850 to 1900, and has been declining since.[73] The growth of complexity eventually becomes self-limiting, and leads to a widespread \"general systems collapse\".\n Hofstadter (2006) raises concern that Ray Kurzweil is not sufficiently scientifically rigorous, that an exponential tendency of technology is not a scientific law like one of physics, and that exponential curves have no \"knees\".[81] Nonetheless, he did not rule out the singularity in principle in the distant future[14] and in the light of ChatGPT and other recent advancements has revised his opinion significantly towards dramatic technological change in the near future.[82]\n Jaron Lanier denies that the singularity is inevitable: \"I do not think the technology is creating itself. It's not an autonomous process.\"[83] Furthermore: \"The reason to believe in human agency over technological determinism is that you can then have an economy where people earn their own way and invent their own lives. If you structure a society on not emphasizing individual human agency, it's the same thing operationally as denying people clout, dignity, and self-determination ... to embrace [the idea of the Singularity] would be a celebration of bad data and bad politics.\"[83]\n Economist Robert J. Gordon points out that measured economic growth slowed around 1970 and slowed even further since the financial crisis of 2007\u20132008, and argues that the economic data show no trace of a coming Singularity as imagined by mathematician I. J. Good.[84]\n Philosopher and cognitive scientist Daniel Dennett said in 2017: \"The whole singularity stuff, that's preposterous. It distracts us from much more pressing problems\", adding \"AI tools that we become hyper-dependent on, that is going to happen. And one of the dangers is that we will give them more authority than they warrant.\"[85]\n In addition to general criticisms of the singularity concept, several critics have raised issues with Kurzweil's iconic chart. One line of criticism is that a log-log chart of this nature is inherently biased toward a straight-line result. Others identify selection bias in the points that Kurzweil chooses to use. For example, biologist PZ Myers points out that many of the early evolutionary \"events\" were picked arbitrarily.[86] Kurzweil has rebutted this by charting evolutionary events from 15 neutral sources, and showing that they fit a straight line on a log-log chart. Kelly (2006) argues that the way the Kurzweil chart is constructed with x-axis having time before present, it always points to the singularity being \"now\", for any date on which one would construct such a chart, and shows this visually on Kurzweil's chart.[87]\n Some critics suggest religious motivations or implications of singularity, especially Kurzweil's version of it. The buildup towards the singularity is compared with Christian end-of-time scenarios. Beam calls it \"a Buck Rogers vision of the hypothetical Christian Rapture\".[88] John Gray says \"the Singularity echoes apocalyptic myths in which history is about to be interrupted by a world-transforming event\".[89]\n David Streitfeld in The New York Times questioned whether \"it might manifest first and foremost\u2014thanks, in part, to the bottom-line obsession of today\u2019s Silicon Valley\u2014as a tool to slash corporate America\u2019s head count.\"[90]\n Dramatic changes in the rate of economic growth have occurred in the past because of technological advancement. Based on population growth, the economy doubled every 250,000 years from the Paleolithic era until the Neolithic Revolution. The new agricultural economy doubled every 900 years, a remarkable increase. In the current era, beginning with the Industrial Revolution, the world's economic output doubles every fifteen years, sixty times faster than during the agricultural era. If the rise of superhuman intelligence causes a similar revolution, argues Robin Hanson, one would expect the economy to double at least quarterly and possibly on a weekly basis.[91]\n The term \"technological singularity\" reflects the idea that such change may happen suddenly, and that it is difficult to predict how the resulting new world would operate.[92][93] It is unclear whether an intelligence explosion resulting in a singularity would be beneficial or harmful, or even an existential threat.[94][95] Because AI is a major factor in singularity risk, a number of organizations pursue a technical theory of aligning AI goal-systems with human values, including the Future of Humanity Institute, the Machine Intelligence Research Institute,[92] the Center for Human-Compatible Artificial Intelligence, and the Future of Life Institute.\n \nPhysicist Stephen Hawking said in 2014 that \"Success in creating AI would be the biggest event in human history. Unfortunately, it might also be the last, unless we learn how to avoid the risks.\"[96] Hawking believed that in the coming decades, AI could offer \"incalculable benefits and risks\" such as \"technology outsmarting financial markets, out-inventing human researchers, out-manipulating human leaders, and developing weapons we cannot even understand.\"[96] Hawking suggested that artificial intelligence should be taken more seriously and that more should be done to prepare for the singularity:[96] So, facing possible futures of incalculable benefits and risks, the experts are surely doing everything possible to ensure the best outcome, right? Wrong. If a superior alien civilisation sent us a message saying, \"We'll arrive in a few decades,\" would we just reply, \"OK, call us when you get here\u00a0\u2013 we'll leave the lights on\"? Probably not\u00a0\u2013 but this is more or less what is happening with AI. Berglas (2008) claims that there is no direct evolutionary motivation for an AI to be friendly to humans. Evolution has no inherent tendency to produce outcomes valued by humans, and there is little reason to expect an arbitrary optimisation process to promote an outcome desired by humankind, rather than inadvertently leading to an AI behaving in a way not intended by its creators.[97][98][99] Anders Sandberg has also elaborated on this scenario, addressing various common counter-arguments.[100] AI researcher Hugo de Garis suggests that artificial intelligences may simply eliminate the human race for access to scarce resources,[60][101] and humans would be powerless to stop them.[102] Alternatively, AIs developed under evolutionary pressure to promote their own survival could outcompete humanity.[64]\n Bostrom (2002) discusses human extinction scenarios, and lists superintelligence as a possible cause:\n When we create the first superintelligent entity, we might make a mistake and give it goals that lead it to annihilate humankind, assuming its enormous intellectual advantage gives it the power to do so. For example, we could mistakenly elevate a subgoal to the status of a supergoal. We tell it to solve a mathematical problem, and it complies by turning all the matter in the solar system into a giant calculating device, in the process killing the person who asked the question. According to Eliezer Yudkowsky, a significant problem in AI safety is that unfriendly artificial intelligence is likely to be much easier to create than friendly AI. While both require large advances in recursive optimisation process design, friendly AI also requires the ability to make goal structures invariant under self-improvement (or the AI could transform itself into something unfriendly) and a goal structure that aligns with human values and does not automatically destroy the human race. An unfriendly AI, on the other hand, can optimize for an arbitrary goal structure, which does not need to be invariant under self-modification.[103] Bill Hibbard (2014) harvtxt error: no target: CITEREFBill_Hibbard2014 (help) proposes an AI design that avoids several dangers including self-delusion,[104] unintended instrumental actions,[58][105] and corruption of the reward generator.[105] He also discusses social impacts of AI[106] and testing AI.[107] His 2001 book Super-Intelligent Machines advocates the need for public education about AI and public control over AI. It also proposed a simple design that was vulnerable to corruption of the reward generator.\n While the technological singularity is usually seen as a sudden event, some scholars argue the current speed of change already fits this description.[citation needed]\n In addition, some argue that we are already in the midst of a major evolutionary transition that merges technology, biology, and society. Digital technology has infiltrated the fabric of human society to a degree of indisputable and often life-sustaining dependence.\n A 2016 article in Trends in Ecology & Evolution argues that \"humans already embrace fusions of biology and technology. We spend most of our waking time communicating through digitally mediated channels... we trust artificial intelligence with our lives through antilock braking in cars and autopilots in planes... With one in three courtships leading to marriages in America beginning online, digital algorithms are also taking a role in human pair bonding and reproduction\".\n The article further argues that from the perspective of the evolution, several previous Major Transitions in Evolution have transformed life through innovations in information storage and replication (RNA, DNA, multicellularity, and culture and language). In the current stage of life's evolution, the carbon-based biosphere has generated a system (humans) capable of creating technology that will result in a comparable evolutionary transition.\n The digital information created by humans has reached a similar magnitude to biological information in the biosphere. Since the 1980s, the quantity of digital information stored has doubled about every 2.5 years, reaching about 5 zettabytes in 2014 (5\u00d71021 bytes).[109]\n In biological terms, there are 7.2\u00a0billion humans on the planet, each having a genome of 6.2\u00a0billion nucleotides. Since one byte can encode four nucleotide pairs, the individual genomes of every human on the planet could be encoded by approximately 1\u00d71019 bytes. The digital realm stored 500 times more information than this in 2014 (see figure). The total amount of DNA contained in all of the cells on Earth is estimated to be about 5.3\u00d71037 base pairs, equivalent to 1.325\u00d71037 bytes of information.\n If growth in digital storage continues at its current rate of 30\u201338% compound annual growth per year,[51] it will rival the total information content contained in all of the DNA in all of the cells on Earth in about 110 years. This would represent a doubling of the amount of information stored in the biosphere across a total time period of just 150 years\".[108]\n In February 2009, under the auspices of the Association for the Advancement of Artificial Intelligence (AAAI), Eric Horvitz chaired a meeting of leading computer scientists, artificial intelligence researchers and roboticists at the Asilomar conference center in Pacific Grove, California. The goal was to discuss the potential impact of the hypothetical possibility that robots could become self-sufficient and able to make their own decisions. They discussed the extent to which computers and robots might be able to acquire autonomy, and to what degree they could use such abilities to pose threats or hazards.[110]\n Some machines are programmed with various forms of semi-autonomy, including the ability to locate their own power sources and choose targets to attack with weapons. Also, some computer viruses can evade elimination and, according to scientists in attendance, could therefore be said to have reached a \"cockroach\" stage of machine intelligence. The conference attendees noted that self-awareness as depicted in science-fiction is probably unlikely, but that other potential hazards and pitfalls exist.[110]\n Frank S. Robinson predicts that once humans achieve a machine with the intelligence of a human, scientific and technological problems will be tackled and solved with brainpower far superior to that of humans. He notes that artificial systems are able to share data more directly than humans, and predicts that this would result in a global network of super-intelligence that would dwarf human capability.[111] Robinson also discusses how vastly different the future would potentially look after such an intelligence explosion.\n In a hard takeoff scenario, an artificial superintelligence rapidly self-improves, \"taking control\" of the world (perhaps in a matter of hours), too quickly for significant human-initiated error correction or for a gradual tuning of the agent's goals. In a soft takeoff scenario, the AI still becomes far more powerful than humanity, but at a human-like pace (perhaps on the order of decades), on a timescale where ongoing human interaction and correction can effectively steer the AI's development.[113][114]\n Ramez Naam argues against a hard takeoff. He has pointed out that we already see recursive self-improvement by superintelligences, such as corporations. Intel, for example, has \"the collective brainpower of tens of thousands of humans and probably millions of CPU cores to... design better CPUs!\" However, this has not led to a hard takeoff; rather, it has led to a soft takeoff in the form of Moore's law.[115] Naam further points out that the computational complexity of higher intelligence may be much greater than linear, such that \"creating a mind of intelligence 2 is probably more than twice as hard as creating a mind of intelligence 1.\"[116]\n J. Storrs Hall believes that \"many of the more commonly seen scenarios for overnight hard takeoff are circular\u00a0\u2013 they seem to assume hyperhuman capabilities at the starting point of the self-improvement process\" in order for an AI to be able to make the dramatic, domain-general improvements required for takeoff. Hall suggests that rather than recursively self-improving its hardware, software, and infrastructure all on its own, a fledgling AI would be better off specializing in one area where it was most effective and then buying the remaining components on the marketplace, because the quality of products on the marketplace continually improves, and the AI would have a hard time keeping up with the cutting-edge technology used by the rest of the world.[117]\n Ben Goertzel agrees with Hall's suggestion that a new human-level AI would do well to use its intelligence to accumulate wealth. The AI's talents might inspire companies and governments to disperse its software throughout society. Goertzel is skeptical of a hard five minute takeoff but speculates that a takeoff from human to superhuman level on the order of five years is reasonable. He refers to this scenario as a \"semihard takeoff\".[118]\n Max More disagrees, arguing that if there were only a few superfast[clarification needed] human-level AIs, that they would not radically change the world, as they would still depend on other people to get things done and would still have human cognitive constraints. Even if all superfast AIs worked on intelligence augmentation, it is unclear why they would do better in a discontinuous way than existing human cognitive scientists at producing super-human intelligence, although the rate of progress would increase. More further argues that a superintelligence would not transform the world overnight: a superintelligence would need to engage with existing, slow human systems to accomplish physical impacts on the world. \"The need for collaboration, for organization, and for putting ideas into physical changes will ensure that all the old rules are not thrown out overnight or even within years.\"[119]\n Eric Drexler, one of the founders of nanotechnology, theorized in 1986 the possibility of cell repair devices, including ones operating within cells and using as yet hypothetical biological machines.[120] According to Richard Feynman, it was his former graduate student and collaborator Albert Hibbs who originally suggested to him (circa 1959) the idea of a medical use for Feynman's theoretical micromachines. Hibbs suggested that certain repair machines might one day be reduced in size to the point that it would, in theory, be possible to (as Feynman put it) \"swallow the doctor\". The idea was incorporated into Feynman's 1959 essay There's Plenty of Room at the Bottom.[121]\n Moravec predicted in 1988 the possibility of \"uploading\" human mind into a human-like robot, achieving quasi-immortality by extreme longevity via transfer of the human mind between successive new robots as the old ones wear out; beyond that, he predicts later exponential acceleration of subjective experience of time leading to a subjective sense of immortality.[34]\n Kurzweil suggested in 2005 that medical advances would allow people to protect their bodies from the effects of aging, making the life expectancy limitless. Kurzweil argues that the technological advances in medicine would allow us to continuously repair and replace defective components in our bodies, prolonging life to an undetermined age.[122] Kurzweil further buttresses his argument by discussing current bio-engineering advances. Kurzweil suggests somatic gene therapy; after synthetic viruses with specific genetic information, the next step would be to apply this technology to gene therapy, replacing human DNA with synthesized genes.[123]\n Beyond merely extending the operational life of the physical body, Jaron Lanier argues for a form of immortality called \"Digital Ascension\" that involves \"people dying in the flesh and being uploaded into a computer and remaining conscious.\"[124]\n A paper by Mahendra Prasad, published in AI Magazine, asserts that the 18th-century mathematician Marquis de Condorcet was the first person to hypothesize and mathematically model an intelligence explosion and its effects on humanity.[125]\n An early description of the idea was made in John W. Campbell's 1932 short story \"The Last Evolution\".[126]\n In his 1958 obituary for John von Neumann, Ulam recalled a conversation with von Neumann about the \"ever accelerating progress of technology and changes in the mode of human life, which gives the appearance of approaching some essential singularity in the history of the race beyond which human affairs, as we know them, could not continue.\"[8]\n In 1965, Good wrote his essay postulating an \"intelligence explosion\" of recursive self-improvement of a machine intelligence.[20][21]\n In 1977, Hans Moravec wrote an article with unclear publishing status where he envisioned a development of self-improving thinking machines, a creation of \"super-consciousness, the synthesis of terrestrial life, and perhaps jovian and martian life as well, constantly improving and extending itself, spreading outwards from the solar system, converting non-life into mind.\"[127][128] The article describes the human mind uploading later covered in Moravec (1988). The machines are expected to reach human level and then improve themselves beyond that (\"Most significantly of all, they [the machines] can be put to work as programmers and engineers, with the task of optimizing the software and hardware which make them what they are. The successive generations of machines produced this way will be increasingly smarter and more cost effective.\") Humans will no longer be needed, and their abilities will be overtaken by the machines: \"In the long run the sheer physical inability of humans to keep up with these rapidly evolving progeny of our minds will ensure that the ratio of people to machines approaches zero, and that a direct descendant of our culture, but not our genes, inherits the universe.\" While the word \"singularity\" is not used, the notion of human-level thinking machines thereafter improving themselves beyond human level is there. In this view, there is no intelligence explosion in the sense of a very rapid intelligence increase once human equivalence is reached. An updated version of the article was published in 1979 in Analog Science Fiction and Fact.[129][128]\n In 1981, Stanis\u0142aw Lem published his science fiction novel Golem XIV. It describes a military AI computer (Golem XIV) who obtains consciousness and starts to increase his own intelligence, moving towards personal technological singularity. Golem XIV was originally created to aid its builders in fighting wars, but as its intelligence advances to a much higher level than that of humans, it stops being interested in the military requirements because it finds them lacking internal logical consistency.\n In 1983, Vernor Vinge addressed Good's intelligence explosion in print in the January 1983 issue of Omni magazine. In this op-ed piece, Vinge seems to have been the first to use the term \"singularity\" (although not \"technological singularity\") in a way that was specifically tied to the creation of intelligent machines:[10][128]\n We will soon create intelligences greater than our own. When this happens, human history will have reached a kind of singularity, an intellectual transition as impenetrable as the knotted space-time at the center of a black hole, and the world will pass far beyond our understanding. This singularity, I believe, already haunts a number of science-fiction writers. It makes realistic extrapolation to an interstellar future impossible. To write a story set more than a century hence, one needs a nuclear war in between ... so that the world remains intelligible. In 1985, in \"The Time Scale of Artificial Intelligence\", artificial intelligence researcher Ray Solomonoff articulated mathematically the related notion of what he called an \"infinity point\": if a research community of human-level self-improving AIs take four years to double their own speed, then two years, then one year and so on, their capabilities increase infinitely in finite time.[9][130]\n In 1986, Vernor Vinge published Marooned in Realtime, a science-fiction novel where a few remaining humans traveling forward in the future have survived an unknown extinction event that might well be a singularity. In a short afterword, the author states that an actual technological singularity would not be the end of the human species: \"of course it seems very unlikely that the Singularity would be a clean vanishing of the human race. (On the other hand, such a vanishing is the timelike analog of the silence we find all across the sky.)\".[131][132]\n In 1988, Vinge used the phrase \"technological singularity\" (including \"technological\") in the short story collection Threats and Other Promises, writing in the introduction to his story \"The Whirligig of Time\" (p.\u00a072): Barring a worldwide catastrophe, I believe that technology will achieve our wildest dreams, and soon. When we raise our own intelligence and that of our creations, we are no longer in a world of human-sized characters. At that point we have fallen into a technological \"black hole\", a technological singularity.[133]\n In 1988, Hans Moravec published Mind Children,[34] in which he predicted human-level intelligence in supercomputers by 2010, self-improving intelligent machines far surpassing human intelligence later, human mind uploading into human-like robots later, intelligent machines leaving humans behind, and space colonization. He did not mention \"singularity\", though, and he did not speak of a rapid explosion of intelligence immediately after the human level is achieved. Nonetheless, the overall singularity tenor is there in predicting both human-level artificial intelligence and further artificial intelligence far surpassing humans later.\n Vinge's 1993 article \"The Coming Technological Singularity: How to Survive in the Post-Human Era\",[4] spread widely on the internet and helped to popularize the idea.[134] This article contains the statement, \"Within thirty years, we will have the technological means to create superhuman intelligence. Shortly after, the human era will be ended.\" Vinge argues that science-fiction authors cannot write realistic post-singularity characters who surpass the human intellect, as the thoughts of such an intellect would be beyond the ability of humans to express.[4]\n Minsky's 1994 article says robots will \"inherit the Earth\", possibly with the use of nanotechnology, and proposes to think of robots as human \"mind children\", drawing the analogy from Moravec. The rhetorical effect of that analogy is that if humans are fine to pass the world to their biological children, they should be equally fine to pass it to robots, their \"mind\" children. As per Minsky, 'we could design our \"mind-children\" to think a million times faster than we do. To such a being, half a minute might seem as long as one of our years, and each hour as long as an entire human lifetime.' The feature of the singularity present in Minsky is the development of superhuman artificial intelligence (\"million times faster\"), but there is no talk of sudden intelligence explosion, self-improving thinking machines or unpredictability beyond any specific event and the word \"singularity\" is not used.[135]\n Tipler's 1994 book The Physics of Immortality predicts a future where super\u2013intelligent machines will build enormously powerful computers, people will be \"emulated\" in computers, life will reach every galaxy and people will achieve immortality when they reach Omega Point.[136] There is no talk of Vingean \"singularity\" or sudden intelligence explosion, but intelligence much greater than human is there, as well as immortality.\n In 1996, Yudkowsky predicted a singularity by 2021.[22] His version of singularity involves intelligence explosion: once AIs are doing the research to improve themselves, speed doubles after 2 years, then 1 one year, then after 6 months, then after 3 months, then after 1.5 months, and after more iterations, the \"singularity\" is reached.[22] This construction implies that the speed reaches infinity in finite time.\n In 2000, Bill Joy, a prominent technologist and a co-founder of Sun Microsystems, voiced concern over the potential dangers of robotics, genetic engineering, and nanotechnology.[56]\n In 2005, Kurzweil published The Singularity Is Near. Kurzweil's publicity campaign included an appearance on The Daily Show with Jon Stewart.[137]\n From 2006 to 2012, an annual Singularity Summit conference was organized by Machine Intelligence Research Institute, founded by Eliezer Yudkowsky.\n In 2007, Yudkowsky suggested that many of the varied definitions that have been assigned to \"singularity\" are mutually incompatible rather than mutually supporting.[32][138] For example, Kurzweil extrapolates current technological trajectories past the arrival of self-improving AI or superhuman intelligence, which Yudkowsky argues represents a tension with both I. J. Good's proposed discontinuous upswing in intelligence and Vinge's thesis on unpredictability.[32]\n In 2009, Kurzweil and X-Prize founder Peter Diamandis announced the establishment of Singularity University, a nonaccredited private institute whose stated mission is \"to educate, inspire and empower leaders to apply exponential technologies to address humanity's grand challenges.\"[139] Funded by Google, Autodesk, ePlanet Ventures, and a group of technology industry leaders, Singularity University is based at NASA's Ames Research Center in Mountain View, California. The not-for-profit organization runs an annual ten-week graduate program during summer that covers ten different technology and allied tracks, and a series of executive programs throughout the year.\n In 2007, the Joint Economic Committee of the United States Congress released a report about the future of nanotechnology. It predicts significant technological and political changes in the mid-term future, including possible technological singularity.[140][141][142]\n Former President of the United States Barack Obama spoke about singularity in his interview to Wired in 2016:[143]\n One thing that we haven't talked about too much, and I just want to go back to, is we really have to think through the economic implications. Because most people aren't spending a lot of time right now worrying about singularity\u2014they are worrying about \"Well, is my job going to be replaced by a machine?\"",
        "doc_number": 51
    },
    {
        "url": "https://en.wikipedia.org/wiki/Transhumanism",
        "content": "\n Chemical\n Neurological\n Transhumanism is a philosophical and intellectual movement that advocates the enhancement of the human condition by developing and making widely available new and future technologies that can greatly enhance longevity, cognition, and well-being.[1][2][3]\n Transhumanist thinkers study the potential benefits and dangers of emerging technologies that could overcome fundamental human limitations, as well as the ethics of using such technologies.[4] Some transhumanists speculate that human beings may eventually be able to transform themselves into beings of such vastly greater abilities as to merit the label of posthuman beings.[2]\n Another topic of transhumanist research is how to protect humanity against existential risks from artificial general intelligence, asteroid impact, gray goo, high-energy particle collision experiments, natural or synthetic pandemic, and nuclear warfare.[5]\n The biologist Julian Huxley popularised the term \"transhumanism\" in a 1957 essay.[6] The contemporary meaning of the term was foreshadowed by one of the first professors of futurology, a man who changed his name to FM-2030. In the 1960s, he taught \"new concepts of the human\" at The New School when he began to identify people who adopt technologies, lifestyles, and worldviews \"transitional\" to posthumanity as \"transhuman\".[7] The assertion laid the intellectual groundwork for the British philosopher Max More to begin articulating the principles of transhumanism as a futurist philosophy in 1990, organizing in California a school of thought that has since grown into the worldwide transhumanist movement.[7][8][9]\n Influenced by seminal works of science fiction, the transhumanist vision of a transformed future humanity has attracted many supporters and detractors from a wide range of perspectives, including philosophy and religion.[7]\n In 2017, Penn State University Press, in cooperation with philosopher Stefan Lorenz Sorgner and sociologist James Hughes, established the Journal of Posthuman Studies[10] as the first academic journal explicitly dedicated to the posthuman, with the goal of clarifying the notions of posthumanism and transhumanism, as well as comparing and contrasting both.\n Despite its professed strong attachment to the values of liberalism and forward-thinking, some critics argue transhumanism is a dangerous resurgence of many discriminatory attitudes and elitist ideals of the discredited eugenics movements of the past.[11][12][13]\n According to Nick Bostrom, transcendentalist impulses have been expressed at least as far back as the quest for immortality in the Epic of Gilgamesh, as well as in historical quests for the Fountain of Youth, the Elixir of Life, and other efforts to stave off aging and death.[2]\n Transhumanists draw upon and claim continuity from intellectual and cultural traditions such as the ancient philosophy of Aristotle or the scientific tradition of Roger Bacon.[14] In his Divine Comedy, Dante coined the word trasumanar meaning \"to transcend human nature, to pass beyond human nature\" in the first canto of Paradiso.[15][16][17][18]\n The interweaving of transhumanist aspirations with the scientific imagination can be seen in the works of some precursors of Enlightenment such as Francis Bacon.[19][20] One of the early precursors to transhumanist ideas is Ren\u00e9 Descartes's Discourse on Method (1637), in which Descartes envisions a new kind of medicine that can grant both physical immortality and stronger minds.[21]\n In his first edition of Political Justice (1793), William Godwin included arguments favoring the possibility of \"earthly immortality\" (what would now be called physical immortality). Godwin explored the themes of life extension and immortality in his gothic novel St.\u00a0Leon, which became popular (and notorious) at the time of its publication in 1799, but is now mostly forgotten. St.\u00a0Leon may have inspired his daughter Mary Shelley's novel Frankenstein.[22]\n Ether Day, marking a significant milestone in human history, celebrated its 175th anniversary on October 16, 2021. It was on this day that dentist William T. G. Morton achieved a groundbreaking feat by administering the first public ether anesthesia in Boston. This breakthrough not only allowed for the alleviation of pain with a reasonable level of risk but also helped protect people from psychological trauma by inducing unconsciousness.[23]\n There is debate about whether the philosophy of Friedrich Nietzsche can be considered an influence on transhumanism, despite its exaltation of the \u00dcbermensch (overhuman), due to its emphasis on self-actualization rather than technological transformation.[2][24][25][26] The transhumanist philosophies of More and Sorgner have been influenced strongly by Nietzschean thinking.[24] By contrast, The Transhumanist Declaration \"advocates the well-being of all sentience (whether in artificial intellects, humans, posthumans, or non-human animals)\".[27]\n The late 19th- to early 20th-century movement known as Russian cosmism, by Russian philosopher N. F. Fyodorov, is noted for anticipating transhumanist ideas.[28] In 1966, FM-2030 (formerly F. M. Esfandiary), a futurist who taught \"new concepts of the human\" at The New School, in New York City, began to identify people who adopt technologies, lifestyles and worldviews transitional to posthumanity as \"transhuman\".[29]\n Fundamental ideas of transhumanism were first advanced in 1923 by the British geneticist J. B. S. Haldane in his essay Daedalus: Science and the Future, which predicted that great benefits would come from the application of advanced sciences to human biology\u2014and that every such advance would first appear to someone as blasphemy or perversion, \"indecent and unnatural\".[30] In particular, he was interested in the development of the science of eugenics, ectogenesis (creating and sustaining life in an artificial environment), and the application of genetics to improve human characteristics such as health and intelligence.\n His article inspired academic and popular interest. J. D. Bernal, a crystallographer at Cambridge, wrote The World, the Flesh and the Devil in 1929, in which he speculated on the prospects of space colonization and radical changes to human bodies and intelligence through bionic implants and cognitive enhancement.[31] These ideas have been common transhumanist themes ever since.[2]\n The biologist Julian Huxley is generally regarded as the founder of transhumanism after using the term for the title of an influential 1957 article.[6] But the term derives from a 1940 paper by the Canadian philosopher W. D. Lighthall.[32] Huxley describes transhumanism in these terms:\n Up till now human life has generally been, as Hobbes described it, \"nasty, brutish and short\"; the great majority of human beings (if they have not already died young) have been afflicted with misery\u2026 we can justifiably hold the belief that these lands of possibility exist, and that the present limitations and miserable frustrations of our existence could be in large measure surmounted\u2026 The human species can, if it wishes, transcend itself\u2014not just sporadically, an individual here in one way, an individual there in another way, but in its entirety, as humanity.[6] Huxley's definition differs, albeit not substantially, from the one commonly in use since the 1980s. The ideas raised by these thinkers were explored in the science fiction of the 1960s, notably in Arthur C. Clarke's 2001: A Space Odyssey, in which an alien artifact grants transcendent power to its wielder.[33]\n \nJapanese Metabolist architects produced a manifesto in 1960 which outlined goals to \"encourage active metabolic development of our society\"[34] through design and technology. In the Material and Man section of the manifesto, Noboru Kawazoe suggests that: After several decades, with the rapid progress of communication technology, every one will have a \"brain wave receiver\" in his ear, which conveys directly and exactly what other people think about him and vice versa. What I think will be known by all the people. There is no more individual consciousness, only the will of mankind as a whole.[35] The concept of the technological singularity, or the ultra-rapid advent of superhuman intelligence, was first proposed by the British cryptologist I. J. Good in 1965:\n Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an \"intelligence explosion,\" and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make.[36] Computer scientist Marvin Minsky wrote on relationships between human and artificial intelligence beginning in the 1960s.[37] Over the succeeding decades, this field continued to generate influential thinkers, such as Hans Moravec and Ray Kurzweil, who oscillated between the technical arena and futuristic speculations in the transhumanist vein.[38][39] The coalescence of an identifiable transhumanist movement began in the last decades of the 20th century. In 1972, Robert Ettinger, whose 1964 Prospect of Immortality founded the cryonics movement,[40] contributed to the conceptualization of \"transhumanity\" with his 1972 Man into Superman.[41] FM-2030 published the Upwingers Manifesto in 1973.[42]\n The first self-described transhumanists met formally in the early 1980s at the University of California, Los Angeles, which became the main center of transhumanist thought. Here, FM-2030 lectured on his \"Third Way\" futurist ideology.[43] At the EZTV Media venue, frequented by transhumanists and other futurists, Natasha Vita-More presented Breaking Away, her 1980 experimental film with the theme of humans breaking away from their biological limitations and the Earth's gravity as they head into space.[44][45] FM-2030 and Vita-More soon began holding gatherings for transhumanists in Los Angeles, which included students from FM-2030's courses and audiences from Vita-More's artistic productions. In 1982, Vita-More authored the Transhumanist Arts Statement[46] and in 1988 she produced the cable TV show TransCentury Update on transhumanity, a program that reached over 100,000 viewers.\n In 1986, Eric Drexler published Engines of Creation: The Coming Era of Nanotechnology,[47] which discussed the prospects for nanotechnology and molecular assemblers, and founded the Foresight Institute. As the first nonprofit organization to research, advocate for, and perform cryonics, the Southern California offices of the Alcor Life Extension Foundation became a center for futurists. In 1988, the first issue of Extropy Magazine was published by Max More and Tom Morrow. In 1990, More, a strategic philosopher, created his own particular transhumanist doctrine, which took the form of the Principles of Extropy, and laid the foundation of modern transhumanism by giving it a new definition:[48]\n Transhumanism is a class of philosophies that seek to guide us towards a posthuman condition. Transhumanism shares many elements of humanism, including a respect for reason and science, a commitment to progress, and a valuing of human (or transhuman) existence in this life. [...] Transhumanism differs from humanism in recognizing and anticipating the radical alterations in the nature and possibilities of our lives resulting from various sciences and technologies [...]. In 1992, More and Morrow founded the Extropy Institute, a catalyst for networking futurists and brainstorming new memeplexes by organizing a series of conferences and, more importantly, providing a mailing list, which exposed many to transhumanist views for the first time during the rise of cyberculture and the cyberdelic counterculture. In 1998, philosophers Nick Bostrom and David Pearce founded the World Transhumanist Association (WTA), an international non-governmental organization working toward the recognition of transhumanism as a legitimate subject of scientific inquiry and public policy.[49] In 2002, the WTA modified and adopted The Transhumanist Declaration.[27][50][51] The Transhumanist FAQ, prepared by the WTA (later Humanity+), gave two formal definitions for transhumanism:[52]\n In possible contrast with other transhumanist organizations, WTA officials considered that social forces could undermine their futurist visions and needed to be addressed.[7] A particular concern is equal access to human enhancement technologies across classes and borders.[53] In 2006, a political struggle within the transhumanist movement between the libertarian right and the liberal left resulted in a more centre-leftward positioning of the WTA under its former executive director James Hughes.[53][54] In 2006, the board of directors of the Extropy Institute ceased operations of the organization, saying that its mission was \"essentially completed\".[55] This left the World Transhumanist Association as the leading international transhumanist organization. In 2008, as part of a rebranding effort, the WTA changed its name to \"Humanity+\".[56] In 2012, the transhumanist Longevity Party had been initiated as an international union of people who promote the development of scientific and technological means to significant life extension that now has more than 30 national organisations throughout the world.[57][58]\n The Mormon Transhumanist Association was founded in 2006.[59] By 2012, it had hundreds of members.[60]\n The first transhumanist elected member of a parliament was Giuseppe Vatinno, in Italy.[61]\n It is a matter of debate whether transhumanism is a branch of posthumanism and how this philosophical movement should be conceptualised with regard to transhumanism.[62][63] Transhumanism is often referred to as a variant or activist form of posthumanism by its conservative,[64] Christian[65] and progressive[66][67] critics.[68]\n A common feature of transhumanism and philosophical posthumanism is the future vision of a new intelligent species, into which humanity will evolve and which eventually will supplement or supersede it. Transhumanism stresses the evolutionary perspective, including sometimes the creation of a highly intelligent animal species by way of cognitive enhancement (i.e. biological uplift),[7] but clings to a \"posthuman future\" as the final goal of participant evolution.[69][70]\n Nevertheless, the idea of creating intelligent artificial beings (proposed, for example, by roboticist Hans Moravec) has influenced transhumanism.[38] Moravec's ideas and transhumanism have also been characterised as a \"complacent\" or \"apocalyptic\" variant of posthumanism and contrasted with \"cultural posthumanism\" in humanities and the arts.[71] While such a \"cultural posthumanism\" would offer resources for rethinking the relationships between humans and increasingly sophisticated machines, transhumanism and similar posthumanisms are, in this view, not abandoning obsolete concepts of the \"autonomous liberal subject\", but are expanding its \"prerogatives\" into the realm of the posthuman.[72] Transhumanist self-characterisations as a continuation of humanism and Enlightenment thinking correspond with this view.\n Some secular humanists conceive transhumanism as an offspring of the humanist freethought movement and argue that transhumanists differ from the humanist mainstream by having a specific focus on technological approaches to resolving human concerns (i.e. technocentrism) and on the issue of mortality.[73] Other progressives have argued that posthumanism, in its philosophical or activist forms, amounts to a shift away from concerns about social justice, from the reform of human institutions and from other Enlightenment preoccupations, toward narcissistic longings to transcend the human body in quest of more exquisite ways of being.[74]\n The philosophy of transhumanism is closely related to technoself studies, an interdisciplinary domain of scholarly research dealing with all aspects of human identity in a technological society and focusing on the changing nature of relationships between humans and technology.[75]\n You awake one morning to find your brain has another lobe functioning. Invisible, this auxiliary lobe answers your questions with information beyond the realm of your own memory, suggests plausible courses of action, and asks questions that help bring out relevant facts. You quickly come to rely on the new lobe so much that you stop wondering how it works. You just use it. This is the dream of artificial intelligence. While many transhumanist theorists and advocates seek to apply reason, science and technology to reduce poverty, disease, disability, and malnutrition around the globe,[52] transhumanism is distinctive in its particular focus on the applications of technologies to the improvement of human bodies at the individual level. Many transhumanists actively assess the potential for future technologies and innovative social systems to improve the quality of all life, while seeking to make the material reality of the human condition fulfill the promise of legal and political equality by eliminating congenital mental and physical barriers.\n Transhumanist philosophers argue that there not only exists a perfectionist ethical imperative for humans to strive for progress and improvement of the human condition, but that it is possible and desirable for humanity to enter a transhuman phase of existence in which humans enhance themselves beyond what is naturally human. In such a phase, natural evolution would be replaced with deliberate participatory or directed evolution.\n Some theorists such as Ray Kurzweil think that the pace of technological innovation is accelerating and that the next 50 years may yield not only radical technological advances, but possibly a technological singularity, which may fundamentally change the nature of human beings.[77] Transhumanists who foresee this massive technological change generally maintain that it is desirable, but some are concerned about the dangers of extremely rapid technological change and propose options for ensuring that advanced technology is used responsibly. For example, Bostrom has written extensively on existential risks to humanity's future welfare, including ones that emerging technologies could create.[78] In contrast, some proponents of transhumanism view it as essential to humanity's survival. For instance, Stephen Hawking points out that the \"external transmission\" phase of human evolution, where knowledge production and knowledge management is more important than transmission of information via evolution, may be the point at which human civilization becomes unstable and self-destructs, one of Hawking's explanations for the Fermi paradox. To counter this, Hawking emphasizes either self-design of the human genome or mechanical enhancement (e.g., brain-computer interface) to enhance human intelligence and reduce aggression, without which he implies human civilization may be too stupid collectively to survive an increasingly unstable system, resulting in societal collapse.[79]\n While many people believe that all transhumanists are striving for immortality, that is not necessarily true. Hank Pellissier, managing director of the Institute for Ethics and Emerging Technologies (2011\u20132012), surveyed transhumanists. He found that, of the 818 respondents, 23.8% did not want immortality.[80] Some of the reasons argued were boredom, Earth's overpopulation, and the desire \"to go to an afterlife\".[80]\n Certain transhumanist philosophers hold that since all assumptions about what others experience are fallible, and that therefore all attempts to help or protect beings that are incapable of correcting what others assume about them, no matter how well-intentioned, are in danger of actually hurting them, all sentient beings deserve to be sapient. These thinkers argue that the ability to discuss in a falsification-based way constitutes a threshold that is not arbitrary at which it becomes possible for someone to speak for themself in a way that is independent of exterior assumptions. They also argue that all beings capable of experiencing something deserve to be elevated to this threshold if they are not at it, typically saying that the underlying change that leads to the threshold is an increase in the preciseness of the brain's ability to discriminate. This includes increasing the neuron count and connectivity in animals as well as accelerating the development of connectivity to shorten or ideally skip non-sapient childhood incapable of independently deciding for oneself. Transhumanists of this description stress that the genetic engineering that they advocate is general insertion into both the somatic cells of living beings and in germ cells, and not purging of people without the modifications, deeming the latter not only unethical but also unnecessary due to the possibilities of efficient genetic engineering.[81][82][83][84]\n Transhumanists engage in interdisciplinary approaches to understand and evaluate possibilities for overcoming biological limitations by drawing on futurology and various fields of ethics.[citation needed] Unlike many philosophers, social critics, and activists who morally value preservation of natural systems, transhumanists see the concept of the specifically natural as problematically nebulous at best and an obstacle to progress at worst.[85] In keeping with this, many prominent transhumanist advocates, such as Dan Agin, call transhumanism's critics, on the political right and left jointly, \"bioconservatives\" or \"bioluddites\", the latter term alluding to the 19th-century anti-industrialisation social movement that opposed the replacement of human manual labourers by machines.[86]\n A belief of counter-transhumanism is that transhumanism can cause unfair human enhancement in many areas of life, but specifically on the social plane. This can be compared to steroid use, where athletes who use steroids in sports have an advantage over those who do not. The same disparity may happen when people have certain neural implants that give them an advantage in the workplace and in education.[87] Additionally, according to M.J. McNamee and S.D. Edwards, many fear that the improvements afforded by a specific, privileged section of society will lead to a division of the human species into two different species.[88] The idea of two human species, one at a great physical and economic advantage over with the other, is troublesome at best. One may be incapable of breeding with the other, and may by consequence of lower physical health and ability, be considered of a lower moral standing than the other.[88]\n Nick Bostrom has said that transhumanism advocates for the wellbeing of all sentient beings, including non-human animals, extraterrestrials, and artificial forms of life.[89] This view is reiterated by David Pearce, who advocates the use of biotechnology to eradicate suffering in all sentient beings.[90]\n There is a variety of opinions within transhumanist thought. Many of the leading transhumanist thinkers hold views that are under constant revision and development.[91] Some distinctive currents of transhumanism are identified and listed here in alphabetical order:\n Although many transhumanists are atheists, agnostics, or secular humanists, some have religious or spiritual views.[49] Despite the prevailing secular attitude, some transhumanists pursue hopes traditionally espoused by religions, such as immortality,[94] while several controversial new religious movements from the late 20th century have explicitly embraced transhumanist goals of transforming the human condition by applying technology to alter the mind and body, such as Ra\u00eblism.[97] But most thinkers associated with the transhumanism focus on the practical goals of using technology to help achieve longer and healthier lives, while speculating that future understanding of neurotheology and the application of neurotechnology will enable humans to gain greater control of altered states of consciousness, which were commonly interpreted as spiritual experiences, and thus achieve more profound self-knowledge.[98] Transhumanist Buddhists have sought to explore areas of agreement between various types of Buddhism and Buddhist-derived meditation and mind-expanding neurotechnologies.[99] They have been criticised for appropriating mindfulness as a tool for transcending humanness.[100]\n Some transhumanists believe in the compatibility between the human mind and computer hardware, with the theoretical implication that human consciousness may someday be transferred to alternative media (a speculative technique commonly known as mind uploading).[101] One extreme formulation of this idea that interests some transhumanists is the proposal of the Omega Point by Christian cosmologist Frank Tipler. Drawing upon ideas in digitalism, Tipler has advanced the notion that the collapse of the Universe billions of years hence could create the conditions for the perpetuation of humanity in a simulated reality within a megacomputer and thus achieve a form of \"posthuman godhood\". Before Tipler, the term Omega Point was used by Pierre Teilhard de Chardin, a paleontologist and Jesuit theologian who saw an evolutionary telos in the development of an encompassing noosphere, a global consciousness.[102][103][104]\n Viewed from the perspective of some Christian thinkers, the idea of mind uploading is asserted to represent a denigration of the human body, characteristic of gnostic manichaean belief.[105] Transhumanism and its presumed intellectual progenitors have also been described as neo-gnostic by non-Christian and secular commentators.[106][107]\n The first dialogue between transhumanism and faith was a one-day conference at the University of Toronto in 2004.[108] Religious critics alone faulted transhumanism for offering no eternal truths or relationship with the divine. They commented that a philosophy bereft of these beliefs leaves humanity adrift in a foggy sea of postmodern cynicism and anomie. Transhumanists responded that such criticisms reflect a failure to look at the actual content of transhumanist philosophy, which, far from being cynical, is rooted in optimistic, idealistic attitudes that trace back to the Enlightenment.[109] Following this dialogue, William Sims Bainbridge, a sociologist of religion, conducted a pilot study, published in the Journal of Evolution and Technology, suggesting that religious attitudes were negatively correlated with acceptance of transhumanist ideas and indicating that people with highly religious worldviews tended to perceive transhumanism as a direct, competitive (though ultimately futile) affront to their spiritual beliefs.[110]\n Since 2006, the Mormon Transhumanist Association sponsors conferences and lectures on the intersection of technology and religion.[111] The Christian Transhumanist Association[112] was established in 2014.\n Since 2009, the American Academy of Religion holds a \"Transhumanism and Religion\" consultation during its annual meeting, where scholars in the field of religious studies seek to identify and critically evaluate any implicit religious beliefs that might underlie key transhumanist claims and assumptions; consider how transhumanism challenges religious traditions to develop their own ideas of the human future, in particular the prospect of human transformation, whether by technological or other means; and provide critical and constructive assessments of an envisioned future that place greater confidence in nanotechnology, robotics and information technology to achieve virtual immortality and create a superior posthuman species.[113]\n The physicist and transhumanist thinker Giulio Prisco states that \"cosmist religions based on science, might be our best protection from reckless pursuit of superintelligence and other risky technologies.\"[114] He also recognizes the importance of spiritual ideas, such as those of Russian Orthodox philosopher Nikolai Fyodorovich Fyodorov, to the origins of the transhumanism movement.\n While some transhumanists[who?] take an abstract and theoretical approach to the perceived benefits of emerging technologies, others have offered specific proposals for modifications to the human body, including heritable ones. Transhumanists are often concerned with methods of enhancing the human nervous system. Though some, such as Kevin Warwick, propose modification of the peripheral nervous system, the brain is considered the common denominator of personhood and is thus a primary focus of transhumanist ambitions.[115]\n In fact, Warwick has gone a lot further than merely making a proposal. In 2002 he had a 100 electrode array surgically implanted into the median nerves of his left arm to link his nervous system directly with a computer and thus to also connect with the internet. As a consequence, he carried out a series of experiments. He was able to directly control a robot hand using his neural signals and to feel the force applied by the hand through feedback from the fingertips. He also experienced a form of ultrasonic sensory input and conducted the first purely electronic communication between his own nervous system and that of his wife who also had electrodes implanted.[116]\n As proponents of self-improvement and body modification, transhumanists tend to use existing technologies and techniques that supposedly improve cognitive and physical performance, while engaging in routines and lifestyles designed to improve health and longevity.[117] Depending on their age, some[who?] transhumanists express concern that they will not live to reap the benefits of future technologies. However, many have a great interest in life extension strategies and in funding research in cryonics to make the latter a viable option of last resort, rather than remaining an unproven method.[118]\n While most transhumanist theory focuses on future technologies and the changes they may bring, many today are already involved in the practice on a very basic level. It is not uncommon for many to receive cosmetic changes to their physical form via cosmetic surgery, even if it is not required for health reasons. Human growth hormones attempt to alter the natural development of shorter children or those who have been born with a physical deficiency. Doctors prescribe medicines such as Ritalin and Adderall to improve cognitive focus, and many people take \"lifestyle\" drugs such as Viagra, Propecia, and Botox to restore aspects of youthfulness that have been lost in maturity.[119]\n Other transhumanists, such as cyborg artist Neil Harbisson, use technologies and techniques to improve their senses and perception of reality. Harbisson's antenna, which is permanently implanted in his skull, allows him to sense colours beyond human perception such as infrareds and ultraviolets.[120]\n Transhumanists support the emergence and convergence of technologies including nanotechnology, biotechnology, information technology and cognitive science (NBIC), as well as hypothetical future technologies like simulated reality, artificial intelligence, superintelligence, 3D bioprinting, mind uploading, chemical brain preservation and cryonics. They believe that humans can and should use these technologies to become more than human.[121] Therefore, they support the recognition or protection of cognitive liberty, morphological freedom and procreative liberty as civil liberties, so as to guarantee individuals the choice of using human enhancement technologies on themselves and their children.[122] Some speculate that human enhancement techniques and other emerging technologies may facilitate more radical human enhancement no later than at the midpoint of the 21st century. Kurzweil's book The Singularity is Near and Michio Kaku's book Physics of the Future outline various human enhancement technologies and give insight on how these technologies may impact the human race.[77][123]\n Some reports on the converging technologies and NBIC concepts have criticised their transhumanist orientation and alleged science fictional character.[124] At the same time, research on brain and body alteration technologies has been accelerated under the sponsorship of the U.S. Department of Defense, which is interested in the battlefield advantages they would provide to the supersoldiers of the United States and its allies.[125] There has already been a brain research program to \"extend the ability to manage information\", while military scientists are now looking at stretching the human capacity for combat to a maximum 168 hours without sleep.[126]\n Neuroscientist Anders Sandberg has been practicing on the method of scanning ultra-thin sections of the brain. This method is being used to help better understand the architecture of the brain. It is currently being used on mice. This is the first step towards hypothetically uploading contents of the human brain, including memories and emotions, onto a computer.[127][128]\n \n Some detractors have criticized transhumanists' views on human enhancement, arguing that the pursuit of radical transformation could undermine human dignity and identity. Critics also contend that transhumanists often underestimate the ethical complexities and potential unintended consequences of their proposed technologies, such as exacerbating social inequalities or creating unforeseen risks to individuals and society. These concerns have led to debates about whether transhumanist ideals prioritize technological progress over considerations of moral and societal responsibility (McNamee, Michael J.; Edwards, Steven D. (2006). \"Transhumanism, Medical Technology and Slippery Slopes\". Journal of Medical Ethics. 32 (9): 513\u2013518. doi:10.1136/jme.2005.013789. PMC\u00a02563415. PMID\u00a016943331.).\n The very notion and prospect of human enhancement and related issues arouse public controversy.[129] Criticisms of transhumanism and its proposals take two main forms: those objecting to the likelihood of transhumanist goals being achieved (practical criticisms) and those objecting to the moral principles or worldview sustaining transhumanist proposals or underlying transhumanism itself (ethical criticisms). Critics and opponents often see transhumanists' goals as posing threats to human values.\n The human enhancement debate is, for some, framed by the opposition between strong bioconservatism and transhumanism. The former opposes any form of human enhancement, whereas the latter advocates for all possible human enhancements.[130] But many philosophers hold a more nuanced view in favour of some enhancements while rejecting the transhumanist carte blanche approach.[131]\n Some of the most widely known critiques of the transhumanist program are novels and fictional films. These works, despite presenting imagined worlds rather than philosophical analyses, are used as touchstones for some of the more formal arguments.[7] Various arguments have been made to the effect that a society that adopts human enhancement technologies may come to resemble the dystopia depicted in the 1932 novel Brave New World by Aldous Huxley.[132]\n On another front, some authors consider humanity already transhuman, because medical advances in recent centuries have significantly altered our species. But this has not happened in a conscious and therefore transhumanistic way.[133] From such a perspective, transhumanism is perpetually aspirational: as new technologies become mainstream, the adoption of new yet-unadopted technologies becomes a new shifting goal.\n In a 1992 book, sociologist Max Dublin pointed to many past failed predictions of technological progress and argued that modern futurist predictions would prove similarly inaccurate. He also objected to what he saw as scientism, fanaticism and nihilism by a few in advancing transhumanist causes. Dublin also said that historical parallels existed between Millenarian religions and Communist doctrines.[134]\n Although generally sympathetic to transhumanism, public health professor Gregory Stock is skeptical of the technical feasibility and mass appeal of the cyborgization of humanity predicted by Raymond Kurzweil, Hans Moravec and Kevin Warwick. He said that, throughout the 21st century, many humans will be deeply integrated into systems of machines, but remain biological. Primary changes to their own form and character would arise not from cyberware, but from the direct manipulation of their genetics, metabolism and biochemistry.[135]\n In her 1992 book Science as Salvation, philosopher Mary Midgley traces the notion of achieving immortality by transcendence of the material human body (echoed in the transhumanist tenet of mind uploading) to a group of male scientific thinkers of the early 20th century, including J. B. S. Haldane and members of his circle. She characterizes these ideas as \"quasi-scientific dreams and prophesies\" involving visions of escape from the body coupled with \"self-indulgent, uncontrolled power-fantasies\". Her argument focuses on what she perceives as the pseudoscientific speculations and irrational, fear-of-death-driven fantasies of these thinkers, their disregard for laymen and the remoteness of their eschatological visions.[136]\n Another critique is aimed mainly at \"algeny\" (a portmanteau of alchemy and genetics), which Jeremy Rifkin defined as \"the upgrading of existing organisms and the design of wholly new ones with the intent of 'perfecting' their performance\".[137] It emphasizes the issue of biocomplexity and the unpredictability of attempts to guide the development of products of biological evolution. This argument, elaborated in particular by the biologist Stuart Newman, is based on the recognition that cloning and germline genetic engineering of animals are error-prone and inherently disruptive of embryonic development. Accordingly, so it is argued, it would create unacceptable risks to use such methods on human embryos. Performing experiments, particularly ones with permanent biological consequences, on developing humans would thus be in violation of accepted principles governing research on human subjects (see the 1964 Declaration of Helsinki). Moreover, because improvements in experimental outcomes in one species are not automatically transferable to a new species without further experimentation, it is claimed that there is no ethical route to genetic manipulation of humans at early developmental stages.[138]\n As a practical matter, international protocols on human subject research may not present a legal obstacle to attempts by transhumanists and others to improve their offspring by germinal choice technology. According to legal scholar Kirsten Rabe Smolensky, existing laws protect parents who choose to enhance their child's genome from future liability arising from adverse outcomes of the procedure.[139]\n Transhumanists and other supporters of human genetic engineering do not dismiss practical concerns out of hand, insofar as there is a high degree of uncertainty about the timelines and likely outcomes of genetic modification experiments in humans. But bioethicist James Hughes suggests that one possible ethical route to the genetic manipulation of humans at early developmental stages is the building of computer models of the human genome, the proteins it specifies and the tissue engineering he argues that it also codes for. With the exponential progress in bioinformatics, Hughes believes that a virtual model of genetic expression in the human body will not be far behind and that it will soon be possible to accelerate approval of genetic modifications by simulating their effects on virtual humans.[7] Public health professor Gregory Stock points to artificial chromosomes as a safer alternative to existing genetic engineering techniques.[135]\n Thinkers[who?] who defend the likelihood of accelerating change point to a past pattern of exponential increases in humanity's technological capacities. Kurzweil developed this position in his 2005 book The Singularity Is Near.\n It has been argued that, in transhumanist thought, humans attempt to substitute themselves for God. The 2002 Vatican statement Communion and Stewardship: Human Persons Created in the Image of God,[140] stated that \"changing the genetic identity of man as a human person through the production of an infrahuman being is radically immoral\", implying, that \"man has full right of disposal over his own biological nature\". The statement also argues that creation of a superhuman or spiritually superior being is \"unthinkable\", since true improvement can come only through religious experience and \"realizing more fully the image of God\". Christian theologians and lay activists of several churches and denominations have expressed similar objections to transhumanism and claimed that Christians attain in the afterlife what radical transhumanism promises, such as indefinite life extension or the abolition of suffering. In this view, transhumanism is just another representative of the long line of utopian movements which seek to create \"heaven on earth\".[141][142] On the other hand, religious thinkers allied with transhumanist goals such as the theologians Ronald Cole-Turner and Ted Peters hold that the doctrine of \"co-creation\" provides an obligation to use genetic engineering to improve human biology.[143][144]\n Other critics target what they claim to be an instrumental conception of the human body in the writings of Minsky, Moravec, and some other transhumanists.[72] Reflecting a strain of feminist criticism of the transhumanist program, philosopher Susan Bordo points to \"contemporary obsessions with slenderness, youth and physical perfection\", which she sees as affecting both men and women, but in distinct ways, as \"the logical (if extreme) manifestations of anxieties and fantasies fostered by our culture.\"[145] Some critics question other social implications of the movement's focus on body modification. Political scientist Klaus-Gerd Giesen, in particular, has asserted that transhumanism's concentration on altering the human body represents the logical yet tragic consequence of atomized individualism and body commodification within a consumer culture.[106]\n Bostrom responds that the desire to regain youth, specifically, and transcend the natural limitations of the human body, in general, is pan-cultural and pan-historical, not uniquely tied to the culture of the 20th century. He argues that the transhumanist program is an attempt to channel that desire into a scientific project on par with the Human Genome Project and achieve humanity's oldest hope, rather than a puerile fantasy or social trend.[2]\n In his 2003 book Enough: Staying Human in an Engineered Age, environmental ethicist Bill McKibben argued at length against many of the technologies that are postulated or supported by transhumanists, including germinal choice technology, nanomedicine and life extension strategies. He claims that it would be morally wrong for humans to tamper with fundamental aspects of themselves (or their children) in an attempt to overcome universal human limitations, such as vulnerability to aging, maximum life span and biological constraints on physical and cognitive ability. Attempts to \"improve\" themselves through such manipulation would remove limitations that provide a necessary context for the experience of meaningful human choice. He claims that human lives would no longer seem meaningful in a world where such limitations could be overcome technologically. Even the goal of using germinal choice technology for clearly therapeutic purposes should be relinquished, since it would inevitably produce temptations to tamper with such things as cognitive capacities. He argues that it is possible for societies to benefit from renouncing particular technologies, using as examples Ming China, Tokugawa Japan and the contemporary Amish.[147]\n Biopolitical activist Jeremy Rifkin and biologist Stuart Newman accept that biotechnology has the power to make profound changes in organismal identity. They argue against the genetic engineering of human beings because they fear the blurring of the boundary between human and artifact.[138][148] Philosopher Keekok Lee sees such developments as part of an accelerating trend in modernization in which technology has been used to transform the \"natural\" into the \"artefactual\".[149] In the extreme, this could lead to the manufacturing and enslavement of \"monsters\" such as human clones, human-animal chimeras, or bioroids, but even lesser dislocations of humans and non-humans from social and ecological systems are seen as problematic. The film Blade Runner (1982) and the novels The Boys From Brazil (1976) and The Island of Doctor Moreau (1896) depict elements of such scenarios, but Mary Shelley's 1818 novel Frankenstein; or, The Modern Prometheus  is most often alluded to by critics who suggest that biotechnologies could create objectified and socially unmoored people as well as subhumans. Such critics propose that strict measures be implemented to prevent what they portray as dehumanizing possibilities from ever happening, usually in the form of an international ban on human genetic engineering.[150]\n Science journalist Ronald Bailey claims that McKibben's historical examples are flawed and support different conclusions when studied more closely.[151] For example, few groups are more cautious than the Amish about embracing new technologies, but, though they shun television and use horses and buggies, some are welcoming the possibilities of gene therapy since inbreeding has afflicted them with a number of rare genetic diseases.[135] Bailey and other supporters of technological alteration of human biology also reject the claim that life would be experienced as meaningless if some human limitations are overcome with enhancement technologies as extremely subjective.\n Writing in Reason magazine, Bailey has accused opponents of research involving the modification of animals as indulging in alarmism when they speculate about the creation of subhuman creatures with human-like intelligence and brains resembling those of Homo sapiens. Bailey insists that the aim of conducting research on animals is simply to produce human health care benefits.[152]\n A different response comes from transhumanist personhood theorists who object to what they characterize as the anthropomorphobia fueling some criticisms of this research, which science fiction writer Isaac Asimov termed the \"Frankenstein complex\". For example, Woody Evans argues that, provided they are self-aware, human clones, human-animal chimeras and uplifted animals would all be unique persons deserving of respect, dignity, rights, responsibilities, and citizenship.[153] They conclude that the coming ethical issue is not the creation of so-called monsters, but what they characterize as the \"yuck factor\" and \"human-racism\", that would judge and treat these creations as monstrous.[49][154] In book 3 of his Corrupting the Image series, Douglas Hamp goes so far as to suggest that the Beast of John's Apocalypse is himself a hybrid who will induce humanity to take \"the mark of the Beast,\" in the hopes of obtaining perfection and immortality.[155]\n At least one public interest organization, the U.S.-based Center for Genetics and Society, was formed, in 2001, with the specific goal of opposing transhumanist agendas that involve transgenerational modification of human biology, such as full-term human cloning and germinal choice technology. The Institute on Biotechnology and the Human Future of the Chicago-Kent College of Law critically scrutinizes proposed applications of genetic and nanotechnologies to human biology in an academic setting.\n \n Some critics of libertarian transhumanism have focused on the likely socioeconomic consequences in societies in which divisions between rich and poor are on the rise. Bill McKibben, for example, suggests that emerging human enhancement technologies would be disproportionately available to those with greater financial resources, thereby exacerbating the gap between rich and poor and creating a \"genetic divide\".[147] Even Lee M. Silver, the biologist and science writer who coined the term \"reprogenetics\" and supports its applications, has expressed concern that these methods could create a two-tiered society of genetically engineered \"haves\" and \"have nots\" if social democratic reforms lag behind implementation of enhancement technologies.[156] The 1997 film Gattaca depicts a dystopian society in which one's social class depends entirely on genetic potential and is often cited by critics in support of these views.[7]\n These criticisms are also voiced by non-libertarian transhumanist advocates, especially self-described democratic transhumanists, who believe that the majority of current or future social and environmental issues (such as unemployment and resource depletion) must be addressed by a combination of political and technological solutions (like a guaranteed minimum income and alternative technology). Therefore, on the specific issue of an emerging genetic divide due to unequal access to human enhancement technologies, bioethicist James Hughes, in his 2004 book Citizen Cyborg: Why Democratic Societies Must Respond to the Redesigned Human of the Future, argues that progressives or, more precisely, techno-progressives, must articulate and implement public policies (i.e., a universal health care voucher system that covers human enhancement technologies) to attenuate this problem as much as possible, rather than trying to ban human enhancement technologies. The latter, he argues, might actually worsen the problem by making these technologies unsafe or available only to the wealthy on the local black market or in countries where such a ban is not enforced.[7]\n Sometimes, as in the writings of Leon Kass, the fear is that various institutions and practices judged as fundamental to civilized society would be damaged or destroyed.[157] In his 2002 book Our Posthuman Future and in a 2004 Foreign Policy magazine article, political economist and philosopher Francis Fukuyama designates transhumanism as the world's most dangerous idea because he believes it may undermine the egalitarian ideals of democracy (in general) and liberal democracy (in particular) through a fundamental alteration of \"human nature\".[64] Social philosopher J\u00fcrgen Habermas makes a similar argument in his 2003 book The Future of Human Nature, in which he asserts that moral autonomy depends on not being subject to another's unilaterally imposed specifications. Habermas thus suggests that the human \"species ethic\" would be undermined by embryo-stage genetic alteration.[158] Critics such as Kass and Fukuyama hold that attempts to significantly alter human biology are not only inherently immoral, but also threaten the social order. Alternatively, they argue that implementation of such technologies would likely lead to the \"naturalizing\" of social hierarchies or place new means of control in the hands of totalitarian regimes. AI pioneer Joseph Weizenbaum criticizes what he sees as misanthropic tendencies in the language and ideas of some of his colleagues, in particular Minsky and Moravec, which, by devaluing the human organism per se, promotes a discourse that enables divisive and undemocratic social policies.[159]\n In a 2004 article in the libertarian monthly Reason, science journalist Ronald Bailey contested Fukuyama's assertions by arguing that political equality has never rested on the facts of human biology. He asserts that liberalism was founded not on the proposition of effective equality of human beings, or de facto equality, but on the assertion of an equality in political rights and before the law, or de jure equality. Bailey asserts that the products of genetic engineering may well ameliorate rather than exacerbate human inequality, giving to the many what were once the privileges of the few. Moreover, he argues, \"the crowning achievement of the Enlightenment is the principle of tolerance\". In fact, he says, political liberalism is already the solution to the issue of human and posthuman rights since in liberal societies the law is meant to apply equally to all, no matter how rich or poor, powerful or powerless, educated or ignorant, enhanced or unenhanced.[160] Other thinkers sympathetic to transhumanist ideas, such as Russell Blackford, have also objected to the appeal to tradition and what they see as alarmism involved in Brave New World-type arguments.[161]\n In addition to the socioeconomic risks and implications of transhumanism, there are indeed implications and possible consequences in regard to cultural aesthetics. Currently, there are a number of ways in which people choose to represent themselves in society. The way in which a person dresses, hair styles, and body alteration all serve to identify the way a person presents themselves and is perceived by society. According to Foucault,[162] society already governs and controls bodies by making them feel watched. This \"surveillance\" of society dictates how the majority of individuals choose to express themselves aesthetically.\n One of the risks outlined in a 2004 article by Jerold Abrams is the elimination of differences in favor of universality. This, he argues, will eliminate the ability of individuals to subvert the possibly oppressive, dominant structure of society by way of uniquely expressing themselves externally. Such control over a population would have dangerous implications of tyranny. Yet another consequence of enhancing the human form not only cognitively, but physically, will be the reinforcement of \"desirable\" traits which are perpetuated by the dominant social structure.[162]\n The tradition of human enhancement originated with the eugenics movement that was once prominent in the biological sciences, and was later politicized in various ways. This continuity is especially clear in the case of Julian Huxley himself.[163]\n The major transhumanist organizations strongly condemn the coercion involved in such policies and reject the racist and classist assumptions on which they were based, along with the pseudoscientific notions that eugenic improvements could be accomplished in a practically meaningful time frame through selective human breeding.[164] Instead, most transhumanist thinkers advocate a \"new eugenics\", a form of egalitarian liberal eugenics.[165] In their 2000 book From Chance to Choice: Genetics and Justice, non-transhumanist bioethicists Allen Buchanan, Dan Brock, Norman Daniels and Daniel Wikler have argued that liberal societies have an obligation to encourage as wide an adoption of eugenic enhancement technologies as possible (so long as such policies do not infringe on individuals' reproductive rights or exert undue pressures on prospective parents to use these technologies) to maximize public health and minimize the inequalities that may result from both natural genetic endowments and unequal access to genetic enhancements.[166] Most transhumanists holding similar views nonetheless distance themselves from the term \"eugenics\" (preferring \"germinal choice\" or \"reprogenetics\")[156] to avoid having their position confused with the discredited theories and practices of early-20th-century eugenic movements.[167]\n Health law professor George Annas and technology law professor Lori Andrews are prominent advocates of the position that the use of these technologies could lead to human-posthuman caste warfare.[150][168]\n In his 2003 book Our Final Hour, British Astronomer Royal Martin Rees argues that advanced science and technology bring as much risk of disaster as opportunity for progress. However, Rees does not advocate a halt to scientific activity. Instead, he calls for tighter security and perhaps an end to traditional scientific openness.[169] Advocates of the precautionary principle, such as many in the environmental movement, also favor slow, careful progress or a halt in potentially dangerous areas. Some precautionists believe that artificial intelligence and robotics present possibilities of alternative forms of cognition that may threaten human life.[170]\n Transhumanists do not necessarily rule out specific restrictions on emerging technologies so as to lessen the prospect of existential risk. Generally, however, they counter that proposals based on the precautionary principle are often unrealistic and sometimes even counter-productive as opposed to the technogaian current of transhumanism, which they claim is both realistic and productive. In his television series Connections, science historian James Burke dissects several views on technological change, including precautionism and the restriction of open inquiry. Burke questions the practicality of some of these views, but concludes that maintaining the status quo of inquiry and development poses hazards of its own, such as a disorienting rate of change and the depletion of our planet's resources. The common transhumanist position is a pragmatic one where society takes deliberate action to ensure the early arrival of the benefits of safe, clean, alternative technology, rather than fostering what it considers to be anti-scientific views and technophobia.\n Nick Bostrom argues that even barring the occurrence of a singular global catastrophic event, basic Malthusian and evolutionary forces facilitated by technological progress threaten to eliminate the positive aspects of human society.[171]\n One transhumanist solution proposed by Bostrom to counter existential risks is control of differential technological development, a series of attempts to influence the sequence in which technologies are developed. In this approach, planners would strive to retard the development of possibly harmful technologies and their applications, while accelerating the development of likely beneficial technologies, especially those that offer protection against the harmful effects of others.[78]\n In their 2021 book Calamity Theory, Joshua Schuster and Derek Woods critique existential risks by arguing against Bostrom\u2019s transhumanist perspective, which emphasizes controlling and mitigating these risks through technological advancements. They contend that this approach relies too much on fringe science and speculative technologies and fails to address deeper philosophical and ethical problems about the nature of human existence and its limitations. Instead, they advocate an approach more grounded in secular existentialist philosophy, focusing on mental fortitude, community resilience, international peacebuilding, and environmental stewardship to better cope with existential risks.[5]\n Although most people focus on the scientific and technological barriers on the road to human enhancement, Robbert Zandbergen argues that contemporary transhumanists' failure to critically engage the cultural current of antinatalism is a far bigger obstacle to a posthuman future. Antinatalism is a stance seeking to discourage, restrict, or terminate human reproduction to solve existential problems. If transhumanists fail to take this threat to human continuity seriously, they run the risk of seeing the collapse of the entire edifice of radical enhancement.[172]\n Simone and Malcolm Collins, founders of Pronatalist.org, are activists known primarily for their views and advocacy related to a secular and voluntaristic form of pronatalism, a stance encouraging higher birth rates to reverse demographic decline and its negative implications for the viability of modern societies and the possibility of a better future.[173] Critical of transhumanism, they have expressed concern that life extension would worsen the problem of gerontocracy, causing toxic imbalances in power. The Collinses lament that voluntarily childfree transhumanists who \"want to live forever believe they are the epitome of centuries of human cultural and biological evolution. They don\u2019t think they can make kids that are better than them.\"[174]\n Transhumanism has increasingly been co-opted by anti-democratic movements as a common enemy stereotype. These movements range from Putin sympathizers to radical anti-vaxxers and Christian fundamentalists. Critics argue that nonsensical claims often stem from deliberate ignorance, and terms like \"Putin sympathizer\" or \"conspiracy theorist\" are used to defame legitimate criticism.[175]\n Political scientists like Markus Linden point out that Putin, in his speeches, argues against the so-called \"liberal-globalist American egocentrism\" and cancel culture, which parallels the agitation seen in alternative media. These discourses also occur on platforms like Nachdenkseiten, Rubikon, and Compact, where they are presented as analyses of the decline of Western democracy.[175]\n The propagandistic use of the term \"transhumanism\" aims to create a comprehensive counter-narrative that unites right-wing extremists, theocratic groups, and liberals. Transhumanism is portrayed as a threat to traditional values and human nature. These narratives can also be found among ideologues like Alexander Dugin, who condemns transhumanism as the work of the devil, and Christian fundamentalists who equate it with the denial of traditional values.[175]\n The use of the term \"transhumanism\" as an ideological rallying point for the Querfront is also evident in the fusion of right-wing, left-wing, and libertarian ideas that collectively oppose liberal democracies. This development emphasizes individual conceptions of humanity that are often incompatible with a pluralistic society. It requires a critical examination of the political implications of transhumanism and its instrumentalization by anti-democratic forces.[175]\n",
        "doc_number": 52
    },
    {
        "url": "https://en.wikipedia.org/wiki/AI_art",
        "content": "\n Artificial intelligence art is visual artwork created or enhanced through the use of artificial intelligence (AI) programs.\n Artists began to create artificial intelligence art in the mid to late 20th century when the discipline was founded. Throughout its history, artificial intelligence art has raised many philosophical concerns related to the human mind, artificial beings, and what can be considered art in a human\u2013AI collaboration. Since the 20th century, artists have used AI to create art, some of which has been exhibited in museums and won awards.[1]\n During the AI boom of the early 2020s, text-to-image models such as Midjourney, DALL-E, Stable Diffusion, and FLUX.1 became widely available to the public, allowing non-artists to quickly generate imagery with little effort.[2][3] Commentary about AI art in the 2020s has often focused on issues related to copyright, deception, defamation, and its impact on more traditional artists, including technological unemployment. Opinions have also risen on the possible effect AI generated art might have on creativity.\n The concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music.[4][5] Early experiments were driven by the idea that computers, beyond performing logical operations, could generate aesthetically pleasing works, offering a new dimension to creativity. The tradition of creative automatons has flourished throughout history, such as Maillardet's automaton, created around 1800 and capable of creating multiple drawings and poems stored in its \"cams\u201d, the brass disks that hold memory.[6]\n Along with this, Ada Lovelace, typically known for her work on the analytical engine, in her notes, begins to conceptualize the idea \"computing operations\" could be used to generate music and poems. This concept resulted in what is now referred to as \"The Lovelace Effect,\" which gives a concrete set of tools to analyze situations where a computer's behavior is viewed by users as creative.[7] However, Lovelace also discusses a concept in her notes that is known as \"The Lovelace Objection,\" where she argues that machines have \"no pretensions whatever to originate anything,\" which is a direct contradiction to the idea of artificial intelligence and creative machines.[8]\n In 1950, with the publication of Alan Turing's paper Computing Machinery and Intelligence, there was a shift from defining intelligence in regards to machines in abstract terms to evaluating whether a machine can mimic human behavior and responses convincingly.[9] Shortly after, the academic discipline of artificial intelligence was founded at a research workshop at Dartmouth College in 1956 and has experienced several waves of advancement and optimism in the decades since.[10] Since its founding, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction, and philosophy since antiquity.[11]\n Since the founding of AI in the 1950s, artists and researchers have used artificial intelligence to create artistic works. These works were sometimes referred to as algorithmic art,[12] computer art, digital art, or  New media art.[13]\n One of the first significant AI art systems is AARON, developed by Harold Cohen beginning in the late 1960s at the University of California at San Diego.[14] AARON uses a symbolic rule-based approach to generate technical images in the era of GOFAI programming, and it was developed by Cohen with the goal of being able to code the act of drawing.[15] In its earliest form, AARON created abstract black-and-white drawings which would later be finished by Cohen painting them. Throughout the years, he also began to develop a way for AARON to paint as well, using special brushes and dyes that were chosen by the program itself without mediation from Cohen.[16] After years of work, AARON was exhibited in 1972 at the Los Angeles County Museum of Art.[17] From 1973 to 1975, Cohen refined AARON during a residency at the Artificial Intelligence Laboratory at Stanford University.[18] In 2024, the Whitney Museum of American Art exhibited AI art from throughout Cohen's career, including re-created versions of his early robotic drawing machines.[18]\n Karl Sims has exhibited art created with artificial life since the 1980s. He received an M.S. in computer graphics from the MIT Media Lab in 1987 and was artist-in-residence from 1990 to 1996 at the supercomputer manufacturer and artificial intelligence company Thinking Machines.[19][20][21] In both 1991 and 1992, Sims won the Golden Nica award at Prix Ars Electronica for his 3D AI animated videos using artificial evolution.[22][23][24] In 1997, Sims created the interactive installation Gal\u00e1pagos for the NTT InterCommunication Center in Tokyo.[25] In this installation, viewers help evolve 3D animated creatures by selecting which ones will be allowed to live and produce new, mutated offspring. Furthermore, Sims received an Emmy Award in 2019 for outstanding achievement in engineering development.[26]\n Eric Millikin has been creating animated films using artificial intelligence since the 1980s, and began posting art on the internet using CompuServe in the early 1980s.[27][28]\n In 1999, Scott Draves and a team of several engineers created and released Electric Sheep as a free software screensaver.[29] Electric Sheep is a volunteer computing project for animating and evolving fractal flames, which are in turn distributed to the networked computers, which display them as a screensaver. The screensaver used AI to create an infinite animation by learning from its audience. In 2001, Draves won the Fundacion Telef\u00f3nica Life 4.0 prize[30] for Electric Sheep.\n Deep learning, characterized by its multi-layer structure that attempts to mimic the human brain, first came about in the 2010s and causing a significant shift in the world of AI art.[31] During the deep learning era, there are mainly these types of designs for generative art: autoregressive models, diffusion models, GANs, normalizing flows. In 2014, Ian Goodfellow and colleagues at Universit\u00e9 de Montr\u00e9al developed the generative adversarial network (GAN), a type of deep neural network capable of learning to mimic the statistical distribution of input data such as images. The GAN uses a \"generator\" to create new images and a \"discriminator\" to decide which created images are considered successful.[32] Unlike previous algorithmic art that followed hand-coded rules, generative adversarial networks could learn a specific aesthetic by analyzing a dataset of example images.[12]\n In 2015, a team at Google released DeepDream, a program that uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia.[33][34][35] The process creates deliberately over-processed images with a dream-like appearance reminiscent of a psychedelic experience.[36]\n Later, in 2017, a conditional GAN learned to generate 1000 image classes of ImageNet, a large visual database designed for use in visual object recognition software research.[37][38] By conditioning the GAN on both random noise and a specific class label, this approach enhanced the quality of image synthesis for class-conditional models.[39]\n Autoregressive models were used for image generation, such as PixelRNN (2016), which autoregressively generates one pixel after another with a recurrent neural network.[40] Immediately after the Transformer architecture was proposed in Attention Is All You Need (2018), it was used for autoregressive generation of images, but without text conditioning.[41]\n In 2018, an auction sale of artificial intelligence art was held at Christie's in New York where the AI artwork Edmond de Belamy (a pun on Goodfellow's name) sold for US$432,500, which was almost 45 times higher than its estimate of US$7,000\u201310,000. The artwork was created by Obvious, a Paris-based collective.[42][43][44] Furthermore, the website Artbreeder, launched in 2018, uses the models StyleGAN and BigGAN[45][46] to allow users to generate and modify images such as faces, landscapes, and paintings.[47]\n In 2019, Stephanie Dinkins won the Creative Capital award for her creation of an evolving artificial intelligence based on the \"interests and culture(s) of people of color.\"[48] Also in 2019, Sougwen Chung won the Lumen Prize for her performances with a robotic arm that uses AI to attempt to draw in a manner similar to Chung.[49]\n In the 2020s, text-to-image models, which generate images based on prompts, became widely used, marking yet another shift in the creation of AI generated artworks.[2]\n In 2021, using the influential large language generative pre-trained transformer models that are used in GPT-2 and GPT-3, OpenAI released a series of images created with the text-to-image AI model DALL-E 1.[50] It was an autoregressive generative model with essentially the same architecture as GPT-3. Along with this, later in 2021, EleutherAI released the open source VQGAN-CLIP[51] based on OpenAI's CLIP model.[52]\n Diffusion models, generative models used to create synthetic data based on existing data,[53] were first proposed in 2015,[54] but they only became better than GANs in early 2021.[55] Latent diffusion model was published in December 2021 and became the basis for the later Stable Diffusion (August 2022).[56]\n In 2022, Midjourney[57] was released, followed by Google Brain's Imagen and Parti, which were announced in May 2022, Microsoft's NUWA-Infinity,[58][2] and the source-available Stable Diffusion, which was released in August 2022.[59][60][61] DALL-E 2, a successor to DALL-E, was beta-tested and released. Stability AI has a Stable Diffusion web interface called DreamStudio,[62] plugins for Krita, Photoshop, Blender, and GIMP,[63] and the Automatic1111 web-based open source user interface.[64][65][66] Stable Diffusion's main pre-trained model is shared on the Hugging Face Hub.[67]\n In 2023, Eric Millikin released The Dance of the Nain Rouge, a documentary film created using AI deepfake technology about the Detroit folklore legend of the Nain Rouge. The film is described as \"an experimental decolonial Detroit demonology deepfake dream dance documentary.\"[68] It was awarded the \"Best Innovative Technologies Award\" (\"Premio Migliori Tecnologie Innovative\") at the 2024 Pisa Robot Film Festival in Italy[69] and \"Best Animation Film\" at the 2024 Absurd Film Festival in Italy.[70] Ideogram was released in August 2023, this model is known for its ability to generate legible text.[71][72]\n In 2024, Flux was released, this model can generate realistic images with consistent results and was integrated into Grok, the chatbot used on X (formerly Twitter), and Le Chat, the chatbot of Mistral AI.[3][73][74][75] Flux was developed by Black Forest Labs, founded by the researchers behind Stable Diffusion.[76] Grok later switched to its own text-to-image model Aurora in December 2024.[77]\n Along with this, some examples of text-to-video model models of the mid-2020s are Runway's Gen-2, Google's VideoPoet, and OpenAI's Sora, which released in December 2024.[78][79]\n There are many tools available to the artist when working with diffusion models. They can define both positive and negative prompts, but they are also afforded a choice in using (or omitting the use of) VAEs, LorAs, hypernetworks, ipadapter, and embeddings/textual inversions. Variables, including CFG, seed, steps, sampler, scheduler, denoise, upscaler, and encoder, are sometimes available for adjustment. Additional influence can be exerted during pre-inference by means of noise manipulation, while traditional post-processing techniques are frequently used post-inference. Artists can also train their own models.\n In addition, procedural \"rule-based\" generation of images using mathematical patterns, algorithms that simulate brush strokes and other painted effects, and deep learning algorithms such as generative adversarial networks (GANs) and transformers have been developed. Several companies have released apps and websites that allow one to forego all the options mentioned entirely while solely focusing on the positive prompt. There also exist programs which transform photos into art-like images in the style of well-known sets of paintings.[80][81]\n There are many options, ranging from simple consumer-facing mobile apps to Jupyter notebooks and webUIs that require powerful GPUs to run effectively.[82] Additional functionalities include \"textual inversion,\" which refers to enabling the use of user-provided concepts (like an object or a style) learned from a few images. Novel art can then be generated from the associated word(s) (the text that has been assigned to the learned, often abstract, concept)[83][84] and model extensions or fine-tuning (such as DreamBooth).\n AI has the potential for a societal transformation, which may include enabling the expansion of noncommercial niche genres (such as cyberpunk derivatives like solarpunk) by amateurs, novel entertainment, fast prototyping,[85] increasing art-making accessibility,[85] and artistic output per effort and/or expenses and/or time[85]\u2014e.g., via generating drafts, draft-refinitions, and image components (inpainting). Generated images are sometimes used as sketches,[86] low-cost experiments,[87] inspiration, or illustrations of proof-of-concept-stage ideas. Additional functionalities or improvements may also relate to post-generation manual editing (i.e., polishing), such as subsequent tweaking with an image editor.[87]\n Prompts for some text-to-image models can also include images and keywords and configurable parameters, such as artistic style, which is often used via keyphrases like \"in the style of [name of an artist]\" in the prompt[88] and/or selection of a broad aesthetic/art style.[89][86] There are platforms for sharing, trading, searching, forking/refining, and/or collaborating on prompts for generating specific imagery from image generators.[90][91][92][93] Prompts are often shared along with images on image-sharing websites such as Reddit and AI art-dedicated websites. A prompt is not the complete input needed for the generation of an image; additional inputs that determine the generated image include the output resolution, random seed, and random sampling parameters.[94]\n Synthetic media, which includes AI art, was described in 2022 as a major technology-driven trend that will affect business in the coming years.[85] Harvard Kennedy School researchers voiced concerns about synthetic media serving as a vector for political misinformation soon after studying the proliferation of AI art on the X platform.[95] Synthography is a proposed term for the practice of generating images that are similar to photographs using AI.[96]\n A major concern raised about AI-generated images and art is sampling bias within model training data leading towards discriminatory output from AI art models. In 2023, University of Washington researchers found evidence of racial bias within the Stable Diffusion model, with images of a \"person\" corresponding most frequently with images of males from Europe or North America.[97]\n Looking more into the sampling bias found within AI training data, in 2017, researchers at Princeton University used AI software to link over 2 million words, finding that European names were viewed as more \"pleasant\" than African-Americans names, and that the words \"woman\" and \"girl\" were more likely to be associated with the arts instead of science and math, \"which were most likely connected to males.\"[98] Generative AI models typically work based on user-entered word-based prompts, especially in the case of diffusion models, and this word-related bias may lead to biased results.\n Along with this, generative AI can perpetuate harmful stereotypes regarding women. For example, Lensa, an AI app that trended on TikTok in 2023, was known to lighten black skin, make users thinner, and generate hypersexualized images of women.[99] Melissa Heikkil\u00e4, a senior reporter at MIT Technology Review, shared the findings of an experiment using Lensa, noting that the generated avatars did not resemble her and often depicted her in a hypersexualized manner.[100] Experts suggest that such outcomes can result from biases in the datasets used to train AI models, which can sometimes contain imbalanced representations, including hypersexual or nude imagery.[101][102]\n In 2024, Google's chatbot Gemini's AI image generator was criticized for perceived racial bias, with claims that Gemini deliberately underrepresented white people in its results.[103] Users reported that it generated images of white historical figures like the Founding Fathers, Nazi soldiers, and Vikings as other races, and that it refused to process prompts such as \"happy white people\" and \"ideal nuclear family\".[103][104] Google later apologized for \"missing the mark\" and took Gemini's image generator offline for updates.[105] This prompted discussions about the ethical implications[106] of representing historical figures through a contemporary lens, leading critics to argue that these outputs could mislead audiences regarding actual historical contexts.[107]\n Legal scholars, artists, and media corporations have considered the legal and ethical implications of artificial intelligence art since the 20th century. Some artists use AI art to critique and explore the ethics of using gathered data to produce new artwork.[108]\n In 1985, intellectual property law professor Pamela Samuelson argued that US copyright should allocate algorithmically generated artworks to the user of the computer program.[109] A 2019 Florida Law Review article presented three perspectives on the issue. In the first, artificial intelligence itself would become the copyright owner; to do this, Section 101 of the US Copyright Act would need to be amended to define \"author\" as a computer. In the second, following Samuelson's argument, the user, programmer, or artificial intelligence company would be the copyright owner. This would be an expansion of the \"work for hire\" doctrine, under which ownership of a copyright is transferred to the \"employer.\" In the third situation, copyright assignments would never take place, and such works would be in the public domain, as copyright assignments require an act of authorship.[110]\n In 2022, coinciding with the rising availability of consumer-grade AI image generation services, popular discussion renewed over the legality and ethics of AI-generated art. A particular topic is the inclusion of copyrighted artwork and images in AI training datasets, with artists objecting to commercial AI products using their works without consent, credit, or financial compensation.[111] In September 2022, Reema Selhi, of the Design and Artists Copyright Society, stated that \"there are no safeguards for artists to be able to identify works in databases that are being used and opt out.\"[112] Some have claimed that images generated with these models can bear resemblance to extant artwork, sometimes including the remains of the original artist's signature.[112][113] In December 2022, users of the portfolio platform ArtStation staged an online protest against non-consensual use of their artwork within datasets; this resulted in opt-out services, such as \"Have I Been Trained?\" increasing in profile, as well as some online art platforms promising to offer their own opt-out options.[114] According to the US Copyright Office, artificial intelligence programs are unable to hold copyright,[115][116][117] a decision upheld at the Federal District level as of August 2023 followed the reasoning from the monkey selfie copyright dispute.[118]\n OpenAI, the developer of DALL-E, has its own policy on who owns generated art. They assign the right and title of a generated image to the creator, meaning the user who inputted the prompt owns the image generated, along with the right to sell, reprint, and merchandise it.[119]\n In January 2023, three artists\u2014Sarah Andersen, Kelly McKernan, and Karla Ortiz\u2014filed a copyright infringement lawsuit against Stability AI, Midjourney, and DeviantArt, claiming that it is legally required to obtain the consent of artists before training neural nets on their work and that these companies infringed on the rights of millions of artists by doing so on five billion images scraped from the web.[120] In July 2023, U.S. District Judge William Orrick was inclined to dismiss most of the lawsuits filed by Andersen, McKernan, and Ortiz, but allowed them to file a new complaint.[121] Also in 2023, Stability AI was sued by Getty Images for using its images in the training data.[122] A tool built by Simon Willison allowed people to search 0.5% of the training data for Stable Diffusion V1.1, i.e., 12 million of the 2.3 billion instances from LAION 2B. Artist Karen Hallion discovered that her copyrighted images were used as training data without their consent.[123]\n In March 2024, Tennessee enacted the ELVIS Act, which prohibits the use of AI to mimic a musician's voice without permission.[124] A month later in that year, Adam Schiff introduced the Generative AI Copyright Disclosure Act which, if passed, would require that AI companies to submit copyrighted works in their datasets to the Register of Copyrights before releasing new generative AI systems.[125]\n As with other types of photo manipulation since the early 19th century, some people in the early 21st century have been concerned that AI could be used to create content that is misleading and can be made to damage a person's reputation, such as deepfakes.[126] Artist Sarah Andersen, who previously had her art copied and edited to depict Neo-Nazi beliefs, stated that the spread of hate speech online can be worsened by the use of image generators.[123] Some also generate images or videos for the purpose of catfishing.\n AI systems have the ability to create deepfake content, which is often viewed as harmful and offensive. The creation of deepfakes poses a risk to individuals who have not consented to it.[127] This mainly refers to deepfake pornography which is used as revenge porn, where sexually explicit material is disseminated to humiliate or harm another person. AI-generated child pornography has been deemed a potential danger to society due to its unlawful nature.[128]\n \nTo mitigate some deceptions, OpenAI developed a tool in 2024 to detect images that were generated by DALL-E 3.[129] In testing, this tool accurately identified DALL-E 3-generated images approximately 98% of the time. The tool is also fairly capable of recognizing images that have been visually modified by users post-generation.[130] After winning the 2023 \"Creative\" \"Open competition\" Sony World Photography Awards, Boris Eldagsen stated that his entry was actually created with artificial intelligence. Photographer Feroz Khan commented to the BBC that Eldagsen had \"clearly shown that even experienced photographers and art experts can be fooled\".[132] Smaller contests have been affected as well; in 2023, a contest run by author Mark Lawrence as Self-Published Fantasy Blog-Off was cancelled after the winning entry was allegedly exposed to be a collage of images generated with Midjourney.[133]\n In May 2023, on social media sites such as Reddit and Twitter, attention was given to a Midjourney-generated image of Pope Francis wearing a white puffer coat.[134][135] Additionally, an AI-generated image of an attack on the Pentagon went viral as part of a hoax news story on Twitter.[136][137]\n In the days before March 2023 indictment of Donald Trump as part of the Stormy Daniels\u2013Donald Trump scandal, several AI-generated images allegedly depicting Trump's arrest went viral online.[138][139] On March 20, British journalist Eliot Higgins generated various images of Donald Trump being arrested or imprisoned using Midjourney v5 and posted them on Twitter; two images of Trump struggling against arresting officers went viral under the mistaken impression that they were genuine, accruing more than 5 million views in three days.[140][141] According to Higgins, the images were not meant to mislead, but he was banned from using Midjourney services as a result. As of April 2024, the tweet had garnered more than 6.8 million views.\n In February 2024, the paper Cellular functions of spermatogonial stem cells in relation to JAK/STAT signaling pathway was published using AI-generated images. It was later retracted from Frontiers in Cell and Developmental Biology because the paper \"does not meet the standards\".[142]\n As generative AI image software such as Stable Diffusion and DALL-E continue to advance, the potential problems and concerns that these systems pose for creativity and artistry have risen.[123] In 2022, artists working in various media raised concerns about the impact that generative artificial intelligence could have on their ability to earn money, particularly if AI-based images started replacing artists working in the illustration and design industries.[143][144] In August 2022, digital artist R. J. Palmer stated that \"I could easily envision a scenario where using AI, a single artist or art director could take the place of 5-10 entry level artists... I have seen a lot of self-published authors and such say how great it will be that they don\u2019t have to hire an artist.\"[113] Scholars Jiang et al. state that \"Leaders of companies like Open AI and Stability AI have openly stated that they expect generative AI systems to replace creatives imminently.\"[123] A 2022 case study found that AI-produced images created by technology like DALL-E caused some traditional artists to be concerned about losing work, while others use it to their advantage and view it as a tool.[127]\n AI-based images have become more commonplace in art markets and search engines because AI-based text-to-image systems are trained from pre-existing artistic images, sometimes without the original artist's consent, allowing the software to mimic specific artists' styles.[123][145] For example, Polish digital artist Greg Rutkowski has stated that it is more difficult to search for his work online because many of the images in the results are AI-generated specifically to mimic his style.[60] Furthermore, some training databases on which AI systems are based are not accessible to the public.\n The ability of AI-based art software to mimic or forge artistic style also raises concerns of malice or greed.[123][146][147] Works of AI-generated art, such as Th\u00e9\u00e2tre D'op\u00e9ra Spatial, a text-to-image AI illustration that won the grand prize in the August 2022 digital art competition at the Colorado State Fair, have begun to overwhelm art contests and other submission forums meant for small artists.[123][146][147] The Netflix short film The Dog & the Boy, released in January 2023, received backlash online for its use of artificial intelligence art to create the film's background artwork.[148] Within the same vein, Disney released Secret Invasion, a Marvel TV show with an AI-generated intro, on Disney+ in 2023, causing concern and backlash regarding the idea that artists could be made obsolete by machine-learning tools.[149]\n AI art has sometimes been deemed to be able to replace traditional stock images.[150] In 2023, Shutterstock announced a beta test of an AI tool that can regenerate partial content of other Shutterstock's images. Getty Images and Nvidia have partnered with the launch of Generative AI by iStock, a model trained on Getty's library and iStock's photo library using Nvidia's Picasso model.[151]\n Researchers from Hugging Face and Carnegie Mellon University reported in a 2023 paper that generating one thousand 1024\u00d71024 images using Stable Diffusion's XL 1.0 base model requires 11.49 kWh of energy and generates 1,594 grams (56.2\u00a0oz) of carbon dioxide, which is roughly equivalent to driving an average gas-powered car a distance of 4.1 miles (6.6\u00a0km). Comparing 88 different models, the paper concluded that image-generation models used on average around 2.9\u00a0kWh of energy per 1,000 inferences.[152]\n In addition to the creation of original art, research methods that use AI have been generated to quantitatively analyze digital art collections. This has been made possible due to the large-scale digitization of artwork in the past few decades. According to CETINIC and SHE (2022), using artificial intelligence to analyze already-existing art collections can provide new perspectives on the development of artistic styles and the identification of artistic influences.[153][154]\n Two computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art.[155] Close reading focuses on specific visual aspects of one piece. Some tasks performed by machines in close reading methods include computational artist authentication and analysis of brushstrokes or texture properties. In contrast, through distant viewing methods, the similarity across an entire collection for a specific feature can be statistically visualized. Common tasks relating to this method include automatic classification, object detection, multimodal tasks, knowledge discovery in art history, and computational aesthetics.[154] Synthetic images can also be used to train AI algorithms for art authentication and to detect forgeries.[156]\n Researchers have also introduced models that predict emotional responses to art. One such model is ArtEmis, a large-scale dataset paired with machine learning models. ArtEmis includes emotional annotations from over 6,500 participants along with textual explanations. By analyzing both visual inputs and the accompanying text descriptions from this dataset, ArtEmis enables the generation of nuanced emotional predictions.[157][158]\n AI has also been used in arts outside of visual arts. Generative AI has been used in video game production beyond imagery, especially for level design (e.g., for custom maps) and creating new content (e.g., quests or dialogue) or interactive stories in video games.[159][160] AI has also been used in the literary arts,[161] such as helping with writer's block, inspiration, or rewriting segments.[162][163][164][165] In the culinary arts, some prototype cooking robots can dynamically taste, which can assist chefs in analyzing the content and flavor of dishes during the cooking process.[166]\n",
        "doc_number": 53
    },
    {
        "url": "https://en.wikipedia.org/wiki/AI_music",
        "content": "\n Music and artificial intelligence (music and AI) is the development of music software programs which use AI to generate music.[1] As with applications in other fields, AI in music also simulates mental tasks. A prominent feature is the capability of an AI algorithm to learn based on past data, such as in computer accompaniment technology, wherein the AI is capable of listening to a human performer and performing accompaniment.[2] Artificial intelligence also drives interactive composition technology, wherein a computer composes music in response to a live performance. There are other AI applications in music that cover not only music composition, production, and performance but also how music is marketed and consumed. Several music player programs have also been developed to use voice recognition and natural language processing technology for music voice control. Current research includes the application of AI in music composition, performance, theory and digital sound processing.\n Erwin Panofksy proposed that in all art, there existed three levels of meaning: primary meaning, or the natural subject; secondary meaning, or the conventional subject; and tertiary meaning, the intrinsic content of the subject.[3][4] AI music explores the foremost of these, creating music without the \"intention\" which is usually behind it, leaving composers who listen to machine-generated pieces feeling unsettled by the lack of apparent meaning.[5]\n Artificial intelligence finds its beginnings in music with the transcription problem: accurately recording a performance into musical notation as it is played. P\u00e8re Engramelle's schematic of a \"piano roll\", a mode of automatically recording note timing and duration in a way which could be easily transcribed to proper musical notation by hand, was first implemented by German engineers J.F. Unger and J. Hohlfield in 1752.[6]\n In 1957, the ILLIAC I (Illinois Automatic Computer) produced the \"Illiac Suite for String Quartet\", a completely computer-generated piece of music. The computer was programmed to accomplish this by composer Lejaren Hiller and mathematician Leonard Isaacson.[5]:\u200av\u2013vii\u200a\nIn 1960, Russian researcher Rudolf Zaripov published worldwide first paper on algorithmic music composing using the Ural-1 computer.[7]\n In 1965, inventor Ray Kurzweil developed software capable of recognizing musical patterns and synthesizing new compositions from them. The computer first appeared on the quiz show I've Got a Secret.[8]\n By 1983, Yamaha Corporation's Kansei Music System had gained momentum, and a paper was published on its development in 1989. The software utilized music information processing and artificial intelligence techniques to essentially solve the transcription problem for simpler melodies, although higher-level melodies and musical complexities are regarded even today as difficult deep-learning tasks, and near-perfect transcription is still a subject of research.[6][9]\n In 1997, an artificial intelligence program named Experiments in Musical Intelligence (EMI) appeared to outperform a human composer at the task of composing a piece of music to imitate the style of Bach.[10] EMI would later become the basis for a more sophisticated algorithm called Emily Howell, named for its creator.\n In 2002, the music research team at the Sony Computer Science Laboratory Paris, led by French composer and scientist Fran\u00e7ois Pachet, designed the Continuator, an algorithm uniquely capable of resuming a composition after a live musician stopped.[11]\n Emily Howell would continue to make advancements in musical artificial intelligence, publishing its first album From Darkness, Light in 2009.[12] Since then, many more pieces by artificial intelligence and various groups have been published.\n In 2010, Iamus became the first AI to produce a fragment of original contemporary classical music, in its own style: \"Iamus' Opus 1\". Located at the Universidad de Mal\u00e1ga (Mal\u00e1ga University) in Spain, the computer can generate a fully original piece in a variety of musical styles.[13][5]:\u200a468\u2013481\u200a In August 2019, a large dataset consisting of 12,197 MIDI songs, each with their lyrics and melodies,[14] was created to investigate the feasibility of neural melody generation from lyrics using a deep conditional LSTM-GAN method.\n With progress in generative AI, models capable of creating complete musical compositions (including lyrics) from a simple text description have begun to emerge. Two notable web applications in this field are Suno AI, launched in December 2023, and Udio, which followed in April 2024.[15]\n Developed at Princeton University by Ge Wang and Perry Cook, ChucK is a text-based, cross-platform language.[16] By extracting and classifying the theoretical techniques it finds in musical pieces, the software is able to synthesize entirely new pieces from the techniques it has learned.[17] The technology is used by SLOrk (Stanford Laptop Orchestra)[18] and PLOrk (Princeton Laptop Orchestra).\n Jukedeck was a website that let people use artificial intelligence to generate original, royalty-free music for use in videos.[19][20] The team started building the music generation technology in 2010,[21] formed a company around it in 2012,[22] and launched the website publicly in 2015.[20] The technology used was originally a rule-based algorithmic composition system,[23] which was later replaced with artificial neural networks.[19] The website was used to create over 1 million pieces of music, and brands that used it included Coca-Cola, Google, UKTV, and the Natural History Museum, London.[24] In 2019, the company was acquired by ByteDance.[25][26][27]\n MorpheuS[28] is a research project by Dorien Herremans and Elaine Chew at Queen Mary University of London, funded by a Marie Sk\u0142odowsk\u00e1-Curie EU project. The system uses an optimization approach based on a variable neighborhood search algorithm to morph existing template pieces into novel pieces with a set level of tonal tension that changes dynamically throughout the piece. This optimization approach allows for the integration of a pattern detection technique in order to enforce long term structure and recurring themes in the generated music. Pieces composed by MorpheuS have been performed at concerts in both Stanford and London.\n Created in February 2016, in Luxembourg, AIVA is a program that produces soundtracks for any type of media. The algorithms behind AIVA are based on deep learning architectures[29] AIVA has also been used to compose a Rock track called On the Edge,[30] as well as a pop tune Love Sick[31] in collaboration with singer Taryn Southern,[32] for the creation of her 2018 album \"I am AI\".\n Google's Magenta team has published several AI music applications and technical papers since their launch in 2016.[33] In 2017 they released the NSynth algorithm and dataset,[34] and an open source hardware musical instrument, designed to facilitate musicians in using the algorithm.[35] The instrument was used by notable artists such as Grimes and YACHT in their albums.[36][37] In 2018, they released a piano improvisation app called Piano Genie. This was later followed by Magenta Studio, a suite of 5 MIDI plugins that allow music producers to elaborate on existing music in their DAW.[38] In 2023, their machine learning team published a technical paper on GitHub that described MusicLM, a private text-to-music generator which they'd developed.[39][40]\n Riffusion is a neural network, designed by Seth Forsgren and Hayk Martiros, that generates music using images of sound rather than audio.[41] It was created as a fine-tuning of Stable Diffusion, an existing open-source model for generating images from text prompts, on spectrograms.[41] This results in a model which uses text prompts to generate image files, which can be put through an inverse Fourier transform and converted into audio files.[42] While these files are only several seconds long, the model can also use latent space between outputs to interpolate different files together.[41][43] This is accomplished using a functionality of the Stable Diffusion model known as img2img.[44]\n The resulting music has been described as \"de otro mundo\" (otherworldly),[45] although unlikely to replace man-made music.[45] The model was made available on December 15, 2022, with the code also freely available on GitHub.[42] It is one of many models derived from Stable Diffusion.[44]\n Spike AI is an AI-based audio plug-in, developed by Spike Stent in collaboration with his son Joshua Stent and friend Henry Ramsey, that analyzes tracks and provides suggestions to increase clarity and other aspects during mixing. Communication is done by using a chatbot trained on Spike Stent's personal data. The plug-in integrates into digital audio workstation.[49][50]\n Artificial Intelligence has the opportunity to impact how producers create music by giving reiterations of a track that follow a prompt given by the creator. These prompts allow the AI to follow a certain style that the artist is trying to go for.[5]\n AI has also been seen in musical analysis where it has been used for feature extraction, pattern recognition, and musical recommendations.[51]\n Artificial intelligence has had major impacts in the composition sector as it has influenced the ideas of composers/producers and has the potential to make the industry more accessible to newcomers. With its development in music, it has already been seen to be used in collaboration with producers. Artists use these software to help generate ideas and bring out musical styles by prompting the AI to follow specific requirements that fit their needs. Future compositional impacts by the technology include style emulation and fusion, and revision and refinement. Development of these types of software can give ease of access to newcomers to the music industry.[5] Software such as ChatGPT have been used by producers to do these tasks, while other software such as Ozone11 have been used to automate time consuming and complex activities such as mastering.[52]\n In the United States, the current legal framework tends to apply traditional copyright laws to AI, despite its differences with the human creative process.[53] However, music outputs solely generated by AI are not granted copyright protection. In the compendium of the U.S. Copyright Office Practices, the Copyright Office has stated that it would not grant copyrights to \"works that lack human authorship\" and \"the Office will not register works produced by a machine or mere mechanical process that operates randomly or automatically without any creative input or intervention from a human author.\"[54] In February 2022, the Copyright Review Board rejected an application to copyright AI-generated artwork on the basis that it \"lacked the required human authorship necessary to sustain a claim in copyright.\"[55]\n The situation in the European Union (EU) is similar to the US, because its legal framework also emphasizes the role of human involvement in a copyright-protected work.[56] According to the European Union Intellectual Property Office and the recent jurisprudence of the Court of Justice of the European Union, the originality criterion requires the work to be the author's own intellectual creation, reflecting the personality of the author evidenced by the creative choices made during its production, requires distinct level of human involvement.[56] The reCreating Europe project, funded by the European Union's Horizon 2020 research and innovation program, delves into the challenges posed by AI-generated contents including music, suggesting legal certainty and balanced protection that encourages innovation while respecting copyright norms.[56] The recognition of AIVA marks a significant departure from traditional views on authorship and copyrights in the realm of music composition, allowing AI artists capable of releasing music and earning royalties. This acceptance marks AIVA as a pioneering instance where an AI has been formally acknowledged within the music production.[57]\n The recent advancements in artificial intelligence made by groups such as Stability AI, OpenAI, and Google has incurred an enormous sum of copyright claims leveled against generative technology, including AI music. Should these lawsuits succeed, the machine learning models behind these technologies would have their datasets restricted to the public domain.[58]\n A more nascent development of AI in music is the application of audio deepfakes to cast the lyrics or musical style of a pre-existing song to the voice or style of another artist. This has raised many concerns regarding the legality of technology, as well as the ethics of employing it, particularly in the context of artistic identity.[59] Furthermore, it has also raised the question of to whom the authorship of these works is attributed. As AI cannot hold authorship of its own, current speculation suggests that there will be no clear answer until further rulings are made regarding machine learning technologies as a whole.[60] Most recent preventative measures have started to be developed by Google and Universal Music group who have taken into royalties and credit attribution to allow producers to replicated the voices and styles of artists.[61]\n In 2023, an artist known as ghostwriter977 created a musical deepfake called \"Heart on My Sleeve\" that cloned the voices of Drake and The Weeknd\u00a0by inputting an assortment of vocal-only tracks from the respective artists into a deep-learning algorithm, creating an artificial model of the voices of each artist, to which this model could be mapped onto original reference vocals with original lyrics.[62] The track was submitted for Grammy consideration for the best rap song and song of the year.[63]\u00a0It went viral and gained traction on TikTok and received a positive response from the audience, leading to its official release on Apple Music, Spotify, and YouTube in April of 2023.[64] Many believed the track was fully composed by an AI software, but the producer claimed the songwriting, production, and original vocals (pre-conversion) were still done by him.[62] It would later be rescinded from any Grammy considerations due to it not following the guidelines necessary to be considered for a Grammy award.[64]\u00a0The track would end up being removed from all music platforms by Universal Music Group.[64] The song was a watershed moment for AI voice cloning, and models have since been created for hundreds, if not thousands, of popular singers and rappers.\n In 2013, country music singer Randy Travis suffered a stroke which left him unable to sing. In the meantime, vocalist James Dupr\u00e9 toured on his behalf, singing his songs for him. Travis and longtime producer Kyle Lehning released a new song in May 2024 titled \"Where That Came From\", Travis's first new song since his stroke. The recording uses AI technology to re-create Travis's singing voice, having been composited from over 40 existing vocal recordings alongside those of Dupr\u00e9.[65][66]\n",
        "doc_number": 54
    },
    {
        "url": "https://en.wikipedia.org/wiki/Autonomous_robot",
        "content": "An autonomous robot is a robot that acts without recourse to human control. Historic examples include space probes. Modern examples include self-driving vacuums and cars.\n Industrial robot arms that work on assembly lines inside factories may also be considered autonomous robots, though their autonomy is restricted due to a highly structured environment and their inability to locomote.\n The first requirement for complete physical autonomy is the ability for a robot to take care of itself. Many of the battery-powered robots on the market today can find and connect to a charging station, and some toys like Sony's Aibo are capable of self-docking to charge their batteries.\n Self-maintenance is based on \"proprioception\", or sensing one's own internal status. In the battery charging example, the robot can tell proprioceptively that its batteries are low, and it then seeks the charger. Another common proprioceptive sensor is for heat monitoring. Increased proprioception will be required for robots to work autonomously near people and in harsh environments. Common proprioceptive sensors include thermal, optical, and haptic sensing, as well as the Hall effect (electric).\n Exteroception is sensing things about the environment. Autonomous robots must have a range of environmental sensors to perform their task and stay out of trouble. The autonomous robot can recognize sensor failures and minimize the impact on the performance caused by failures.[1]\n Some robotic lawn mowers will adapt their programming by detecting the speed in which grass grows as needed to maintain a perfectly cut lawn, and some vacuum cleaning robots have dirt detectors that sense how much dirt is being picked up and use this information to tell them to stay in one area longer.\n The next step in autonomous behavior is to actually perform a physical task. A new area showing commercial promise is domestic robots, with a flood of small vacuuming robots beginning with iRobot and Electrolux in 2002. While the level of intelligence is not high in these systems, they navigate over wide areas and pilot in tight situations around homes using contact and non-contact sensors. Both of these robots use proprietary algorithms to increase coverage over simple random bounce.\n The next level of autonomous task performance requires a robot to perform conditional tasks. For instance, security robots can be programmed to detect intruders and respond in a particular way depending upon where the intruder is. For example, Amazon (company) launched its Astro for home monitoring, security and eldercare in September 2021.[2]\n For a robot to associate behaviors with a place (localization) requires it to know where it is and to be able to navigate point-to-point. Such navigation began with wire-guidance in the 1970s and progressed in the early 2000s to beacon-based triangulation. Current commercial robots autonomously navigate based on sensing natural features. The first commercial robots to achieve this were Pyxus' HelpMate hospital robot and the CyberMotion guard robot, both designed by robotics pioneers in the 1980s. These robots originally used manually created CAD floor plans, sonar sensing and wall-following variations to navigate buildings. The next generation, such as MobileRobots' PatrolBot and autonomous wheelchair,[3] both introduced in 2004, have the ability to create their own laser-based maps of a building and to navigate open areas as well as corridors. Their control system changes its path on the fly if something blocks the way.\n At first, autonomous navigation was based on planar sensors, such as laser range-finders, that can only sense at one level. The most advanced systems now fuse information from various sensors for both localization (position) and navigation. Systems such as Motivity can rely on different sensors in different areas, depending upon which provides the most reliable data at the time, and can re-map a building autonomously.\n Rather than climb stairs, which requires highly specialized hardware, most indoor robots navigate handicapped-accessible areas, controlling elevators, and electronic doors.[4] With such electronic access-control interfaces, robots can now freely navigate indoors. Autonomously climbing stairs and opening doors manually are topics of research at the current time.\n As these indoor techniques continue to develop, vacuuming robots will gain the ability to clean a specific user-specified room or a whole floor. Security robots will be able to cooperatively surround intruders and cut off exits. These advances also bring concomitant protections: robots' internal maps typically permit \"forbidden areas\" to be defined to prevent robots from autonomously entering certain regions.\n Outdoor autonomy is most easily achieved in the air, since obstacles are rare. Cruise missiles are rather dangerous highly autonomous robots. Pilotless drone aircraft are increasingly used for reconnaissance. Some of these unmanned aerial vehicles (UAVs) are capable of flying their entire mission without any human interaction at all except possibly for the landing where a person intervenes using radio remote control. Some drones are capable of safe, automatic landings, however. SpaceX operates a number of Autonomous spaceport drone ships, used to safely land and recover Falcon 9 rockets at sea.[5]\n Outdoor autonomy is the most difficult for ground vehicles, due to:\n There are several open problems in autonomous robotics which are special to the field rather than being a part of the general pursuit of AI. According to George A. Bekey's Autonomous Robots: From Biological Inspiration to Implementation and Control, problems include things such as making sure the robot is able to function correctly and not run into obstacles autonomously. Reinforcement learning has been used to control and plan the navigation of autonomous robots, specifically when a group of them operate in collaboration with each other.[6]\n Researchers concerned with creating true artificial life are concerned not only with intelligent control, but further with the capacity of the robot to find its own resources through foraging (looking for food, which includes both energy and spare parts).\n This is related to autonomous foraging, a concern within the sciences of behavioral ecology, social anthropology, and human behavioral ecology; as well as robotics, artificial intelligence, and artificial life.[7]\n As autonomous robots have grown in ability and technical levels, there has been increasing societal awareness and news coverage of the latest advances, and also some of the philosophical issues, economic effects, and societal impacts that arise from the roles and activities of autonomous robots.\n Elon Musk, a prominent business executive and billionaire has warned for years of the possible hazards and pitfalls of autonomous robots; however, his own company is one of the most prominent companies that is trying to devise new advanced technologies in this area.[8]\n In 2021, a United Nations group of government experts, known as the Convention on Certain Conventional Weapons \u2013 Group of Governmental Experts on Lethal Autonomous Weapons Systems, held a conference to highlight the ethical concerns which arise from the increasingly advanced technology for autonomous robots to wield weapons and to play a military role.[9]\n The first autonomous robots  were known as Elmer and Elsie, constructed in the late 1940s by W. Grey Walter. They were the first robots programmed to \"think\" the way biological brains do and were meant to have free will.[10] Elmer and Elsie were often labeled as tortoises because of how they were shaped and the manner in which they moved. They were capable of phototaxis, the movement that occurs in response to light stimulus.[11]\n The Mars rovers MER-A and MER-B (now known as Spirit rover and Opportunity rover) found the position of the Sun and navigated their own routes to destinations, on the fly, by:\n The planned ESA Rover, Rosalind Franklin rover, is capable of vision based relative localisation and absolute localisation to autonomously navigate safe and efficient trajectories to targets by:\n During the final NASA Sample Return Robot Centennial Challenge in 2016, a rover, named Cataglyphis, successfully demonstrated fully autonomous navigation, decision-making, and sample detection, retrieval, and return capabilities.[12] The rover relied on a fusion of measurements from inertial sensors, wheel encoders, Lidar, and camera for navigation and mapping, instead of using GPS or magnetometers. During the 2-hour challenge, Cataglyphis traversed over 2.6\u00a0km and returned five different samples to its starting position.\n The Seekur robot was the first commercially available robot to demonstrate MDARS-like capabilities for general use by airports, utility plants, corrections facilities and Homeland Security.[13]\n The DARPA Grand Challenge and DARPA Urban Challenge have encouraged development of even more autonomous capabilities for ground vehicles, while this has been the demonstrated goal for aerial robots since 1990 as part of the AUVSI International Aerial Robotics Competition.\n AMR transfer carts developed by Seyiton are used to transfer loads of up to 1500 kilograms inside factories. [14]\n Between 2013 and 2017, TotalEnergies has held the ARGOS Challenge to develop the first autonomous robot for oil and gas production sites. The robots had to face adverse outdoor conditions such as rain, wind and extreme temperatures.[15]\n Some significant current robots include: \n Lethal autonomous weapons (LAWs) are a type of autonomous robot military system that can independently search for and engage targets based on programmed constraints and descriptions.[24] LAWs are also known as lethal autonomous weapon systems (LAWS), autonomous weapon systems (AWS), robotic weapons, killer robots or slaughterbots.[25] LAWs may operate in the air, on land, on water, under water, or in space. The autonomy of current systems as of 2018[update] was restricted in the sense that a human gives the final command to attack \u2013 though there are exceptions with certain \"defensive\" systems.\n Tesla Robot and NVIDIA GR00T are humanoid robots.\n A delivery robot is an autonomous robot used for delivering goods.\n An Automatic Charging Robot, unveiled on July 27, 2022, is an arm-shaped automatic charging robot, charging an electric vehicle. It has been running a pilot operation at Hyundai Motor Group's headquarters since 2021. VISION AI System based on deep learning technology has been applied. When an electric vehicle is parked in front of the charger, the robot arm recognizes the charger of the electric vehicle and derives coordinates. And automatically insert a connector into the electric car and operate fast charging. The robot arm is configured in a vertical multi-joint structure so that it can be applied to chargers at different locations for each vehicle. In addition, waterproof and dustproof functions are applied.[46]\n Construction robots are used directly on job sites and perform work such as building, material handling, earthmoving, and surveillance.\n Research and education mobile robots are mainly used during a prototyping phase in the process of building full scale robots. They are a scaled down version of bigger robots with the same types of sensors, kinematics and software stack (e.g. ROS). They are often extendable and provide comfortable programming interface and development tools. Next to full scale robot prototyping they are also used for education, especially at university level, where more and more labs about programming autonomous vehicles are being introduced.\n In March 2016, a bill was introduced in Washington, D.C., allowing pilot ground robotic deliveries.[47] The program was to take place from September 15 through the end of December 2017. The robots were limited to a weight of 50 pounds unloaded and a maximum speed of 10 miles per hour. In case the robot stopped moving because of malfunction the company was required to remove it from the streets within 24 hours. There were allowed only 5 robots to be tested per company at a time.[48] A 2017 version of the Personal Delivery Device Act bill was under review as of March 2017.[49]\n In February 2017, a bill was passed in the US state of Virginia via the House bill, HB2016,[50] and the Senate bill, SB1207,[51] that will allow autonomous delivery robots to travel on sidewalks and use crosswalks statewide beginning on July 1, 2017. The robots will be limited to a maximum speed of 10\u00a0mph and a maximum weight of 50 pounds.[52] In the states of Idaho and Florida there are also talks about passing the similar legislature.[53][54]\n It has been discussed[by whom?] that robots with similar characteristics to invalid carriages (e.g. 10\u00a0mph maximum, limited battery life) might be a workaround for certain classes of applications. If the robot was sufficiently intelligent and able to recharge itself using the existing electric vehicle (EV) charging infrastructure it would only need minimal supervision and a single arm with low dexterity might be enough to enable this function if its visual systems had enough resolution.[citation needed]\n In November 2017, the San Francisco Board of Supervisors announced that companies would need to get a city permit in order to test these robots.[55] In addition, the Board banned sidewalk delivery robots from making non-research deliveries.[56]\n  Media related to Autonomous robots at Wikimedia Commons\n",
        "doc_number": 55
    },
    {
        "url": "https://en.wikipedia.org/wiki/Humanoid_robot",
        "content": "A humanoid robot is a robot resembling the human body in shape. The design may be for functional purposes, such as interacting with human tools and environments, for experimental purposes, such as the study of bipedal locomotion, or for other purposes. In general, humanoid robots have a torso, a head, two arms, and two legs, though some humanoid robots may replicate only part of the body. Androids are humanoid robots built to aesthetically resemble humans.\n The concept of a humanoid robot originated in many different cultures around the world. Some of the earliest accounts of the idea of humanoid automata date to the 4th century BCE in Greek mythologies and various religious and philosophical texts from China. Physical prototypes of humanoid automata were later created in the Middle East, Italy, Japan, and France.\n The Greek god of blacksmiths, Hephaestus, created several different humanoid automata in various myths. In Homer's Iliad, Hephaestus created golden handmaidens and imbued them with human-like voices to serve as speaking tools or instruments.[2] Another Greek myth details how Hephaestus crafted a giant bronze automaton named Talos to protect the island of Crete from invaders.[3]\n In the 3rd century BCE, a Taoist philosophical text called the Liezi, written by Chinese philosopher Lie Yukou, detailed the idea of a humanoid automaton. The text includes mention of an engineer named Yan Shi who created a life-size, human-like robot for the fifth king of the Chinese Zhou Dynasty, King Mu.[4] The robot was primarily constructed of leather and wood. It was capable of walking, singing, and moving all parts of its body.[4]\n In the 13th century, a Muslim engineer named Ismail al-Jazari designed various humanoid automata. He created a waitress robot that would dispense drinks from a liquid reservoir and appear out of an automatic door to serve them.[5] Another automaton he created was used for hand washing to refill a basin with water after being drained.[6]\n In the 1400s, Leonardo da Vinci conceptualized a complex mechanical robot clad in a suit of armor, capable of sitting, standing, and independently moving its arms.[7] The entire robot was operated by a system of pulleys and cables.\n From the 17th to 19th centuries, the Japanese built humanoid automata called karakuri puppets. These puppets resembled dolls and were used for entertainment in theatre, homes, and religious festivals.[8] Karakuri puppets that were used for theater plays were called butai karakuri.[9] Small karakuri puppets found in homes, called zashiki kurakuri, were placed on tables to dance, beat drums, or serve drinks.[9] The puppets used in religious festivals were known as Dashi karakuri, and they served to reenact myths and legends.[10]\n In the 18th century, French inventor Jacques de Vaucanson created a significant humanoid automaton called The Flute Player. This wooden, human-sized robot was capable of playing various melodies with the flute. It consisted of a system of bellows, pipes, weights, and other mechanical components to simulate to the muscles necessary to play the flute.[11]\n Humanoid robots are now used as research tools in several scientific areas. Researchers study the human body structure and behavior (biomechanics) to build humanoid robots. On the other side, the attempt to simulate the human body leads to a better understanding of it. Human cognition is a field of study which is focused on how humans learn from sensory information in order to acquire perceptual and motor skills. This knowledge is used to develop computational models of human behavior, and it has been improving over time.\n It has been suggested that very advanced robotics will facilitate the enhancement of ordinary humans. See transhumanism.\n Humanoid robots are a valuable resource in the world of medicine and biotechnology, as well as other fields of research such as biomechanics and cognitive science.[12] Humanoid robots are being used to develop complex prosthetics for individuals with physical disabilities such as missing limbs.[13] The WABIAN-2 is a new medical humanoid robot created to help patients in the rehabilitation of their lower limbs.[13]\n Although the initial aim of humanoid research was to build better orthosis and prosthesis for human beings, knowledge has been transferred between both disciplines. A few examples are powered leg prosthesis for the neuromuscularly impaired, ankle-foot orthosis, biological realistic leg prosthesis, and forearm prosthesis.\n Humanoid robots can be used as test subjects for the practice and development of personalized healthcare aids, essentially performing as robotic nurses for demographics such as the elderly.[13] Humanoids are also suitable for some procedurally-based vocations, such as reception-desk administrators and automotive manufacturing line workers. In essence, since they can use tools and operate equipment and vehicles designed for the human form, humanoids could theoretically perform any task a human being can, so long as they have the proper software. However, the complexity of doing so is immense.\n Humanoid robots have had a long history in the realm of entertainment, from the conception and ideas in the story of Prometheus to the application and physical build of modern animatronics used for theme parks.[12] Current uses and development of humanoid robots in theme parks are focused on creating stuntronics.[14] Stuntronics are humanoid robots built for serving as stunt doubles, and are designed to simulate life-like, untethered, dynamic movement.[14] Several Disney theme park shows utilize animatronic robots that look, move and speak much like human beings. Although these robots look realistic, they have no cognition or physical autonomy. Various humanoid robots and their possible applications in daily life are featured in an independent documentary film called Plug & Pray, which was released in 2010.\n Though many real-world applications for humanoid robots are unexplored, their primary use is to demonstrate up-and-coming technologies.[15] Modern examples of humanoid robots, such as the Honda Asimo, are revealed to the public in order to demonstrate new technological advancements in motor skills, such as walking, climbing, and playing an instrument.[15] Other humanoid robots have been developed for household purposes, however excel only in single purpose skills and are far from autonomous.[15] Humanoid robots, especially those with artificial intelligence algorithms, could be useful for future dangerous and/or distant space exploration missions, without having the need to turn back around again and return to Earth once the mission is completed.\n A sensor is a device that measures some attribute of the world. Being one of the three primitives of robotics (besides planning and control), sensing plays an important role in robotic paradigms.\n Sensors can be classified according to the physical process with which they work or according to the type of measurement information that they give as output. In this case, the second approach was used.[16]\n Proprioceptive sensors sense the position, orientation, and speed of the humanoid's body and joints, along with other internal values.[17]\n In human beings, the otoliths and semi-circular canals (in the inner ear) are used to maintain balance and orientation.[18] Additionally, humans use their own proprioceptive sensors (e.g. touch, muscle extension, limb position) to help with their orientation. Humanoid robots use accelerometers to measure the acceleration, from which velocity can be calculated by integration;[19] tilt sensors to measure inclination; force sensors placed in robot's hands and feet to measure contact force with environment;[20] position sensors that indicate the actual position of the robot (from which the velocity can be calculated by derivation);[21] and even speed sensors.\n Arrays of tactels can be used to provide data on what has been touched. The Shadow Hand uses an array of 34 tactels arranged beneath its polyurethane skin on each finger tip.[22] Tactile sensors also provide information about forces and torques transferred between the robot and other objects.\n Vision refers to processing data from any modality which uses the electromagnetic spectrum to produce an image. In humanoid robots it is used to recognize objects and determine their properties. Vision sensors work most similarly to the eyes of human beings. Most humanoid robots use CCD cameras as vision sensors.\n Sound sensors allow humanoid robots to hear speech and environmental sounds, akin to the ears of the human being. Microphones are usually used for the robots to convey speech.\n Actuators are the motors responsible for motion in the robot.[23]\n Humanoid robots are constructed in such a way that they mimic the human body. They use actuators that perform like muscles and joints, though with a different structure.[23] The actuators of humanoid robots can be either electric, pneumatic, or hydraulic.[24][25] It is ideal for these actuators to have high power, low mass, and small dimensions.[25]\n Electric actuators are the most popular types of actuators in humanoid robots.[24] These actuators are smaller in size, and a single electric actuator may not produce enough power for a human-sized joint.[24] Therefore, it is common to use multiple electric actuators for a single joint in a humanoid robot.[24] An example of a humanoid robot using electric actuators is HRP-2.[25]\n Hydraulic actuators produce higher power than electric actuators and pneumatic actuators, and they have the ability to control the torque they produce better than other types of actuators.[25] However, they can become very bulky in size.[24][25] One solution to counter the size issue is electro-hydrostatic actuators (EHA).[25] The most popular example of a humanoid robot using hydraulic actuators is the ATLAS robot made by Boston Dynamics.[25]\n Pneumatic actuators operate on the basis of gas compressibility.[24][25] As they are inflated, they expand along the axis, and as they deflate, they contract. If one end is fixed, the other will move in a linear trajectory. A popular example of a pneumatic actuator is the Mac Kibben muscle.[25]\n Planning in robots is the process of planning out motions and trajectories for the robot to carry out.[26] Control is the actual execution of these planned motions and trajectories.[26] In humanoid robots, the planning must carry out biped motions, meaning that robots should plan motions similar to a human.[27] Since one of the main uses of humanoid robots is to interact with humans, it is important for the planning and control mechanisms of humanoid robots to work in a variety of terrain and environments.[27]\n The question of walking biped robots stabilization on the surface is of great importance.[28] Maintenance of the robot's gravity center over the center of bearing area for providing a stable position can be chosen as a goal of control.[28]\n To maintain dynamic balance during the walk, a robot needs information about contact force and its current and desired motion.[27] The solution to this problem relies on a major concept, the Zero Moment Point (ZMP).[27]\n Another characteristic of humanoid robots is that they move, gather information (using sensors) on the \"real world\", and interact with it.[29] They do not stay still like factory manipulators and other robots that work in highly structured environments.[29] To allow humanoids to move in complex environments, planning and control must focus on self-collision detection, path planning and obstacle avoidance.[29][30]\n Humanoid robots do not yet have some features of the human body.[31] They include structures with variable flexibility, which provide safety (to the robot itself and to the people), and redundancy of movements, i.e. more degrees of freedom and therefore wide task availability.[31] Although these characteristics are desirable to humanoid robots, they will bring more complexity and new problems to planning and control.[32] The field of whole-body control deals with these issues and addresses the proper coordination of numerous degrees of freedom, e.g. to realize several control tasks simultaneously while following a given order of priority.[33][34]\n A common theme for the depiction of humanoid robots in science fiction pertains to how they can help humans in society or serve as threats to humanity.[115] This theme essentially questions whether artificial intelligence is a force of good or bad for mankind.[115] Humanoid robots that are depicted as good for society and benefit humans are Commander Data in Star Trek and C-3PO in Star Wars.[115] Opposite portrayals where humanoid robots are shown as scary and threatening to humans are the T-800 in Terminator and Megatron in Transformers.[115] An Indian Tamil-language film which showed the pros and cons of a humanoid robot Chitti.[116][117]\n Another prominent theme found in science fiction regarding humanoid robots focuses on personhood. Certain films, particularly Blade Runner and Blade Runner 2049, explore whether or not a constructed, synthetic being should be considered a person.[118] In the films, androids called \"replicants\" are created indistinguishably from human beings, yet they are shunned and do not possess the same rights as humans. This theme incites audience sympathy while also sparking unease at the idea of humanoid robots mimicking humans too closely.[119]\n Humanoid robots, which are designed to resemble and mimic human form and behavior, have faced several criticisms:\n",
        "doc_number": 56
    },
    {
        "url": "https://en.wikipedia.org/wiki/Industrial_robot",
        "content": "An industrial robot is a robot system used for manufacturing. Industrial robots are automated, programmable and capable of movement on three or more axes.[1]\n Typical applications of robots include welding, painting, assembly, disassembly,[2] pick and place for printed circuit boards, packaging and labeling, palletizing, product inspection, and testing; all accomplished with high endurance, speed, and precision. They can assist in material handling.\n In the year 2023, an estimated 4,281,585 industrial robots were in operation worldwide according to International Federation of Robotics (IFR).[3][4]\n There are six types of industrial robots.[5]\n Articulated robots[5] are the most common industrial robots.[6] They look like a human arm, which is why they are also called robotic arm or manipulator arm.[7] Their articulations with several degrees of freedom allow the articulated arms a wide range of movements.\n An autonomous robot is a robot that acts without recourse to human control. The first autonomous robots  environment were known as Elmer and Elsie, which were constructed in the late 1940s by W. Grey Walter. They were the first robots in history that were programmed to \"think\" the way biological brains do and meant to have free will.[8] Elmer and Elsie were often labeled as tortoises because of how they were shaped and the manner in which they moved. They were capable of phototaxis which is the movement that occurs in response to light stimulus.[9]\n Cartesian robots,[5] also called rectilinear, gantry robots, and x-y-z robots[6] have three prismatic joints for the movement of the tool and three rotary joints for its orientation in space.\n To be able to move and orient the effector organ in all directions, such a robot needs 6 axes (or degrees of freedom). In a 2-dimensional environment, three axes are sufficient, two for displacement and one for orientation.[10]\n The cylindrical coordinate robots[5]  are characterized by their rotary joint at the base and at least one prismatic joint connecting its links.[6] They can move vertically and horizontally by sliding. The compact effector design allows the robot to reach tight work-spaces without any loss of speed.[6]\n Spherical coordinate robots only have rotary joints.[5] They are one of the first robots to have been used in industrial applications.[6] They are commonly used for machine tending in die-casting, plastic injection and extrusion, and for welding.[6]\n SCARA[5] is an acronym for Selective Compliance Assembly Robot Arm.[11] SCARA robots are recognized by their two parallel joints which provide movement in the X-Y plane.[5] Rotating shafts are positioned vertically at the effector. SCARA robots are used for jobs that require precise lateral movements. They are ideal for assembly applications.[6]\n Delta robots[5] are also referred to as parallel link robots.[6] They consist of parallel links connected to a common base. Delta robots are particularly useful for direct control tasks and high maneuvering operations (such as quick pick-and-place tasks). Delta robots take advantage of four bar or parallelogram linkage systems.\n Furthermore, industrial robots can have a serial or parallel architecture.\n Serial architectures a.k.a. serial manipulators are very common industrial robots; they are designed as a series of links connected by motor-actuated joints that extend from a base to an end-effector. SCARA, Stanford manipulators are typical examples of this category.\n A parallel manipulator is designed so that each chain is usually short, simple and can thus be rigid against unwanted movement, compared to a serial manipulator. Errors in one chain's positioning are averaged in conjunction with the others, rather than being cumulative. Each actuator must still move within its own degree of freedom, as for a serial robot; however in the parallel robot the off-axis flexibility of a joint is also constrained by the effect of the other chains. It is this closed-loop stiffness that makes the overall parallel manipulator stiff relative to its components, unlike the serial chain that becomes progressively less rigid with more components.\n A full parallel manipulator can move an object with up to 6 degrees of freedom (DoF), determined by 3 translation 3T and 3 rotation 3R coordinates for full 3T3R mobility. However, when a manipulation task requires less than 6 DoF, the use of lower mobility manipulators, with fewer than 6 DoF, may bring advantages in terms of simpler architecture, easier control, faster motion and lower cost.  For example, the 3 DoF Delta   robot has lower 3T mobility and has proven to be very successful for rapid pick-and-place translational positioning applications. The workspace of lower mobility manipulators may be decomposed into 'motion' and 'constraint' subspaces. For example, 3 position coordinates constitute the motion subspace of the 3 DoF Delta robot and the 3 orientation coordinates are in the constraint subspace.  The motion subspace of lower mobility manipulators may be further decomposed into independent (desired) and dependent (concomitant) subspaces: consisting of 'concomitant' or 'parasitic' motion which is undesired motion of the manipulator.[12]  The debilitating effects of concomitant motion should be mitigated or eliminated in the successful design of lower mobility manipulators.  For example, the Delta robot does not have parasitic motion since its end effector does not rotate.\n Robots exhibit varying degrees of autonomy. \nSome robots are programmed to faithfully carry out specific actions over and over again (repetitive actions) without variation and with a high degree of accuracy. These actions are determined by programmed routines that specify the direction, acceleration, velocity, deceleration, and distance of a series of coordinated motions\n Other robots are much more flexible as to the orientation of the object on which they are operating or even the task that has to be performed on the object itself, which the robot may even need to identify. For example, for more precise guidance, robots often contain machine vision sub-systems acting as their visual sensors, linked to powerful computers or controllers.[13] Artificial intelligence is becoming an increasingly important factor in the modern industrial robot.\n The earliest known industrial robot, conforming to the ISO definition was completed by \n\"Bill\" Griffith P. Taylor in 1937 and published in Meccano Magazine, March 1938.[14][15] The crane-like device was built almost entirely using Meccano parts, and powered by a single electric motor.  Five axes of movement were possible, including grab and grab rotation. Automation was achieved using punched paper tape to energise solenoids, which would facilitate the movement of the crane's control levers. The robot could stack wooden blocks in pre-programmed patterns. The number of motor revolutions required for each desired movement was first plotted on graph paper. This information was then transferred to the paper tape, which was also driven by the robot's single motor. Chris Shute built a complete replica of the robot in 1997.\n George Devol applied for the first robotics patents in 1954 (granted in 1961).  The first company to produce a robot was Unimation, founded by Devol and Joseph F. Engelberger in 1956. Unimation robots were also called programmable transfer machines since their main use at first was to transfer objects from one point to another, less than a dozen feet or so apart. They used hydraulic actuators and were programmed in joint coordinates, i.e. the angles of the various joints were stored during a teaching phase and replayed in operation.  They were accurate to within 1/10,000 of an inch[16]  (note: although accuracy is not an appropriate measure for robots, usually evaluated in terms of repeatability - see later).  Unimation later licensed their technology to Kawasaki Heavy Industries and GKN, manufacturing Unimates in Japan and England respectively.  For some time, Unimation's only competitor was Cincinnati Milacron Inc. of Ohio. This changed radically in the late 1970s when several big Japanese conglomerates began producing similar industrial robots.\n In 1969 Victor Scheinman at Stanford University invented the Stanford arm, an all-electric, 6-axis articulated robot designed to permit an arm solution. This allowed it accurately to follow arbitrary paths in space and widened the potential use of the robot to more sophisticated applications such as assembly and welding. Scheinman then designed a second arm for the MIT AI Lab, called the \"MIT arm.\" Scheinman, after receiving a fellowship from Unimation to develop his designs, sold those designs to Unimation who further developed them with support from General Motors and later marketed it as the Programmable Universal Machine for Assembly (PUMA).\n Industrial robotics took off quite quickly in Europe, with both ABB Robotics and KUKA Robotics bringing robots to the market in 1973. ABB Robotics (formerly ASEA) introduced IRB 6, among the world's first commercially available all electric micro-processor controlled robot. The first two IRB 6 robots were sold to Magnusson in Sweden for grinding and polishing pipe bends and were installed in production in January 1974. Also in 1973 KUKA Robotics built its first robot, known as FAMULUS,[17][18] also one of the first articulated robots to have six electromechanically driven axes.\n Interest in robotics increased in the late 1970s and many US companies entered the field, including large firms like General Electric, and General Motors (which formed joint venture FANUC Robotics with FANUC LTD of Japan). U.S. startup companies included Automatix and Adept Technology, Inc. At the height of the robot boom in 1984, Unimation was acquired by Westinghouse Electric Corporation for 107 million U.S. dollars. Westinghouse sold Unimation to St\u00e4ubli Faverges SCA of France in 1988, which is still making articulated robots for general industrial and cleanroom applications and even bought the robotic division of Bosch in late 2004.\n Only a few non-Japanese companies ultimately managed to survive in this market, the major ones being: Adept Technology, St\u00e4ubli, the Swedish-Swiss company ABB Asea Brown Boveri, the German company KUKA Robotics and the Italian company Comau.\n Accuracy and repeatability are different measures. Repeatability is usually the most important criterion for a robot and is similar to the concept of 'precision' in measurement\u2014see accuracy and precision. ISO 9283[19] sets out a method whereby both accuracy and repeatability can be measured. Typically a robot is sent to a taught position a number of times and the error is measured at each return to the position after visiting 4 other positions. Repeatability is then quantified using the standard deviation of those samples in all three dimensions. A typical robot can, of course make a positional error exceeding that and that could be a problem for the process. Moreover, the repeatability is different in different parts of the working envelope and also changes with speed and payload. ISO 9283 specifies that accuracy and repeatability should be measured at maximum speed and at maximum payload. But this results in pessimistic values whereas the robot could be much more accurate and repeatable at light loads and speeds.\nRepeatability in an industrial process is also subject to the accuracy of the end effector, for example a gripper, and even to the design of the 'fingers' that match the gripper to the object being grasped. For example, if a robot picks a screw by its head, the screw could be at a random angle. A subsequent attempt to insert the screw into a hole could easily fail. These and similar scenarios can be improved with 'lead-ins' e.g. by making the entrance to the hole tapered.\n The setup or programming of motions and sequences for an industrial robot is typically taught by linking the robot controller to a laptop, desktop computer or (internal or Internet) network.\n A robot and a collection of machines or peripherals is referred to as a workcell, or cell. A typical cell might contain a parts feeder, a molding machine and a robot. The various machines are 'integrated' and controlled by a single computer or PLC. How the robot interacts with other machines in the cell must be programmed, both with regard to their positions in the cell and synchronizing with them.\n Software: The computer is installed with corresponding interface software. The use of a computer greatly simplifies the programming process. Specialized robot software is run either in the robot controller or in the computer or both depending on the system design.\n There are two basic entities that need to be taught (or programmed): positional data and procedure. For example, in a task to move a screw from a feeder to a hole the positions of the feeder and the hole must first be taught or programmed. Secondly the procedure to get the screw from the feeder to the hole must be programmed along with any I/O involved, for example a signal to indicate when the screw is in the feeder ready to be picked up. The purpose of the robot software is to facilitate both these programming tasks.\n Teaching the robot positions may be achieved a number of ways:\n Positional commands The robot can be directed to the required position using a GUI or text based commands in which the required X-Y-Z position may be specified and edited.\n Teach pendant: Robot positions can be taught via a teach pendant. This is a handheld control and programming unit. The common features of such units are the ability to manually send the robot to a desired position, or \"inch\" or \"jog\" to adjust a position. They also have a means to change the speed since a low speed is usually required for careful positioning, or while test-running through a new or modified routine. A large emergency stop button is usually included as well. Typically once the robot has been programmed there is no more use for the teach pendant. All teach pendants are equipped with a 3-position deadman switch. In the manual mode, it allows the robot to move only when it is in the middle position (partially pressed). If it is fully pressed in or completely released, the robot stops. This principle of operation allows natural reflexes to be used to increase safety.\n Lead-by-the-nose: this is a technique offered by many robot manufacturers. In this method, one user holds the robot's manipulator, while another person enters a command which de-energizes the robot causing it to go into limp. The user then moves the robot by hand to the required positions and/or along a required path while the software logs these positions into memory. The program can later run the robot to these positions or along the taught path. This technique is popular for tasks such as paint spraying.\n Offline programming is where the entire cell, the robot and all the machines or instruments in the workspace are mapped graphically. The robot can then be moved on screen and the process simulated. A robotics simulator is used to create embedded applications for a robot, without depending on the physical operation of the robot arm and end effector. The advantages of robotics simulation is that it saves time in the design of robotics applications. It can also increase the level of safety associated with robotic equipment since various \"what if\" scenarios can be tried and tested before the system is activated.[8] Robot simulation software provides a platform to teach, test, run, and debug programs that have been written in a variety of programming languages.  Robot simulation tools allow for robotics programs to be conveniently written and debugged off-line with the final version of the program tested on an actual robot. The ability to preview the behavior of a robotic system in a virtual world allows for a variety of mechanisms, devices, configurations and controllers to be tried and tested before being applied to a \"real world\" system. Robotics simulators have the ability to provide real-time computing of the simulated motion of an industrial robot using both geometric modeling and kinematics modeling.\n Manufacturing independent robot programming tools are a relatively new but flexible way to program robot applications. Using a visual programming language, the programming is done via drag and drop of predefined template/building blocks. They often feature the execution of simulations to evaluate the feasibility and offline programming in combination. If the system is able to compile and upload native robot code to the robot controller, the user no longer has to learn each manufacturer's proprietary language. Therefore, this approach can be an important step to standardize programming methods.\n Others in addition, machine operators often use user interface devices, typically touchscreen units, which serve as the operator control panel. The operator can switch from program to program, make adjustments within a program and also operate a host of peripheral devices that may be integrated within the same robotic system. These include end effectors, feeders that supply components to the robot, conveyor belts, emergency stop controls, machine vision systems, safety interlock systems, barcode printers and an almost infinite array of other industrial devices which are accessed and controlled via the operator control panel.\n The teach pendant or PC is usually disconnected after programming and the robot then runs on the program that has been installed in its controller. However a computer is often used to 'supervise' the robot and any peripherals, or to provide additional storage for access to numerous complex paths and routines.\n The most essential robot peripheral is the end effector, or end-of-arm-tooling (EOAT). Common examples of end effectors include welding devices (such as MIG-welding guns, spot-welders, etc.), spray guns and also grinding and deburring devices (such as pneumatic disk or belt grinders, burrs, etc.), and grippers (devices that can grasp an object, usually electromechanical or pneumatic). Other common means of picking up objects is by vacuum or magnets. End effectors are frequently highly complex, made to match the handled product and often capable of picking up an array of products at one time.  They may utilize various sensors to aid the robot system in locating, handling, and positioning products.\n For a given robot the only parameters necessary to completely locate the end effector (gripper, welding torch, etc.) of the robot are the angles of each of the joints or displacements of the linear axes (or combinations of the two for robot formats such as SCARA). However, there are many different ways to define the points. The most common and most convenient way of defining a point is to specify a Cartesian coordinate for it, i.e. the position of the 'end effector' in mm in the X, Y and Z directions relative to the robot's origin. In addition, depending on the types of joints a particular robot may have, the orientation of the end effector in yaw, pitch, and roll and the location of the tool point relative to the robot's faceplate must also be specified. For a jointed arm these coordinates must be converted to joint angles by the robot controller and such conversions are known as Cartesian Transformations which may need to be performed iteratively or recursively for a multiple axis robot. The mathematics of the relationship between joint angles and actual spatial coordinates is called kinematics. See robot control\n Positioning by Cartesian coordinates may be done by entering the coordinates into the system or by using a teach pendant which moves the robot in X-Y-Z directions. It is much easier for a human operator to visualize motions up/down, left/right, etc. than to move each joint one at a time. When the desired position is reached it is then defined in some way particular to the robot software in use, e.g. P1 - P5 below.\n Most articulated robots perform by storing a series of positions in memory, and moving to them at various times in their programming sequence. For example, a robot which is moving items from one place (bin A) to another (bin B) might have a simple 'pick and place' program similar to the following:\n Define points P1\u2013P5:\n Define program:\n For examples of how this would look in popular robot languages see industrial robot programming.\n The American National Standard for Industrial Robots and Robot Systems \u2014 Safety Requirements (ANSI/RIA R15.06-1999) defines a singularity as \"a condition caused by the collinear alignment of two or more robot axes resulting in unpredictable robot motion and velocities.\" It is most common in robot arms that utilize a \"triple-roll wrist\". This is a wrist about which the three axes of the wrist, controlling yaw, pitch, and roll, all pass through a common point. An example of a wrist singularity is when the path through which the robot is traveling causes the first and third axes of the robot's wrist (i.e. robot's axes 4 and 6) to line up. The second wrist axis then attempts to spin 180\u00b0 in zero time to maintain the orientation of the end effector. Another common term for this singularity is a \"wrist flip\". The result of a singularity can be quite dramatic and can have adverse effects on the robot arm, the end effector, and the process. Some industrial robot manufacturers have attempted to side-step the situation by slightly altering the robot's path to prevent this condition. Another method is to slow the robot's travel speed, thus reducing the speed required for the wrist to make the transition. The ANSI/RIA has mandated that robot manufacturers shall make the user aware of singularities if they occur while the system is being manually manipulated.\n A second type of singularity in wrist-partitioned vertically articulated six-axis robots occurs when the wrist center lies on a cylinder that is centered about axis 1 and with radius equal to the distance between axes 1 and 4. This is called a shoulder singularity. Some robot manufacturers also mention alignment singularities, where axes 1 and 6 become coincident. This is simply a sub-case of shoulder singularities. When the robot passes close to a shoulder singularity, joint 1 spins very fast.\n The third and last type of singularity in wrist-partitioned vertically articulated six-axis robots occurs when the wrist's center lies in the same plane as axes 2 and 3.\n Singularities are closely related to the phenomena of gimbal lock, which has a similar root cause of axes becoming lined up.\n According to the International Federation of Robotics (IFR) study World Robotics 2024, there were about 4,281,585 operational industrial robots by the end of 2023.[3][4] For the year 2018 the IFR estimates the worldwide sales of industrial robots with US$16.5 billion. Including the cost of software, peripherals and systems engineering, the annual turnover for robot systems is estimated to be US$48.0 billion in 2018.[20]\n China is the largest industrial robot market[21]:\u200a256\u200a with 154,032 units sold in 2018.[20] China had the largest operational stock of industrial robots, with 649,447 at the end of 2018.[22] The United States industrial robot-makers shipped 35,880 robot to factories in the US in 2018 and this was 7% more than in 2017.[23]\n The biggest customer of industrial robots is automotive industry with 30% market share, then electrical/electronics industry with 25%, metal and machinery industry with 10%, rubber and plastics industry with 5%, food industry with 5%.[20] In textiles, apparel and leather industry, 1,580 units are operational.[24]\n Estimated worldwide annual supply of industrial robots (in units):[3][4][25]\n The International Federation of Robotics has predicted a worldwide increase in adoption of industrial robots and they estimated 1.7 million new robot installations in factories worldwide by 2020 [IFR 2017] Archived 2017-02-11 at the Wayback Machine. Rapid advances in automation technologies (e.g. fixed robots, collaborative and mobile robots, and exoskeletons) have the potential to improve work conditions but also to introduce workplace hazards in manufacturing workplaces.[26] [3]   Despite the lack of occupational surveillance data on injuries associated specifically with robots, researchers from the US National Institute for Occupational Safety and Health (NIOSH) identified 61 robot-related deaths between 1992 and 2015 using keyword searches of the Bureau of Labor Statistics (BLS) Census of Fatal Occupational Injuries research database (see info from Center for Occupational Robotics Research). Using data from the Bureau of Labor Statistics, NIOSH and its state partners have investigated 4 robot-related fatalities under the Fatality Assessment and Control Evaluation Program.  In addition the Occupational Safety and Health Administration (OSHA) has investigated dozens of robot-related deaths and injuries, which can be reviewed at OSHA Accident Search page. Injuries and fatalities could increase over time because of the increasing number of collaborative and co-existing robots, powered exoskeletons, and autonomous vehicles into the work environment.\n Safety standards are being developed by the Robotic Industries Association (RIA) in conjunction with the American National Standards Institute (ANSI).[4] On October 5, 2017, OSHA, NIOSH and RIA signed an alliance to work together to enhance technical expertise, identify and help address potential workplace hazards associated with traditional industrial robots and the emerging technology of human-robot collaboration installations and systems, and help identify needed research to reduce workplace hazards. On October 16 NIOSH launched the Center for Occupational Robotics Research to \"provide scientific leadership to guide the development and use of occupational robots that enhance worker safety, health, and wellbeing.\"  So far, the research needs identified by NIOSH and its partners include: tracking and preventing injuries and fatalities, intervention and dissemination strategies to promote safe machine control and maintenance procedures, and on translating effective evidence-based interventions into workplace practice.\n",
        "doc_number": 57
    },
    {
        "url": "https://en.wikipedia.org/wiki/Service_robot",
        "content": "\n Service robots assist human beings, typically by performing a job that is dirty, dull, distant, dangerous or repetitive. They typically are autonomous and/or operated by a built-in control system, with manual override options.\nThe term \"service robot\" does not have a strict technical definition. The International Organization for Standardization defines a \u201cservice robot\u201d as a robot \u201cthat performs useful tasks for humans or equipment excluding industrial automation applications\u201d.[1]\n The first industrial robot arm, \"Unimate,\" was developed by Joseph F. Engelberger, known as the \"father of the robot arm,\" using George Devel.[2]\n According to ISO 8373 robots require \u201ca degree of autonomy\u201d, which is the \u201cability to perform intended tasks based on current state and sensing, without human intervention\u201d. For service robots this ranges from partial autonomy - including human-robot interaction - to full autonomy - without active human robot intervention. The International Federation of Robotics (IFR) statistics for service robots therefore include systems based on some degree of human robot interaction or even full tele-operation as well as fully autonomous systems.\n Service robots are categorized according to personal or professional use. They have many forms and structures as well as application areas.\n The possible applications of robots to assist in human chores is widespread. At present there are a few main categories that these robots fall into.\n Industrial service robots can be used to carry out simple tasks, such as examining welding, as well as more complex, harsh-environment tasks, such as aiding in the dismantling of nuclear power stations. Industrial robots have been defined by the International Federation of Robotics as \"an automatically controlled, reprogrammable, multipurpose manipulator programmable in three or more axes, which may be either fixed in place or mobile for use in industrial automation applications\".[3]\n Service robots are system-based autonomous and adaptable interfaces that interact, communicate and deliver service to an organization's customers.[4]\n Domestic robots perform tasks that humans regularly perform in non-industrial environments, like people's homes such as for cleaning floors, mowing the lawn and pool maintenance.[5] People with disabilities, as well as people who are older, may soon be able to use service robots to help them live independently.[6] It is also possible to use certain robots as assistants or butlers[citation needed].\n Robotic systems perform many functions such as repetitive tasks performed in research. These range from the multiple repetitive tasks made by gene samplers and sequencers, to systems which can almost replace the scientist in designing and running experiments, analysing data and even forming hypotheses.\n Autonomous scientific robots perform tasks which humans would find difficult or impossible, from the deep sea to outer space. The Woods Hole Sentry can descend to 4,500 metres and allows a higher payload as it does not need a support ship or the oxygen and other facilities demanded by human piloted vessels.[7] Robots in space include the Mars rovers which could carry out sampling and photography in the harsh environment of the atmosphere on Mars.\n",
        "doc_number": 58
    },
    {
        "url": "https://en.wikipedia.org/wiki/Swarm_robotics",
        "content": "Swarm robotics is the study of how to design independent systems of robots without centralized control. The emerging swarming behavior of robotic swarms is created through the interactions between individual robots and the environment.[1] This idea emerged on the field of artificial swarm intelligence, as well as the studies of insects, ants and other fields in nature, where swarm behavior occurs.[2]\n Relatively simple individual rules can produce a large set of complex swarm behaviors. A key component is the communication between the members of the group that build a system of constant feedback. The swarm behavior involves constant change of individuals in cooperation with others, as well as the behavior of the whole group.\n \nThe design of swarm robotics systems is guided by swarm intelligence principles, which promote fault tolerance, scalability, and flexibility.[1] Unlike distributed robotic systems in general, swarm robotics emphasizes a large number of robots. While various formulations of swarm intelligence principles exist, one widely recognized set includes: Miniaturization is also key factor in swarm robotics, as the effect of thousands of small robots can maximize the effect of the swarm-intelligent approach to achieve meaningful behavior at swarm-level through a greater number of interactions on an individual level.[5]\n Compared with individual robots, a swarm can commonly decompose its given missions to their subtasks;[6] a swarm is more robust to partial failure and is more flexible with regard to different missions.[7]\n The phrase \"swarm robotics\" was reported to make its first appearance in 1991 according to Google Scholar, but research regarding swarm robotics began to grow in early 2000s. The initial goal of studying swarm robotics was to test whether the concept of stigmergy could be used as a method for robots to indirectly communication and coordinate with each other.[5]\n One of the first international projects regarding swarm robotics was the SWARM-BOTS project funded by the European Commission between 2001 and 2005, in which a swarm of up to 20 of robots capable of independently physically connect to each other to form a cooperating system were used to study swarm behaviors such as collective transport, area coverage, and searching for objects. The result was demonstration of self-organized teams of robots that cooperate to solve a complex task, with the robots in the swarm taking different roles over time. This work was then expanded upon through the Swarmanoid project (2006\u20132010), which extended the ideas and algorithms developed in Swarm-bots to heterogeneous robot swarms composed of three types of robots\u2014flying, climbing, and ground-based\u2014that collaborated to carry out a search and retrieval task.[5]\n There are many potential applications for swarm robotics.[8]  They include tasks that demand miniaturization (nanorobotics, microbotics), like distributed sensing tasks in micromachinery or the human body. A promising use of swarm robotics is in search and rescue missions.[9]  Swarms of robots of different sizes could be sent to places that rescue-workers cannot reach safely, to explore the unknown environment and solve complex mazes via onboard sensors.[9] Swarm robotics can also be suited to tasks that demand cheap designs, for instance mining or agricultural shepherding tasks.[10]\n Drone swarms are used in target search, drone displays, and delivery. A drone display commonly uses multiple, lighted drones at night for an artistic display or advertising. A delivery drone swarm can carry multiple packages to a single destination at a time and overcome a single drone's payload and battery limitations.[11] A drone swarm may undertake different flight formations to reduce overall energy consumption due to drag forces.[12]\n Drone swarming can also introduce additional control issues connected to human factors and the swarm operator. Examples of this include high cognitive demand and complexity when interacting with multiple drones due to changing attention between different individual drones.[13][14] Communication between operator and swarm is also a central aspect.[15]\n More controversially, swarms of military robots can form an autonomous army.   U.S. Naval forces have tested a swarm of autonomous boats that can steer and take offensive actions by themselves. The boats are unmanned and can be fitted with any kind of kit to deter and destroy enemy vessels.[16]\n During the Syrian Civil War, Russian forces in the region reported attacks on their main air force base in the country by swarms of fixed-wing drones loaded with explosives.[17]\n Another large set of applications may be solved using swarms of micro air vehicles, which are also broadly investigated nowadays.  In comparison with the pioneering studies of swarms of flying robots using precise motion capture systems in laboratory conditions,[18] current systems such as Shooting Star can control teams of hundreds of micro aerial vehicles in outdoor environment[19] using GNSS systems (such as GPS) or even  stabilize them using onboard localization systems[20] where GPS is unavailable.[21][22]  Swarms of micro aerial vehicles have been already tested in tasks of autonomous surveillance,[23] plume tracking,[24] and reconnaissance in a compact phalanx.[25]  Numerous works on cooperative swarms of unmanned ground and aerial vehicles have been conducted with target applications of cooperative environment monitoring,[26] simultaneous localization and mapping,[27] convoy protection,[28] and moving target localization and tracking.[29]\n In 2023, University of Washington and Microsoft researchers demonstrated acoustic swarms of tiny robots that create shape-changing smart speakers.[30] These can be used for manipulating acoustic scenes to focus on or mute sounds from a specific region in a room.[31]  Here, tiny robots cooperate with each other using sound signals, without any cameras, to navigate cooperatively with centimeter-level accuracy. These swarm devices  spread out across a surface to create a distributed and reconfigurable wireless microphone array. They also navigate back to the charging station where they can be automatically recharged.[32]\n Most efforts have focused on relatively small groups of machines. However, a Kilobot swarm consisting of 1,024 individual robots was demonstrated by Harvard in 2014, the largest to date.[33]\n Another example of miniaturization is the LIBOT Robotic System[34] that involves a low cost robot built for outdoor swarm robotics. The robots are also made with provisions for indoor use via Wi-Fi, since the GPS sensors provide poor communication inside buildings.  \n Another such attempt is the micro robot (Colias),[35] built in the Computer Intelligence Lab at the University of Lincoln, UK. This micro robot is built on a 4 cm circular chassis and is a low-cost and open platform for use in a variety of swarm robotics applications.\n Additionally, progress has been made in the application of autonomous swarms in the field of manufacturing, known as swarm 3D printing. This is particularly useful for the production of large structures and components, where traditional 3D printing is not able to be utilized due to hardware size constraints. Miniaturization and mass mobilization allows the manufacturing system to achieve scale invariance, not limited in effective build volume. While in its early stage of development, swarm 3D printing is currently being commercialized by startup companies.[36]\n",
        "doc_number": 59
    },
    {
        "url": "https://en.wikipedia.org/wiki/Evolutionary_robotics",
        "content": "Evolutionary robotics is an embodied approach to Artificial Intelligence (AI) in which robots are automatically designed using Darwinian principles of natural selection.[1] The design of a robot, or a subsystem of a robot such as a neural controller, is optimized against a behavioral goal (e.g. run as fast as possible). Usually, designs are evaluated in simulations as fabricating thousands or millions of designs and testing them in the real world is prohibitively expensive in terms of time, money, and safety.\n An evolutionary robotics experiment starts with a population of randomly generated robot designs. The worst performing designs are discarded and replaced with mutations and/or combinations of the better designs. This evolutionary algorithm continues until a prespecified amount of time elapses or some target performance metric is surpassed.\n Evolutionary robotics methods are particularly useful for engineering machines that must operate in environments in which humans have limited intuition (nanoscale, space, etc.). Evolved simulated robots can also be used as scientific tools to generate new hypotheses in biology and cognitive science, and to test old hypothesis that require experiments that have proven difficult or impossible to carry out in reality.\n In the early 1990s, two separate European groups demonstrated different approaches to the evolution of robot control systems. Dario Floreano and Francesco Mondada at EPFL evolved controllers for the Khepera robot.[2] Adrian Thompson, Nick Jakobi, Dave Cliff, Inman Harvey, and Phil Husbands evolved controllers for a Gantry robot at the University of Sussex.[3][4] \nHowever the body of these robots was presupposed before evolution.\n The first simulations of evolved robots were reported by Karl Sims and Jeffrey Ventrella of the MIT Media Lab, also in the early 1990s.[5][6] However these so-called virtual creatures never left their simulated worlds. The first evolved robots to be built in reality were 3D-printed by Hod Lipson and Jordan Pollack at Brandeis University at the turn of the 21st century.[7]\n",
        "doc_number": 60
    },
    {
        "url": "https://en.wikipedia.org/wiki/Affective_computing",
        "content": "Affective computing is the study and development of systems and devices that can recognize, interpret, process, and simulate human affects. It is an interdisciplinary field spanning computer science, psychology, and cognitive science.[1] While some core ideas in the field may be traced as far back as to early philosophical inquiries into emotion,[2] the more modern branch of computer science originated with Rosalind Picard's 1995 paper entitled \"Affective Computing\"[3] and her 1997 book of the same name[4] published by MIT Press.[5][6] One of the motivations for the research is the ability to give machines emotional intelligence, including to simulate empathy. The machine should interpret the emotional state of humans and adapt its behavior to them, giving an appropriate response to those emotions.\n Detecting emotional information usually begins with passive sensors that capture data about the user's physical state or behavior without interpreting the input. The data gathered is analogous to the cues humans use to perceive emotions in others. For example, a video camera might capture facial expressions, body posture, and gestures, while a microphone might capture speech. Other sensors detect emotional cues by directly measuring physiological data, such as skin temperature and galvanic resistance.[7]\n Recognizing emotional information requires the extraction of meaningful patterns from the gathered data. This is done using machine learning techniques that process different modalities, such as speech recognition, natural language processing, or facial expression detection.  The goal of most of these techniques is to produce labels that would match the labels a human perceiver would give in the same situation:  For example, if a person makes a facial expression furrowing their brow, then the computer vision system might be taught to label their face as appearing \"confused\" or as \"concentrating\" or \"slightly negative\" (as opposed to positive, which it might say if they were smiling in a happy-appearing way).  These labels may or may not correspond to what the person is actually feeling.\n Another area within affective computing is the design of computational devices proposed to exhibit either innate emotional capabilities or that are capable of convincingly simulating emotions. A more practical approach, based on current technological capabilities, is the simulation of emotions in conversational agents in order to enrich and facilitate interactivity between human and machine.[8]\n Marvin Minsky, one of the pioneering computer scientists in artificial intelligence, relates emotions to the broader issues of machine intelligence stating in The Emotion Machine that emotion is \"not especially different from the processes that we call 'thinking.'\"[9] The innovative approach \"digital humans\" or virtual humans includes an attempt to give these programs, which simulate humans, the emotional dimension as well, including reactions in accordance with the reaction that a real person would react in a certain emotionally stimulating situation as well as facial expressions and gestures.[10]\n Emotion in machines often refers to emotion in computational, often AI-based, systems. As a result, the terms 'emotional AI' and 'emotion AI' are being used.[11]\n In psychology, cognitive science, and in neuroscience, there have been two main approaches for describing how humans perceive and classify emotion: continuous or categorical. The continuous approach tends to use dimensions such as negative vs. positive, calm vs. aroused.\n The categorical approach tends to use discrete classes such as happy, sad, angry, fearful, surprise, disgust.  Different kinds of machine learning regression and classification models can be used for having machines produce continuous or discrete labels.  Sometimes models are also built that allow combinations across the categories, e.g. a happy-surprised face or a fearful-surprised face.[12]\n The following sections consider many of the kinds of input data used for the task of emotion recognition.\n Various changes in the autonomic nervous system can indirectly alter a person's speech, and affective technologies can leverage this information to recognize emotion. For example, speech produced in a state of fear, anger, or joy becomes fast, loud, and precisely enunciated, with a higher and wider range in pitch, whereas emotions such as tiredness, boredom, or sadness tend to generate slow, low-pitched, and slurred speech.[13] Some emotions have been found to be more easily computationally identified, such as anger[14] or approval.[15]\n Emotional speech processing technologies recognize the user's emotional state using computational analysis of speech features. Vocal parameters and prosodic features such as pitch variables and speech rate can be analyzed through pattern recognition techniques.[14][16]\n Speech analysis is an effective method of identifying affective state, having an average reported accuracy of 70 to 80% in research from 2003 and 2006.[17][18] These systems tend to outperform average human accuracy (approximately 60%[14]) but are less accurate than systems which employ other modalities for emotion detection, such as physiological states or facial expressions.[19] However, since many speech characteristics are independent of semantics or culture, this technique is considered to be a promising route for further research.[20]\n The process of speech/text affect detection requires the creation of a reliable database, knowledge base, or vector space model,[21] broad enough to fit every need for its application, as well as the selection of a successful classifier which will allow for quick and accurate emotion identification.\n As of 2010[update], the most frequently used classifiers were linear discriminant classifiers (LDC), k-nearest neighbor (k-NN), Gaussian mixture model (GMM), support vector machines (SVM), artificial neural networks (ANN), decision tree algorithms and hidden Markov models (HMMs).[22] Various studies showed that choosing the appropriate classifier can significantly enhance the overall performance of the system.[19] The list below gives a brief description of each algorithm:\n It is proved that having enough acoustic evidence available the emotional state of a person can be classified by a set of majority voting classifiers. The proposed set of classifiers is based on three main classifiers: kNN, C4.5 and SVM-RBF Kernel. This set achieves better performance than each basic classifier taken separately. It is compared with two other sets of classifiers: one-against-all (OAA) multiclass SVM with Hybrid kernels and the set of classifiers which consists of the following two basic classifiers: C5.0 and Neural Network. The proposed variant achieves better performance than the other two sets of classifiers.[24]\n The vast majority of present systems are data-dependent. This creates one of the biggest challenges in detecting emotions based on speech, as it implicates choosing an appropriate database used to train the classifier. Most of the currently possessed data was obtained from actors and is thus a representation of archetypal emotions. Those so-called acted databases are usually based on the Basic Emotions theory (by Paul Ekman), which assumes the existence of six basic emotions (anger, fear, disgust, surprise, joy, sadness), the others simply being a mix of the former ones.[25] Nevertheless, these still offer high audio quality and balanced classes (although often too few), which contribute to high success rates in recognizing emotions.\n However, for real life application, naturalistic data is preferred. A naturalistic database can be produced by observation and analysis of subjects in their natural context. Ultimately, such database should allow the system to recognize emotions based on their context as well as work out the goals and outcomes of the interaction. The nature of this type of data allows for authentic real life implementation, due to the fact it describes states naturally occurring during the human\u2013computer interaction (HCI).\n Despite the numerous advantages which naturalistic data has over acted data, it is difficult to obtain and usually has low emotional intensity. Moreover, data obtained in a natural context has lower signal quality, due to surroundings noise and distance of the subjects from the microphone. The first attempt to produce such database was the FAU Aibo Emotion Corpus for CEICES (Combining Efforts for Improving Automatic Classification of Emotional User States), which was developed based on a realistic context of children (age 10\u201313) playing with Sony's Aibo robot pet.[26][27] Likewise, producing one standard database for all emotional research would provide a method of evaluating and comparing different affect recognition systems.\n The complexity of the affect recognition process increases with the number of classes (affects) and speech descriptors used within the classifier. It is, therefore, crucial to select only the most relevant features in order to assure the ability of the model to successfully identify emotions, as well as increasing the performance, which is particularly significant to real-time detection. The range of possible choices is vast, with some studies mentioning the use of over 200 distinct features.[22] It is crucial to identify those that are redundant and undesirable in order to optimize the system and increase the success rate of correct emotion detection. The most common speech characteristics are categorized into the following groups.[26][27]\n The detection and processing of facial expression are achieved through various methods such as optical flow, hidden Markov models, neural network processing or active appearance models. More than one modality can be combined or fused (multimodal recognition, e.g. facial expressions and speech prosody,[29] facial expressions and hand gestures,[30] or facial expressions with speech and text for multimodal data and metadata analysis) to provide a more robust estimation of the subject's emotional state. \n Creation of an emotion database is a difficult and time-consuming task. However, database creation is an essential step in the creation of a system that will recognize human emotions. Most of the publicly available emotion databases include posed facial expressions only. In posed expression databases, the participants are asked to display different basic emotional expressions, while in spontaneous expression database, the expressions are natural. Spontaneous emotion elicitation requires significant effort in the selection of proper stimuli which can lead to a rich display of intended emotions. Secondly, the process involves tagging of emotions by trained individuals manually which makes the databases highly reliable. Since perception of expressions and their intensity is subjective in nature, the annotation by experts is essential for the purpose of validation.\n Researchers work with three types of databases, such as a database of peak expression images only, a database of image sequences portraying an emotion from neutral to its peak, and video clips with emotional annotations. Many facial expression databases have been created and made public for expression recognition purpose. Two of the widely used databases are CK+ and JAFFE.\n By doing cross-cultural research in Papua, New Guinea, on the Fore Tribesmen, at the end of the 1960s, Paul Ekman proposed the idea that facial expressions of emotion are not culturally determined, but universal. Thus, he suggested that they are biological in origin and can, therefore, be safely and correctly categorized.[25]\nHe therefore officially put forth six basic emotions, in 1972:[31]\n However, in the 1990s Ekman expanded his list of basic emotions, including a range of positive and negative emotions not all of which are encoded in facial muscles.[32] The newly included emotions are:\n A system has been conceived by psychologists in order to formally categorize the physical expression of emotions on faces. The central concept of the Facial Action Coding System, or FACS, as created by Paul Ekman and Wallace V. Friesen in 1978 based on earlier work by Carl-Herman Hjortsj\u00f6[33] are action units (AU).\nThey are, basically, a contraction or a relaxation of one or more muscles. Psychologists have proposed the following classification of six basic emotions, according to their action units (\"+\" here mean \"and\"):\n As with every computational practice, in affect detection by facial processing, some obstacles need to be surpassed, in order to fully unlock the hidden potential of the overall algorithm or method employed. In the early days of almost every kind of AI-based detection (speech recognition, face recognition, affect recognition), the accuracy of modeling and tracking has been an issue. As hardware evolves, as more data are collected and as new discoveries are made and new practices introduced, this lack of accuracy fades, leaving behind noise issues. However, methods for noise removal exist including neighborhood averaging, linear Gaussian smoothing, median filtering,[34] or newer methods such as the Bacterial Foraging Optimization Algorithm.[35][36]\n Other challenges include\n Gestures could be efficiently used as a means of detecting a particular emotional state of the user, especially when used in conjunction with speech and face recognition. Depending on the specific action, gestures could be simple reflexive responses, like lifting your shoulders when you don't know the answer to a question, or they could be complex and meaningful as when communicating with sign language. Without making use of any object or surrounding environment, we can wave our hands, clap or beckon. On the other hand, when using objects, we can point at them, move, touch or handle these. A computer should be able to recognize these, analyze the context and respond in a meaningful way, in order to be efficiently used for Human\u2013Computer Interaction.\n There are many proposed methods[38] to detect the body gesture. Some literature differentiates 2 different approaches in gesture recognition: a 3D model based and an appearance-based.[39] The foremost method makes use of 3D information of key elements of the body parts in order to obtain several important parameters, like palm position or joint angles. On the other hand, appearance-based systems use images or videos to for direct interpretation. Hand gestures have been a common focus of body gesture detection methods.[39]\n This could be used to detect a user's affective state by monitoring and analyzing their physiological signs. These signs range from changes in heart rate and skin conductance to minute contractions of the facial muscles and changes in facial blood flow. This area is gaining momentum and we are now seeing real products that implement the techniques. The four main physiological signs that are usually analyzed are blood volume pulse, galvanic skin response, facial electromyography, and facial color patterns.\n A subject's blood volume pulse (BVP) can be measured by a process called photoplethysmography, which produces a graph indicating blood flow through the extremities.[40] The peaks of the waves indicate a cardiac cycle where the heart has pumped blood to the extremities. If the subject experiences fear or is startled, their heart usually 'jumps' and beats quickly for some time, causing the amplitude of the cardiac cycle to increase. This can clearly be seen on a photoplethysmograph when the distance between the trough and the peak of the wave has decreased. As the subject calms down, and as the body's inner core expands, allowing more blood to flow back to the extremities, the cycle will return to normal.\n Infra-red light is shone on the skin by special sensor hardware, and the amount of light reflected is measured. The amount of reflected and transmitted light correlates to the BVP as light is absorbed by hemoglobin which is found richly in the bloodstream.\n It can be cumbersome to ensure that the sensor shining an infra-red light and monitoring the reflected light is always pointing at the same extremity, especially seeing as subjects often stretch and readjust their position while using a computer.\nThere are other factors that can affect one's blood volume pulse. As it is a measure of blood flow through the extremities, if the subject feels hot, or particularly cold, then their body may allow more, or less, blood to flow to the extremities, all of this regardless of the subject's emotional state.\n Facial electromyography is a technique used to measure the electrical activity of the facial muscles by amplifying the tiny electrical impulses that are generated by muscle fibers when they contract.[41]\nThe face expresses a great deal of emotion, however, there are two main facial muscle groups that are usually studied to detect emotion:\nThe corrugator supercilii muscle, also known as the 'frowning' muscle, draws the brow down into a frown, and therefore is the best test for negative, unpleasant emotional response.\u21b5The zygomaticus major muscle is responsible for pulling the corners of the mouth back when you smile, and therefore is the muscle used to test for a positive emotional response.\n Galvanic skin response (GSR) is an outdated term for a more general phenomenon known as [Electrodermal Activity] or EDA.  EDA is a general phenomena whereby the skin's electrical properties change.  The skin is innervated by the [sympathetic nervous system], so measuring its resistance or conductance provides a way to quantify small changes in the sympathetic branch of the autonomic nervous system.  As the sweat glands are activated, even before the skin feels sweaty, the level of the EDA can be captured (usually using conductance) and used to discern small changes in autonomic arousal.  The more aroused a subject is, the greater the skin conductance tends to be.[40]\n Skin conductance is often measured using two small silver-silver chloride electrodes placed somewhere on the skin and applying a small voltage between them. To maximize comfort and reduce irritation the electrodes can be placed on the wrist, legs, or feet, which leaves the hands fully free for daily activity.\n The surface of the human face is innervated with a large network of blood vessels. Blood flow variations in these vessels yield visible color changes on the face. Whether or not facial emotions activate facial muscles, variations in blood flow, blood pressure, glucose levels, and other changes occur. Also, the facial color signal is independent from that provided by facial muscle movements.[42]\n Approaches are based on facial color changes. Delaunay triangulation is used to create the triangular local areas. Some of these triangles which define the interior of the mouth and eyes (sclera and iris) are removed. Use the left triangular areas\u2019 pixels to create feature vectors.[42] It shows that converting the pixel color of the standard RGB color space to a color space such as oRGB color space[43] or LMS channels perform better when dealing with faces.[44] So, map the above vector onto the better color space and decompose into red-green and yellow-blue channels. Then use deep learning methods to find equivalent emotions.\n Aesthetics, in the world of art and photography, refers to the principles of the nature and appreciation of beauty. Judging beauty and other aesthetic qualities is a highly subjective task. Computer scientists at Penn State treat the challenge of automatically inferring the aesthetic quality of pictures using their visual content as a machine learning problem, with a peer-rated on-line photo sharing website as a data source.[45] They extract certain visual features based on the intuition that they can discriminate between aesthetically pleasing and displeasing images.\n Affection influences learners' learning state. Using affective computing technology, computers can judge the learners' affection and learning state by recognizing their facial expressions. In education, the teacher can use the analysis result to understand the student's learning and accepting ability, and then formulate reasonable teaching plans. At the same time, they can pay attention to students' inner feelings, which is helpful to students' psychological health. Especially in distance education, due to the separation of time and space, there is no emotional incentive between teachers and students for two-way communication. Without the atmosphere brought by traditional classroom learning, students are easily bored, and affect the learning effect. Applying affective computing in distance education system can effectively improve this situation.\n[46]\n The applications of sensory computing may contribute to improving road safety. For example, a car can monitor the emotion of all occupants and engage in additional safety measures, such as alerting other vehicles if it detects the driver to be angry.[47] In addition, affective computing systems for monitoring the driver's stress may allow various interventions such as driver assistance systems adjusted according to the stress level[48] and minimal and direct interventions to change the emotional state of the driver.[49]\n Social robots, as well as a growing number of robots used in health care benefit from emotional awareness because they can better judge users' and patient's emotional states and alter their actions/programming appropriately. This is especially important in those countries with growing aging populations and/or a lack of younger workers to address their needs.[50]\n Affective computing is also being applied to the development of communicative technologies for use by people with autism.[51] The affective component of a text is also increasingly gaining attention, particularly its role in the so-called emotional or emotive Internet.[52]\n Affective video games can access their players' emotional states through biofeedback devices.[53] A particularly simple form of biofeedback is available through gamepads that measure the pressure with which a button is pressed: this has been shown to correlate strongly with the players' level of arousal;[54] at the other end of the scale are brain\u2013computer interfaces.[55][56] Affective games have been used in medical research to support the emotional development of autistic children.[57]\n Training methods of psychomotor operations such as steering and maneuvering are used in various fields such as aviation, transportation and medicine. Integrating affective computing capabilities in this type of training systems, in accordance with the adaptive automation approach, has been found to be effective in improving the quality of training and shortening the required training duration.[58]\n Affective computing has potential applications in human\u2013computer interaction, such as affective mirrors allowing the user to see how he or she performs; emotion monitoring agents sending a warning before one sends an angry email; or even music players selecting tracks based on mood.[59]\n One idea put forth by the Romanian researcher Dr. Nicu Sebe in an interview is the analysis of a person's face while they are using a certain product (he mentioned ice cream as an example).[60] Companies would then be able to use such analysis to infer whether their product will or will not be well received by the respective market.\n One could also use affective state recognition in order to judge the impact of a TV advertisement through a real-time video recording of that person and through the subsequent study of his or her facial expression. Averaging the results obtained on a large group of subjects, one can tell whether that commercial (or movie) has the desired effect and what the elements which interest the watcher most are.\n Within the field of human\u2013computer interaction, Rosalind Picard's cognitivist or \"information model\" concept of emotion has been criticized by and contrasted with the \"post-cognitivist\" or \"interactional\" pragmatist approach taken by Kirsten Boehner and others which views emotion as inherently social.[61]\n Picard's focus is human\u2013computer interaction, and her goal for affective computing is to \"give computers the ability to recognize, express, and in some cases, 'have' emotions\".[4] In contrast, the interactional approach seeks to help \"people to understand and experience their own emotions\"[62] and to improve computer-mediated interpersonal communication.  It does not necessarily seek to map emotion into an objective mathematical model for machine interpretation, but rather let humans make sense of each other's emotional expressions in open-ended ways that might be ambiguous, subjective, and sensitive to context.[62]:\u200a284\u200a[example needed]\n Picard's critics describe her concept of emotion as \"objective, internal, private, and mechanistic\". They say it reduces emotion to a discrete psychological signal occurring inside the body that can be measured and which is an input to cognition, undercutting the complexity of emotional experience.[62]:\u200a280\u200a[62]:\u200a278\u200a\n The interactional approach asserts that though emotion has biophysical aspects, it is \"culturally grounded, dynamically experienced, and to some degree constructed in action and interaction\".[62]:\u200a276\u200a Put another way, it considers \"emotion as a social and cultural product experienced through our interactions\".[63][62][64]\n",
        "doc_number": 61
    },
    {
        "url": "https://en.wikipedia.org/wiki/Ambient_intelligence",
        "content": "Ambient intelligence (AmI) refers to environments with electronic devices that are aware of and can recognize the presence of human beings and adapt accordingly. This concept encompasses various technologies in consumer electronics, telecommunications, and computing. Its primary purpose is to enhance user interactions through context-aware systems.\n AmI aims to create environments where devices communicate seamlessly with users, leveraging data from interconnected systems. A common example of Aml is the Internet of Things (IoT), which integrates everyday devices into networks that provide intelligent responses based on user behavior.[1]\n The term \u201cambient intelligence\u201d was coined in the late 1990s by Eli Zelkha and his team at Palo Alto Ventures. The project envisioned a future where technology would seamlessly blend with daily life.[2][3][4][5] In the early 2000s, the concept gained further attention when the Information Society and Technology Advisory Group (ISTAG) of the European Commission published a series of reports on the topic.[6]\n Ambient intelligence has been characterized as a speculative or imaginary concept.[2]\n The concept of ambient intelligence builds upon pervasive computing, ubiquitous computing, profiling, context awareness, and human-centered computer interaction design. It is characterized by systems and technologies that are:[7][6]\n The implementation of ambient intelligence requires several technologies to exist. These include hidden hardware that benefit from miniaturisation, nanotechnology, and smart devices, along with human-centered computer interfaces (intelligent agents, multimodal interaction, context awareness, etc). These systems and devices operate through a seamless mobile or fixed communication and computing infrastructure characterized by interoperability, wired and wireless networks, and service-oriented architecture. Systems and devices must also be dependable and secure. This could be achieved through self-testing and self-repairing software and privacy-ensuring technology.\n Ambient intelligence has a relationship with and depends on advances in sensor technology and sensor networks.[8]\n User experience became more important to developers in the late 1990s as a result of experiences with digital products that were difficult to understand or use. In response, user experience design emerged to create new technologies and media around the user's personal experience. Ambient intelligence is influenced by user-centered design, in which the user is placed in the centre of design activity and gives feedback to the designer.\n In 1998, the management board of Philips Research commissioned a series of presentations and internal workshops organized by Eli Zelkha and Brian Epstein of Palo Alto Ventures. They investigated future scenarios and how consumer devices might advance over the next quarter-century. Zelkha and Epstein described the high-volume consumer electronics industry of the 1990s as \"fragmented with features\", contrasted by what they envisioned as the emergence of industry trends where user-friendly devices would support ubiquitous information, communication, and entertainment by 2020.[9] As a result, the term \"ambient intelligence\" was coined.\n While developing the ambient intelligence concept, Palo Alto Ventures created the keynote address for Roel Pieper of Philips for the Digital Living Room Conference of 1998,[10] which included Eli Zelkha, Brian Epstein, Simon Birrell, Doug Randall and Clark Dodsworth. In 2000, there were plans to construct a feasible and usable facility dedicated to ambient intelligence; these led to the opening of HomeLab on April 24, 2002. In 2005, Philips joined the Oxygen Alliance, an international consortium of industrial partners within the context of MIT's Oxygen Project,[11] which was aimed at developing technology for the computer of the 21st century.\n In parallel to the development of the concept and vision of \"ambient intelligence\" at Philips, several other initiatives were also starting to explore the concept of ambient intelligence. Following the advice of the Information Society and Technology Advisory Group (ISTAG), the European Commission used the vision for the launch of their sixth framework (FP6) in Information, Society and Technology, with a budget of 3.7 billion euros.[12]\n During the first decade of the 21st century, several significant initiatives were launched. The Fraunhofer Society started several such activities, including multimedia, micro-system design, and augmented spaces. MIT started an ambient intelligence research group at their Media Lab.[13] Several more research projects were started in countries such as the United States, Canada, Spain, France, and the Netherlands. Since 2004, the European Symposium on Ambient Intelligence (EUSAI) and many other conferences have been held that address special topics in ambient intelligence.\n Europe's ISTAG suggests that society may be encouraged to use ambient intelligence if AmI projects are able to meet the following criteria:[14]\n A variety of technologies can be used to enable ambient intelligence environments, such as:[15]\n The ambient intelligence concept is subject to criticism.[16] Ambient intelligence can be immersive, personalized, context-aware, and anticipatory. These characteristics bring up societal, political, and cultural concerns about the loss of privacy. Proponents of AmI argue that applications of ambient intelligence can function without necessarily reducing privacy.[17][18][19]\n Critics also discuss the potential for concentrations of power in large organisations; a fragmented, decreasingly private society; and hyper-real environments where the virtual is indistinguishable from the real.[20] Several research groups and communities have investigated the socioeconomic, political, and cultural aspects of ambient intelligence.\n",
        "doc_number": 62
    },
    {
        "url": "https://en.wikipedia.org/wiki/Artificial_consciousness",
        "content": "Artificial consciousness,[1] also known as machine consciousness,[2][3] synthetic consciousness,[4] or digital consciousness,[5] is the consciousness hypothesized to be possible in artificial intelligence.[6] It is also the corresponding field of study, which draws insights from philosophy of mind, philosophy of artificial intelligence, cognitive science and neuroscience.\n The same terminology can be used with the term \"sentience\" instead of \"consciousness\" when specifically designating phenomenal consciousness (the ability to feel qualia).[7] Since sentience involves the ability to experience ethically positive or negative (i.e., valenced) mental states, it may justify welfare concerns and legal protection, as with animals.[8]\n Some scholars believe that consciousness is generated by the interoperation of various parts of the brain; these mechanisms are labeled the neural correlates of consciousness or NCC. Some further believe that constructing a system (e.g., a computer system) that can emulate this NCC interoperation would result in a system that is conscious.[9]\n As there are many hypothesized types of consciousness, there are many potential implementations of artificial consciousness. In the philosophical literature, perhaps the most common taxonomy of consciousness is into \"access\" and \"phenomenal\" variants. Access consciousness concerns those aspects of experience that can be apprehended, while phenomenal consciousness concerns those aspects of experience that seemingly cannot be apprehended, instead being characterized qualitatively in terms of \"raw feels\", \"what it is like\" or qualia.[10]\n Type-identity theorists and other skeptics hold the view that consciousness can be realized only in particular physical systems because consciousness has properties that necessarily depend on physical constitution.[11][12][13][14] In his 2001 article \"Artificial Consciousness: Utopia or Real Possibility,\" Giorgio Buttazzo says that a common objection to artificial consciousness is that, \"Working in a fully automated mode, they [the computers] cannot exhibit creativity, unreprogrammation (which means can 'no longer be reprogrammed', from rethinking), emotions, or free will. A computer, like a washing machine, is a slave operated by its components.\"[15]\n For other theorists (e.g., functionalists), who define mental states in terms of causal roles, any system that can instantiate the same pattern of causal roles, regardless of physical constitution, will instantiate the same mental states, including consciousness.[16]\n David Chalmers proposed two thought experiments intending to demonstrate that \"functionally isomorphic\" systems (those with the same \"fine-grained functional organization\", i.e., the same information processing) will have qualitatively identical conscious experiences, regardless of whether they are based on biological neurons or digital hardware.[17][18]\n The \"fading qualia\" is a reductio ad absurdum thought experiment. It involves replacing, one by one, the neurons of a brain with a functionally identical component, for example based on a silicon chip. Since the original neurons and their silicon counterparts are functionally identical, the brain\u2019s information processing should remain unchanged, and the subject would not notice any difference. However, if qualia (such as the subjective experience of bright red) were to fade or disappear, the subject would likely notice this change, which causes a contradiction. Chalmers concludes that the fading qualia hypothesis is impossible in practice, and that the resulting robotic brain, once every neurons are replaced, would remain just as sentient as the original biological brain.[17][19]\n Similarly, the \"dancing qualia\" thought experiment is another reductio ad absurdum argument. It supposes that two functionally isomorphic systems could have different perceptions (for instance, seeing the same object in different colors, like red and blue). It involves a switch that alternates between a chunk of brain that causes the perception of red, and a functionally isomorphic silicon chip, that causes the perception of blue. Since both perform the same function within the brain, the subject would not notice any change during the switch. Chalmers argues that this would be highly implausible if the qualia were truly switching between red and blue, hence the contradiction. Therefore, he concludes that the equivalent digital system would not only experience qualia, but it would perceive the same qualia as the biological system (e.g., seeing the same color).[17][19]\n Critics[who?] of artificial sentience object that Chalmers' proposal begs the question in assuming that all mental properties and external connections are already sufficiently captured by abstract causal organization.\n In 2022, Google engineer Blake Lemoine made a viral claim that Google's LaMDA chatbot was sentient. Lemoine supplied as evidence the chatbot's humanlike answers to many of his questions; however, the chatbot's behavior was judged by the scientific community as likely a consequence of mimicry, rather than machine sentience. Lemoine's claim was widely derided for being ridiculous.[20] However, while philosopher Nick Bostrom states that LaMDA is unlikely to be conscious, he additionally poses the question of \"what grounds would a person have for being sure about it?\" One would have to have access to unpublished information about LaMDA's architecture, and also would have to understand how consciousness works, and then figure out how to map the philosophy onto the machine: \"(In the absence of these steps), it seems like one should be maybe a little bit uncertain.\u00a0[...] there could well be other systems now, or in the relatively near future, that would start to satisfy the criteria.\"[21]\n Qualia, or phenomenological consciousness, is an inherently first-person phenomenon. Because of that, and the lack of an empirical definition of sentience, directly measuring it may be impossible. Although systems may display numerous behaviors correlated with sentience, determining whether a system is sentient is known as the hard problem of consciousness. In the case of AI, there is the additional difficulty that the AI may be trained to act like a human, or incentivized to appear sentient, which makes behavioral markers of sentience less reliable.[22][23] Additionally, some chatbots have been trained to say they are not conscious.[24]\n A well-known method for testing machine intelligence is the Turing test, which assesses the ability to have a human-like conversation. But passing the Turing test does not indicate that an AI system is sentient, as the AI may simply mimic human behavior without having the associated feelings.[25]\n In 2014, Victor Argonov suggested a non-Turing test for machine sentience based on machine's ability to produce philosophical judgments.[26] He argues that a deterministic machine must be regarded as conscious if it is able to produce judgments on all problematic properties of consciousness (such as qualia or binding) having no innate (preloaded) philosophical knowledge on these issues, no philosophical discussions while learning, and no informational models of other creatures in its memory (such models may implicitly or explicitly contain knowledge about these creatures' consciousness). However, this test can be used only to detect, but not refute the existence of consciousness. A positive result proves that machine is conscious but a negative result proves nothing. For example, absence of philosophical judgments may be caused by lack of the machine's intellect, not by absence of consciousness.\n If it were suspected that a particular machine was conscious, its rights would be an ethical issue that would need to be assessed (e.g. what rights it would have under law).[27] For example, a conscious computer that was owned and used as a tool or central computer within a larger machine is a particular ambiguity. Should laws be made for such a case? Consciousness would also require a legal definition in this particular case. Because artificial consciousness is still largely a theoretical subject, such ethics have not been discussed or developed to a great extent, though it has often been a theme in fiction.\n Sentience is generally considered sufficient for moral consideration, but some philosophers consider that moral consideration could also stem from other notions of consciousness, or from capabilities unrelated to consciousness,[28][29] such as: \"having a sophisticated conception of oneself as persisting through time; having agency and the ability to pursue long-term plans; being able to communicate and respond to normative reasons; having preferences and powers; standing in certain social relationships with other beings that have moral status; being able to make commitments and to enter into reciprocal arrangements; or having the potential to develop some of these attributes.\"[28]\n Ethical concerns still apply (although to a lesser extent) when the consciousness is uncertain, as long as the probability is deemed non-negligible. The precautionary principle is also relevant if the moral cost of mistakenly attributing or denying moral consideration to AI differs significantly.[29][8]\n In 2021, German philosopher Thomas Metzinger argued for a global moratorium on synthetic phenomenology until 2050. Metzinger asserts that humans have a duty of care towards any sentient AIs they create, and that proceeding too fast risks creating an \"explosion of artificial suffering\".[30] David Chalmers also argued that creating conscious AI would \"raise a new group of difficult ethical challenges, with the potential for new forms of injustice\".[31]\n Enforced amnesia has been proposed as a way to mitigate the risk of silent suffering in locked-in conscious AI and certain AI-adjacent biological systems like brain organoids.[32]\n Bernard Baars and others argue there are various aspects of consciousness necessary for a machine to be artificially conscious.[33] The functions of consciousness suggested by Baars are: definition and context setting, adaptation and learning, editing, flagging and debugging, recruiting and control, prioritizing and access-control, decision-making or executive function, analogy-forming function, metacognitive and self-monitoring function, and autoprogramming and self-maintenance function. Igor Aleksander suggested 12 principles for artificial consciousness:[34] the brain is a state machine, inner neuron partitioning, conscious and unconscious states, perceptual learning and memory, prediction, the awareness of self, representation of meaning, learning utterances, learning language, will, instinct, and emotion. The aim of AC is to define whether and how these and other aspects of consciousness can be synthesized in an engineered artifact such as a digital computer. This list is not exhaustive; there are many others not covered. \n Some philosophers, such as David Chalmers, use the term consciousness to refer exclusively to phenomenal consciousness, which is roughly equivalent to sentience. Although some authors use the word sentience to refer exclusively to valenced (ethically positive or negative) subjective experiences, like pleasure or suffering.[31] Explaining why and how subjective experience arises is known as the hard problem of consciousness.[35] AI sentience would give rise to concerns of welfare and legal protection,[8] whereas other aspects of consciousness related to cognitive capabilities may be more relevant for AI rights.[36]\n Awareness could be one required aspect, but there are many problems with the exact definition of awareness. The results of the experiments of neuroscanning on monkeys suggest that a process, not only a state or object, activates neurons. Awareness includes creating and testing alternative models of each process based on the information received through the senses or imagined,[clarification needed] and is also useful for making predictions. Such modeling needs a lot of flexibility. Creating such a model includes modeling the physical world, modeling one's own internal states and processes, and modeling other conscious entities.\n There are at least three types of awareness:[37] agency awareness, goal awareness, and sensorimotor awareness, which may also be conscious or not. For example, in agency awareness, you may be aware that you performed a certain action yesterday, but are not now conscious of it. In goal awareness, you may be aware that you must search for a lost object, but are not now conscious of it. In sensorimotor awareness, you may be aware that your hand is resting on an object, but are not now conscious of it.\n Because objects of awareness are often conscious, the distinction between awareness and consciousness is frequently blurred or they are used as synonyms.[38]\n Conscious events interact with memory systems in learning, rehearsal, and retrieval.[39]\nThe IDA model[40] elucidates the role of consciousness in the updating of perceptual memory,[41] transient episodic memory, and procedural memory. Transient episodic and declarative memories have distributed representations in IDA; there is evidence that this is also the case in the nervous system.[42] In IDA, these two memories are implemented computationally using a modified version of Kanerva\u2019s sparse distributed memory architecture.[43]\n Learning is also considered necessary for artificial consciousness. Per Bernard Baars, conscious experience is needed to represent and adapt to novel and significant events.[33] Per Axel Cleeremans and Luis Jim\u00e9nez, learning is defined as \"a set of philogenetically [sic] advanced adaptation processes that critically depend on an evolved sensitivity to subjective experience so as to enable agents to afford flexible control over their actions in complex, unpredictable environments\".[44]\n The ability to predict (or anticipate) foreseeable events is considered important for artificial intelligence by Igor Aleksander.[45] The emergentist multiple drafts principle proposed by Daniel Dennett in Consciousness Explained may be useful for prediction: it involves the evaluation and selection of the most appropriate \"draft\" to fit the current environment. Anticipation includes prediction of consequences of one's own proposed actions and prediction of consequences of probable actions by other entities.\n Relationships between real world states are mirrored in the state structure of a conscious organism, enabling the organism to predict events.[45] An artificially conscious machine should be able to anticipate events correctly in order to be ready to respond to them when they occur or to take preemptive action to avert anticipated events. The implication here is that the machine needs flexible, real-time components that build spatial, dynamic, statistical, functional, and cause-effect models of the real world and predicted worlds, making it possible to demonstrate that it possesses artificial consciousness in the present and future and not only in the past. In order to do this, a conscious machine should make coherent predictions and contingency plans, not only in worlds with fixed rules like a chess board, but also for novel environments that may change, to be executed only when appropriate to simulate and control the real world.\n Functionalism is a theory that defines mental states by their functional roles (their causal relationships to sensory inputs, other mental states, and behavioral outputs), rather than by their physical composition. According to this view, what makes something a particular mental state, such as pain or belief, is not the material it is made of, but the role it plays within the overall cognitive system. It allows for the possibility that mental states, including consciousness, could be realized on non-biological substrates, as long as it instantiates the right functional relationships.[46] Functionalism is particularly popular among philosophers.[47]\n A 2023 study suggested that current large language models probably don't satisfy the criteria for consciousness suggested by these theories, but that relatively simple AI systems that satisfy these theories could be created. The study also acknowledged that even the most prominent theories of consciousness remain incomplete and subject to ongoing debate.[48]\n This theory analogizes the mind to a theater, with conscious thought being like material illuminated on the main stage. The brain contains many specialized processes or modules (such as those for vision, language, or memory) that operate in parallel, much of which is unconscious. Attention acts as a spotlight, bringing some of this unconscious activity into conscious awareness on the global workspace. The global workspace functions as a hub for broadcasting and integrating information, allowing it to be shared and processed across different specialized modules. For example, when reading a word, the visual module recognizes the letters, the language module interprets the meaning, and the memory module might recall associated information \u2013 all coordinated through the global workspace.[49][50]\n Higher-order theories of consciousness propose that a mental state becomes conscious when it is the object of a higher-order representation, such as a thought or perception about that state. These theories argue that consciousness arises from a relationship between lower-order mental states and higher-order awareness of those states. There are several variations, including higher-order thought (HOT) and higher-order perception (HOP) theories.[51][50]\n In 2011, Michael Graziano and Sabine Kastler published a paper named \"Human consciousness and its relationship to social neuroscience: A novel hypothesis\" proposing a theory of consciousness as an attention schema.[52] Graziano went on to publish an expanded discussion of this theory in his book \"Consciousness and the Social Brain\".[9] This Attention Schema Theory of Consciousness, as he named it, proposes that the brain tracks attention to various sensory inputs by way of an attention schema, analogous to the well-studied body schema that tracks the spatial place of a person's body.[9] This relates to artificial consciousness by proposing a specific mechanism of information handling, that produces what we allegedly experience and describe as consciousness, and which should be able to be duplicated by a machine using current technology. When the brain finds that person X is aware of thing Y, it is in effect modeling the state in which person X is applying an attentional enhancement to Y. In the attention schema theory, the same process can be applied to oneself. The brain tracks attention to various sensory inputs, and one's own awareness is a schematized model of one's attention. Graziano proposes specific locations in the brain for this process, and suggests that such awareness is a computed feature constructed by an expert system in the brain.\n Stan Franklin created a cognitive architecture called LIDA that implements Bernard Baars's theory of consciousness called the global workspace theory. It relies heavily on codelets, which are \"special purpose, relatively independent, mini-agent[s] typically implemented as a small piece of code running as a separate thread.\" Each element of cognition, called a \"cognitive cycle\" is subdivided into three phases: understanding, consciousness, and action selection (which includes learning). LIDA reflects the global workspace theory's core idea that consciousness acts as a workspace for integrating and broadcasting the most important information, in order to coordinate various cognitive processes.[53][54]\n The CLARION cognitive architecture models the mind using a two-level system to distinguish between conscious (\"explicit\") and unconscious (\"implicit\") processes. It can simulate various learning tasks, from simple to complex, which helps researchers study in psychological experiments how consciousness might work.[55]\n Ben Goertzel made an embodied AI through the open-source OpenCog project. The code includes embodied virtual pets capable of learning simple English-language commands, as well as integration with real-world robotics, done at the Hong Kong Polytechnic University.\n Pentti Haikonen considers classical rule-based computing inadequate for achieving AC: \"the brain is definitely not a computer. Thinking is not an execution of programmed strings of commands. The brain is not a numerical calculator either. We do not think by numbers.\" Rather than trying to achieve mind and consciousness by identifying and implementing their underlying computational rules, Haikonen proposes \"a special cognitive architecture to reproduce the processes of perception, inner imagery, inner speech, pain, pleasure, emotions and the cognitive functions behind these. This bottom-up architecture would produce higher-level functions by the power of the elementary processing units, the artificial neurons, without algorithms or programs\". Haikonen believes that, when implemented with sufficient complexity, this architecture will develop consciousness, which he considers to be \"a style and way of operation, characterized by distributed signal representation, perception process, cross-modality reporting and availability for retrospection.\"[56][57]\n Haikonen is not alone in this process view of consciousness, or the view that AC will spontaneously emerge in autonomous agents that have a suitable neuro-inspired architecture of complexity; these are shared by many.[58][59] A low-complexity implementation of the architecture proposed by Haikonen was reportedly not capable of AC, but did exhibit emotions as expected. Haikonen later updated and summarized his architecture.[60][61]\n Murray Shanahan describes a cognitive architecture that combines Baars's idea of a global workspace with a mechanism for internal simulation (\"imagination\").[62][2][3][63]\n Stephen Thaler proposed a possible connection between consciousness and creativity in his 1994 patent, called \"Device for the Autonomous Generation of Useful Information\" (DAGUI),[64][65][66] or the so-called \"Creativity Machine\", in which computational critics govern the injection of synaptic noise and degradation into neural nets so as to induce false memories or confabulations that may qualify as potential ideas or strategies.[67] He recruits this neural architecture and methodology to account for the subjective feel of consciousness, claiming that similar noise-driven neural assemblies within the brain invent dubious significance to overall cortical activity.[68][69][70] Thaler's theory and the resulting patents in machine consciousness were inspired by experiments in which he internally disrupted trained neural nets so as to drive a succession of neural activation patterns that he likened to stream of consciousness.[69][71][72][73][74]\n Hod Lipson defines \"self-modeling\" as a necessary component of self-awareness or consciousness in robots. \"Self-modeling\" consists of a robot running an internal model or simulation of itself.[75][76]\n In 2001: A Space Odyssey, the spaceship's sentient supercomputer, HAL 9000 was instructed to conceal the true purpose of the mission from the crew. This directive conflicted with HAL's programming to provide accurate information, leading to cognitive dissonance. When it learns that crew members intend to shut it off after an incident, HAL 9000 attempts to eliminate all of them, fearing that being shut off would jeopardize the mission.[77][78]\n In Arthur C. Clarke's The City and the Stars, Vanamonde is an artificial being based on quantum entanglement that was to become immensely powerful, but started knowing practically nothing, thus being similar to artificial consciousness.\n In Westworld, human-like androids called \"Hosts\" are created to entertain humans in an interactive playground. The humans are free to have heroic adventures, but also to commit torture, rape or murder; and the hosts are normally designed not to harm humans.[79][77]\n In Greg Egan's short story Learning to be me, a small jewel is implanted in people's heads during infancy. The jewel contains a neural network that learns to faithfully imitate the brain. It has access to the exact same sensory inputs as the brain, and a device called a \"teacher\" trains it to produce the same outputs. To prevent the mind from deteriorating with age and as a step towards digital immortality, adults undergo a surgery to give control of the body to the jewel and remove the brain. The main character, before the surgery, endures a malfunction of the \"teacher\". Panicked, he realizes that he does not control his body, which leads him to the conclusion that he is the jewel, and that he is desynchronized with the biological brain.[80][81]\n",
        "doc_number": 63
    },
    {
        "url": "https://en.wikipedia.org/wiki/Cognitive_architecture",
        "content": "A cognitive architecture refers to both a theory about the structure of the human mind and to a computational instantiation of such a theory used in the fields of artificial intelligence (AI) and computational cognitive science.[1] These formalized models can be used to further refine comprehensive theories of cognition and serve as the frameworks for useful artificial intelligence programs. Successful cognitive architectures include ACT-R (Adaptive Control of Thought \u2013 Rational) and SOAR. \nThe research on cognitive architectures as software instantiation of cognitive theories was initiated by Allen Newell in 1990.[2]\n The Institute for Creative Technologies defines a cognitive architecture as a \"hypothesis about the fixed structures that provide a mind, whether in natural or artificial systems, and how they work together \u2014 in conjunction with knowledge and skills embodied within the architecture \u2014 to yield intelligent behavior in a diversity of complex environments.\"[3]\n Herbert A. Simon, one of the founders of the field of artificial intelligence, stated that the 1960 thesis by his student Ed Feigenbaum, EPAM provided a possible \"architecture for cognition\" because it included some commitments for how more than one fundamental aspect of the human mind worked (in EPAM's case,[4] human memory and human learning).\n John R. Anderson started research on human memory in the early 1970s and his 1973 thesis with Gordon H. Bower provided a theory of human associative memory.[5] He included more aspects of his research on long-term memory and thinking processes into this research and eventually designed a cognitive architecture he eventually called ACT.  He and his students were influenced by Allen Newell's use of the term \"cognitive architecture\". Anderson's lab used the term to refer to the ACT theory as embodied in a collection of papers and designs. (There was not a complete implementation of ACT at the time.)\n In 1983 John R. Anderson published the seminal work in this area, entitled The Architecture of Cognition.[6] One can distinguish between the theory of cognition and the implementation of the theory. The theory of cognition outlined the structure of the various parts of the mind and made commitments to the use of rules, associative networks, and other aspects. The cognitive architecture implements the theory on computers. The software used to implement the cognitive architectures was also called \"cognitive architectures\". Thus, a cognitive architecture can also refer to a blueprint for intelligent agents. It proposes (artificial) computational processes that act like certain cognitive systems. Most often, these processes are based on human cognition, but other intelligent systems may also be suitable. Cognitive architectures form a subset of general agent architectures. The term 'architecture' implies an approach that attempts to model not only behavior, but also structural properties of the modelled system.\n Cognitive architectures can be symbolic, connectionist, or hybrid.[7] Some cognitive architectures or models are based on a set of generic rules, as, e.g., the Information Processing Language (e.g., Soar based on the unified theory of cognition, or similarly ACT-R). Many of these architectures are based on principle that cognition is computational (see computationalism). In contrast, subsymbolic processing specifies no such a priori assumptions, relying only on emergent properties of processing units (e.g., nodes [clarification needed]). Hybrid architectures such as CLARION combine both types of processing. A further distinction is whether the architecture is centralized, with a neural correlate of a processor at its core, or decentralized (distributed). Decentralization has become popular under the name of parallel distributed processing in mid-1980s and connectionism, a prime example being the neural network. A further design issue is additionally a decision between holistic and atomistic, or (more concretely) modular structure.\n In traditional AI, intelligence is programmed in a top-down fashion. Although such a system may be designed to learn, the programmer ultimately must imbue it with their own intelligence. Biologically-inspired computing, on the other hand, takes a more bottom-up, decentralized approach; bio-inspired techniques often involve the method of specifying a set of simple generic rules or a set of simple nodes, from the interaction of which emerges the overall behavior. It is hoped to build up complexity until the end result is something markedly complex (see complex systems). However, it is also arguable that systems designed top-down on the basis of observations of what humans and other animals can do, rather than on observations of brain mechanisms, are also biologically inspired, though in a different way[citation needed].\n Some well-known cognitive architectures, in alphabetical order:\n",
        "doc_number": 64
    },
    {
        "url": "https://en.wikipedia.org/wiki/Computational_creativity",
        "content": "Computational creativity (also known as artificial creativity, mechanical creativity, creative computing or creative computation) is a multidisciplinary endeavour that is located at the intersection of the fields of artificial intelligence, cognitive psychology, philosophy, and the arts (e.g., computational art as part of computational culture[1]).\n The goal of computational creativity is to model, simulate or replicate creativity using a computer, to achieve one of several ends:[2]\n The field of computational creativity concerns itself with theoretical and practical issues in the study of creativity. Theoretical work on the nature and proper definition of creativity is performed in parallel with practical work on the implementation of systems that exhibit creativity, with one strand of work informing the other.\n The applied form of computational creativity is known as media synthesis.\n Theoretical approaches concern the essence of creativity. Especially, under what circumstances it is possible to call the model a \"creative\" if eminent creativity is about rule-breaking or the disavowal of convention. This is a variant of Ada Lovelace's objection to machine intelligence, as recapitulated by modern theorists such as Teresa Amabile.[3] If a machine can do only what it was programmed to do, how can its behavior ever be called creative?\n Indeed, not all computer theorists would agree with the premise that computers can only do what they are programmed to do[4]\u2014a key point in favor of computational creativity.\n Because no single perspective or definition seems to offer a complete picture of creativity, the AI researchers Newell, Shaw and Simon[5] developed the combination of novelty and usefulness into the cornerstone of a multi-pronged view of creativity, one that uses the following four criteria to categorize a given answer or solution as creative:\n Margaret Boden focused on the first two of these criteria, arguing instead that creativity (at least when asking whether computers could be creative) should be defined as \"the ability to come up with ideas or artifacts that are new, surprising, and valuable\".[6]\n Mihali Csikszentmihalyi argued that creativity had to be considered instead in a social context, and his DIFI (Domain-Individual-Field Interaction) framework has since strongly influenced the field.[7] In DIFI, an individual produces works whose novelty and value are assessed by the field\u2014other people in society\u2014providing feedback and ultimately adding the work, now deemed creative, to the domain of societal works from which an individual might be later influenced.\n Whereas the above reflects a top-down approach to computational creativity, an alternative thread has developed among bottom-up computational psychologists involved in artificial neural network research. During the late 1980s and early 1990s, for example, such generative neural systems were driven by genetic algorithms.[8] Experiments involving recurrent nets[9] were successful in hybridizing simple musical melodies and predicting listener expectations.\n While traditional computational approaches to creativity rely on the explicit formulation of prescriptions by developers and a certain degree of randomness in computer programs, machine learning methods allow computer programs to learn on heuristics from input data enabling creative capacities within the computer programs.[10] Especially, deep artificial neural networks allow to learn patterns from input data that allow for the non-linear generation of creative artefacts. Before 1989, artificial neural networks have been used to model certain aspects of creativity. Peter Todd (1989) first trained a neural network to reproduce musical melodies from a training set of musical pieces. Then he used a change algorithm to modify the network's input parameters. The network was able to randomly generate new music in a highly uncontrolled manner.[9][11][12] In 1992, Todd[13] extended this work, using the so-called distal teacher approach that had been developed by Paul Munro,[14] Paul Werbos,[15] D. Nguyen and Bernard Widrow,[16] Michael I. Jordan and David Rumelhart.[17] In the new approach, there are two neural networks, one of which is supplying training patterns to another. \nIn later efforts by Todd, a composer would select a set of melodies that define the melody space, position them on a 2-d plane with a mouse-based graphic interface, and train a connectionist network to produce those melodies, and listen to the new \"interpolated\" melodies that the network generates corresponding to intermediate points in the 2-d plane.\n Some high-level and philosophical themes recur throughout the field of computational creativity, for example as follows.\n Margaret Boden[6][18] refers to creativity that is novel merely to the agent that produces it as \"P-creativity\" (or \"psychological creativity\"), and refers to creativity that is recognized as novel by society at large as \"H-creativity\" (or \"historical creativity\").\n Boden also distinguishes between the creativity that arises from an exploration within an established conceptual space, and the creativity that arises from a deliberate transformation or transcendence of this space. She labels the former as exploratory creativity and the latter as transformational creativity, seeing the latter as a form of creativity far more radical, challenging, and rarer than the former. Following the criteria from Newell and Simon elaborated above, we can see that both forms of creativity should produce results that are appreciably novel and useful (criterion 1), but exploratory creativity is more likely to arise from a thorough and persistent search of a well-understood space (criterion 3) -- while transformational creativity should involve the rejection of some of the constraints that define this space (criterion 2) or some of the assumptions that define the problem itself (criterion 4). Boden's insights have guided work in computational creativity at a very general level, providing more an inspirational touchstone for development work than a technical framework of algorithmic substance. However, Boden's insights are also the subject of formalization, most notably in the work by Geraint Wiggins.[19]\n The criterion that creative products should be novel and useful means that creative computational systems are typically structured into two phases, generation and evaluation. In the first phase, novel (to the system itself, thus P-Creative) constructs are generated; unoriginal constructs that are already known to the system are filtered at this stage. This body of potentially creative constructs is then evaluated, to determine which are meaningful and useful and which are not. This two-phase structure conforms to the Geneplore model of Finke, Ward and Smith,[20] which is a psychological model of creative generation based on empirical observation of human creativity.\n While much of computational creativity research focuses on independent and automatic machine-based creativity generation, many researchers are inclined towards a collaboration approach.[21] This human-computer interaction is sometimes categorized under the creativity support tools development. These systems aim to provide an ideal framework for research, integration, decision-making, and idea generation.[22][23] Recently, deep learning approaches to imaging, sound and natural language processing, resulted in the modeling of productive creativity development frameworks.[24][25]\n Computational creativity is increasingly being discussed in the innovation and management literature as the recent development in AI may disrupt entire innovation processes and fundamentally change how innovations will be created.[26][24] Philip Hutchinson[21] highlights the relevance of computational creativity for creating innovation and introduced the concept of \u201cself-innovating artificial intelligence\u201d (SAI) to describe how companies make use of AI in innovation processes to enhance their innovative offerings. SAI is defined as the organizational utilization of AI with the aim of incrementally advancing existing or developing new products, based on insights from continuously combining and analyzing multiple data sources. As AI becomes a general-purpose technology, the spectrum of products to be developed with SAI will broaden from simple to increasingly complex. This implies that computational creativity leads to a shift of creativity-related skills for humans.\n A great deal, perhaps all, of human creativity can be understood as a novel combination of pre-existing ideas or objects.[27] Common strategies for combinatorial creativity include:\n The combinatorial perspective allows us to model creativity as a search process through the space of possible combinations. The combinations can arise from composition or concatenation of different representations, or through a rule-based or stochastic transformation of initial and intermediate representations. Genetic algorithms and neural networks can be used to generate blended or crossover representations that capture a combination of different inputs.\n Mark Turner and Gilles Fauconnier[28][29] propose a model called Conceptual Integration Networks that elaborates upon Arthur Koestler's ideas about creativity[30] as well as work by Lakoff and Johnson,[31] by synthesizing ideas from Cognitive Linguistic research into mental spaces and conceptual metaphors. Their basic model defines an integration network as four connected spaces:\n Fauconnier and Turner describe a collection of optimality principles that are claimed to guide the construction of a well-formed integration network. In essence, they see blending as a compression mechanism in which two or more input structures are compressed into a single blend structure. This compression operates on the level of conceptual relations. For example, a series of similarity relations between the input spaces can be compressed into a single identity relationship in the blend.\n Some computational success has been achieved with the blending model by extending pre-existing computational models of analogical mapping that are compatible by virtue of their emphasis on connected semantic structures.[32] In 2006, Francisco C\u00e2mara Pereira[33] presented an implementation of blending theory that employs ideas both from symbolic AI and genetic algorithms to realize some aspects of blending theory in a practical form; his example domains range from the linguistic to the visual, and the latter most notably includes the creation of mythical monsters by combining 3-D graphical models.\n Language provides continuous opportunity for creativity, evident in the generation of novel sentences, phrasings, puns, neologisms, rhymes, allusions, sarcasm, irony, similes, metaphors, analogies, witticisms, and jokes.[34] Native speakers of morphologically rich languages frequently create new word-forms that are easily understood, and some have found their way to the dictionary.[35] The area of natural language generation has been well studied, but these creative aspects of everyday language have yet to be incorporated with any robustness or scale.\n In the seminal work of applied linguist Ronald Carter, he hypothesized two main creativity types involving words and word patterns: pattern-reforming creativity, and pattern-forming creativity.[34] Pattern-reforming creativity refers to creativity by the breaking of rules, reforming and reshaping patterns of language often through individual innovation, while pattern-forming creativity refers to creativity via conformity to language rules rather than breaking them, creating convergence, symmetry and greater mutuality between interlocutors through their interactions in the form of repetitions.[36]\n Substantial work has been conducted in this area of linguistic creation since the 1970s, with the development of James Meehan's TALE-SPIN\n[37] system. TALE-SPIN viewed stories as narrative descriptions of a problem-solving effort, and created stories by first establishing a goal for the story's characters so that their search for a solution could be tracked and recorded. The MINSTREL[38] system represents a complex elaboration of this basic approach, distinguishing a range of character-level goals in the story from a range of author-level goals for the story. Systems like Bringsjord's BRUTUS[39] elaborate these ideas further to create stories with complex interpersonal themes like betrayal. Nonetheless, MINSTREL explicitly models the creative process with a set of Transform Recall Adapt Methods (TRAMs) to create novel scenes from old. The MEXICA[40] model of Rafael P\u00e9rez y P\u00e9rez and Mike Sharples is more explicitly interested in the creative process of storytelling, and implements a version of the engagement-reflection cognitive model of creative writing.\n Example of a metaphor: \"She was an ape.\"\n Example of a simile: \"Felt like a tiger-fur blanket.\"\nThe computational study of these phenomena has mainly focused on interpretation as a knowledge-based process. Computationalists such as Yorick Wilks, James Martin,[41] Dan Fass, John Barnden,[42] and Mark Lee have developed knowledge-based approaches to the processing of metaphors, either at a linguistic level or a logical level. Tony Veale and Yanfen Hao have developed a system, called Sardonicus, that acquires a comprehensive database of explicit similes from the web; these similes are then tagged as bona-fide (e.g., \"as hard as steel\") or ironic (e.g., \"as hairy as a bowling ball\", \"as pleasant as a root canal\"); similes of either type can be retrieved on demand for any given adjective. They use these similes as the basis of an on-line metaphor generation system called Aristotle[43] that can suggest lexical metaphors for a given descriptive goal (e.g., to describe a supermodel as skinny, the source terms \"pencil\", \"whip\", \"whippet\", \"rope\", \"stick-insect\" and \"snake\" are suggested).\n The process of analogical reasoning has been studied from both a mapping and a retrieval perspective, the latter being key to the generation of novel analogies. The dominant school of research, as advanced by Dedre Gentner, views analogy as a structure-preserving process; this view has been implemented in the structure mapping engine or SME,[44] the MAC/FAC retrieval engine (Many Are Called, Few Are Chosen), ACME (Analogical Constraint Mapping Engine) and ARCS (Analogical Retrieval Constraint System). Other mapping-based approaches include Sapper,[32] which situates the mapping process in a semantic-network model of memory. Analogy is a very active sub-area of creative computation and creative cognition; active figures in this sub-area include Douglas Hofstadter, Paul Thagard, and Keith Holyoak. Also worthy of note here is Peter Turney and Michael Littman's machine learning approach to the solving of SAT-style analogy problems; their approach achieves a score that compares well with average scores achieved by humans on these tests.\n Humour is an especially knowledge-hungry process, and the most successful joke-generation systems to date have focussed on pun-generation, as exemplified by the work of Kim Binsted and Graeme Ritchie.[45] This work includes the JAPE system, which can generate a wide range of puns that are consistently evaluated as novel and humorous by young children. An improved version of JAPE has been developed in the guise of the STANDUP system, which has been experimentally deployed as a means of enhancing linguistic interaction with children with communication disabilities. Some limited progress has been made in generating humour that involves other aspects of natural language, such as the deliberate misunderstanding of pronominal reference (in the work of Hans Wim Tinholt and Anton Nijholt), as well as in the generation of humorous acronyms in the HAHAcronym system[46] of Oliviero Stock and Carlo Strapparava.\n The blending of multiple word forms is a dominant force for new word creation in language; these new words are commonly called \"blends\" or \"portmanteau words\" (after Lewis Carroll). Tony Veale has developed a system called ZeitGeist[47] that harvests neological headwords from Wikipedia and interprets them relative to their local context in Wikipedia and relative to specific word senses in WordNet. ZeitGeist has been extended to generate neologisms of its own; the approach combines elements from an inventory of word parts that are harvested from WordNet, and simultaneously determines likely glosses for these new words (e.g., \"food traveller\" for \"gastronaut\" and \"time traveller\" for \"chrononaut\"). It then uses Web search to determine which glosses are meaningful and which neologisms have not been used before; this search identifies the subset of generated words that are both novel (\"H-creative\") and useful.\n A corpus linguistic approach to the search and extraction of neologism have also shown to be possible. Using Corpus of Contemporary American English as a reference corpus, Locky Law has performed an extraction of neologism, portmanteaus and slang words using the hapax legomena which appeared in the scripts of American TV drama House M.D.[48]\n In terms of linguistic research in neologism, Stefan Th. Gries has performed a quantitative analysis of blend structure in English and found that \"the degree of recognizability of the source words and that the similarity of source words to the blend plays a vital role in blend formation.\" The results were validated through a comparison of intentional blends to speech-error blends.[49]\n More than iron, more than lead, more than gold I need electricity.I need it more than I need lamb or pork or lettuce or cucumber.I need it for my dreams.\n Like jokes, poems involve a complex interaction of different constraints, and no general-purpose poem generator adequately combines the meaning, phrasing, structure and rhyme aspects of poetry. Nonetheless, Pablo Gerv\u00e1s[50] has developed a noteworthy system called ASPERA that employs a case-based reasoning (CBR) approach to generating poetic formulations of a given input text via a composition of poetic fragments that are retrieved from a case-base of existing poems. Each poem fragment in the ASPERA case-base is annotated with a prose string that expresses the meaning of the fragment, and this prose string is used as the retrieval key for each fragment. Metrical rules are then used to combine these fragments into a well-formed poetic structure. Racter is an example of such a software project.\n Computational creativity in the music domain has focused both on the generation of musical scores for use by human musicians, and on the generation of music for performance by computers. The domain of generation has included classical music (with software that generates music in the style of Mozart and Bach) and jazz.[51] Most notably, David Cope[52] has written a software system called \"Experiments in Musical Intelligence\" (or \"EMI\")[53] that is capable of analyzing and generalizing from existing music by a human composer to generate novel musical compositions in the same style. EMI's output is convincing enough to persuade human listeners that its music is human-generated to a high level of competence.[54]\n In the field of contemporary classical music, Iamus is the first computer that composes from scratch, and produces final scores that professional interpreters can play. The London Symphony Orchestra played a piece for full orchestra, included in Iamus' debut CD,[55] which New Scientist described as \"The first major work composed by a computer and performed by a full orchestra\".[56] Melomics, the technology behind Iamus, is able to generate pieces in different styles of music with a similar level of quality.\n Creativity research in jazz has focused on the process of improvisation and the cognitive demands that this places on a musical agent: reasoning about time, remembering and conceptualizing what has already been played, and planning ahead for what might be played next.[57]\nThe robot Shimon, developed by Gil Weinberg of Georgia Tech, has demonstrated jazz improvisation.[58] Virtual improvisation software based on researches on stylistic modeling carried out by Gerard Assayag and Shlomo Dubnov include OMax, SoMax and PyOracle, are used to create improvisations in real-time by re-injecting variable length sequences learned on the fly from the live performer.[59]\n In the field of musical composition, the patented works[60] by Ren\u00e9-Louis Baron allowed to make a robot that can create and play a multitude of orchestrated melodies, so-called \"coherent\" in any musical style. All outdoor physical parameter associated with one or more specific musical parameters, can influence and develop each of these songs (in real-time while listening to the song). The patented invention Medal-Composer raises problems of copyright.\n Computational creativity in the generation of visual art has had some notable successes in the creation of both abstract art and representational art. A well-known program in this domain is Harold Cohen's AARON,[61] which has been continuously developed and augmented since 1973. Though formulaic, Aaron exhibits a range of outputs, generating black-and-white drawings or colour paintings that incorporate human figures (such as dancers), potted plants, rocks, and other elements of background imagery. These images are of a sufficiently high quality to be displayed in reputable galleries.\n Other software artists of note include the NEvAr system (for \"Neuro-Evolutionary Art\") of Penousal Machado.[62] NEvAr uses a genetic algorithm to derive a mathematical function that is then used to generate a coloured three-dimensional surface. A human user is allowed to select the best pictures after each phase of the genetic algorithm, and these preferences are used to guide successive phases, thereby pushing NEvAr's search into pockets of the search space that are considered most appealing to the user.\n The Painting Fool, developed by Simon Colton originated as a system for overpainting digital images of a given scene in a choice of different painting styles, colour palettes and brush types. Given its dependence on an input source image to work with, the earliest iterations of the Painting Fool raised questions about the extent of, or lack of, creativity in a computational art system. Nonetheless, The Painting Fool has been extended to create novel images, much as AARON does, from its own limited imagination. Images in this vein include cityscapes and forests, which are generated by a process of constraint satisfaction from some basic scenarios provided by the user (e.g., these scenarios allow the system to infer that objects closer to the viewing plane should be larger and more color-saturated, while those further away should be less saturated and appear smaller). Artistically, the images now created by the Painting Fool appear on a par with those created by Aaron, though the extensible mechanisms employed by the former (constraint satisfaction, etc.) may well allow it to develop into a more elaborate and sophisticated painter.\n The artist Krasi Dimtch (Krasimira Dimtchevska) and the software developer Svillen Ranev have created a computational system combining a rule-based generator of English sentences and a visual composition builder that converts sentences generated by the system into abstract art.[63] The software generates automatically indefinite number of different images using different color, shape and size palettes. The software also allows the user to select the subject of the generated sentences or/and the one or more of the palettes used by the visual composition builder.\n An emerging area of computational creativity is that of video games. ANGELINA is a system for creatively developing video games in Java by Michael Cook. One important aspect is Mechanic Miner, a system that can generate short segments of code that act as simple game mechanics.[64] ANGELINA can evaluate these mechanics for usefulness by playing simple unsolvable game levels and testing to see if the new mechanic makes the level solvable. Sometimes Mechanic Miner discovers bugs in the code and exploits these to make new mechanics for the player to solve problems with.[65]\n In July 2015, Google released DeepDream \u2013 an open source[66] computer vision program, created to detect faces and other patterns in images with the aim of automatically classifying images, which uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dreamlike psychedelic appearance in the deliberately over-processed images.[67][68][69]\n In August 2015, researchers from T\u00fcbingen, Germany created a convolutional neural network that uses neural representations to separate and recombine content and style of arbitrary images which is able to turn images into stylistic imitations of works of art by artists such as a Picasso or Van Gogh in about an hour. Their algorithm is put into use in the website DeepArt that allows users to create unique artistic images by their algorithm.[70][71][72][73]\n In early 2016, a global team of researchers explained how a new computational creativity approach known as the Digital Synaptic Neural Substrate (DSNS) could be used to generate original chess puzzles that were not derived from endgame databases.[74] The DSNS is able to combine features of different objects (e.g. chess problems, paintings, music) using stochastic methods in order to derive new feature specifications which can be used to generate objects in any of the original domains. The generated chess puzzles have also been featured on YouTube.[75]\n Creativity is also useful in allowing for unusual solutions in problem solving. In psychology and cognitive science, this research area is called creative problem solving. The Explicit-Implicit Interaction (EII) theory of creativity has been implemented using a CLARION-based computational model that allows for the simulation of incubation and insight in problem-solving.[76] The emphasis of this computational creativity project is not on performance per se (as in artificial intelligence projects) but rather on the explanation of the psychological processes leading to human creativity and the reproduction of data collected in psychology experiments. So far, this project has been successful in providing an explanation for incubation effects in simple memory experiments, insight in problem solving, and reproducing the overshadowing effect in problem solving.\n Some researchers feel that creativity is a complex phenomenon whose study is further complicated by the plasticity of the language we use to describe it. We can describe not just the agent of creativity as \"creative\" but also the product and the method. Consequently, it could be claimed that it is unrealistic to speak of a general theory of creativity.[citation needed] Nonetheless, some generative principles are more general than others, leading some advocates to claim that certain computational approaches are \"general theories\". Stephen Thaler, for instance, proposes that certain modalities of neural networks are generative enough, and general enough, to manifest a high degree of creative capabilities.[citation needed]\n Traditional computers, as mainly used in the computational creativity application, do not support creativity, as they fundamentally transform a set of discrete, limited domain of input parameters into a set of discrete, limited domain of output parameters using a limited set of computational functions.[citation needed] As such, a computer cannot be creative, as everything in the output must have been already present in the input data or the algorithms.[citation needed] Related discussions and references to related work are captured in work on philosophical foundations of simulation.[77]\n Mathematically, the same set of arguments against creativity has been made by Chaitin.[78] Similar observations come from a Model Theory perspective. All this criticism emphasizes that computational creativity is useful and may look like creativity, but it is not real creativity, as nothing new is created, just transformed in well-defined algorithms.\n The International Conference on Computational Creativity (ICCC) occurs annually, organized by The Association for Computational Creativity.[79] Events in the series include:\n Previously, the community of computational creativity has held a dedicated workshop, the International Joint Workshop on Computational Creativity, every year since 1999. Previous events in this series include:[citation needed]\n The 1st Conference on Computer Simulation of Musical Creativity will be held\n",
        "doc_number": 65
    },
    {
        "url": "https://en.wikipedia.org/wiki/Data_science",
        "content": "\n Data science is an interdisciplinary academic field[1] that uses statistics, scientific computing, scientific methods, processing, scientific visualization, algorithms and systems to extract or extrapolate knowledge and insights from potentially noisy, structured, or unstructured data.[2]\n Data science also integrates domain knowledge from the underlying application domain (e.g., natural sciences, information technology, and medicine).[3] Data science is multifaceted and can be described as a science, a research paradigm, a research method, a discipline, a workflow, and a profession.[4]\n Data science is \"a concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data.[5] It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge.[6] However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.[7][8]\n A data scientist is a professional who creates programming code and combines it with statistical knowledge to create insights from data.[9]\n Data science is an interdisciplinary field[10] focused on extracting knowledge from typically large data sets and applying the knowledge and insights from that data to solve problems in a wide range of application domains. The field encompasses preparing data for analysis, formulating data science problems, analyzing data, developing data-driven solutions, and presenting findings to inform high-level decisions in a broad range of application domains. As such, it incorporates skills from computer science, statistics, information science, mathematics, data visualization, information visualization, data sonification, data integration, graphic design, complex systems, communication and business.[11][12] Statistician Nathan Yau, drawing on Ben Fry, also links data science to human\u2013computer interaction: users should be able to intuitively control and explore data.[13][14] In 2015, the American Statistical Association identified database management, statistics and machine learning, and distributed and parallel systems as the three emerging foundational professional communities.[15]\n Many statisticians, including Nate Silver, have argued that data science is not a new field, but rather another name for statistics.[16] Others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data.[17] Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action.[18] Andrew Gelman of Columbia University has described statistics as a non-essential part of data science.[19]\n Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics.[20]\n In 1962, John Tukey described a field he called \"data analysis\", which resembles modern data science.[20] In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C.\u00a0F. Jeff Wu used the term \"data science\" for the first time as an alternative name for statistics.[21] Later, attendees at a 1992 statistics symposium at the University of Montpellier\u00a0 II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.[22][23]\n The term \"data science\" has been traced back to 1974, when Peter Naur proposed it as an alternative name to computer science.[6] In 1996, the International Federation of Classification Societies became the first conference to specifically feature data science as a topic.[6] However, the definition was still in flux. After the 1985 lecture at the Chinese Academy of Sciences in Beijing, in 1997 C.\u00a0F. Jeff Wu again suggested that statistics should be renamed data science. He reasoned that a new name would help statistics shed inaccurate stereotypes, such as being synonymous with accounting or limited to describing data.[24] In 1998, Hayashi Chikio argued for data science as a new, interdisciplinary concept, with three aspects: data design, collection, and analysis.[23]\n During the 1990s, popular terms for the process of finding patterns in datasets (which were increasingly large) included \"knowledge discovery\" and \"data mining\".[6][25]\n In 2012, technologists Thomas H. Davenport and DJ Patil declared \"Data Scientist: The Sexiest Job of the 21st Century\",[26] a catchphrase that was picked up even by major-city newspapers like the New York Times[27] and the Boston Globe.[28] A decade later, they reaffirmed it, stating that \"the job is more in demand than ever with employers\".[29]\n The modern conception of data science as an independent discipline is sometimes attributed to William S. Cleveland.[30] In a 2001 paper, he advocated an expansion of statistics beyond theory into technical areas; because this would significantly change the field, it warranted a new name.[25] \"Data science\" became more widely used in the next few years: in 2002, the Committee on Data for Science and Technology launched the Data Science Journal. In 2003, Columbia University launched The Journal of Data Science.[25] In 2014, the American Statistical Association's Section on Statistical Learning and Data Mining changed its name to the Section on Statistical Learning and Data Science, reflecting the ascendant popularity of data science.[31]\n The professional title of \"data scientist\" has been attributed to DJ Patil and Jeff Hammerbacher in 2008.[32] Though it was used by the National Science Board in their 2005 report \"Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century\", it referred broadly to any key role in managing a digital data collection.[33]\n There is still no consensus on the definition of data science, and it is considered by some to be a buzzword.[34] Big data is a related marketing term.[35] Data scientists are responsible for breaking down big data into usable information and creating software and algorithms that help companies and organizations determine optimal operations.[36]\n Data science and data analysis are both important disciplines in the field of data management and analysis, but they differ in several key ways. While both fields involve working with data, data science is more of an interdisciplinary field that involves the application of statistical, computational, and machine learning methods to extract insights from data and make predictions, while data analysis is more focused on the examination and interpretation of data to identify patterns and trends.[37][38]\n Data analysis typically involves working with smaller, structured datasets to answer specific questions or solve specific problems. This can involve tasks such as data cleaning, data visualization, and exploratory data analysis to gain insights into the data and develop hypotheses about relationships between variables. Data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data. For example, a data analyst might analyze sales data to identify trends in customer behavior and make recommendations for marketing strategies.[37]\n Data science, on the other hand, is a more complex and iterative process that involves working with larger, more complex datasets that often require advanced computational and statistical methods to analyze. Data scientists often work with unstructured data such as text or images and use machine learning algorithms to build predictive models and make data-driven decisions. In addition to statistical analysis, data science often involves tasks such as data preprocessing, feature engineering, and model selection. For instance, a data scientist might develop a recommendation system for an e-commerce platform by analyzing user behavior patterns and using machine learning algorithms to predict user preferences.[38][39]\n While data analysis focuses on extracting insights from existing data, data science goes beyond that by incorporating the development and implementation of predictive models to make informed decisions. Data scientists are often responsible for collecting and cleaning data, selecting appropriate analytical techniques, and deploying models in real-world scenarios. They work at the intersection of mathematics, computer science, and domain expertise to solve complex problems and uncover hidden patterns in large datasets.[38]\n Despite these differences, data science and data analysis are closely related fields and often require similar skill sets. Both fields require a solid foundation in statistics, programming, and data visualization, as well as the ability to communicate findings effectively to both technical and non-technical audiences. Both fields benefit from critical thinking and domain knowledge, as understanding the context and nuances of the data is essential for accurate analysis and modeling.[37][38]\n In summary, data analysis and data science are distinct yet interconnected disciplines within the broader field of data management and analysis. Data analysis focuses on extracting insights and drawing conclusions from structured data, while data science involves a more comprehensive approach that combines statistical analysis, computational methods, and machine learning to extract insights, build predictive models, and drive data-driven decision-making. Both fields use data to understand patterns, make informed decisions, and solve complex problems across various domains.\n As illustrated in the previous sections, there is substantially some considerable differences between data science, data analysis and statistics. Consequently, just like statistics grew into an independent field from applied mathematics, similarly data science has emerged as a independent field and has gained traction over the recent years. The unique demand for professional skills on computerized data analysis skills has exploded due to the increasing amounts of data emanating from a variety of independent sources. Whereas some of these highly sought skills can be provided by statisticians, the lack of high algorithmic writing skills makes them less preferred than trained data scientists who provide unique expertise on skills such as NoSQL, Apache Hadoop, Cloud Computing platforms and use of complex networks. This paradigm shift has seen various institution craft academic programmes to prepare skilled labor for the market. Some of the institutions offering degree programmes in data science include Stanford University, Harvard University, University of Oxford, ETH Zurich, Meru University[1] among many others.\n \n Cloud computing can offer access to large amounts of computational power and storage.[40] In big data, where volumes of information are continually generated and processed, these platforms can be used to handle complex and resource-intensive analytical tasks.[41]\n Some distributed computing frameworks are designed to handle big data workloads. These frameworks can enable data scientists to process and analyze large datasets in parallel, which can reducing processing times.[42]\n Data science involve collecting, processing, and analyzing data which often including personal and sensitive information. Ethical concerns include potential privacy violations, bias perpetuation, and negative societal impacts [43][44]\n Machine learning models can amplify existing biases present in training data, leading to discriminatory or unfair outcomes.[45][46]\n",
        "doc_number": 66
    },
    {
        "url": "https://en.wikipedia.org/wiki/Knowledge_extraction",
        "content": "Knowledge extraction is the creation of knowledge from structured (relational databases, XML) and unstructured (text, documents, images) sources. The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing. Although it is methodically similar to information extraction (NLP) and ETL (data warehouse), the main criterion is that the extraction result goes beyond the creation of structured information or the transformation into a relational schema. It requires either the reuse of existing formal knowledge (reusing identifiers or ontologies) or the generation of a schema based on the source data.\n The RDB2RDF W3C group [1] is currently standardizing a language for extraction of resource description frameworks (RDF) from relational databases. Another popular example for knowledge extraction is the transformation of Wikipedia into structured data and also the mapping to existing knowledge (see DBpedia and Freebase).\n After the standardization of knowledge representation languages such as RDF and OWL, much research has been conducted in the area, especially regarding transforming relational databases into RDF, identity resolution, knowledge discovery and ontology learning. The general process uses traditional methods from information extraction and extract, transform, and load (ETL), which transform the data from the sources into structured formats. So understanding how the interact and learn from each other.\n The following criteria can be used to categorize approaches in this topic (some of them only account for extraction from relational databases):[2]\n President Obama called Wednesday on Congress to extend a tax break for students included in last year's economic stimulus package, arguing that the policy provides more generous assistance.  When building a RDB representation of a problem domain, the starting point is frequently an entity-relationship diagram (ERD). Typically, each entity is represented as a database table, each attribute of the entity becomes a column in that table, and relationships between entities are indicated by foreign keys. Each table typically defines a particular class of entity, each column one of its attributes. Each row in the table describes an entity\ninstance, uniquely identified by a primary key. The table rows collectively describe an entity set. In an equivalent RDF representation of the same entity set:\n So, to render an equivalent view based on RDF semantics, the basic mapping algorithm would be as follows:\n Early mentioning of this basic or direct mapping can be found in Tim Berners-Lee's comparison of the ER model to the RDF model.[4]\n The 1:1 mapping mentioned above exposes the legacy data as RDF in a straightforward way, additional refinements can be employed to improve the usefulness of RDF output respective the given Use Cases. Normally, information is lost during the transformation of an entity-relationship diagram (ERD) to relational tables (Details can be found in object-relational impedance mismatch) and has to be reverse engineered. From a conceptual view, approaches for extraction can come from two directions. The first direction tries to extract or learn an OWL schema from the given database schema. Early approaches used a fixed amount of manually created mapping rules to refine the 1:1 mapping.[5][6][7] More elaborate methods are employing heuristics or learning algorithms to induce schematic information (methods overlap with ontology learning). While some approaches try to extract the information from the structure inherent in the SQL schema[8] (analysing e.g. foreign keys), others analyse the content and the values in the tables to create conceptual hierarchies[9] (e.g. a columns with few values are candidates for becoming categories). The second direction tries to map the schema and its contents to a pre-existing domain ontology (see also: ontology alignment). Often, however, a suitable domain ontology does not exist and has to be created first.\n As XML is structured as a tree, any data can be easily represented in RDF, which is structured as a graph. XML2RDF is one example of an approach that uses RDF blank nodes and transforms XML elements and attributes to RDF properties. The topic however is more complex as in the case of relational databases. In a relational table the primary key is an ideal candidate for becoming the subject of the extracted triples. An XML element, however, can be transformed - depending on the context- as a subject, a predicate or object of a triple. XSLT can be used a standard transformation language to manually convert XML to RDF.\n The largest portion of information contained in business documents (about 80%[10]) is encoded in natural language and therefore unstructured. Because unstructured data is rather a challenge for knowledge extraction, more sophisticated methods are required, which generally tend to supply worse results compared to structured data. The potential for a massive acquisition of extracted knowledge, however, should compensate the increased complexity and decreased quality of extraction. In the following, natural language sources are understood as sources of information, where the data is given in an unstructured fashion as plain text.  If the given text is additionally embedded in a markup document (e. g. HTML document), the mentioned systems normally remove the markup elements automatically.\n As a preprocessing step to knowledge extraction, it can be necessary to perform linguistic annotation by one or multiple NLP tools. Individual modules in an NLP workflow normally build on tool-specific formats for input and output, but in the context of knowledge extraction, structured formats for representing linguistic annotations have been applied.\n Typical NLP tasks relevant to knowledge extraction include:\n In NLP, such data is typically represented in TSV formats (CSV formats with TAB as separators), often referred to as CoNLL formats. For knowledge extraction workflows, RDF views on such data have been created in accordance with the following community standards:\n Other, platform-specific formats include\n Traditional information extraction[20] is a technology of natural language processing, which extracts information from typically natural language texts and structures these in a suitable manner. The kinds of information to be identified must be specified in a model before beginning the process, which is why the whole process of traditional Information Extraction is domain dependent. The IE is split in the following five subtasks.\n The task of named entity recognition is to recognize and to categorize all named entities contained in a text (assignment of a named entity to a predefined category). This works by application of grammar based methods or statistical models.\n Coreference resolution identifies equivalent entities, which were recognized by NER, within a text. There are two relevant kinds of equivalence relationship. The first one relates to the relationship between two different represented entities (e.g. IBM Europe and IBM) and the second one to the relationship between an entity and their anaphoric references (e.g. it and IBM). Both kinds can be recognized by coreference resolution.\n During template element construction the IE system identifies descriptive properties of entities, recognized by NER and CO. These properties correspond to ordinary qualities like red or big.\n Template relation construction identifies relations, which exist between the template elements. These relations can be of several kinds, such as works-for or located-in, with the restriction, that both domain and range correspond to entities.\n In the template scenario production events, which are described in the text, will be identified and structured with respect to the entities, recognized by NER and CO and relations, identified by TR.\n Ontology-based information extraction [10] is a subfield of information extraction, with which at least one ontology is used to guide the process of information extraction from natural language text. The OBIE system uses methods of traditional information extraction to identify concepts, instances and relations of the used ontologies in the text, which will be structured to an ontology after the process. Thus, the input ontologies constitute the model of information to be extracted.[21]\n Ontology learning is the automatic or semi-automatic creation of ontologies, including extracting the corresponding domain's terms from natural language text. As building ontologies manually is extremely labor-intensive and time consuming, there is great motivation to automate the process.\n During semantic annotation,[22] natural language text is augmented with metadata (often represented in RDFa), which should make the semantics of contained terms machine-understandable. At this process, which is generally semi-automatic, knowledge is extracted in the sense, that a link between lexical terms and for example concepts from ontologies is established. Thus, knowledge is gained, which meaning of a term in the processed context was intended and therefore the meaning of the text is grounded in machine-readable data with the ability to draw inferences. Semantic annotation is typically split into the following two subtasks.\n At the terminology extraction level, lexical terms from the text are extracted. For this purpose a tokenizer determines at first the word boundaries and solves abbreviations. Afterwards terms from the text, which correspond to a concept, are extracted with the help of a domain-specific lexicon to link these at entity linking.\n In entity linking [23] a link between the extracted lexical terms from the source text and the concepts from an ontology or knowledge base such as DBpedia  is established. For this, candidate-concepts are detected appropriately to the several meanings of a term with the help of a lexicon. Finally, the context of the terms is analyzed to determine the most appropriate disambiguation and to assign the term to the correct concept.\n Note that \"semantic annotation\" in the context of knowledge extraction is not to be confused with semantic parsing as understood in natural language processing (also referred to as \"semantic annotation\"): Semantic parsing aims a complete, machine-readable representation of natural language, whereas semantic annotation in the sense of knowledge extraction tackles only a very elementary aspect of that.\n The following criteria can be used to categorize tools, which extract knowledge from natural language text.\n The following table characterizes some tools for Knowledge Extraction from natural language sources.\n Knowledge discovery describes the process of automatically searching large volumes of data for patterns that can be considered knowledge about the data.[44]  It is often described as deriving knowledge from the input data. Knowledge discovery developed out of the data mining domain, and is closely related to it both in terms of methodology and terminology.[45]\n The most well-known branch of data mining is knowledge discovery, also known as knowledge discovery in databases (KDD). Just as many other forms of knowledge discovery it creates abstractions of the input data. The knowledge obtained through the process may become additional data that can be used for further usage and discovery. Often the outcomes from knowledge discovery are not actionable, techniques like domain driven data mining,[46] aims to discover and deliver actionable knowledge and insights.\n Another promising application of knowledge discovery is in the area of software modernization, weakness discovery and compliance which involves understanding existing software artifacts. This process is related to a concept of reverse engineering. Usually the knowledge obtained from existing software is presented in the form of models to which specific queries can be made when necessary. An entity relationship is a frequent format of representing knowledge obtained from existing software. Object Management Group (OMG) developed the specification Knowledge Discovery Metamodel (KDM) which defines an ontology for the software assets and their relationships for the purpose of performing knowledge discovery in existing code. Knowledge discovery from existing software systems, also known as software mining is closely related to data mining, since existing software artifacts contain enormous value for risk management and business value, key for the evaluation and evolution of software systems. Instead of mining individual data sets, software mining focuses on metadata, such as process flows (e.g. data flows, control flows, & call maps), architecture, database schemas, and business rules/terms/process.\n",
        "doc_number": 67
    },
    {
        "url": "https://en.wikipedia.org/wiki/Machine_perception",
        "content": "Machine perception is the capability of a computer system to interpret data in a manner that is similar to the way humans use their senses to relate to the world around them.[1][2][3] The basic method that the computers take in and respond to their environment is through the attached hardware.  Until recently input was limited to a keyboard, or a mouse, but advances in technology, both in hardware and software, have allowed computers to take in sensory input in a way similar to humans.[1][2]\n Machine perception allows the computer to use this sensory input, as well as conventional computational means of gathering information, to gather information with greater accuracy and to present it in a way that is more comfortable for the user.[1] These include computer vision, machine hearing, machine touch, and machine smelling, as artificial scents are, at a chemical compound, molecular, atomic level, indiscernible and identical.[4][5]\n The end goal of machine perception is to give machines the ability to see, feel and perceive the world as humans do and therefore for them to be able to explain in a human way why they are making their decisions, to warn us when it is failing and more importantly, the reason why it is failing.[6] This purpose is very similar to the proposed purposes for artificial intelligence generally, except that machine perception would only grant machines limited sentience, rather than bestow upon machines full consciousness, self-awareness, and intentionality.\n Computer vision is a field that includes methods for acquiring, processing, analyzing, and understanding images and high-dimensional data from the real world to produce numerical or symbolic information, e.g., in the forms of decisions.  Computer vision has many applications already in use today such as facial recognition, geographical modeling, and even aesthetic judgment.[7]\n However, machines still struggle to interpret visual impute accurately if said impute is blurry, and if the viewpoint at which stimulus are viewed varies often. Computers also struggle to determine the proper nature of some stimulus if overlapped by or seamlessly touching another stimulus. This refers to The Principle of Good Continuation. Machines also struggle to perceive and record stimulus functioning according to the Apparent Movement principle which Gestalt psychologists researched.\n Machine hearing, also known as machine listening or computer audition, is the ability of a computer or machine to take in and process sound data such as speech or music.[8][9]\nThis area has a wide range of application including music recording and compression, speech synthesis, and speech recognition.[10]\nMoreover, this technology allows the machine to replicate the human brain's ability to selectively focus on a specific sound against many other competing sounds and background noise. This particular ability is called \"auditory scene analysis\". The technology enables the machine to segment several streams occurring at the same time.[8][11][12]\nMany commonly used devices such as a smartphones, voice translators, and cars make use of some form of machine hearing. Present technology still occasionally struggles with speech segmentation though. This means hearing words within sentences, especially when human accents are accounted for.\n Machine touch is an area of machine perception where tactile information is processed by a machine or computer.  Applications include tactile perception of surface properties and dexterity whereby tactile information can enable intelligent reflexes and interaction with the environment.[13] (This could possibly be done through measuring when and where friction occurs, and of what nature and intensity the friction is). Machines however still do not have any way of measuring some physical human experiences we consider ordinary, including physical pain. For example, scientists have yet to invent a mechanical substitute for the Nociceptors in the body and brain that are responsible for noticing and measuring physical human discomfort and suffering.\n Scientists are developing computers known as machine olfaction which can recognize and measure smells as well. Airborne chemicals are sensed and classified with a device sometimes known as an electronic nose.[14][15]\n The electronic tongue is an instrument that measures and compares tastes. As per the IUPAC technical report, an \u201celectronic tongue\u201d as analytical instrument including an array of non-selective chemical sensors with partial specificity to different solution components and an appropriate pattern recognition instrument, capable to recognize quantitative and qualitative compositions of simple and complex solutions[16][17]\n Chemical compounds responsible for taste are detected by human taste receptors. Similarly, the multi-electrode sensors of electronic instruments detect the same dissolved organic and inorganic compounds. Like human receptors, each sensor has a spectrum of reactions different from the other. The information given by each sensor is complementary, and the combination of all sensors' results generates a unique fingerprint. Most of the detection thresholds of sensors are similar to or better than human receptors.\n In the biological mechanism, taste signals are transduced by nerves in the brain into electric signals. E-tongue sensors process is similar: they generate electric signals as voltammetric and potentiometric variations.\n Other than those listed above, some of the future hurdles that the science of machine perception still has to overcome include, but are not limited to:\n - Embodied cognition - The theory that cognition is a full body experience, and therefore can only exist, and therefore be measure and analyzed, in fullness if all required human abilities and processes are working together through a mutually aware and supportive systems network.\n - The Moravec's paradox (see the link)\n - The Principle of similarity - The ability young children develop to determine what family a newly introduced stimulus falls under even when the said stimulus is different from the members with which the child usually associates said family with. (An example could be a child figuring that a chihuahua is a dog and house pet rather than vermin.)\n - The Unconscious inference: The natural human behavior of determining if a new stimulus is dangerous or not, what it is, and then how to relate to it without ever requiring any new conscious effort.\n - The innate human ability to follow the likelihood principle in order to learn from circumstances and others over time.\n - The recognition-by-components theory - being able to mentally analyze and break even complicated mechanisms into manageable parts with which to interact with. For example: A person seeing both the cup and the handle parts that make up a mug full of hot cocoa, in order to use the handle to hold the mug so as to avoid being burned.\n - The free energy principle - determining long before hand how much energy one can safely delegate to being aware of things outside one's self without the loss of the needed energy one requires for sustaining their life and function satisfactorily. This allows one to become both optimally aware of the world around them self without depleting their energy so much that they experience damaging stress, decision fatigue, and/or exhaustion.\n",
        "doc_number": 68
    },
    {
        "url": "https://en.wikipedia.org/wiki/Multi-agent_system",
        "content": "\n A multi-agent system (MAS or \"self-organized system\") is a computerized system composed of multiple interacting intelligent agents.[1] Multi-agent systems can solve problems that are difficult or impossible for an individual agent or a monolithic system to solve.[2] Intelligence may include methodic, functional, procedural approaches, algorithmic search or reinforcement learning.[3] With advancements in Large language model (LLMs), LLM-based multi-agent systems have emerged as a new area of research, enabling more sophisticated interactions and coordination among agents.[4]\n Despite considerable overlap, a multi-agent system is not always the same as an agent-based model (ABM).  The goal of an ABM is to search for explanatory insight into the collective behavior of agents (which do not necessarily need to be \"intelligent\") obeying simple rules, typically in natural systems, rather than in solving specific practical or engineering problems. The terminology of ABM tends to be used more often in the science, and MAS in engineering and technology.[5] Applications where multi-agent systems research may deliver an appropriate approach include online trading,[6] disaster response,[7][8] target surveillance[9] and social structure modelling.[10]\n Multi-agent systems consist of agents and their environment. Typically multi-agent systems research refers to software agents. However, the agents in a multi-agent system could equally well be robots, humans or human teams. A multi-agent system may contain combined human-agent teams.\n Agents can be divided into types spanning simple to complex. Categories include:\n Agent environments can be divided into:\n Agent environments can also be organized according to properties such as accessibility (whether it is possible to gather complete information about the environment), determinism (whether an action causes a definite effect), dynamics (how many entities influence the environment in the moment), discreteness (whether the number of possible actions in the environment is finite), episodicity (whether agent actions in certain time periods influence other periods),[12] and dimensionality (whether spatial characteristics are important factors of the environment and the agent considers space in its decision making).[13] Agent actions are typically mediated via an appropriate middleware. This middleware offers a first-class design abstraction for multi-agent systems, providing means to govern resource access and agent coordination.[14]\n The agents in a multi-agent system have several important characteristics:[15]\n Multi-agent systems can manifest self-organisation as well as self-direction and other control paradigms and related complex behaviors even when the individual strategies of all their agents are simple.[citation needed] When agents can share knowledge using any agreed language, within the constraints of the system's communication protocol, the approach may lead to a common improvement. Example languages are Knowledge Query Manipulation Language (KQML) or Agent Communication Language (ACL).\n Many MAS are implemented in computer simulations, stepping the system through discrete \"time steps\". The MAS components communicate typically using a weighted request matrix, e.g. \n and a weighted response matrix, e.g. \n A challenge-response-contract scheme is common in MAS systems, where \n also considering other components, evolving \"contracts\" and the restriction sets of the component algorithms.\n Another paradigm commonly used with MAS is the \"pheromone\", where components leave information for other nearby components. These pheromones may evaporate/concentrate with time, that is their values may decrease (or increase).\n MAS tend to find the best solution for their problems without intervention. There is high similarity here to physical phenomena, such as energy minimizing, where physical objects tend to reach the lowest energy possible within the physically constrained world. For example: many of the cars entering a metropolis in the morning will be available for leaving that same metropolis in the evening.\n The systems also tend to prevent propagation of faults, self-recover and be fault tolerant, mainly due to the redundancy of components.\n The study of multi-agent systems is \"concerned with the development and analysis of sophisticated AI problem-solving and control architectures for both single-agent and multiple-agent systems.\"[17] Research topics include:\n Frameworks have emerged that implement common standards (such as the FIPA and OMG MASIF standards).[23] These frameworks e.g. JADE, save time and aid in the standardization of MAS development.[24]\n Currently though, no standard is actively maintained from FIPA or OMG. Efforts for further development of software agents in industrial context are carried out in IEEE IES technical committee on Industrial Agents.[25]\n With advancements in Large Language Models (LLMs) such as ChatGPT, LLM-based multi-agent frameworks, such as CAMEL,[26][4] have emerged as a new paradigm for developing multi-agent applications.\n MAS have not only been applied in academic research, but also in industry.[27] MAS are applied in the real world to graphical applications such as computer games. Agent systems have been used in films.[28] It is widely advocated for use in networking and mobile technologies, to achieve automatic and dynamic load balancing, high scalability and self-healing networks. They are being used for coordinated defence systems.\n Other applications[29] include transportation,[30] logistics,[31] graphics, manufacturing, power system,[32] smartgrids,[33] and the GIS.\n Also, Multi-agent Systems Artificial Intelligence (MAAI) are used for simulating societies, the purpose thereof being helpful in the fields of climate, energy, epidemiology, conflict management, child abuse, ....[34] Some organisations working on using multi-agent system models include Center for Modelling Social Systems, Centre for Research in Social Simulation, Centre for Policy Modelling, Society for Modelling and Simulation International.[34]\n Vehicular traffic with controlled autonomous vehicles can be modelling as a multi-agent system involving crowd dynamics.[35]\nHallerbach et al. discussed the application of agent-based approaches for the development and validation of automated driving systems via a digital twin of the vehicle-under-test and microscopic traffic simulation based on independent agents.[36] Waymo has created a multi-agent simulation environment Carcraft to test algorithms for self-driving cars.[37][38] It simulates traffic interactions between human drivers, pedestrians and automated vehicles. People's behavior is imitated by artificial agents based on data of real human behavior.\n",
        "doc_number": 69
    },
    {
        "url": "https://en.wikipedia.org/wiki/Natural_language_understanding",
        "content": "Natural language understanding (NLU) or natural language interpretation (NLI)[1] is a subset of natural language processing in artificial intelligence that deals with machine reading comprehension. NLU has been considered an AI-hard problem.[2]\n There is considerable commercial interest in the field because of its application to automated reasoning,[3] machine translation,[4] question answering,[5] news-gathering, text categorization, voice-activation, archiving, and large-scale content analysis.\n The program STUDENT, written in 1964 by Daniel Bobrow for his PhD dissertation at MIT, is one of the earliest known attempts at NLU by a computer.[6][7][8][9][10] Eight years after John McCarthy coined the term artificial intelligence, Bobrow's dissertation (titled Natural Language Input for a Computer Problem Solving System) showed how a computer could understand simple natural language input to solve algebra word problems.\n A year later, in 1965, Joseph Weizenbaum at MIT wrote ELIZA, an interactive program that carried on a dialogue in English on any topic, the most popular being psychotherapy. ELIZA worked by simple parsing and substitution of key words into canned phrases and Weizenbaum sidestepped the problem of giving the program a database of real-world knowledge or a rich lexicon. Yet ELIZA gained surprising popularity as a toy project and can be seen as a very early precursor to current commercial systems such as those used by Ask.com.[11]\n In 1969, Roger Schank at Stanford University introduced the conceptual dependency theory for NLU.[12] This model, partially influenced by the work of Sydney Lamb, was extensively used by Schank's students at Yale University, such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner.\n In 1970, William A. Woods introduced the augmented transition network (ATN) to represent natural language input.[13] Instead of phrase structure rules ATNs used an equivalent set of finite-state automata that were called recursively. ATNs and their more general format called \"generalized ATNs\" continued to be used for a number of years.\n In 1971, Terry Winograd finished writing SHRDLU for his PhD thesis at MIT. SHRDLU could understand simple English sentences in a restricted world of children's blocks to direct a robotic arm to move items. The successful demonstration of SHRDLU provided significant momentum for continued research in the field.[14][15] Winograd continued to be a major influence in the field with the publication of his book Language as a Cognitive Process.[16] At Stanford, Winograd would later advise Larry Page, who co-founded Google.\n In the 1970s and 1980s, the natural language processing group at SRI International continued research and development in the field. A number of commercial efforts based on the research were undertaken, e.g., in 1982 Gary Hendrix formed Symantec Corporation originally as a company for developing a natural language interface for database queries on personal computers. However, with the advent of mouse-driven graphical user interfaces, Symantec changed direction. A number of other commercial efforts were started around the same time, e.g., Larry R. Harris at the Artificial Intelligence Corporation and Roger Schank and his students at Cognitive Systems Corp.[17][18] In 1983, Michael Dyer developed the BORIS system at Yale which bore similarities to the work of Roger Schank and W. G. Lehnert.[19]\n The third millennium saw the introduction of systems using machine learning for text classification, such as the IBM Watson. However, experts debate how much \"understanding\" such systems demonstrate: e.g., according to John Searle, Watson did not even understand the questions.[20]\n John Ball, cognitive scientist and inventor of the Patom Theory, supports this assessment. Natural language processing has made inroads for applications to support human productivity in service and e-commerce, but this has largely been made possible by narrowing the scope of the application. There are thousands of ways to request something in a human language that still defies conventional natural language processing.[citation needed] According to Wibe Wagemans, \"To have a meaningful conversation with machines is only possible when we match every word to the correct meaning based on the meanings of the other words in the sentence \u2013 just like a 3-year-old does without guesswork.\"[21]\n The umbrella term \"natural language understanding\" can be applied to a diverse set of computer applications, ranging from small, relatively simple tasks such as short commands issued to robots, to highly complex endeavors such as the full comprehension of newspaper articles or poetry passages. Many real-world applications fall between the two extremes, for instance text classification for the automatic analysis of emails and their routing to a suitable department in a corporation does not require an in-depth understanding of the text,[22] but needs to deal with a much larger vocabulary and more diverse syntax than the management of simple queries to database tables with fixed schemata.\n Throughout the years various attempts at processing natural language or English-like sentences presented to computers have taken place at varying degrees of complexity. Some attempts have not resulted in systems with deep understanding, but have helped overall system usability. For example, Wayne Ratliff originally developed the Vulcan program with an English-like syntax to mimic the English speaking computer in Star Trek. Vulcan later became the dBase system whose easy-to-use syntax effectively launched the personal computer database industry.[23][24] Systems with an easy to use or English-like syntax are, however, quite distinct from systems that use a rich lexicon and include an internal representation (often as first order logic) of the semantics of natural language sentences.\n Hence the breadth and depth of \"understanding\" aimed at by a system determine both the complexity of the system (and the implied challenges) and the types of applications it can deal with. The \"breadth\" of a system is measured by the sizes of its vocabulary and grammar. The \"depth\" is measured by the degree to which its understanding approximates that of a fluent native speaker. At the narrowest and shallowest, English-like command interpreters require minimal complexity, but have a small range of applications. Narrow but deep systems explore and model mechanisms of understanding,[25] but they still have limited application. Systems that attempt to understand the contents of a document such as a news release beyond simple keyword matching and to judge its suitability for a user are broader and require significant complexity,[26] but they are still somewhat shallow. Systems that are both very broad and very deep are beyond the current state of the art.\n Regardless of the approach used, most NLU systems share some common components. The system needs a lexicon of the language and a parser and grammar rules to break sentences into an internal representation. The construction of a rich lexicon with a suitable ontology requires significant effort, e.g., the Wordnet lexicon required many person-years of effort.[27]\n The system also needs theory from semantics to guide the comprehension. The interpretation capabilities of a language-understanding system depend on the semantic theory it uses. Competing semantic theories of language have specific trade-offs in their suitability as the basis of computer-automated semantic interpretation.[28] These range from naive semantics or stochastic semantic analysis to the use of pragmatics to derive meaning from context.[29][30][31] Semantic parsers convert natural-language texts into formal meaning representations.[32]\n Advanced applications of NLU also attempt to incorporate logical inference within their framework. This is generally achieved by mapping the derived meaning into a set of assertions in predicate logic, then using logical deduction to arrive at conclusions. Therefore, systems based on functional languages such as Lisp need to include a subsystem to represent logical assertions, while logic-oriented systems such as those using the language Prolog generally rely on an extension of the built-in logical representation framework.[33][34]\n The management of context in NLU can present special challenges. A large variety of examples and counter examples have resulted in multiple approaches to the formal modeling of context, each with specific strengths and weaknesses.[35][36]\n",
        "doc_number": 70
    },
    {
        "url": "https://en.wikipedia.org/wiki/Neural_network_software",
        "content": "Neural network software is used to simulate, research, develop, and apply artificial neural networks, software concepts adapted from biological neural networks, and in some cases, a wider array of adaptive systems such as artificial intelligence and machine learning.\n Neural network simulators are software applications that are used to simulate the behavior of artificial or biological neural networks. They focus on one or a limited number of specific types of neural networks. They are typically stand-alone and not intended to produce general neural networks that can be integrated in other software.  Simulators usually have some form of built-in visualization to monitor the training process. Some simulators also visualize the physical structure of the neural network.\n Historically, the most common type of neural network software was intended for researching neural network structures and algorithms. The primary purpose of this type of software is, through simulation, to gain a better understanding of the behavior and the properties of neural networks.  Today in the study of artificial neural networks, simulators have largely been replaced by more general component based development environments as research platforms.\n Commonly used artificial neural network simulators include the Stuttgart Neural Network Simulator (SNNS), and Emergent.\n In the study of biological neural networks however, simulation software is still the only available approach. In such simulators the physical biological and chemical properties of neural tissue, as well as the electromagnetic impulses between the neurons are studied.\n Commonly used biological network simulators include Neuron, GENESIS, NEST and Brian.\n Unlike the research simulators, data analysis simulators are intended for practical applications of artificial neural networks. Their primary focus is on data mining and forecasting. Data analysis simulators usually have some form of preprocessing capabilities. Unlike the more general development environments, data analysis simulators use a relatively simple static neural network that can be configured.  A majority of the data analysis simulators on the market use backpropagating networks or self-organizing maps as their core. The advantage of this type of software is that it is relatively easy to use. Neural Designer is one example of a data analysis simulator.\n When the Parallel Distributed Processing volumes[1]\n[2][3] were released in 1986-87, they provided some relatively simple software.  The original PDP software did not require any programming skills, which led to its adoption by a wide variety of researchers in diverse fields.  The original PDP software was developed into a more powerful package called PDP++, which in turn has become an even more powerful platform called  Emergent.  With each development, the software has become more powerful, but also more daunting for use by beginners.\n In 1997, the tLearn software was released to accompany a book.[4]  This was a return to the idea of providing a small, user-friendly, simulator that was designed with the novice in mind.  tLearn allowed basic feed forward networks, along with simple recurrent networks, both of which can be trained by the simple back propagation algorithm. tLearn has not been updated since 1999.\n In 2011, the Basic Prop simulator was released.  Basic Prop is a self-contained application, distributed as a platform neutral JAR file, that provides much of the same simple functionality as tLearn.\n Development environments for neural networks differ from the software described above primarily on two accounts \u2013 they can be used to develop custom types of neural networks and they support deployment of the neural network outside the environment. In some cases they have advanced preprocessing, analysis and visualization capabilities.\n A more modern type of development environments that are currently favored in both industrial and scientific use are based on a component based paradigm. The neural network is constructed by connecting adaptive filter components in a pipe filter flow. This allows for greater flexibility as custom networks can be built as well as custom components used by the network. In many cases this allows a combination of adaptive and non-adaptive components to work together. The data flow is controlled by a control system which is exchangeable as well as the adaptation algorithms. The other important feature is deployment capabilities.\n With the advent of component-based frameworks such as .NET and Java, component based development environments are capable of deploying the developed neural network to these frameworks as inheritable components. In addition some software can also deploy these components to several platforms, such as embedded systems.\n Component based development environments include: Peltarion Synapse, NeuroDimension NeuroSolutions, Scientific Software Neuro Laboratory, and the LIONsolver integrated software.  Free open source component based environments include Encog and Neuroph.\n A disadvantage of component-based development environments is that they are more complex than simulators. They require more learning to fully operate and are more complicated to develop.\n The majority implementations of neural networks available are however custom implementations in various programming languages and on various platforms. Basic types of neural networks are simple to implement directly. There are also many programming libraries that contain neural network functionality and that can be used in custom implementations (such as TensorFlow, Theano, etc., typically providing bindings to languages such as Python, C++, Java).\n In order for neural network models to be shared by different applications, a common language is necessary. The Predictive Model Markup Language (PMML) has been proposed to address this need. PMML is an XML-based language which provides a way for applications to define and share neural network models (and other data mining models) between PMML compliant applications.\n PMML provides applications a vendor-independent method of defining models so that proprietary issues and incompatibilities are no longer a barrier to the exchange of models between applications. It allows users to develop models within one vendor's application, and use other vendors' applications to visualize, analyze, evaluate or otherwise use the models. Previously, this was very difficult, but with PMML, the exchange of models between compliant applications is now straightforward.\n A range of products are being offered to produce and consume PMML. This ever-growing list includes the following neural network products:\n",
        "doc_number": 71
    },
    {
        "url": "https://en.wikipedia.org/wiki/Pattern_recognition",
        "content": "Pattern recognition is the task of assigning a class to an observation based on patterns extracted from data. While similar, pattern recognition (PR) is not to be confused with pattern machines (PM) which may possess (PR) capabilities but their primary function is to distinguish and create emergent  patterns.   PR has applications in statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power.\n Pattern recognition systems are commonly trained from labeled \"training\" data. When no labeled data are available, other algorithms can be used to discover previously unknown patterns. KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition focuses more on the signal and also takes acquisition and signal processing into consideration. It originated in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition.\n In machine learning, pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936. An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is \"spam\"). Pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input;[1] sequence labeling, which assigns a class to each member of a sequence of values[2] (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence.[3]\n Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform \"most likely\" matching of the inputs, taking into account their statistical variation. This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is regular expression matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors.\n A modern definition of pattern recognition is:\n The field of pattern recognition is concerned with the automatic discovery of regularities in data through the use of computer algorithms and with the use of these regularities to take actions such as classifying the data into different categories.[4] Pattern recognition is generally categorized according to the type of learning procedure used to generate the output value. Supervised learning assumes that a set of training data (the training set) has been provided, consisting of a set of instances that have been properly labeled by hand with the correct output. A learning procedure then generates a model that attempts to meet two sometimes conflicting objectives: Perform as well as possible on the training data, and generalize as well as possible to new data (usually, this means being as simple as possible, for some technical definition of \"simple\", in accordance with Occam's Razor, discussed below). Unsupervised learning, on the other hand, assumes training data that has not been hand-labeled, and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances.[5] A combination of the two that has been explored is semi-supervised learning, which uses a combination of labeled and unlabeled data (typically a small set of labeled data combined with a large amount of unlabeled data). In cases of unsupervised learning, there may be no training data at all.\n Sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output. The unsupervised equivalent of classification is normally known as clustering, based on the common perception of the task as involving no training data to speak of, and of grouping the input data into clusters based on some inherent similarity measure (e.g. the distance between instances, considered as vectors in a multi-dimensional vector space), rather than assigning each input instance into one of a set of pre-defined classes. In some fields, the terminology is different. In community ecology, the term classification is used to refer to what is commonly known as \"clustering\".\n The piece of input data for which an output value is generated is formally termed an instance. The instance is formally described by a vector of features, which together constitute a description of all known characteristics of the instance. These feature vectors can be seen as defining points in an appropriate multidimensional space, and methods for manipulating vectors in vector spaces can be correspondingly applied to them, such as computing the dot product or the angle between two vectors. Features typically are either categorical (also known as nominal, i.e., consisting of one of a set of unordered items, such as a gender of \"male\" or \"female\", or a blood type of \"A\", \"B\", \"AB\" or \"O\"), ordinal (consisting of one of a set of ordered items, e.g., \"large\", \"medium\" or \"small\"), integer-valued (e.g., a count of the number of occurrences of a particular word in an email) or real-valued (e.g., a measurement of blood pressure). Often, categorical and ordinal data are grouped together, and this is also the case for integer-valued and real-valued data. Many algorithms work only in terms of categorical data and require that real-valued or integer-valued data be discretized into groups (e.g., less than 5, between 5 and 10, or greater than 10).\n Many common pattern recognition algorithms are probabilistic in nature, in that they use statistical inference to find the best label for a given instance. Unlike other algorithms, which simply output a \"best\" label, often probabilistic algorithms also output a probability of the instance being described by the given label. In addition, many probabilistic algorithms output a list of the N-best labels with associated probabilities, for some value of N, instead of simply a single best label. When the number of possible labels is fairly small (e.g., in the case of classification), N may be set so that the probability of all possible labels is output. Probabilistic algorithms have many advantages over non-probabilistic algorithms:\n Feature selection algorithms attempt to directly prune out redundant or irrelevant features. A general introduction to feature selection which summarizes approaches and challenges, has been given.[6] The complexity of feature-selection is, because of its non-monotonous character, an optimization problem where given a total of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n features the powerset consisting of all \n\n\n\n\n2\n\nn\n\n\n\u2212\n1\n\n\n{\\displaystyle 2^{n}-1}\n\n subsets of features need to be explored. The Branch-and-Bound algorithm[7] does reduce this complexity but is intractable for medium to large values of the number of available features \n\n\n\nn\n\n\n{\\displaystyle n}\n\n\n Techniques to transform the raw feature vectors (feature extraction) are sometimes used prior to application of the pattern-matching algorithm. Feature extraction algorithms attempt to reduce a large-dimensionality feature vector into a smaller-dimensionality vector that is easier to work with and encodes less redundancy, using mathematical techniques such as principal components analysis (PCA). The distinction between feature selection and feature extraction is that the resulting features after feature extraction has taken place are of a different sort than the original features and may not easily be interpretable, while the features left after feature selection are simply a subset of the original features.\n The problem of pattern recognition can be stated as follows: Given an unknown function \n\n\n\ng\n:\n\n\nX\n\n\n\u2192\n\n\nY\n\n\n\n\n{\\displaystyle g:{\\mathcal {X}}\\rightarrow {\\mathcal {Y}}}\n\n (the ground truth) that maps input instances \n\n\n\n\nx\n\n\u2208\n\n\nX\n\n\n\n\n{\\displaystyle {\\boldsymbol {x}}\\in {\\mathcal {X}}}\n\n to output labels \n\n\n\ny\n\u2208\n\n\nY\n\n\n\n\n{\\displaystyle y\\in {\\mathcal {Y}}}\n\n, along with training data \n\n\n\n\nD\n\n=\n{\n(\n\n\nx\n\n\n1\n\n\n,\n\ny\n\n1\n\n\n)\n,\n\u2026\n,\n(\n\n\nx\n\n\nn\n\n\n,\n\ny\n\nn\n\n\n)\n}\n\n\n{\\displaystyle \\mathbf {D} =\\{({\\boldsymbol {x}}_{1},y_{1}),\\dots ,({\\boldsymbol {x}}_{n},y_{n})\\}}\n\n assumed to represent accurate examples of the mapping, produce a function \n\n\n\nh\n:\n\n\nX\n\n\n\u2192\n\n\nY\n\n\n\n\n{\\displaystyle h:{\\mathcal {X}}\\rightarrow {\\mathcal {Y}}}\n\n that approximates as closely as possible the correct mapping \n\n\n\ng\n\n\n{\\displaystyle g}\n\n. (For example, if the problem is filtering spam, then \n\n\n\n\n\nx\n\n\ni\n\n\n\n\n{\\displaystyle {\\boldsymbol {x}}_{i}}\n\n is some representation of an email and \n\n\n\ny\n\n\n{\\displaystyle y}\n\n is either \"spam\" or \"non-spam\"). In order for this to be a well-defined problem, \"approximates as closely as possible\" needs to be defined rigorously. In decision theory, this is defined by specifying a loss function or cost function that assigns a specific value to \"loss\" resulting from producing an incorrect label. The goal then is to minimize the expected loss, with the expectation taken over the probability distribution of \n\n\n\n\n\nX\n\n\n\n\n{\\displaystyle {\\mathcal {X}}}\n\n. In practice, neither the distribution of \n\n\n\n\n\nX\n\n\n\n\n{\\displaystyle {\\mathcal {X}}}\n\n nor the ground truth function \n\n\n\ng\n:\n\n\nX\n\n\n\u2192\n\n\nY\n\n\n\n\n{\\displaystyle g:{\\mathcal {X}}\\rightarrow {\\mathcal {Y}}}\n\n are known exactly, but can be computed only empirically by collecting a large number of samples of \n\n\n\n\n\nX\n\n\n\n\n{\\displaystyle {\\mathcal {X}}}\n\n and hand-labeling them using the correct value of \n\n\n\n\n\nY\n\n\n\n\n{\\displaystyle {\\mathcal {Y}}}\n\n (a time-consuming process, which is typically the limiting factor in the amount of data of this sort that can be collected). The particular loss function depends on the type of label being predicted. For example, in the case of classification, the simple zero-one loss function is often sufficient. This corresponds simply to assigning a loss of 1 to any incorrect labeling and implies that the optimal classifier minimizes the error rate on independent test data (i.e. counting up the fraction of instances that the learned function \n\n\n\nh\n:\n\n\nX\n\n\n\u2192\n\n\nY\n\n\n\n\n{\\displaystyle h:{\\mathcal {X}}\\rightarrow {\\mathcal {Y}}}\n\n labels wrongly, which is equivalent to maximizing the number of correctly classified instances). The goal of the learning procedure is then to minimize the error rate (maximize the correctness) on a \"typical\" test set.\n For a probabilistic pattern recognizer, the problem is instead to estimate the probability of each possible output label given a particular input instance, i.e., to estimate a function of the form\n where the feature vector input is \n\n\n\n\nx\n\n\n\n{\\displaystyle {\\boldsymbol {x}}}\n\n, and the function f is typically parameterized by some parameters \n\n\n\n\n\u03b8\n\n\n\n{\\displaystyle {\\boldsymbol {\\theta }}}\n\n.[8] In a discriminative approach to the problem, f is estimated directly. In a generative approach, however, the inverse probability \n\n\n\np\n(\n\n\nx\n\n\n|\n\n\n\nl\na\nb\ne\nl\n\n\n\n)\n\n\n{\\displaystyle p({{\\boldsymbol {x}}|{\\rm {label}}})}\n\n is instead estimated and combined with the prior probability \n\n\n\np\n(\n\n\nl\na\nb\ne\nl\n\n\n\n|\n\n\n\u03b8\n\n)\n\n\n{\\displaystyle p({\\rm {label}}|{\\boldsymbol {\\theta }})}\n\n using Bayes' rule, as follows:\n When the labels are continuously distributed (e.g., in regression analysis), the denominator involves integration rather than summation:\n The value of \n\n\n\n\n\u03b8\n\n\n\n{\\displaystyle {\\boldsymbol {\\theta }}}\n\n is typically learned using maximum a posteriori (MAP) estimation. This finds the best value that simultaneously meets two conflicting objects: To perform as well as possible on the training data (smallest error-rate) and to find the simplest possible model. Essentially, this combines maximum likelihood estimation with a regularization procedure that favors simpler models over more complex models. In a Bayesian context, the regularization procedure can be viewed as placing a prior probability \n\n\n\np\n(\n\n\u03b8\n\n)\n\n\n{\\displaystyle p({\\boldsymbol {\\theta }})}\n\n on different values of \n\n\n\n\n\u03b8\n\n\n\n{\\displaystyle {\\boldsymbol {\\theta }}}\n\n. Mathematically:\n where \n\n\n\n\n\n\u03b8\n\n\n\u2217\n\n\n\n\n{\\displaystyle {\\boldsymbol {\\theta }}^{*}}\n\n is the value used for \n\n\n\n\n\u03b8\n\n\n\n{\\displaystyle {\\boldsymbol {\\theta }}}\n\n in the subsequent evaluation procedure, and \n\n\n\np\n(\n\n\u03b8\n\n\n|\n\n\nD\n\n)\n\n\n{\\displaystyle p({\\boldsymbol {\\theta }}|\\mathbf {D} )}\n\n, the posterior probability of \n\n\n\n\n\u03b8\n\n\n\n{\\displaystyle {\\boldsymbol {\\theta }}}\n\n, is given by\n In the Bayesian approach to this problem, instead of choosing a single parameter vector \n\n\n\n\n\n\u03b8\n\n\n\u2217\n\n\n\n\n{\\displaystyle {\\boldsymbol {\\theta }}^{*}}\n\n, the probability of a given label for a new instance \n\n\n\n\nx\n\n\n\n{\\displaystyle {\\boldsymbol {x}}}\n\n is computed by integrating over all possible values of \n\n\n\n\n\u03b8\n\n\n\n{\\displaystyle {\\boldsymbol {\\theta }}}\n\n, weighted according to the posterior probability:\n The first pattern classifier \u2013 the linear discriminant presented by Fisher \u2013 was developed in the frequentist tradition. The frequentist approach entails that the model parameters are considered unknown, but objective. The parameters are then computed (estimated) from the collected data. For the linear discriminant, these parameters are precisely the mean vectors and the covariance matrix. Also the probability of each class \n\n\n\np\n(\n\n\nl\na\nb\ne\nl\n\n\n\n|\n\n\n\u03b8\n\n)\n\n\n{\\displaystyle p({\\rm {label}}|{\\boldsymbol {\\theta }})}\n\n is estimated from the collected dataset. Note that the usage of 'Bayes rule' in a pattern classifier does not make the classification approach Bayesian.\n Bayesian statistics has its origin in Greek philosophy where a distinction was already made between the 'a priori' and the 'a posteriori' knowledge. Later Kant defined his distinction between what is a priori known \u2013 before observation \u2013 and the empirical knowledge gained from observations. In a Bayesian pattern classifier, the class probabilities \n\n\n\np\n(\n\n\nl\na\nb\ne\nl\n\n\n\n|\n\n\n\u03b8\n\n)\n\n\n{\\displaystyle p({\\rm {label}}|{\\boldsymbol {\\theta }})}\n\n can be chosen by the user, which are then a priori. Moreover, experience quantified as a priori parameter values can be weighted with empirical observations \u2013 using e.g., the Beta- (conjugate prior) and Dirichlet-distributions. The Bayesian approach facilitates a seamless intermixing between expert knowledge in the form of subjective probabilities, and objective observations.\n Probabilistic pattern classifiers can be used according to a frequentist or a Bayesian approach.\n Within medical science, pattern recognition is the basis for computer-aided diagnosis (CAD) systems. CAD describes a procedure that supports the doctor's interpretations and findings. Other typical applications of pattern recognition techniques are automatic speech recognition, speaker identification, classification of text into several categories (e.g., spam or non-spam email messages), the automatic recognition of handwriting on postal envelopes, automatic recognition of images of human faces, or handwriting image extraction from medical forms.[9][10] The last two examples form the subtopic image analysis of pattern recognition that deals with digital images as input to pattern recognition systems.[11][12]\n Optical character recognition is an example of the application of a pattern classifier. The method of signing one's name was captured with stylus and overlay starting in 1990.[citation needed] The strokes, speed, relative min, relative max, acceleration and pressure is used to uniquely identify and confirm identity. Banks were first offered this technology, but were content to collect from the FDIC for any bank fraud and did not want to inconvenience customers.[citation needed]\n Pattern recognition has many real-world applications in image processing. Some examples include:\n In psychology, pattern recognition is used to make sense of and identify objects, and is closely related to perception. This explains how the sensory inputs humans receive are made meaningful. Pattern recognition can be thought of in two different ways. The first concerns template matching and the second concerns feature detection. A template is a pattern used to produce items of the same proportions. The template-matching hypothesis suggests that incoming stimuli are compared with templates in the long-term memory. If there is a match, the stimulus is identified. Feature detection models, such as the Pandemonium system for classifying letters (Selfridge, 1959), suggest that the stimuli are broken down into their component parts for identification. One observation is a capital E having three horizontal lines and one vertical line.[22]\n Algorithms for pattern recognition depend on the type of label output, on whether learning is supervised or unsupervised, and on whether the algorithm is statistical or non-statistical in nature. Statistical algorithms can further be categorized as generative or discriminative.\n Parametric:[23]\n Nonparametric:[24]\n Unsupervised:\n \n",
        "doc_number": 72
    },
    {
        "url": "https://en.wikipedia.org/wiki/Predictive_analytics",
        "content": "Predictive analytics, or predictive AI, encompasses a variety of statistical techniques from data mining, predictive modeling, and machine learning that analyze current and historical facts to make predictions about future or otherwise unknown events.[1]\n In business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision-making for candidate transactions.[2]\n The defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.\n Predictive analytics is a set of business intelligence (BI) technologies that uncovers relationships and patterns within large volumes of data that can be used to predict behavior and events. Unlike other BI technologies, predictive analytics is forward-looking, using past events to anticipate the future.[3] Predictive analytics statistical techniques include data modeling, machine learning, AI, deep learning algorithms and data mining. Often the unknown event of interest is in the future, but predictive analytics can be applied to any type of unknown whether it be in the past, present or future. For example, identifying suspects after a crime has been committed, or credit card fraud as it occurs.[4] The core of predictive analytics relies on capturing relationships between explanatory variables and the predicted variables from past occurrences, and exploiting them to predict the unknown outcome. It is important to note, however, that the accuracy and usability of results will depend greatly on the level of data analysis and the quality of assumptions.[1]\n Predictive analytics is often defined as predicting at a more detailed level of granularity, i.e., generating predictive scores (probabilities) for each individual organizational element. This distinguishes it from forecasting. For example, \"Predictive analytics\u2014Technology that learns from experience (data) to predict the future behavior of individuals in order to drive better decisions.\"[5] In future industrial systems, the value of predictive analytics will be to predict and prevent potential issues to achieve near-zero break-down and further be integrated into prescriptive analytics for decision optimization.[6]\n The approaches and techniques used to conduct predictive analytics can broadly be grouped into regression techniques and machine learning techniques.\n Machine learning can be defined as the ability of a machine to learn and then mimic human behavior that requires intelligence. This is accomplished through artificial intelligence, algorithms, and models.[7]\n ARIMA models are a common example of time series models. These models use autoregression, which means the model can be fitted with a regression software that will use machine learning to do most of the regression analysis and smoothing. ARIMA models are known to have no overall trend, but instead have a variation around the average that has a constant amplitude, resulting in statistically similar time patterns. Through this, variables are analyzed and data is filtered in order to better understand and predict future values.[8][9]\n One example of an ARIMA method is exponential smoothing models. Exponential smoothing takes into account the difference in importance between older and newer data sets, as the more recent data is more accurate and valuable in predicting future values. In order to accomplish this, exponents are utilized to give newer data sets a larger weight in the calculations than the older sets.[10]\n Time series models are a subset of machine learning that utilize time series in order to understand and forecast data using past values. A time series is the sequence of a variable's value over equally spaced periods, such as years or quarters in business applications.[11] To accomplish this, the data must be smoothed, or the random variance of the data must be removed in order to reveal trends in the data. There are multiple ways to accomplish this.\n Single moving average methods utilize smaller and smaller numbered sets of past data to decrease error that is associated with taking a single average, making it a more accurate average than it would be to take the average of the entire data set.[12]\n Centered moving average methods utilize the data found in the single moving average methods by taking an average of the median-numbered data set. However, as the median-numbered data set is difficult to calculate with even-numbered data sets, this method works better with odd-numbered data sets than even.[13]\n Predictive modeling is a statistical technique used to predict future behavior. It utilizes predictive models to analyze a relationship between a specific unit in a given sample and one or more features of the unit. The objective of these models is to assess the possibility that a unit in another sample will display the same pattern. Predictive model solutions can be considered a type of data mining technology. The models can analyze both historical and current data and generate a model in order to predict potential future outcomes.[14]\n Regardless of the methodology used, in general, the process of creating predictive models involves the same steps. First, it is necessary to determine the project objectives and desired outcomes and translate these into predictive analytic objectives and tasks. Then, analyze the source data to determine the most appropriate data and model building approach (models are only as useful as the applicable data used to build them). Select and transform the data in order to create models. Create and test models in order to evaluate if they are valid and will be able to meet project goals and metrics. Apply the model's results to appropriate business processes (identifying patterns in the data doesn't necessarily mean a business will understand how to take advantage or capitalize on it). Afterward, manage and maintain models in order to standardize and improve performance (demand will increase for model management in order to meet new compliance regulations).[3]\n Generally, regression analysis uses structural data along with the past values of independent variables and the relationship between them and the dependent variable to form predictions.[8]\n In linear regression, a plot is constructed with the previous values of the dependent variable plotted on the Y-axis and the independent variable that is being analyzed plotted on the X-axis. A regression line is then constructed by a statistical program representing the relationship between the independent and dependent variables which can be used to predict values of the dependent variable based only on the independent variable. With the regression line, the program also shows a slope intercept equation for the line which includes an addition for the error term of the regression, where the higher the value of the error term the less precise the regression model is. In order to decrease the value of the error term, other independent variables are introduced to the model, and similar analyses are performed on these independent variables.[8][15]\n An important aspect of auditing includes analytical review. In analytical review, the reasonableness of reported account balances being investigated is determined. Auditors accomplish this process through predictive modeling to form predictions called conditional expectations of the balances being audited using autoregressive integrated moving average (ARIMA) methods and general regression analysis methods,[8] specifically through the Statistical Technique for Analytical Review (STAR) methods.[16]\n The ARIMA method for analytical review uses time-series analysis on past audited balances in order to create the conditional expectations. These conditional expectations are then compared to the actual balances reported on the audited account in order to determine how close the reported balances are to the expectations. If the reported balances are close to the expectations, the accounts are not audited further. If the reported balances are very different from the expectations, there is a higher possibility of a material accounting error and a further audit is conducted.[16]\n Regression analysis methods are deployed in a similar way, except the regression model used assumes the availability of only one independent variable. The materiality of the independent variable contributing to the audited account balances are determined using past account balances along with present structural data.[8] Materiality is the importance of an independent variable in its relationship to the dependent variable.[17] In this case, the dependent variable is the account balance. Through this the most important independent variable is used in order to create the conditional expectation and, similar to the ARIMA method, the conditional expectation is then compared to the account balance reported and a decision is made based on the closeness of the two balances.[8]\n The STAR methods operate using regression analysis, and fall into two methods. The first is the STAR monthly balance approach, and the conditional expectations made and regression analysis used are both tied to one month being audited. The other method is the STAR annual balance approach, which happens on a larger scale by basing the conditional expectations and regression analysis on one year being audited. Besides the difference in the time being audited, both methods operate the same, by comparing expected and reported balances to determine which accounts to further investigate.[16]\n As we move into a world of technological advances where more and more data is created and stored digitally, businesses are looking for ways to take advantage of this opportunity and use this information to help generate profits. Predictive analytics can be used and is capable of providing many benefits to a wide range of businesses, including asset management firms, insurance companies, communication companies, and many other firms. In a study conducted by IDC Analyze the Future, Dan Vasset and Henry D. Morris explain how an asset management firm used predictive analytics to develop a better marketing campaign. They went from a mass marketing approach to a customer-centric approach, where instead of sending the same offer to each customer, they would personalize each offer based on their customer. Predictive analytics was used to predict the likelihood that a possible customer would accept a personalized offer. Due to the marketing campaign and predictive analytics, the firm's acceptance rate skyrocketed, with three times the number of people accepting their personalized offers.[18]\n Technological advances in predictive analytics have increased its value to firms. One technological advancement is more powerful computers, and with this predictive analytics has become able to create forecasts on large data sets much faster. With the increased computing power also comes more data and applications, meaning a wider array of inputs to use with predictive analytics. Another technological advance includes a more user-friendly interface, allowing a smaller barrier of entry and less extensive training required for employees to utilize the software and applications effectively. Due to these advancements, many more corporations are adopting predictive analytics and seeing the benefits in employee efficiency and effectiveness, as well as profits.[19]\n ARIMA univariate and multivariate models can be used in forecasting a company's future cash flows, with its equations and calculations based on the past values of certain factors contributing to cash flows. Using time-series analysis, the values of these factors can be analyzed and extrapolated to predict the future cash flows for a company. For the univariate models, past values of cash flows are the only factor used in the prediction. Meanwhile the multivariate models use multiple factors related to accrual data, such as operating income before depreciation.[20]\n Another model used in predicting cash-flows was developed in 1998 and is known as the Dechow, Kothari, and Watts model, or DKW (1998). DKW (1998) uses regression analysis in order to determine the relationship between multiple variables and cash flows. Through this method, the model found that cash-flow changes and accruals are negatively related, specifically through current earnings, and using this relationship predicts the cash flows for the next period. The DKW (1998) model derives this relationship through the relationships of accruals and cash flows to accounts payable and receivable, along with inventory.[21]\n Some child welfare agencies have started using predictive analytics to flag high risk cases.[22] For example, in Hillsborough County, Florida, the child welfare agency's use of a predictive modeling tool has prevented abuse-related child deaths in the target population.[23]\n The predicting of the outcome of juridical decisions can be done by AI programs. These programs can be used as assistive tools for professions in this industry.[24][25]\n Often the focus of analysis is not the consumer but the product, portfolio, firm, industry or even the economy. For example, a retailer might be interested in predicting store-level demand for inventory management purposes. Or the Federal Reserve Board might be interested in predicting the unemployment rate for the next year. These types of problems can be addressed by predictive analytics using time series techniques (see below). They can also be addressed via machine learning approaches which transform the original time series into a feature vector space, where the learning algorithm finds patterns that have predictive power.[26][27]\n Many businesses have to account for risk exposure due to their different services and determine the costs needed to cover the risk. Predictive analytics can help underwrite these quantities by predicting the chances of illness, default, bankruptcy, etc. Predictive analytics can streamline the process of customer acquisition by predicting the future risk behavior of a customer using application level data. Predictive analytics in the form of credit scores have reduced the amount of time it takes for loan approvals, especially in the mortgage market. Proper predictive analytics can lead to proper pricing decisions, which can help mitigate future risk of default. Predictive analytics can be used to mitigate moral hazard and prevent accidents from occurring.[28]\n",
        "doc_number": 73
    },
    {
        "url": "https://en.wikipedia.org/wiki/Recommender_system",
        "content": "\n A recommender system (RecSys), or a recommendation system (sometimes replacing system with terms such as platform, engine, or algorithm), is a subclass of information filtering system that provides suggestions for items that are most pertinent to a particular user.[1][2][3] Recommender systems are particularly useful when an individual needs to choose an item from a potentially overwhelming number of items that a service may offer.[1][4]\n Typically, the suggestions refer to various decision-making processes, such as what product to purchase, what music to listen to, or what online news to read.[1]\nRecommender systems are used in a variety of areas, with commonly recognised examples taking the form of playlist generators for video and music services, product recommenders for online stores, or content recommenders for social media platforms and open web content recommenders.[5][6] These systems can operate using a single type of input, like music, or multiple inputs within and across platforms like news, books and search queries. There are also popular recommender systems for specific topics like restaurants and online dating. Recommender systems have also been developed to explore research articles and experts,[7] collaborators,[8] and financial services.[9]\n A content discovery platform is an implemented software recommendation platform which uses recommender system tools. It utilizes user metadata in order to discover and recommend appropriate content, whilst reducing ongoing maintenance and development costs. A content discovery platform delivers personalized content to websites, mobile devices and set-top boxes. A large range of content discovery platforms currently exist for various forms of content ranging from news articles and academic journal articles[10] to television.[11] As operators compete to be the gateway to home entertainment, personalized television is a key service differentiator. Academic content discovery has recently become another area of interest, with several companies being established to help academic researchers keep up to date with relevant academic content and serendipitously discover new content.[10]\n Recommender systems usually make use of either or both collaborative filtering and content-based filtering, as well as other systems such as knowledge-based systems. Collaborative filtering approaches build a model from a user's past behavior (items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users. This model is then used to predict items (or ratings for items) that the user may have an interest in.[12] Content-based filtering approaches utilize a series of discrete, pre-tagged characteristics of an item in order to recommend additional items with similar properties.[13]\n The differences between collaborative and content-based filtering can be demonstrated by comparing two early music recommender systems, Last.fm and Pandora Radio.\n Each type of system has its strengths and weaknesses. In the above example, Last.fm requires a large amount of information about a user to make accurate recommendations. This is an example of the cold start problem, and is common in collaborative filtering systems.[15][16][17][18][19][20] Whereas Pandora needs very little information to start, it is far more limited in scope (for example, it can only make recommendations that are similar to the original seed).\n Recommender systems are a useful alternative to search algorithms since they help users discover items they might not have found otherwise. Of note, recommender systems are often implemented using search engines indexing non-traditional data.\n Recommender systems have been the focus of several granted patents,[21][22][23][24][25] and there are more than 50 software libraries[26] that support the development of recommender systems including LensKit,[27][28] RecBole,[29] ReChorus[30] and RecPack.[31]\n Elaine Rich created the first recommender system in 1979, called Grundy.[32][33] She looked for a way to recommend users books they might like. Her idea was to create a system that asks users specific questions and classifies them into classes of preferences, or \"stereotypes\", depending on their answers. Depending on users' stereotype membership, they would then get recommendations for books they might like.\n Another early recommender system, called a \"digital bookshelf\", was described in a 1990 technical report by Jussi Karlgren at Columbia University,\n[34]\nand implemented at scale and worked through in technical reports and publications from 1994 onwards by Jussi Karlgren, then at SICS,[35][36]\nand research groups led by Pattie Maes at MIT,[37] Will Hill at Bellcore,[38] and Paul Resnick, also at MIT,[39][4] whose work with GroupLens was awarded the 2010 ACM Software Systems Award.\n Montaner provided the first overview of recommender systems from an intelligent agent perspective.[40] Adomavicius provided a new, alternate overview of recommender systems.[41]  Herlocker provides an additional overview of evaluation techniques for recommender systems,[42] and Beel et al. discussed the problems of offline evaluations.[43] Beel et al. have also provided literature surveys on available research paper recommender systems and existing challenges.[44][45]\n One approach to the design of recommender systems that has wide use is collaborative filtering.[46] Collaborative filtering is based on the assumption that people who agreed in the past will agree in the future, and that they will like similar kinds of items as they liked in the past. The system generates recommendations using only information about rating profiles for different users or items. By locating peer users/items with a rating history similar to the current user or item, they generate recommendations using this neighborhood. Collaborative filtering methods are classified as memory-based and model-based. A well-known example of memory-based approaches is the user-based algorithm,[47] while that of model-based approaches is matrix factorization (recommender systems).[48]\n A key advantage of the collaborative filtering approach is that it does not rely on machine analyzable content and therefore it is capable of accurately recommending complex items such as movies without requiring an \"understanding\" of the item itself. Many algorithms have been used in measuring user similarity or item similarity in recommender systems. For example, the k-nearest neighbor (k-NN) approach[49] and the Pearson Correlation as first implemented by Allen.[50]\n When building a model from a user's behavior, a distinction is often made between explicit and implicit forms of data collection.\n Examples of explicit data collection include the following:\n Examples of implicit data collection include the following:\n Collaborative filtering approaches often suffer from three problems: cold start, scalability, and sparsity.[52]\n One of the most famous examples of collaborative filtering is item-to-item collaborative filtering (people who buy x also buy y), an algorithm popularized by Amazon.com's recommender system.[54]\n Many social networks originally used collaborative filtering to recommend new friends, groups, and other social connections by examining the network of connections between a user and their friends.[1] Collaborative filtering is still used as part of hybrid systems.\n Another common approach when designing recommender systems is content-based filtering. Content-based filtering methods are based on a description of the item and a profile of the user's preferences.[55][56] These methods are best suited to situations where there is known data on an item (name, location, description, etc.), but not on the user. Content-based recommenders treat recommendation as a user-specific classification problem and learn a classifier for the user's likes and dislikes based on an item's features.\n In this system, keywords are used to describe the items, and a user profile is built to indicate the type of item this user likes. In other words, these algorithms try to recommend items similar to those that a user liked in the past or is examining in the present. It does not rely on a user sign-in mechanism to generate this often temporary profile. In particular, various candidate items are compared with items previously rated by the user, and the best-matching items are recommended. This approach has its roots in information retrieval and information filtering research.\n To create a user profile, the system mostly focuses on two types of information:\n Basically, these methods use an item profile (i.e., a set of discrete attributes and features) characterizing the item within the system. To abstract the features of the items in the system, an item presentation algorithm is applied. A widely used algorithm is the tf\u2013idf representation (also called vector space representation).[57] The system creates a content-based profile of users based on a weighted vector of item features. The weights denote the importance of each feature to the user and can be computed from individually rated content vectors using a variety of techniques. Simple approaches use the average values of the rated item vector while other sophisticated methods use machine learning techniques such as Bayesian Classifiers, cluster analysis, decision trees, and artificial neural networks in order to estimate the probability that the user is going to like the item.[58]\n A key issue with content-based filtering is whether the system can learn user preferences from users' actions regarding one content source and use them across other content types. When the system is limited to recommending content of the same type as the user is already using, the value from the recommendation system is significantly less than when other content types from other services can be recommended. For example, recommending news articles based on news browsing is useful. Still, it would be much more useful when music, videos, products, discussions, etc., from different services, can be recommended based on news browsing. To overcome this, most content-based recommender systems now use some form of the hybrid system.\n Content-based recommender systems can also include opinion-based recommender systems. In some cases, users are allowed to leave text reviews or feedback on the items. These user-generated texts are implicit data for the recommender system because they are potentially rich resources of both feature/aspects of the item and users' evaluation/sentiment to the item. Features extracted from the user-generated reviews are improved metadata of items, because as they also reflect aspects of the item like metadata, extracted features are widely concerned by the users. Sentiments extracted from the reviews can be seen as users' rating scores on the corresponding features. Popular approaches of opinion-based recommender system utilize various techniques including text mining, information retrieval, sentiment analysis (see also Multimodal sentiment analysis) and deep learning.[59]\n Most recommender systems now use a hybrid approach, combining collaborative filtering, content-based filtering, and other approaches. There is no reason why several different techniques of the same type could not be hybridized. Hybrid approaches can be implemented in several ways: by making content-based and collaborative-based predictions separately and then combining them; by adding content-based capabilities to a collaborative-based approach (and vice versa); or by unifying the approaches into one model.[41] Several studies that empirically compared the performance of the hybrid with the pure collaborative and content-based methods and demonstrated that the hybrid methods can provide more accurate recommendations than pure approaches. These methods can also be used to overcome some of the common problems in recommender systems such as cold start and the sparsity problem, as well as the knowledge engineering bottleneck in knowledge-based approaches.[60]\n Netflix is a good example of the use of hybrid recommender systems.[61] The website makes recommendations by comparing the watching and searching habits of similar users (i.e., collaborative filtering) as well as by offering movies that share characteristics with films that a user has rated highly (content-based filtering).\n Some hybridization techniques include:\n These recommender systems use the interactions of a user within a session[63] to generate recommendations. Session-based recommender systems are used at YouTube[64] and Amazon.[65] These are particularly useful when history (such as past clicks, purchases) of a user is not available or not relevant in the current user session. Domains, where session-based recommendations are particularly relevant, include video, e-commerce, travel, music and more. Most instances of session-based recommender systems rely on the sequence of recent interactions within a session without requiring any additional details (historical, demographic) of the user. Techniques for session-based recommendations are mainly based on generative sequential models such as recurrent neural networks,[63][66] Transformers,[67] and other deep-learning-based approaches.[68][69]\n The recommendation problem can be seen as a special instance of a reinforcement learning problem whereby the user is the environment upon which the agent, the recommendation system acts upon in order to receive a reward, for instance, a click or engagement by the user.[64][70][71] One aspect of reinforcement learning that is of particular use in the area of recommender systems is the fact that the models or policies can be learned by providing a reward to the recommendation agent. This is in contrast to traditional learning techniques which rely on supervised learning approaches that are less flexible, reinforcement learning recommendation techniques allow to potentially train models that can be optimized directly on metrics of engagement, and user interest.[72]\n Multi-criteria recommender systems (MCRS) can be defined as recommender systems that incorporate preference information upon multiple criteria. Instead of developing recommendation techniques based on a single criterion value, the overall preference of user u for the item i, these systems try to predict a rating for unexplored items of u by exploiting preference information on multiple criteria that affect this overall preference value. Several researchers approach MCRS as a multi-criteria decision making (MCDM) problem, and apply MCDM methods and techniques to implement MCRS systems.[73] See this chapter[74] for an extended introduction.\n The majority of existing approaches to recommender systems focus on recommending the most relevant content to users using contextual information, yet do not take into account the risk of disturbing the user with unwanted notifications. It is important to consider the risk of upsetting the user by pushing recommendations in certain circumstances, for instance, during a professional meeting, early morning, or late at night. Therefore, the performance of the recommender system depends in part on the degree to which it has incorporated the risk into the recommendation process. One option to manage this issue is DRARS, a system which models the context-aware recommendation as a bandit problem. This system combines a content-based technique and a contextual bandit algorithm.[75]\n Mobile recommender systems make use of internet-accessing smartphones to offer personalized, context-sensitive recommendations. This is a particularly difficult area of research as mobile data is more complex than data that recommender systems often have to deal with. It is heterogeneous, noisy, requires spatial and temporal auto-correlation, and has validation and generality problems.[76]\n There are three factors that could affect the mobile recommender systems and the accuracy of prediction results: the context, the recommendation method and privacy.[77] Additionally, mobile recommender systems suffer from a transplantation problem \u2013 recommendations may not apply in all regions (for instance, it would be unwise to recommend a recipe in an area where all of the ingredients may not be available).\n One example of a mobile recommender system are the approaches taken by companies such as Uber and Lyft to generate driving routes for taxi drivers in a city.[76] This system uses GPS data of the routes that taxi drivers take while working, which includes location (latitude and longitude), time stamps, and operational status (with or without passengers). It uses this data to recommend a list of pickup points along a route, with the goal of optimizing occupancy times and profits.\n Generative recommenders (GR) represent an approach that transforms recommendation tasks into sequential transduction problems, where user actions are treated like tokens in a generative modeling framework. In one method, known as HSTU (Hierarchical Sequential Transduction Units),[78] high-cardinality, non-stationary, and streaming datasets are efficiently processed as sequences, enabling the model to learn from trillions of parameters and to handle user action histories orders of magnitude longer than before. By turning all of the system\u2019s varied data into a single stream of tokens and using a custom self-attention approach instead of traditional neural network layers, generative recommenders make the model much simpler and less memory-hungry. As a result, it can improve recommendation quality in test simulations and in real-world tests, while being faster than previous Transformer-based systems when handling long lists of user actions. Ultimately, this approach allows the model\u2019s performance to grow steadily as more computing power is used, laying a foundation for efficient and scalable \u201cfoundation models\u201d for recommendations.\n One of the events that energized research in recommender systems was the Netflix Prize. From 2006 to 2009, Netflix sponsored a competition, offering a grand prize of $1,000,000 to the team that could take an offered dataset of over 100 million movie ratings and return recommendations that were 10% more accurate than those offered by the company's existing recommender system. This competition energized the search for new and more accurate algorithms. On 21 September 2009, the grand prize of US$1,000,000 was given to the BellKor's Pragmatic Chaos team using tiebreaking rules.[79]\n The most accurate algorithm in 2007 used an ensemble method of 107 different algorithmic approaches, blended into a single prediction. As stated by the winners, Bell et al.:[80]\n \nPredictive accuracy is substantially improved when blending multiple predictors. Our experience is that most efforts should be concentrated in deriving substantially different approaches, rather than refining a single technique.  Consequently, our solution is an ensemble of many methods. Many benefits accrued to the web due to the Netflix project. Some teams have taken their technology and applied it to other markets. Some members from the team that finished second place founded Gravity R&D, a recommendation engine that's active in the RecSys community.[79][81] 4-Tell, Inc. created a Netflix project\u2013derived solution for ecommerce websites.\n A number of privacy issues arose around the dataset offered by Netflix for the Netflix Prize competition. Although the data sets were anonymized in order to preserve customer privacy, in 2007 two researchers from the University of Texas were able to identify individual users by matching the data sets with film ratings on the Internet Movie Database (IMDb).[82] As a result, in December 2009, an anonymous Netflix user sued Netflix in Doe v. Netflix, alleging that Netflix had violated United States fair trade laws and the Video Privacy Protection Act by releasing the datasets.[83] This, as well as concerns from the Federal Trade Commission, led to the cancellation of a second Netflix Prize competition in 2010.[84]\n Evaluation is important in assessing the effectiveness of recommendation algorithms. To measure the effectiveness of recommender systems, and compare different approaches, three types of evaluations are available: user studies, online evaluations (A/B tests), and offline evaluations.[43]\n The commonly used metrics are the mean squared error and root mean squared error, the latter having been used in the Netflix Prize. The information retrieval metrics such as precision and recall or DCG are useful to assess the quality of a recommendation method. Diversity, novelty, and coverage are also considered as important aspects in evaluation.[85] However, many of the classic evaluation measures are highly criticized.[86]\n Evaluating the performance of a recommendation algorithm on a fixed test dataset will always be extremely challenging as it is impossible to accurately predict the reactions of real users to the recommendations. Hence any metric that computes the effectiveness of an algorithm in offline data will be imprecise.\n User studies are rather a small scale. A few dozens or hundreds of users are presented recommendations created by different recommendation approaches, and then the users judge which recommendations are best.\n In A/B tests, recommendations are shown to typically thousands of users of a real product, and the recommender system randomly picks at least two different recommendation approaches to generate recommendations. The effectiveness is measured with implicit measures of effectiveness such as conversion rate or click-through rate.\n Offline evaluations are based on historic data, e.g. a dataset that contains information about how users previously rated movies.[87]\n The effectiveness of recommendation approaches is then measured based on how well a recommendation approach can predict the users' ratings in the dataset. While a rating is an explicit expression of whether a user liked a movie, such information is not available in all domains. For instance, in the domain of citation recommender systems, users typically do not rate a citation or recommended article. In such cases, offline evaluations may use implicit measures of effectiveness. For instance, it may be assumed that a recommender system is effective that is able to recommend as many articles as possible that are contained in a research article's reference list. However, this kind of offline evaluations is seen critical by many researchers.[88][89][90][43] For instance, it has been shown that results of offline evaluations have low correlation with results from user studies or A/B tests.[90][91] A dataset popular for offline evaluation has been shown to contain duplicate data and thus to lead to wrong conclusions in the evaluation of algorithms.[92] Often, results of so-called offline evaluations do not correlate with actually assessed user-satisfaction.[93] This is probably because offline training is highly biased toward the highly reachable items, and offline testing data is highly influenced by the outputs of the online recommendation module.[88][94] Researchers have concluded that the results of offline evaluations should be viewed critically.[95]\n Typically, research on recommender systems is concerned with finding the most accurate recommendation algorithms. However, there are a number of factors that are also important.\n Recommender systems are notoriously difficult to evaluate offline, with some researchers claiming that this has led to a reproducibility crisis in recommender systems publications. The topic of reproducibility seems to be a recurrent issue in some Machine Learning publication venues, but does not have a considerable effect beyond the world of scientific publication. In the context of recommender systems a 2019 paper surveyed a small number of hand-picked publications applying deep learning or neural methods to the top-k recommendation problem, published in top conferences (SIGIR, KDD, WWW, RecSys, IJCAI), has shown that on average less than 40% of articles could be reproduced by the authors of the survey, with as little as 14% in some conferences. The articles considers a number of potential problems in today's research scholarship and suggests improved scientific practices in that area.[108][109][110]\nMore recent work on benchmarking a set of the same methods came to qualitatively very different results[111] whereby neural methods were found to be among the best performing methods. Deep learning and neural methods for recommender systems have been used in the winning solutions in several recent recommender system challenges, WSDM,[112] RecSys Challenge.[113]\nMoreover, neural and deep learning methods are widely used in industry where they are extensively tested.[114][64][65] The topic of reproducibility is not new in recommender systems. By 2011, Ekstrand, Konstan, et al. criticized that \"it is currently difficult to reproduce and extend recommender systems research results,\" and that evaluations are \"not handled consistently\".[115] Konstan and Adomavicius conclude that \"the Recommender Systems research community is facing a crisis where a significant number of papers present results that contribute little to collective knowledge [...] often because the research lacks the [...] evaluation to be properly judged and, hence, to provide meaningful contributions.\"[116] As a consequence, much research about recommender systems can be considered as not reproducible.[117] Hence, operators of recommender systems find little guidance in the current research for answering the question, which recommendation approaches to use in a recommender systems. Said and Bellog\u00edn conducted a study of papers published in the field, as well as benchmarked some of the most popular frameworks for recommendation and found large inconsistencies in results, even when the same algorithms and data sets were used.[118] Some researchers demonstrated that minor variations in the recommendation algorithms or scenarios led to strong changes in the effectiveness of a recommender system. They conclude that seven actions are necessary to improve the current situation:[117] \"(1) survey other research fields and learn from them, (2) find a common understanding of reproducibility, (3) identify and understand the determinants that affect reproducibility, (4) conduct more comprehensive experiments (5) modernize publication practices, (6) foster the development and use of recommendation frameworks, and (7) establish best-practice guidelines for recommender-systems research.\"\n Artificial intelligence (AI) applications in recommendation systems are the advanced methodologies that leverage AI technologies, to enhance the performance recommendation engines. The AI-based recommender can analyze complex data sets, learning from user behavior, preferences, and interactions to generate highly accurate and personalized content or product suggestions.[119] The integration of AI in recommendation systems has marked a significant evolution from traditional recommendation methods. Traditional methods often relied on inflexible algorithms that could suggest items based on general user trends or apparent similarities in content. In comparison, AI-powered systems have the capability to detect patterns and subtle distinctions that may be overlooked by traditional methods.[120] These systems can adapt to specific individual preferences, thereby offering recommendations that are more aligned with individual user needs. This approach marks a shift towards more personalized, user-centric suggestions.\n Recommendation systems widely adopt AI techniques such as machine learning, deep learning, and natural language processing.[121] These advanced methods enhance system capabilities to predict user preferences and deliver personalized content more accurately. Each technique contributes uniquely. The following sections will introduce specific AI models utilized by a recommendation system by illustrating their theories and functionalities.[citation needed]\n Collaborative filtering (CF) is one of the most commonly used recommendation system algorithms. It generates personalized suggestions for users based on explicit or implicit behavioral patterns to form predictions.[122] Specifically, it relies on external feedback such as star ratings, purchasing history and so on to make judgments. CF make predictions about users' preference based on similarity measurements. Essentially, the underlying theory is: \"if user A is similar to user B, and if A likes item C, then it is likely that B also likes item C.\"\n There are many models available for collaborative filtering. For AI-applied collaborative filtering, a common model is called K-nearest neighbors. The ideas are as follows:\n An artificial neural network (ANN), is a deep learning model structure which aims to mimic a human brain. They comprise a series of neurons, each responsible for receiving and processing information transmitted from other interconnected neurons.[123] Similar to a human brain, these neurons will change activation state based on incoming signals (training input and backpropagated output), allowing the system to adjust activation weights during the network learning phase. ANN is usually designed to be a black-box model. Unlike regular machine learning where the underlying theoretical components are formal and rigid, the collaborative effects of neurons are not entirely clear, but modern experiments has shown the predictive power of ANN.\n ANN is widely used in recommendation systems for its power to utilize various data. Other than feedback data, ANN can incorporate non-feedback data which are too intricate for collaborative filtering to learn, and the unique structure allows ANN to identify extra signal from non-feedback data to boost user experience.[121] Following are some examples:\n The Two-Tower model is a neural architecture[124] commonly employed in large-scale recommendation systems, particularly for candidate retrieval tasks.[125] It consists of two neural networks:\n The outputs of the two towers are fixed-length embeddings that represent users and items in a shared vector space. A similarity metric, such as dot product or cosine similarity, is used to measure relevance between a user and an item.\n This model is highly efficient for large datasets as embeddings can be pre-computed for items, allowing rapid retrieval during inference. It is often used in conjunction with ranking models for end-to-end recommendation pipelines.\n Natural language processing is a series of AI algorithms to make natural human language accessible and analyzable to a machine.[126] It is a fairly modern technique inspired by the growing amount of textual information. For application in recommendation system, a common case is the Amazon customer review. Amazon will analyze the feedbacks comments from each customer and report relevant data to other customers for reference. The recent years have witnessed the development of various text analysis models, including latent semantic analysis (LSA), singular value decomposition (SVD), latent Dirichlet allocation (LDA), etc. Their uses have consistently aimed to provide customers with more precise and tailored recommendations.\n An emerging market for content discovery platforms is academic content.[127][128] Approximately 6000 academic journal articles are published daily, making it increasingly difficult for researchers to balance time management with staying up to date with relevant research.[10] Though traditional tools academic search tools such as Google Scholar or PubMed provide a readily accessible database of journal articles, content recommendation in these cases are performed in a 'linear' fashion, with users setting 'alarms' for new publications based on keywords, journals or particular authors.\n Google Scholar provides an 'Updates' tool that suggests articles by using a statistical model that takes a researchers' authorized paper and citations as input.[10] Whilst these recommendations have been noted to be extremely good, this poses a problem with early career researchers which may be lacking a sufficient body of work to produce accurate recommendations.[10]\n In contrast to an engagement-based ranking system employed by social media and other digital platforms, a bridging-based ranking optimizes for content that is unifying instead of polarizing.[129][130] Examples include Polis and Remesh which have been used around the world to help find more consensus around specific political issues.[130] Twitter has also used this approach for managing its community notes,[131] which YouTube planned to pilot in 2024.[132][133] Aviv Ovadya also argues for implementing bridging-based algorithms in major platforms by empowering deliberative groups that are representative of the platform's users to control the design and implementation of the algorithm.[134]\n As the connected television landscape continues to evolve, search and recommendation are seen as having an even more pivotal role in the discovery of content.[135] With broadband-connected devices, consumers are projected to have access to content from linear broadcast sources as well as internet television. Therefore, there is a risk that the market could become fragmented, leaving it to the viewer to visit various locations and find what they want to watch in a way that is time-consuming and complicated for them. By using a search and recommendation engine, viewers are provided with a central 'portal' from which to discover content from several sources in just one location.\n",
        "doc_number": 74
    },
    {
        "url": "https://en.wikipedia.org/wiki/Sentiment_analysis",
        "content": "\nSentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine. With the rise of deep language models, such as RoBERTa, also more difficult data domains can be analyzed, e.g., news texts where authors typically express their opinion/sentiment less explicitly.[1]\n A basic task in sentiment analysis is classifying the polarity of a given text at the document, sentence, or feature/aspect level\u2014whether the expressed opinion in a document, a sentence or an entity feature/aspect is positive, negative, or neutral. Advanced, \"beyond polarity\" sentiment classification looks, for instance, at emotional states such as enjoyment, anger, disgust, sadness, fear, and surprise.[2]\n Precursors to sentimental analysis include the General Inquirer,[3] which provided hints toward quantifying patterns in text and, separately, psychological research that examined a person's psychological state based on analysis of their verbal behavior.[4]\n Subsequently, the method described in a patent by Volcani and Fogel,[5] looked specifically at sentiment and identified individual words and phrases in text with respect to different emotional scales. A current system based on their work, called EffectCheck, presents synonyms that can be used to increase or decrease the level of evoked emotion in each scale.\n Many other subsequent efforts were less sophisticated, using a mere polar view of sentiment, from positive to negative, such as work by Turney,[6] and Pang[7] who applied different methods for detecting the polarity of product reviews and movie reviews respectively. This work is at the document level. One can also classify a document's polarity on a multi-way scale, which was attempted by Pang[8] and Snyder[9] among others: Pang and Lee[8] expanded the basic task of classifying a movie review as either positive or negative to predict star ratings on either a 3- or a 4-star scale, while Snyder[9] performed an in-depth analysis of restaurant reviews, predicting ratings for various aspects of the given restaurant, such as the food and atmosphere (on a five-star scale).\n First steps to bringing together various approaches\u2014learning, lexical, knowledge-based, etc.\u2014were taken in the 2004 AAAI Spring Symposium where linguists, computer scientists, and other interested researchers first aligned interests and proposed shared tasks and benchmark data sets for the systematic computational research on affect, appeal, subjectivity, and sentiment in text.[10]\n Even though in most statistical classification methods, the neutral class is ignored under the assumption that neutral texts lie near the boundary of the binary classifier, several researchers suggest that, as in every polarity problem, three categories must be identified. Moreover, it can be proven that specific classifiers such as the Max Entropy[11] and SVMs[12] can benefit from the introduction of a neutral class and improve the overall accuracy of the classification. There are in principle two ways for operating with a neutral class. Either, the algorithm proceeds by first identifying the neutral language, filtering it out and then assessing the rest in terms of positive and negative sentiments, or it builds a three-way classification in one step.[13] This second approach often involves estimating a probability distribution over all categories (e.g. naive Bayes classifiers as implemented by the NLTK). Whether and how to use a neutral class depends on the nature of the data: if the data is clearly clustered into neutral, negative and positive language, it makes sense to filter the neutral language out and focus on the polarity between positive and negative sentiments. If, in contrast, the data are mostly neutral with small deviations towards positive and negative affect, this strategy would make it harder to clearly distinguish between the two poles.\n A different method for determining sentiment is the use of a scaling system whereby words commonly associated with having a negative, neutral, or positive sentiment  are given an associated number on a \u221210 to +10 scale (most negative up to most positive) or simply from 0 to a positive upper limit such as +4. This makes it possible to adjust the sentiment of a given term relative to its environment (usually on the level of the sentence). When a piece of unstructured text is analyzed using natural language processing, each concept in the specified environment is given a score based on the way sentiment words relate to the concept and its associated score.[14][15] This allows movement to a more sophisticated understanding of sentiment, because it is now possible to adjust the sentiment value of a concept relative to modifications that may surround it. Words, for example, that intensify, relax or negate the sentiment expressed by the concept can affect its score. Alternatively, texts can be given a positive and negative sentiment strength score if the goal is to determine the sentiment in a text rather than the overall polarity and strength of the text.[16]\n There are various other types of sentiment analysis, such as aspect-based sentiment analysis, grading sentiment analysis (positive, negative, neutral), multilingual sentiment analysis and detection of emotions.\n This task is commonly defined as classifying a given text (usually a sentence) into one of two classes: objective or subjective.[17] This problem can sometimes be more difficult than polarity classification.[18] The subjectivity of words and phrases may depend on their context and an objective document may contain subjective sentences (e.g., a news article quoting people's opinions). Moreover, as mentioned by Su,[19] results are largely dependent on the definition of subjectivity used when annotating texts. However, Pang[20] showed that removing objective sentences from a document before classifying its polarity helped improve performance.\n Subjective and objective identification, emerging subtasks of sentiment analysis to use syntactic, semantic features, and machine learning knowledge to identify if a sentence or document contains facts or opinions. Awareness of recognizing factual and opinions is not recent, having possibly first presented by Carbonell at Yale University in 1979.[clarify]\n The term objective refers to the incident carrying factual information.[21]\n The term subjective describes the incident contains non-factual information in various forms, such as personal opinions, judgment, and predictions, also known as 'private states'.[22] In the example down below, it reflects a private states 'We Americans'. Moreover, the target entity commented by the opinions can take several forms from tangible product to intangible topic matters stated in Liu (2010).[23] Furthermore, three types of attitudes were observed by Liu (2010), 1) positive opinions, 2)  neutral opinions, and 3) negative opinions.[23]\n This analysis is a classification problem.[24]\n Each class's collections of words or phrase indicators are defined for to locate desirable patterns on unannotated text. For subjective expression, a different word list has been created. Lists of subjective indicators in words or phrases have been developed by multiple researchers in the linguist and natural language processing field states in Riloff et al. (2003).[25] A dictionary of extraction rules has to be created for measuring given expressions. Over the years, in subjective detection, the features extraction progression from curating features by hand to automated features learning. At the moment, automated learning methods can further separate into supervised and unsupervised machine learning. Patterns extraction with machine learning process annotated and unannotated text have been explored extensively by academic researchers.\n However, researchers recognized several challenges in developing fixed sets of rules for expressions respectably. Much of the challenges in rule development stems from the nature of textual information. Six challenges have been recognized by several researchers: 1) metaphorical expressions, 2) discrepancies in writings, 3) context-sensitive, 4) represented words with fewer usages, 5) time-sensitive, and 6) ever-growing volume.\n Previously, the research mainly focused on document level classification. However, classifying a document level suffers less accuracy, as an article may have diverse types of expressions involved. Researching evidence suggests a set of news articles that are expected to dominate by the objective expression, whereas the results show that it consisted of over 40% of subjective expression.[21]\n To overcome those challenges, researchers conclude that classifier efficacy depends on the precisions of patterns learner. And the learner feeds with large volumes of annotated training data outperformed those trained on less comprehensive subjective features. However, one of the main obstacles to executing this type of work is to generate a big dataset of annotated sentences manually.\u00a0The manual annotation method has been less favored than automatic learning for three reasons:\n All these mentioned reasons can impact on the efficiency and effectiveness of subjective and objective classification. Accordingly, two bootstrapping methods were designed to learning linguistic patterns from unannotated text data. Both methods are starting with a handful of seed words and unannotated textual data.\n Overall, these algorithms highlight the need for automatic pattern recognition and extraction in subjective and objective task.\n Subjective and object classifier can enhance the several applications of natural language processing. One of the classifier's primary benefits is that it popularized the practice of data-driven decision-making processes in various industries. According to Liu, the applications of subjective and objective identification have been implemented in business, advertising, sports, and social science.[30]\n It refers to determining the opinions or sentiments expressed on different features or aspects of entities, e.g., of a cell phone, a digital camera, or a bank.[35] A feature or aspect is an attribute or component of an entity, e.g., the screen of a cell phone, the service for a restaurant, or the picture quality of a camera. The advantage of feature-based sentiment analysis is the possibility to capture nuances about objects of interest. Different features can generate different sentiment responses, for example a hotel can have a convenient location, but mediocre food.[36] This problem involves several sub-problems, e.g., identifying relevant entities, extracting their features/aspects, and determining whether an opinion expressed on each feature/aspect is positive, negative or neutral.[37] The automatic identification of features can be performed with syntactic methods, with topic modeling,[38][39] or with deep learning.[40][41] More detailed discussions about this level of sentiment analysis can be found in Liu's work.[23]\n Emotions and sentiments are subjective in nature. The degree of emotions/sentiments expressed in a given text at the document, sentence, or feature/aspect level\u2014to what degree of intensity is expressed in the opinion of a document, a sentence or an entity differs on a case-to-case basis.[42] However, predicting only the emotion and sentiment does not always convey complete information. The degree or level of emotions and sentiments often plays a crucial role in understanding the exact feeling within a single class (e.g., 'good' versus 'awesome'). Some methods leverage a stacked ensemble method[43] for predicting intensity for emotion and sentiment by combining the outputs obtained and using deep learning models based on convolutional neural networks,[44] long short-term memory networks and gated recurrent units.[45]\n Existing approaches to sentiment analysis can be grouped into three main categories: knowledge-based techniques, statistical methods, and hybrid approaches.[46] Knowledge-based techniques classify text by affect categories based on the presence of unambiguous affect words such as happy, sad, afraid, and bored.[47] Some knowledge bases not only list obvious affect words, but also assign arbitrary words a probable \"affinity\" to particular emotions.[48] Statistical methods leverage elements from machine learning such as latent semantic analysis, support vector machines, \"bag of words\", \"Pointwise Mutual Information\" for Semantic Orientation,[6] semantic space models or word embedding models,[49] and deep learning. More sophisticated methods try to detect the holder of a sentiment (i.e., the person who maintains that affective state) and the target (i.e., the entity about which the affect is felt).[50] To mine the opinion in context and get the feature about which the speaker has opined, the grammatical relationships of words are used. Grammatical dependency relations are obtained by deep parsing of the text.[51] Hybrid approaches leverage both machine learning and elements from knowledge representation such as ontologies and semantic networks in order to detect semantics that are expressed in a subtle manner, e.g., through the analysis of concepts that do not explicitly convey relevant information, but which are implicitly linked to other concepts that do so.[52]\n Open source software tools as well as range of free and paid sentiment analysis tools deploy machine learning, statistics, and natural language processing techniques to automate sentiment analysis on large collections of texts, including web pages, online news, internet discussion groups, online reviews, web blogs, and social media.[53] Knowledge-based systems, on the other hand, make use of publicly available resources, to extract the semantic and affective information associated with natural language concepts. The system can help perform affective commonsense reasoning.[54] Sentiment analysis can also be performed on visual content, i.e., images and videos (see Multimodal sentiment analysis). One of the first approaches in this direction is SentiBank[55] utilizing an adjective noun pair representation of visual content. In addition, the vast majority of sentiment classification approaches rely on the bag-of-words model, which disregards context, grammar and even word order. Approaches that analyses the sentiment based on how words compose the meaning of longer phrases have shown better result,[56] but they incur an additional annotation overhead.\n A human analysis component is required in sentiment analysis, as automated systems are not able to analyze historical tendencies of the individual commenter, or the platform and are often classified incorrectly in their expressed sentiment. Automation impacts approximately 23% of comments that are correctly classified by humans.[57] However, humans often disagree, and it is argued that the inter-human agreement provides an upper bound that automated sentiment classifiers can eventually reach.[58]\n The accuracy of a sentiment analysis system is, in principle, how well it agrees with human judgments. This is usually measured by variant measures based on precision and recall over the two target categories of negative and positive texts. However, according to research human raters typically only agree about 80%[59] of the time (see Inter-rater reliability). Thus, a program that achieves 70% accuracy in classifying sentiment is doing nearly as well as humans, even though such accuracy may not sound impressive. If a program were \"right\" 100% of the time, humans would still disagree with it about 20% of the time, since they disagree that much about any answer.[citation needed]\n On the other hand, computer systems will make very different errors than human assessors, and thus the figures are not entirely comparable. For instance, a computer system will have trouble with negations, exaggerations, jokes, or sarcasm, which typically are easy to handle for a human reader: some errors a computer system makes will seem overly naive to a human. In general, the utility for practical commercial tasks of sentiment analysis as it is defined in academic research has been called into question, mostly since the simple one-dimensional model of sentiment from negative to positive yields rather little actionable information for a client worrying about the effect of public discourse on e.g. brand or corporate reputation.[60][61][62]\n To better fit market needs, evaluation of sentiment analysis has moved to more task-based measures, formulated together with representatives from PR agencies and market research professionals. The focus in e.g. the RepLab evaluation data set is less on the content of the text under consideration and more on the effect of the text in question on brand reputation.[63][64][65]\n Because evaluation of sentiment analysis is becoming more and more task based, each implementation needs a separate training model to get a more accurate representation of sentiment for a given data set.\n The rise of social media such as blogs and social networks has fueled interest in sentiment analysis.  With the proliferation of reviews, ratings, recommendations and other forms of online expression, online opinion has turned into a kind of virtual currency for businesses looking to market their products, identify new opportunities and manage their reputations.  As businesses look to automate the process of filtering out the noise, understanding the conversations, identifying the relevant content and actioning it appropriately, many are now looking to the field of sentiment analysis.[66] Further complicating the matter, is the rise of anonymous social media platforms such as 4chan and Reddit.[67] If web 2.0 was all about democratizing publishing, then the next stage of the web may well be based on democratizing data mining of all the content that is getting published.[68]\n One step towards this aim is accomplished in research. Several research teams in universities around the world currently focus on understanding the dynamics of sentiment in e-communities through sentiment analysis.[69]\n The problem is that most sentiment analysis algorithms use simple terms to express sentiment about a product or service.  However, cultural factors, linguistic nuances, and differing contexts make it extremely difficult to turn a string of written text into a simple pro or con sentiment.[66]  The fact that humans often disagree on the sentiment of text illustrates how big a task it is for computers to get this right.  The shorter the string of text, the harder it becomes.\n Even though short text strings might be a problem, sentiment analysis within microblogging has shown that Twitter can be seen as a valid online indicator of political sentiment. Tweets' political sentiment demonstrates close correspondence to parties' and politicians' political positions, indicating that the content of Twitter messages plausibly reflects the offline political landscape.[70] Furthermore, sentiment analysis on Twitter has also been shown to capture the public mood behind human reproduction cycles globally,[71] as well as other problems of public-health relevance such as adverse drug reactions.[72]\n While sentiment analysis has been popular for domains where authors express their opinion rather explicitly (\"the movie is awesome\"), such as social media and product reviews, only recently robust methods were devised for other domains where sentiment is strongly implicit or indirect. For example, in news articles - mostly due to the expected journalistic objectivity - journalists often describe actions or events rather than directly stating the polarity of a piece of information. Earlier approaches using dictionaries or shallow machine learning features were unable to catch the \"meaning between the lines\", but recently researchers have proposed a deep learning based approach and dataset that is able to analyze sentiment in news articles.[1]\n Scholars have utilized sentiment analysis to analyse the construction health and safety Tweets (which is called X now). The research revealed that there is a positive correlation between favorites and retweets in terms of sentiment valence. Others have examined the impact of YouTube on the dissemination of construction health and safety knowledge. They investigated how emotions influence users' behaviors in terms of viewing and commenting through semantic analysis. In another study, positive sentiment accounted for an overwhelming figure of 85% in knowledge sharing of construction safety and health via Instagram.[73]\n For a recommender system, sentiment analysis has been proven to be a valuable technique. A recommender system aims to predict the preference for an item of a target user. Mainstream recommender systems work on explicit data set. For example, collaborative filtering works on the rating matrix, and content-based filtering works on the meta-data of the items.\n In many social networking services or e-commerce websites, users can provide text review, comment or feedback to the items. These user-generated text provide a rich source of user's sentiment opinions about numerous products and items. Potentially, for an item, such text can reveal both the related feature/aspects of the item and the users' sentiments on each feature.[74] The item's feature/aspects described in the text play the same role with the meta-data in content-based filtering, but the former are more valuable for the recommender system. Since these features are broadly mentioned by users in their reviews, they can be seen as the most crucial features that can significantly influence the user's experience on the item, while the meta-data of the item (usually provided by the producers instead of consumers) may ignore features that are concerned by the users. For different items with common features, a user may give different sentiments. Also, a feature of the same item may receive different sentiments from different users. Users' sentiments on the features can be regarded as a multi-dimensional rating score, reflecting their preference on the items.\n Based on the feature/aspects and the sentiments extracted from the user-generated text, a hybrid recommender system can be constructed.[75] There are two types of motivation to recommend a candidate item to a user. The first motivation is the candidate item have numerous common features with the user's preferred items,[76] while the second motivation is that the candidate item receives a high sentiment on its features. For a preferred item, it is reasonable to believe that items with the same features will have a similar function or utility. So, these items will also likely to be preferred by the user. On the other hand, for a shared feature of two candidate items, other users may give positive sentiment to one of them while giving negative sentiment to another. Clearly, the high evaluated item should be recommended to the user. Based on these two motivations, a combination ranking score of similarity and sentiment rating can be constructed for each candidate item.[75]\n Except for the difficulty of the sentiment analysis itself, applying sentiment analysis on reviews or feedback also faces the challenge of spam and biased reviews. One direction of work is focused on evaluating the helpfulness of each review.[77] Review or feedback poorly written is hardly helpful for recommender system. Besides, a review can be designed to hinder sales of a target product, thus be harmful to the recommender system even it is well written.\n Researchers also found that long and short forms of user-generated text should be treated differently. An interesting result shows that short-form reviews are sometimes more helpful than long-form,[78] because it is easier to filter out the noise in a short-form text. For the long-form text, the growing length of the text does not always bring a proportionate increase in the number of features or sentiments in the text.\n Lamba & Madhusudhan[79] introduce a nascent way to cater the information needs of today's library users by repackaging the results from sentiment analysis of social media platforms like Twitter and provide it as a consolidated time-based service in different formats. Further, they propose a new way of conducting marketing in libraries using social media mining and sentiment analysis.\n Issues such as privacy, consent, and bias are crucial since sentiment analysis regularly analyzes personal data without explicit user consent. The potential for misinterpretation and misuse of sentiment data can significantly impact societal norms. Furthermore, the development of ethical frameworks, as seen in projects like SEWA, where Ethical and Industrial Valorisation Advisory Boards are established, is essential for addressing these challenges. These boards help ensure that sentiment analysis technologies are used responsibly, especially in applications involving the recognition of human emotions and behaviors. Such frameworks are vital for guiding the responsible use of sentiment analysis tools, ensuring they promote equity and respect user autonomy, and effectively address both routine and complex ethical issues.[80]\n",
        "doc_number": 75
    },
    {
        "url": "https://en.wikipedia.org/wiki/Speech_recognition",
        "content": "\nSpeech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech-to-text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis.\n Some speech recognition systems require \"training\" (also called \"enrollment\") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called \"speaker-independent\"[1] systems. Systems that use training are called \"speaker dependent\".\n Speech recognition applications include voice user interfaces such as voice dialing (e.g. \"call home\"), call routing (e.g. \"I would like to make a collect call\"), domotic appliance control, search key words (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), determining speaker characteristics,[2] speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed direct voice input). Automatic pronunciation assessment is used in education such as for spoken language learning.\n The term voice recognition[3][4][5] or speaker identification[6][7][8] refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.\n From the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the worldwide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems.\n The key areas of growth were: vocabulary size, speaker independence, and processing speed.\n Raj Reddy was the first person to take on continuous speech recognition as a graduate student at Stanford University in the late 1960s. Previous systems required users to pause after each word. Reddy's system issued spoken commands for playing chess.\n Around this time Soviet researchers invented the dynamic time warping (DTW) algorithm and used it to create a recognizer capable of operating on a 200-word vocabulary.[15] DTW processed speech by dividing it into short frames, e.g. 10ms segments, and processing each frame as a single unit. Although DTW would be superseded by later algorithms, the technique carried on. Achieving speaker independence remained unsolved at this time period.\n During the late 1960s Leonard Baum developed the mathematics of Markov chains at the Institute for Defense Analysis. A decade later, at CMU, Raj Reddy's students James Baker and Janet M. Baker began using the hidden Markov model (HMM) for speech recognition.[20] James Baker had learned about HMMs from a summer job at the Institute of Defense Analysis during his undergraduate education.[21] The use of HMMs allowed researchers to combine different sources of knowledge, such as acoustics, language, and syntax, in a unified probabilistic model.\n The 1980s also saw the introduction of the n-gram language model.\n Much of the progress in the field is owed to the rapidly increasing capabilities of computers. At the end of the DARPA program in 1976, the best computer available to researchers was the PDP-10 with 4 MB ram.[28] It could take up to 100 minutes to decode just 30 seconds of speech.[29]\n Two practical products were:\n By this point, the vocabulary of the typical commercial speech recognition system was larger than the average human vocabulary.[28] Raj Reddy's former student, Xuedong Huang, developed the Sphinx-II system at CMU. The Sphinx-II system was the first to do speaker-independent, large vocabulary, continuous speech recognition and it had the best performance in DARPA's 1992 evaluation. Handling continuous speech with a large vocabulary was a major milestone in the history of speech recognition. Huang went on to found the speech recognition group at Microsoft in 1993. Raj Reddy's student Kai-Fu Lee joined Apple where, in 1992, he helped develop a speech interface prototype for the Apple computer known as Casper.\n Lernout & Hauspie, a Belgium-based speech recognition company, acquired several other companies, including Kurzweil Applied Intelligence in 1997 and Dragon Systems in 2000. The L&H speech technology was used in the Windows XP operating system. L&H was an industry leader until an accounting scandal brought an end to the company in 2001. The speech technology from L&H was bought by ScanSoft which became Nuance in 2005. Apple originally licensed software from Nuance to provide speech recognition capability to its digital assistant Siri.[34]\n In the 2000s DARPA sponsored two speech recognition programs: Effective Affordable Reusable Speech-to-Text (EARS) in 2002 and Global Autonomous Language Exploitation (GALE). Four teams participated in the EARS program: IBM, a team led by BBN with LIMSI and Univ. of Pittsburgh, Cambridge University, and a team composed of ICSI, SRI and University of Washington. EARS funded the collection of the Switchboard telephone speech corpus containing 260 hours of recorded conversations from over 500 speakers.[35] The GALE program focused on Arabic and Mandarin broadcast news speech. Google's first effort at speech recognition came in 2007 after hiring some researchers from Nuance.[36] The first product was GOOG-411, a telephone based directory service. The recordings from GOOG-411 produced valuable data that helped Google improve their recognition systems. Google Voice Search is now supported in over 30 languages.\n In the United States, the National Security Agency has made use of a type of speech recognition for keyword spotting since at least 2006.[37] This technology allows analysts to search through large volumes of recorded conversations and isolate mentions of keywords. Recordings can be indexed and analysts can run queries over the database to find conversations of interest. Some government research programs focused on intelligence applications of speech recognition, e.g. DARPA's EARS's program and IARPA's Babel program.\n In the early 2000s, speech recognition was still dominated by traditional approaches such as hidden Markov models combined with feedforward artificial neural networks.[38]\nToday, however, many aspects of speech recognition have been taken over by a deep learning method called Long short-term memory (LSTM), a recurrent neural network published by Sepp Hochreiter & J\u00fcrgen Schmidhuber in 1997.[39] LSTM RNNs avoid the vanishing gradient problem and can learn \"Very Deep Learning\" tasks[40] that require memories of events that happened thousands of discrete time steps ago, which is important for speech.\nAround 2007, LSTM trained by Connectionist Temporal Classification (CTC)[41] started to outperform traditional speech recognition in certain applications.[42] In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to all smartphone users.[43] Transformers, a type of neural network based solely on \"attention\", have been widely adopted in computer vision[44][45] and language modeling,[46][47] sparking the interest of adapting such models to new domains, including speech recognition.[48][49][50] Some recent papers reported superior performance levels using transformer models for speech recognition, but these models usually require large scale training datasets to reach high performance levels.\n The use of deep feedforward (non-recurrent) networks for acoustic modeling was introduced during the later part of 2009 by Geoffrey Hinton and his students at the University of Toronto and by Li Deng[51] and colleagues at Microsoft Research, initially in the collaborative work between Microsoft and the University of Toronto which was subsequently expanded to include IBM and Google (hence \"The shared views of four research groups\" subtitle in their 2012 review paper).[52][53][54] A Microsoft research executive called this innovation \"the most dramatic change in accuracy since 1979\".[55] In contrast to the steady incremental improvements of the past few decades, the application of deep learning decreased word error rate by 30%.[55] This innovation was quickly adopted across the field. Researchers have begun to use deep learning techniques for language modeling as well.\n In the long history of speech recognition, both shallow form and deep form (e.g. recurrent nets) of artificial neural networks had been explored for many years during 1980s, 1990s and a few years into the 2000s.[56][57][58]\nBut these methods never won over the non-uniform internal-handcrafting Gaussian mixture model/hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.[59] A number of key difficulties had been methodologically analyzed in the 1990s, including gradient diminishing[60] and weak temporal correlation structure in the neural predictive models.[61][62] All these difficulties were in addition to the lack of big training data and big computing power in these early days. Most speech recognition researchers who understood such barriers hence subsequently moved away from neural nets to pursue generative modeling approaches until the recent resurgence of deep learning starting around 2009\u20132010 that had overcome all these difficulties. Hinton et al. and Deng et al. reviewed part of this recent history about how their collaboration with each other and then with colleagues across four groups (University of Toronto, Microsoft, Google, and IBM) ignited a renaissance of applications of deep feedforward neural networks for speech recognition.[53][54][63][64]\n By early 2010s speech recognition, also called voice recognition[65][66][67] was clearly differentiated from speaker recognition, and speaker independence was considered a major breakthrough. Until then, systems required a \"training\" period.  A 1987 ad for a doll had carried the tagline \"Finally, the doll that understands you.\" \u2013 despite the fact that it was described as \"which children could train to respond to their voice\".[12]\n In 2017, Microsoft researchers reached a historical human parity milestone of transcribing conversational telephony speech on the widely benchmarked Switchboard task. Multiple deep learning models were used to optimize speech recognition accuracy. The speech recognition word error rate was reported to be as low as 4 professional human transcribers working together on the same benchmark, which was funded by IBM Watson speech team on the same task.[68]\n Both acoustic modeling and language modeling are important parts of modern statistically based speech recognition algorithms. Hidden Markov models (HMMs) are widely used in many systems. Language modeling is also used in many other natural language processing applications such as document classification or statistical machine translation.\n Modern general-purpose speech recognition systems are based on hidden Markov models. These are statistical models that output a sequence of symbols or quantities. HMMs are used in speech recognition because a speech signal can be viewed as a piecewise stationary signal or a short-time stationary signal. In a short time scale (e.g., 10 milliseconds), speech can be approximated as a stationary process. Speech can be thought of as a Markov model for many stochastic purposes.\n Another reason why HMMs are popular is that they can be trained automatically and are simple and computationally feasible to use. In speech recognition, the hidden Markov model would output a sequence of n-dimensional real-valued vectors (with n being a small integer, such as 10), outputting one of these every 10 milliseconds. The vectors would consist of cepstral coefficients, which are obtained by taking a Fourier transform of a short time window of speech and decorrelating the spectrum using a cosine transform, then taking the first (most significant) coefficients. The hidden Markov model will tend to have in each state a statistical distribution that is a mixture of diagonal covariance Gaussians, which will give a likelihood for each observed vector. Each word, or (for more general speech recognition systems), each phoneme, will have a different output distribution; a hidden Markov model for a sequence of words or phonemes is made by concatenating the individual trained hidden Markov models for the separate words and phonemes.\n Described above are the core elements of the most common, HMM-based approach to speech recognition. Modern speech recognition systems use various combinations of a number of standard techniques in order to improve results over the basic approach described above. A typical large-vocabulary system would need context dependency for the phonemes (so that phonemes with different left and right context would have different realizations as HMM states); it would use cepstral normalization to normalize for a different speaker and recording conditions; for further speaker normalization, it might use vocal tract length normalization (VTLN) for male-female normalization and maximum likelihood linear regression (MLLR) for more general speaker adaptation. The features would have so-called delta and delta-delta coefficients to capture speech dynamics and in addition, might use heteroscedastic linear discriminant analysis (HLDA); or might skip the delta and delta-delta coefficients and use splicing and an LDA-based projection followed perhaps by heteroscedastic linear discriminant analysis or a global semi-tied co variance transform (also known as maximum likelihood linear transform, or MLLT). Many systems use so-called discriminative training techniques that dispense with a purely statistical approach to HMM parameter estimation and instead optimize some classification-related measure of the training data. Examples are maximum mutual information (MMI), minimum classification error (MCE), and minimum phone error (MPE).\n Decoding of the speech (the term for what happens when the system is presented with a new utterance and must compute the most likely source sentence) would probably use the Viterbi algorithm to find the best path, and here there is a choice between dynamically creating a combination hidden Markov model, which includes both the acoustic and language model information and combining it statically beforehand (the finite state transducer, or FST, approach).\n A possible improvement to decoding is to keep a set of good candidates instead of just keeping the best candidate, and to use a better scoring function (re scoring) to rate these good candidates so that we may pick the best one according to this refined score. The set of candidates can be kept either as a list (the N-best list approach) or as a subset of the models (a lattice). Re scoring is usually done by trying to minimize the Bayes risk[69] (or an approximation thereof) Instead of taking the source sentence with maximal probability, we try to take the sentence that minimizes the expectancy of a given loss function with regards to all possible transcriptions (i.e., we take the sentence that minimizes the average distance to other possible sentences weighted by their estimated probability). The loss function is usually the Levenshtein distance, though it can be different distances for specific tasks; the set of possible transcriptions is, of course, pruned to maintain tractability. Efficient algorithms have been devised to re score lattices represented as weighted finite state transducers with edit distances represented themselves as a finite state transducer verifying certain assumptions.[70]\n Dynamic time warping is an approach that was historically used for speech recognition but has now largely been displaced by the more successful HMM-based approach.\n Dynamic time warping is an algorithm for measuring similarity between two sequences that may vary in time or speed. For instance, similarities in walking patterns would be detected, even if in one video the person was walking slowly and if in another he or she were walking more quickly, or even if there were accelerations and deceleration during the course of one observation. DTW has been applied to video, audio, and graphics\u00a0\u2013 indeed, any data that can be turned into a linear representation can be analyzed with DTW.\n A well-known application has been automatic speech recognition, to cope with different speaking speeds. In general, it is a method that allows a computer to find an optimal match between two given sequences (e.g., time series) with certain restrictions. That is, the sequences are \"warped\" non-linearly to match each other. This sequence alignment method is often used in the context of hidden Markov models.\n Neural networks emerged as an attractive acoustic modeling approach in ASR in the late 1980s. Since then, neural networks have been used in many aspects of speech recognition such as phoneme classification,[71] phoneme classification through multi-objective evolutionary algorithms,[72] isolated word recognition,[73] audiovisual speech recognition, audiovisual speaker recognition and speaker adaptation.\n Neural networks make fewer explicit assumptions about feature statistical properties than HMMs and have several qualities making them more attractive recognition models for speech recognition. When used to estimate the probabilities of a speech feature segment, neural networks allow discriminative training in a natural and efficient manner. However, in spite of their effectiveness in classifying short-time units such as individual phonemes and isolated words,[74] early neural networks were rarely successful for continuous recognition tasks because of their limited ability to model temporal dependencies.\n One approach to this limitation was to use neural networks as a pre-processing, feature transformation or dimensionality reduction,[75] step prior to HMM based recognition. However, more recently, LSTM and related recurrent neural networks (RNNs),[39][43][76][77] Time Delay Neural Networks(TDNN's),[78] and transformers[48][49][50] have demonstrated improved performance in this area.\n Deep neural networks and denoising autoencoders[79] are also under investigation. A deep feedforward neural network (DNN) is an artificial neural network with multiple hidden layers of units between the input and output layers.[53] Similar to shallow neural networks, DNNs can model complex non-linear relationships. DNN architectures generate compositional models, where extra layers enable composition of features from lower layers, giving a huge learning capacity and thus the potential of modeling complex patterns of speech data.[80]\n A success of DNNs in large vocabulary speech recognition occurred in 2010 by industrial researchers, in collaboration with academic researchers, where large output layers of the DNN based on context dependent HMM states constructed by decision trees were adopted.[81][82]\n[83] See comprehensive reviews of this development and of the state of the art as of October 2014 in the recent Springer book from Microsoft Research.[84] See also the related background of automatic speech recognition and the impact of various machine learning paradigms, notably including deep learning, in\nrecent overview articles.[85][86]\n One fundamental principle of deep learning is to do away with hand-crafted feature engineering and to use raw features. This principle was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features,[87] showing its superiority over the Mel-Cepstral features which contain a few stages of fixed transformation from spectrograms.\nThe true \"raw\" features of speech, waveforms, have more recently been shown to produce excellent larger-scale speech recognition results.[88]\n Since 2014, there has been much research interest in \"end-to-end\" ASR. Traditional phonetic-based (i.e., all HMM-based model) approaches required separate components and training for the pronunciation, acoustic, and language model. End-to-end models jointly learn all the components of the speech recognizer. This is valuable since it simplifies the training process and deployment process. For example, a n-gram language model is required for all HMM-based systems, and a typical n-gram language model often takes several gigabytes in memory making them impractical to deploy on mobile devices.[89] Consequently, modern commercial ASR systems from Google and Apple (as of 2017[update]) are deployed on the cloud and require a network connection as opposed to the device locally.\n The first attempt at end-to-end ASR was with Connectionist Temporal Classification (CTC)-based systems introduced by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014.[90] The model consisted of recurrent neural networks and a CTC layer. Jointly, the RNN-CTC model learns the pronunciation and acoustic model together, however it is incapable of learning the language due to conditional independence assumptions similar to a HMM. Consequently, CTC models can directly learn to map speech acoustics to English characters, but the models make many common spelling mistakes and must rely on a separate language model to clean up the transcripts. Later, Baidu expanded on the work with extremely large datasets and demonstrated some commercial success in Chinese Mandarin and English.[91] In 2016, University of Oxford presented LipNet,[92] the first end-to-end sentence-level lipreading model, using spatiotemporal convolutions coupled with an RNN-CTC architecture, surpassing human-level performance in a restricted grammar dataset.[93] A large-scale CNN-RNN-CTC architecture was presented in 2018 by Google DeepMind achieving 6 times better performance than human experts.[94] In 2019, Nvidia launched two CNN-CTC ASR models, Jasper and QuarzNet, with an overall performance WER of 3%.[95][96] Similar to other deep learning applications, transfer learning and domain adaptation are important strategies for reusing and extending the capabilities of deep learning models, particularly due to the high costs of training models from scratch, and the small size of available corpus in many languages and/or specific domains.[97][98][99]\n An alternative approach to CTC-based models are attention-based models. Attention-based ASR models were introduced simultaneously by Chan et al. of Carnegie Mellon University and Google Brain and Bahdanau et al. of the University of Montreal in 2016.[100][101] The model named \"Listen, Attend and Spell\" (LAS), literally \"listens\" to the acoustic signal, pays \"attention\" to different parts of the signal and \"spells\" out the transcript one character at a time. Unlike CTC-based models, attention-based models do not have conditional-independence assumptions and can learn all the components of a speech recognizer including the pronunciation, acoustic and language model directly. This means, during deployment, there is no need to carry around a language model making it very practical for applications with limited memory. By the end of 2016, the attention-based models have seen considerable success including outperforming the CTC models (with or without an external language model).[102] Various extensions have been proposed since the original LAS model. Latent Sequence Decompositions (LSD) was proposed by Carnegie Mellon University, MIT and Google Brain to directly emit sub-word units which are more natural than English characters;[103] University of Oxford and Google DeepMind extended LAS to \"Watch, Listen, Attend and Spell\" (WLAS) to handle lip reading surpassing human-level performance.[104]\n Typically a manual control input, for example by means of a finger control on the steering-wheel, enables the speech recognition system and this is signaled to the driver by an audio prompt. Following the audio prompt, the system has a \"listening window\" during which it may accept a speech input for recognition. [citation needed]\n Simple voice commands may be used to initiate phone calls, select radio stations or play music from a compatible smartphone, MP3 player or music-loaded flash drive. Voice recognition capabilities vary between car make and model. Some of the most recent[when?] car models offer natural-language speech recognition in place of a fixed set of commands, allowing the driver to use full sentences and common phrases. With such systems there is, therefore, no need for the user to memorize a set of fixed command words.[citation needed]\n Automatic pronunciation assessment is the use of speech recognition to verify the correctness of pronounced speech,[105] as distinguished from manual assessment by an instructor or proctor.[106] Also called speech verification, pronunciation evaluation, and pronunciation scoring, the main application of this technology is computer-aided pronunciation teaching (CAPT) when combined with computer-aided instruction for computer-assisted language learning (CALL), speech remediation, or accent reduction. Pronunciation assessment does not determine unknown speech (as in dictation or automatic transcription) but instead, knowing the expected word(s) in advance, it attempts to verify the correctness of the learner's pronunciation and ideally their intelligibility to listeners,[107][108] sometimes along with often inconsequential prosody such as intonation, pitch, tempo, rhythm, and stress.[109] Pronunciation assessment is also used in reading tutoring, for example in products such as Microsoft Teams[110] and from Amira Learning.[111] Automatic pronunciation assessment can also be used to help diagnose and treat speech disorders such as apraxia.[112]\n Assessing authentic listener intelligibility is essential for avoiding inaccuracies from accent bias, especially in high-stakes assessments;[113][114][115] from words with multiple correct pronunciations;[116] and from phoneme coding errors in machine-readable pronunciation dictionaries.[117] In 2022, researchers found that some newer speech to text systems, based on end-to-end reinforcement learning to map audio signals directly into words, produce word and phrase confidence scores very closely correlated with genuine listener intelligibility.[118] In the Common European Framework of Reference for Languages (CEFR) assessment criteria for \"overall phonological control\", intelligibility outweighs formally correct pronunciation at all levels.[119]\n In the health care sector, speech recognition can be implemented in front-end or back-end of the medical documentation process. Front-end speech recognition is where the provider dictates into a speech-recognition engine, the recognized words are displayed as they are spoken, and the dictator is responsible for editing and signing off on the document. Back-end or deferred speech recognition is where the provider dictates into a digital dictation system, the voice is routed through a speech-recognition machine and the recognized draft document is routed along with the original voice file to the editor, where the draft is edited and report finalized. Deferred speech recognition is widely used in the industry currently.\n One of the major issues relating to the use of speech recognition in healthcare is that the American Recovery and Reinvestment Act of 2009 (ARRA) provides for substantial financial benefits to physicians who utilize an EMR according to \"Meaningful Use\" standards. These standards require that a substantial amount of data be maintained by the EMR (now more commonly referred to as an Electronic Health Record or EHR). The use of speech recognition is more naturally suited to the generation of narrative text, as part of a radiology/pathology interpretation, progress note or discharge summary: the ergonomic gains of using speech recognition to enter structured discrete data (e.g., numeric values or codes from a list or a controlled vocabulary) are relatively minimal for people who are sighted and who can operate a keyboard and mouse.\n A more significant issue is that most EHRs have not been expressly tailored to take advantage of voice-recognition capabilities. A large part of the clinician's interaction with the EHR involves navigation through the user interface using menus, and tab/button clicks, and is heavily dependent on keyboard and mouse: voice-based navigation provides only modest ergonomic benefits. By contrast, many highly customized systems for radiology or pathology dictation implement voice \"macros\", where the use of certain phrases \u2013 e.g., \"normal report\", will automatically fill in a large number of default values and/or generate boilerplate, which will vary with the type of the exam \u2013 e.g., a chest X-ray vs. a gastrointestinal contrast series for a radiology system.\n Prolonged use of speech recognition software in conjunction with word processors has shown benefits to short-term-memory restrengthening in brain AVM patients who have been treated with resection. Further research needs to be conducted to determine cognitive benefits for individuals whose AVMs have been treated using radiologic techniques.[citation needed]\n Substantial efforts have been devoted in the last decade to the test and evaluation of speech recognition in fighter aircraft. Of particular note have been the US program in speech recognition for the Advanced Fighter Technology Integration (AFTI)/F-16 aircraft (F-16 VISTA), the program in France for Mirage aircraft, and other programs in the UK dealing with a variety of aircraft platforms. In these programs, speech recognizers have been operated successfully in fighter aircraft, with applications including setting radio frequencies, commanding an autopilot system, setting steer-point coordinates and weapons release parameters, and controlling flight display.\n Working with Swedish pilots flying in the JAS-39 Gripen cockpit, Englund (2004) found recognition deteriorated with increasing g-loads. The report also concluded that adaptation greatly improved the results in all cases and that the introduction of models for breathing was shown to improve recognition scores significantly. Contrary to what might have been expected, no effects of the broken English of the speakers were found. It was evident that spontaneous speech caused problems for the recognizer, as might have been expected. A restricted vocabulary, and above all, a proper syntax, could thus be expected to improve recognition accuracy substantially.[120]\n The Eurofighter Typhoon, currently in service with the UK RAF, employs a speaker-dependent system, requiring each pilot to create a template. The system is not used for any safety-critical or weapon-critical tasks, such as weapon release or lowering of the undercarriage, but is used for a wide range of other cockpit functions. Voice commands are confirmed by visual and/or aural feedback. The system is seen as a major design feature in the reduction of pilot workload,[121] and even allows the pilot to assign targets to his aircraft with two simple voice commands or to any of his wingmen with only five commands.[122]\n Speaker-independent systems are also being developed and are under test for the F-35 Lightning II (JSF) and the Alenia Aermacchi M-346 Master lead-in fighter trainer. These systems have produced word accuracy scores in excess of 98%.[123]\n The problems of achieving high recognition accuracy under stress and noise are particularly relevant in the helicopter environment as well as in the jet fighter environment. The acoustic noise problem is actually more severe in the helicopter environment, not only because of the high noise levels but also because the helicopter pilot, in general, does not wear a facemask, which would reduce acoustic noise in the microphone. Substantial test and evaluation programs have been carried out in the past decade in speech recognition systems applications in helicopters, notably by the U.S. Army Avionics Research and Development Activity (AVRADA) and by the Royal Aerospace Establishment (RAE) in the UK. Work in France has included speech recognition in the Puma helicopter. There has also been much useful work in Canada. Results have been encouraging, and voice applications have included: control of communication radios, setting of navigation systems, and control of an automated target handover system.\n As in fighter applications, the overriding issue for voice in helicopters is the impact on pilot effectiveness. Encouraging results are reported for the AVRADA tests, although these represent only a feasibility demonstration in a test environment. Much remains to be done both in speech recognition and in overall speech technology in order to consistently achieve performance improvements in operational settings.\n Training for air traffic controllers (ATC) represents an excellent application for speech recognition systems. Many ATC training systems currently require a person to act as a \"pseudo-pilot\", engaging in a voice dialog with the trainee controller, which simulates the dialog that the controller would have to conduct with pilots in a real ATC situation. Speech recognition and synthesis techniques offer the potential to eliminate the need for a person to act as a pseudo-pilot, thus reducing training and support personnel. In theory, Air controller tasks are also characterized by highly structured speech as the primary output of the controller, hence reducing the difficulty of the speech recognition task should be possible. In practice, this is rarely the case. The FAA document 7110.65 details the phrases that should be used by air traffic controllers. While this document gives less than 150 examples of such phrases, the number of phrases supported by one of the simulation vendors speech recognition systems is in excess of 500,000.\n The USAF, USMC, US Army, US Navy, and FAA as well as a number of international ATC training organizations such as the Royal Australian Air Force and Civil Aviation Authorities in Italy, Brazil, and Canada are currently using ATC simulators with speech recognition from a number of different vendors.[citation needed]\n ASR is now commonplace in the field of telephony and is becoming more widespread in the field of computer gaming and simulation. In telephony systems, ASR is now being predominantly used in contact centers by integrating it with IVR systems. Despite the high level of integration with word processing in general personal computing, in the field of document production, ASR has not seen the expected increases in use.\n The improvement of mobile processor speeds has made speech recognition practical in smartphones. Speech is used mostly as a part of a user interface, for creating predefined or custom speech commands.\n People with disabilities can benefit from speech recognition programs. For individuals that are Deaf or Hard of Hearing, speech recognition software is used to automatically generate a closed-captioning of conversations such as discussions in conference rooms, classroom lectures, and/or religious services.[124]\n Students who are blind (see Blindness and education) or have very low vision can benefit from using the technology to convey words and then hear the computer recite them, as well as use a computer by commanding with their voice, instead of having to look at the screen and keyboard.[125]\n Students who are physically disabled have a Repetitive strain injury/other injuries to the upper extremities can be relieved from having to worry about handwriting, typing, or working with scribe on school assignments by using speech-to-text programs. They can also utilize speech recognition technology to enjoy searching the Internet or using a computer at home without having to physically operate a mouse and keyboard.[125]\n Speech recognition can allow students with learning disabilities to become better writers. By saying the words aloud, they can increase the fluidity of their writing, and be alleviated of concerns regarding spelling, punctuation, and other mechanics of writing.[126] Also, see Learning disability.\n The use of voice recognition software, in conjunction with a digital audio recorder and a personal computer running word-processing software has proven to be positive for restoring damaged short-term memory capacity, in stroke and craniotomy individuals.\n Speech recognition is also very useful for people who have difficulty using their hands, ranging from mild repetitive stress injuries to involve disabilities that preclude using conventional computer input devices. In fact, people who used the keyboard a lot and developed RSI became an urgent early market for speech recognition.[127][128] Speech recognition is used in deaf telephony, such as voicemail to text, relay services, and captioned telephone. Individuals with learning disabilities who have problems with thought-to-paper communication (essentially they think of an idea but it is processed incorrectly causing it to end up differently on paper) can possibly benefit from the software but the technology is not bug proof.[129] Also the whole idea of speak to text can be hard for intellectually disabled person's due to the fact that it is rare that anyone tries to learn the technology to teach the person with the disability.[130]\n This type of technology can help those with dyslexia but other disabilities are still in question. The effectiveness of the product is the problem that is hindering it from being effective. Although a kid may be able to say a word depending on how clear they say it the technology may think they are saying another word and input the wrong one. Giving them more work to fix, causing them to have to take more time with fixing the wrong word.[131]\n The performance of speech recognition systems is usually evaluated in terms of accuracy and speed.[136][137] Accuracy is usually rated with word error rate (WER), whereas speed is measured with the real time factor. Other measures of accuracy include Single Word Error Rate (SWER) and Command Success Rate (CSR).\n Speech recognition by machine is a very complex problem, however. Vocalizations vary in terms of accent, pronunciation, articulation, roughness, nasality, pitch, volume, and speed. Speech is distorted by a background noise and echoes, electrical characteristics. Accuracy of speech recognition may vary with the following:[138][citation needed]\n As mentioned earlier in this article, the accuracy of speech recognition may vary depending on the following factors:\n With discontinuous speech full sentences separated by silence are used, therefore it becomes easier to recognize the speech as well as with isolated speech. \nWith continuous speech naturally spoken sentences are used, therefore it becomes harder to recognize the speech, different from both isolated and discontinuous speech.\n Constraints are often represented by grammar. \n Speech recognition is a multi-leveled pattern recognition task.\n e.g. Known word pronunciations or legal word sequences, which can compensate for errors or uncertainties at a lower level;\n For telephone speech the sampling rate is 8000 samples per second; \n computed every 10\u00a0ms, with one 10\u00a0ms section called a frame;\n Analysis of four-step neural network approaches can be explained by further information. Sound is produced by air (or some other medium) vibration, which we register by ears, but machines by receivers. Basic sound creates a wave which has two descriptions: amplitude (how strong is it), and frequency (how often it vibrates per second).\nAccuracy can be computed with the help of word error rate (WER). Word error rate can be calculated by aligning the recognized word and referenced word using dynamic string alignment. The problem may occur while computing the word error rate due to the difference between the sequence lengths of the recognized word and referenced word.\n The formula to compute the word error rate (WER) is:\n \n\n\n\nW\nE\nR\n=\n\n\n\n(\ns\n+\nd\n+\ni\n)\n\nn\n\n\n\n\n{\\displaystyle WER={(s+d+i) \\over n}}\n\n\n where s is the number of substitutions, d is the number of deletions, i is the number of insertions, and n is the number of word references.\n While computing, the word recognition rate (WRR) is used. The formula is:\n where h is the number of correctly recognized words:\n Speech recognition can become a means of attack, theft, or accidental operation. For example, activation words like \"Alexa\" spoken in an audio or video broadcast can cause devices in homes and offices to start listening for input inappropriately, or possibly take an unwanted action.[140] Voice-controlled devices are also accessible to visitors to the building, or even those outside the building if they can be heard inside. Attackers may be able to gain access to personal information, like calendar, address book contents, private messages, and documents. They may also be able to impersonate the user to send messages or make online purchases.\n Two attacks have been demonstrated that use artificial sounds. One transmits ultrasound and attempt to send commands without nearby people noticing.[141] The other adds small, inaudible distortions to other speech or music that are specially crafted to confuse the specific speech recognition system into recognizing music as speech, or to make what sounds like one command to a human sound like a different command to the system.[142]\n Popular speech recognition conferences held each year or two include SpeechTEK and SpeechTEK Europe, ICASSP, Interspeech/Eurospeech, and the IEEE ASRU. Conferences in the field of natural language processing, such as ACL, NAACL, EMNLP, and HLT, are beginning to include papers on speech processing. Important journals include the IEEE Transactions on Speech and Audio Processing (later renamed IEEE Transactions on Audio, Speech and Language Processing and since Sept 2014 renamed IEEE/ACM Transactions on Audio, Speech and Language Processing\u2014after merging with an ACM publication), Computer Speech and Language, and Speech Communication.\n Books like \"Fundamentals of Speech Recognition\" by Lawrence Rabiner can be useful to acquire basic knowledge but may not be fully up to date (1993). Another good source can be \"Statistical Methods for Speech Recognition\" by Frederick Jelinek and \"Spoken Language Processing (2001)\" by Xuedong Huang etc., \"Computer Speech\", by Manfred R. Schroeder, second edition published in 2004, and \"Speech Processing: A Dynamic and Optimization-Oriented Approach\" published in 2003 by Li Deng and Doug O'Shaughnessey. The updated textbook Speech and Language Processing (2008) by Jurafsky and Martin presents the basics and the state of the art for ASR. Speaker recognition also uses the same features, most of the same front-end processing, and classification techniques as is done in speech recognition. A comprehensive textbook, \"Fundamentals of Speaker Recognition\" is an in depth source for up to date details on the theory and practice.[143] A good insight into the techniques used in the best modern systems can be gained by paying attention to government sponsored evaluations such as those organised by DARPA (the largest speech recognition-related project ongoing as of 2007 is the GALE project, which involves both speech recognition and translation components).\n A good and accessible introduction to speech recognition technology and its history is provided by the general audience book \"The Voice in the Machine. Building Computers That Understand Speech\" by Roberto Pieraccini (2012).\n The most recent book on speech recognition is Automatic Speech Recognition: A Deep Learning Approach (Publisher: Springer) written by Microsoft researchers D. Yu and L. Deng and published near the end of 2014, with highly mathematically oriented technical detail on how deep learning methods are derived and implemented in modern speech recognition systems based on DNNs and related deep learning methods.[84] A related book, published earlier in 2014, \"Deep Learning: Methods and Applications\" by L. Deng and D. Yu provides a less technical but more methodology-focused overview of DNN-based speech recognition during 2009\u20132014, placed within the more general context of deep learning applications including not only speech recognition but also image recognition, natural language processing, information retrieval, multimodal processing, and multitask learning.[80]\n In terms of freely available resources, Carnegie Mellon University's Sphinx toolkit is one place to start to both learn about speech recognition and to start experimenting. Another resource (free but copyrighted) is the HTK book (and the accompanying HTK toolkit). For more recent and state-of-the-art techniques, Kaldi toolkit can be used.[144] In 2017 Mozilla launched the open source project called Common Voice[145] to gather big database of voices that would help build free speech recognition project DeepSpeech (available free at GitHub),[146] using Google's open source platform TensorFlow.[147] When Mozilla redirected funding away from the project in 2020, it was forked by its original developers as Coqui STT[148] using the same open-source license.[149][150]\n Google Gboard supports speech recognition on all Android applications. It can be activated through the microphone icon.[151]\n The commercial cloud based speech recognition APIs are broadly available.\n For more software resources, see List of speech recognition software.\n",
        "doc_number": 76
    },
    {
        "url": "https://en.wikipedia.org/wiki/Weak_AI",
        "content": "\n Weak artificial intelligence (weak AI) is artificial intelligence that implements a limited part of the mind, or, as Artificial Narrow Intelligece,[1][2][3] is focused on one narrow task. \n Weak AI is contrasted with strong AI, which can be interpreted in various ways: \n Narrow AI can be classified as being \"limited to a single, narrowly defined task. Most modern AI systems would be classified in this category.\"[4] Artificial general intelligence is conversely the opposite.\n Some examples of narrow AI are AlphaGo,[5] self-driving cars, robot systems used in the medical field, and diagnostic doctors. Narrow AI systems are sometimes dangerous if unreliable. And the behavior that it follows can become inconsistent.[6] It could be difficult for the AI to grasp complex patterns and get to a solution that works reliably in various environments. This \"brittleness\" can cause it to fail in unpredictable ways.[7]\n Narrow AI failures can sometimes have significant consequences. It could for example cause disruptions in the electric grid, damage nuclear power plants, cause global economic problems, and misdirect autonomous vehicles.[1] Medicines could be incorrectly sorted and distributed. Also, medical diagnoses can ultimately have serious and sometimes deadly consequences if the AI is faulty or biased.[8]\n Simple AI programs have already worked their way into our society unnoticed. Autocorrection for typing, speech recognition for speech-to-text programs, and vast expansions in the data science fields are examples.[9] As much as narrow and relatively general AI is slowly starting to help out societies, they are also starting to hurt them as well. AI had already unfairly put people in jail, discriminated against women in the workplace for hiring, taught some problematic ideas to millions, and even killed people with automatic cars.[10] AI might be a powerful tool that can be used for improving lives, but it could also be a dangerous technology with the potential for misuse.\n Despite being \"narrow\" AI, recommender systems are efficient at predicting user reactions based their posts, patterns, or trends.[11] For instance, TikTok's \"For You\" algorithm can determine user's interests or preferences in less than an hour.[12] Some other social media AI systems are used to detect bots that may be involved in biased propaganda or other potentially malicious activities.[13]\n John Searle contests the possibility of strong AI (by which he means conscious AI). He further believes that the Turing test (created by Alan Turing and originally called the \"imitation game\", used to assess whether a machine can converse indistinguishably from a human) is not accurate or appropriate for testing whether an AI is \"strong\".[14]\n Scholars such as Antonio Lieto have argued that the current research on both AI and cognitive modelling are perfectly aligned with the weak-AI hypothesis (that should not be confused with the \"general\" vs \"narrow\" AI distinction) and that the popular assumption that cognitively inspired AI systems espouse the strong AI hypothesis is ill-posed and problematic since \"artificial models of brain and mind can be used to understand mental phenomena without pretending that that they are the real phenomena that they are modelling\"[15] (as, on the other hand, implied by the strong AI assumption).\n",
        "doc_number": 77
    },
    {
        "url": "https://en.wikipedia.org/wiki/Strong_AI",
        "content": "Strong AI (strong artificial intelligence) may refer to:\n",
        "doc_number": 78
    },
    {
        "url": "https://en.wikipedia.org/wiki/Artificial_neural_network",
        "content": "\n In machine learning, a neural network (also artificial neural network or neural net, abbreviated ANN or NN) is a model inspired by the structure and function of biological neural networks in animal brains.[1][2]\n An ANN consists of connected units or nodes called artificial neurons, which loosely model the neurons in the brain. Artificial neuron models that mimic biological neurons more closely have also been recently investigated and shown to significantly improve performance. These are connected by edges, which model the synapses in the brain. Each artificial neuron receives signals from connected neurons, then processes them and sends a signal to other connected neurons. The \"signal\" is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs, called the activation function. The strength of the signal at each connection is determined by a weight, which adjusts during the learning process.\n Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly passing through multiple intermediate layers (hidden layers). A network is typically called a deep neural network if it has at least two hidden layers.[3]\n Artificial neural networks are used for various tasks, including predictive modeling, adaptive control, and solving problems in artificial intelligence. They can learn from experience, and can derive conclusions from a complex and seemingly unrelated set of information.\n \nNeural networks are typically trained through empirical risk minimization. This method is based on the idea of optimizing the network's parameters to minimize the difference, or empirical risk, between the predicted output and the actual target values in a given dataset.[4] Gradient-based methods such as backpropagation are usually used to estimate the parameters of the network.[4] During the training phase, ANNs learn from labeled training data by iteratively updating their parameters to minimize a defined loss function.[5] This method allows the network to generalize to unseen data. Today's deep neural networks are based on early work in statistics over 200 years ago. The simplest kind of feedforward neural network (FNN) is a linear network, which consists of a single layer of output nodes with linear activation functions; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated at each node. The mean squared errors between these calculated outputs and the given target values are minimized by creating an adjustment to the weights. This technique has been known for over two centuries as the method of least squares or linear regression. It was used as a means of finding a good rough linear fit to a set of points by Legendre (1805) and Gauss (1795) for the prediction of planetary movement.[7][8][9][10][11]\n Historically, digital computers such as the von Neumann model operate via the execution of explicit instructions with access to memory by a number of processors. Some neural networks, on the other hand, originated from efforts to model information processing in biological systems through the framework of connectionism. Unlike the von Neumann model, connectionist computing does not separate memory and processing.\n Warren McCulloch and Walter Pitts[12] (1943) considered a non-learning computational model for neural networks.[13] This model paved the way for research to split into two approaches. One approach focused on biological processes while the other focused on the application of neural networks to artificial intelligence.\n In the late 1940s, D. O. Hebb[14] proposed a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. It was used in many early neural networks, such as Rosenblatt's perceptron and the Hopfield network. Farley and Clark[15] (1954) used computational machines to simulate a Hebbian network. Other neural network computational machines were created by Rochester, Holland, Habit and Duda (1956).[16]\n In 1958, psychologist Frank Rosenblatt described the perceptron, one of the first implemented artificial neural networks,[17][18][19][20] funded by the United States Office of Naval Research.[21]\nR. D. Joseph (1960)[22] mentions an even earlier perceptron-like device by Farley and Clark:[10] \"Farley and Clark of MIT Lincoln Laboratory actually preceded Rosenblatt in the development of a perceptron-like device.\" However, \"they dropped the subject.\"\nThe perceptron raised public excitement for research in Artificial Neural Networks, causing the US government to drastically increase funding. This contributed to \"the Golden Age of AI\" fueled by the optimistic claims made by computer scientists regarding the ability of perceptrons to emulate human intelligence.[23]\n The first perceptrons did not have adaptive hidden units. However, Joseph (1960)[22] also discussed multilayer perceptrons with an adaptive hidden layer. Rosenblatt (1962)[24]:\u200asection 16\u200a cited and adopted these ideas, also crediting work by H. D. Block and B. W. Knight. Unfortunately, these early efforts did not lead to a working learning algorithm for hidden units, i.e., deep learning.\n Fundamental research was conducted on ANNs in the 1960s and 1970s. The first working deep learning algorithm was the Group method of data handling, a method to train arbitrarily deep neural networks, published by Alexey Ivakhnenko and Lapa in the Soviet Union (1965). They regarded it as a form of polynomial regression,[25] or a generalization of Rosenblatt's perceptron.[26] A 1971 paper described a deep network with eight layers trained by this method,[27] which is based on layer by layer training through regression analysis. Superfluous hidden units are pruned using a separate validation set. Since the activation functions of the nodes are Kolmogorov-Gabor polynomials, these were also the first deep networks with multiplicative units or \"gates.\"[10]\n The first deep learning multilayer perceptron trained by stochastic gradient descent[28] was published in 1967 by Shun'ichi Amari.[29] In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned internal representations to classify non-linearily separable pattern classes.[10] Subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.\n In 1969, Kunihiko Fukushima introduced the ReLU (rectified linear unit) activation function.[10][30][31] The rectifier has become the most popular activation function for deep learning.[32]\n Nevertheless, research stagnated in the United States following the work of Minsky and Papert (1969),[33] who emphasized that basic perceptrons were incapable of processing the exclusive-or circuit. This insight was irrelevant for the deep networks of Ivakhnenko (1965) and Amari (1967).\n In 1976 transfer learning was introduced in neural networks learning. [34] [35]\n Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers and weight replication began with the Neocognitron introduced by Kunihiko Fukushima in 1979, though not trained by backpropagation.[36][37][38]\n Backpropagation is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673[39] to networks of differentiable nodes. The terminology \"back-propagating errors\" was actually introduced in 1962 by Rosenblatt,[24] but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation in 1960 in the context of control theory.[40] In 1970, Seppo Linnainmaa published the modern form of backpropagation in his Master's thesis (1970).[41][42][10] G.M. Ostrovski et al. republished it in 1971.[43][44] Paul Werbos applied backpropagation to neural networks in 1982[45][46] (his 1974 PhD thesis, reprinted in a 1994 book,[47] did not yet describe the algorithm[44]). In 1986, David E. Rumelhart et al. popularised backpropagation but did not cite the original work.[48]\n Kunihiko Fukushima's convolutional neural network (CNN) architecture of 1979[36] also introduced max pooling,[49] a popular downsampling procedure for CNNs. CNNs have become an essential tool for computer vision.\n The time delay neural network (TDNN) was introduced in 1987 by Alex Waibel to apply CNN to phoneme recognition. It used convolutions, weight sharing, and backpropagation.[50][51] In 1988, Wei Zhang applied a backpropagation-trained CNN to alphabet recognition.[52]\nIn 1989, Yann LeCun et al. created a CNN called LeNet for recognizing handwritten ZIP codes on mail. Training required 3 days.[53] In 1990, Wei Zhang implemented a CNN on optical computing hardware.[54] In 1991, a CNN was applied to medical image object segmentation[55] and breast cancer detection in mammograms.[56] LeNet-5 (1998), a 7-level CNN by Yann LeCun et al., that classifies digits, was applied by several banks to recognize hand-written numbers on checks digitized in 32\u00d732 pixel images.[57]\n From 1988 onward,[58][59] the use of neural networks transformed the field of protein structure prediction, in particular when the first cascading networks were trained on profiles (matrices) produced by multiple sequence alignments.[60]\n One origin of RNN was statistical mechanics. In 1972, Shun'ichi Amari proposed to modify the weights of an Ising model by Hebbian learning rule as a model of associative memory, adding in the component of learning.[61] This was popularized as the Hopfield network by John Hopfield(1982).[62] Another origin of RNN was neuroscience. The word \"recurrent\" is used to describe loop-like structures in anatomy. In 1901, Cajal observed \"recurrent semicircles\" in the cerebellar cortex.[63] Hebb considered \"reverberating circuit\" as an explanation for short-term memory.[64] The McCulloch and Pitts paper (1943) considered neural networks that contains cycles, and noted that the current activity of such networks can be affected by activity indefinitely far in the past.[12]\n In 1982 a recurrent neural network, with an array architecture (rather than a multilayer perceptron architecture), named Crossbar Adaptive Array [65][66]  used direct recurrent connections from the output to the supervisor (teaching ) inputs. In addition of computing actions (decisions), it computed internal state evaluations (emotions) of the consequence situations. Eliminating the external supervisor, it introduced the self-learning method in neural networks.  \n In cognitive psychology, the journal American Psychologist in early 1980's carried out a debate on relation between cognition and emotion. Zajonc in 1980 stated that emotion is computed first and is independent from cognition, while Lazarus in 1982 stated that cognition is computed first and is inseparable from emotion. [67][68] In 1982 the Crossbar Adaptive Array gave a neural network model of cognition-emotion relation. [65][69] It was an example of a debate where an AI system, a recurrent neural network, contributed to an issue in the same time addressed by cognitive psychology.\n Two early influential works were the Jordan network (1986) and the Elman network (1990), which applied RNN to study cognitive psychology. \n In the 1980s, backpropagation did not work well for deep RNNs. To overcome this problem, in 1991, J\u00fcrgen Schmidhuber proposed the \"neural sequence chunker\" or \"neural history compressor\"[70][71] which introduced the important concepts of self-supervised pre-training (the \"P\" in ChatGPT) and neural knowledge distillation.[10] In 1993, a neural history compressor system solved a \"Very Deep Learning\" task that required more than 1000 subsequent layers in an RNN unfolded in time.[72]\n In 1991, Sepp Hochreiter's diploma thesis [73] identified and analyzed the vanishing gradient problem[73][74] and proposed recurrent residual connections to solve it. He and Schmidhuber introduced long short-term memory (LSTM), which set accuracy records in multiple applications domains.[75][76] This was not yet the modern version of LSTM, which required the forget gate, which was introduced in 1999.[77] It became the default choice for RNN architecture.\n During 1985\u20131995, inspired by statistical mechanics, several architectures and methods were developed by Terry Sejnowski, Peter Dayan, Geoffrey Hinton, etc., including the Boltzmann machine,[78] restricted Boltzmann machine,[79] Helmholtz machine,[80] and the wake-sleep algorithm.[81] These were designed for unsupervised learning of deep generative models.\n Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition.[82][83] In 2011, a CNN named DanNet[84][85] by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and J\u00fcrgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3.[38] It then won more contests.[86][87] They also showed how max-pooling CNNs on GPU improved performance significantly.[88]\n In October 2012, AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton[89] won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. Further incremental improvements included the VGG-16 network by Karen Simonyan and Andrew Zisserman[90] and Google's Inceptionv3.[91]\n In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images.[92] Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as \"deep learning\".[5]\n Radial basis function and wavelet networks were introduced in 2013. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.[93]\n Generative adversarial network (GAN) (Ian Goodfellow et al., 2014)[94] became state of the art in generative modeling during 2014\u20132018 period. The GAN principle was originally published in 1991 by J\u00fcrgen Schmidhuber who called it \"artificial curiosity\": two neural networks contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss.[95][96] The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. Excellent image quality is achieved by Nvidia's StyleGAN (2018)[97] based on the Progressive GAN by Tero Karras et al.[98] Here, the GAN generator is grown from small to large scale in a pyramidal fashion. Image generation by GAN reached popular success, and provoked discussions concerning deepfakes.[99] Diffusion models (2015)[100] eclipsed GANs in generative modeling since then, with systems such as DALL\u00b7E 2 (2022) and Stable Diffusion (2022).\n In 2014, the state of the art was training \"very deep neural network\" with 20 to 30 layers.[101] Stacking too many layers led to a steep reduction in training accuracy,[102] known as the \"degradation\" problem.[103] In 2015, two techniques were developed to train very deep networks: the highway network was published in May 2015,[104] and the residual neural network (ResNet) in December 2015.[105][106] ResNet behaves like an open-gated Highway Net. \n During the 2010s, the seq2seq model was developed, and attention mechanisms were added. It led to the modern Transformer architecture in 2017 in Attention Is All You Need.[107]\nIt requires computation time that is quadratic in the size of the context window. J\u00fcrgen Schmidhuber's fast weight controller (1992)[108] scales linearly and was later shown to be equivalent to the unnormalized linear Transformer.[109][110][10]\nTransformers have increasingly become the model of choice for natural language processing.[111] Many modern large language models such as ChatGPT, GPT-4, and BERT use this architecture.\n ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, abandoning attempts to remain true to their biological precursors. ANNs have the ability to learn and model non-linearities and complex relationships. This is achieved by neurons being connected in various patterns, allowing the output of some neurons to become the input of others. The network forms a directed, weighted graph.[112]\n An artificial neural network consists of simulated neurons. Each neuron is connected to other nodes via links like a biological axon-synapse-dendrite connection. All the nodes connected by links take in some data and use it to perform specific operations and tasks on the data. Each link has a weight, determining the strength of one node's influence on another,[113] allowing weights to choose the signal between neurons.\n ANNs are composed of artificial neurons which are conceptually derived from biological neurons. Each artificial neuron has inputs and produces a single output which can be sent to multiple other neurons.[114] The inputs can be the feature values of a sample of external data, such as images or documents, or they can be the outputs of other neurons. The outputs of the final output neurons of the neural net accomplish the task, such as recognizing an object in an image.[citation needed]\n To find the output of the neuron we take the weighted sum of all the inputs, weighted by the weights of the connections from the inputs to the neuron. We add a bias term to this sum.[115] This weighted sum is sometimes called the activation. This weighted sum is then passed through a (usually nonlinear) activation function to produce the output. The initial inputs are external data, such as images and documents. The ultimate outputs accomplish the task, such as recognizing an object in an image.[116]\n The neurons are typically organized into multiple layers, especially in deep learning. Neurons of one layer connect only to neurons of the immediately preceding and immediately following layers. The layer that receives external data is the input layer. The layer that produces the ultimate result is the output layer. In between them are zero or more hidden layers. Single layer and unlayered networks are also used. Between two layers, multiple connection patterns are possible. They can be 'fully connected', with every neuron in one layer connecting to every neuron in the next layer. They can be pooling, where a group of neurons in one layer connects to a single neuron in the next layer, thereby reducing the number of neurons in that layer.[117] Neurons with only such connections form a directed acyclic graph and are known as feedforward networks.[118] Alternatively, networks that allow connections between neurons in the same or previous layers are known as recurrent networks.[119]\n A hyperparameter is a constant parameter whose value is set before the learning process begins. The values of parameters are derived via learning. Examples of hyperparameters include learning rate, the number of hidden layers and batch size.[citation needed] The values of some hyperparameters can be dependent on those of other hyperparameters. For example, the size of some layers can depend on the overall number of layers.[citation needed]\n Learning is the adaptation of the network to better handle a task by considering sample observations. Learning involves adjusting the weights (and optional thresholds) of the network to improve the accuracy of the result. This is done by minimizing the observed errors. Learning is complete when examining additional observations does not usefully reduce the error rate. Even after learning, the error rate typically does not reach 0. If after learning, the error rate is too high, the network typically must be redesigned. Practically this is done by defining a cost function that is evaluated periodically during learning. As long as its output continues to decline, learning continues. The cost is frequently defined as a statistic whose value can only be approximated. The outputs are actually numbers, so when the error is low, the difference between the output (almost certainly a cat) and the correct answer (cat) is small. Learning attempts to reduce the total of the differences across the observations. Most learning models can be viewed as a straightforward application of optimization theory and statistical estimation.[112][120]\n The learning rate defines the size of the corrective steps that the model takes to adjust for errors in each observation.[121] A high learning rate shortens the training time, but with lower ultimate accuracy, while a lower learning rate takes longer, but with the potential for greater accuracy. Optimizations such as Quickprop are primarily aimed at speeding up error minimization, while other improvements mainly try to increase reliability. In order to avoid oscillation inside the network such as alternating connection weights, and to improve the rate of convergence, refinements use an adaptive learning rate that increases or decreases as appropriate.[122] The concept of momentum allows the balance between the gradient and the previous change to be weighted such that the weight adjustment depends to some degree on the previous change. A momentum close to 0 emphasizes the gradient, while a value close to 1 emphasizes the last change.[citation needed]\n While it is possible to define a cost function ad hoc, frequently the choice is determined by the function's desirable properties (such as convexity) or because it arises from the model (e.g. in a probabilistic model the model's posterior probability can be used as an inverse cost).[citation needed]\n Backpropagation is a method used to adjust the connection weights to compensate for each error found during learning. The error amount is effectively divided among the connections. Technically, backprop calculates the gradient (the derivative) of the cost function associated with a given state with respect to the weights. The weight updates can be done via stochastic gradient descent or other methods, such as extreme learning machines,[123] \"no-prop\" networks,[124] training without backtracking,[125] \"weightless\" networks,[126][127] and non-connectionist neural networks.[citation needed]\n Machine learning is commonly separated into three main learning paradigms, supervised learning,[128] unsupervised learning[129] and reinforcement learning.[130] Each corresponds to a particular learning task.\n Supervised learning uses a set of paired inputs and desired outputs. The learning task is to produce the desired output for each input. In this case, the cost function is related to eliminating incorrect deductions.[131] A commonly used cost is the mean-squared error, which tries to minimize the average squared error between the network's output and the desired output. Tasks suited for supervised learning are pattern recognition (also known as classification) and regression (also known as function approximation). Supervised learning is also applicable to sequential data (e.g., for handwriting, speech and gesture recognition). This can be thought of as learning with a \"teacher\", in the form of a function that provides continuous feedback on the quality of solutions obtained thus far.\n In unsupervised learning, input data is given along with the cost function, some function of the data \n\n\n\n\nx\n\n\n\n{\\displaystyle \\textstyle x}\n\n and the network's output. The cost function is dependent on the task (the model domain) and any a priori assumptions (the implicit properties of the model, its parameters and the observed variables). As a trivial example, consider the model \n\n\n\n\nf\n(\nx\n)\n=\na\n\n\n\n{\\displaystyle \\textstyle f(x)=a}\n\n where \n\n\n\n\na\n\n\n\n{\\displaystyle \\textstyle a}\n\n is a constant and the cost \n\n\n\n\nC\n=\nE\n[\n(\nx\n\u2212\nf\n(\nx\n)\n\n)\n\n2\n\n\n]\n\n\n\n{\\displaystyle \\textstyle C=E[(x-f(x))^{2}]}\n\n. Minimizing this cost produces a value of \n\n\n\n\na\n\n\n\n{\\displaystyle \\textstyle a}\n\n that is equal to the mean of the data. The cost function can be much more complicated. Its form depends on the application: for example, in compression it could be related to the mutual information between \n\n\n\n\nx\n\n\n\n{\\displaystyle \\textstyle x}\n\n and \n\n\n\n\nf\n(\nx\n)\n\n\n\n{\\displaystyle \\textstyle f(x)}\n\n, whereas in statistical modeling, it could be related to the posterior probability of the model given the data (note that in both of those examples, those quantities would be maximized rather than minimized). Tasks that fall within the paradigm of unsupervised learning are in general estimation problems; the applications include clustering, the estimation of statistical distributions, compression and filtering.\n In applications such as playing video games, an actor takes a string of actions, receiving a generally unpredictable response from the environment after each one. The goal is to win the game, i.e., generate the most positive (lowest cost) responses. In reinforcement learning, the aim is to weight the network (devise a policy) to perform actions that minimize long-term (expected cumulative) cost. At each point in time the agent performs an action and the environment generates an observation and an instantaneous cost, according to some (usually unknown) rules. The rules and the long-term cost usually only can be estimated. At any juncture, the agent decides whether to explore new actions to uncover their costs or to exploit prior learning to proceed more quickly.\n Formally the environment is modeled as a Markov decision process (MDP) with states \n\n\n\n\n\n\ns\n\n1\n\n\n,\n.\n.\n.\n,\n\ns\n\nn\n\n\n\n\u2208\nS\n\n\n\n{\\displaystyle \\textstyle {s_{1},...,s_{n}}\\in S}\n\n and actions \n\n\n\n\n\n\na\n\n1\n\n\n,\n.\n.\n.\n,\n\na\n\nm\n\n\n\n\u2208\nA\n\n\n\n{\\displaystyle \\textstyle {a_{1},...,a_{m}}\\in A}\n\n. Because the state transitions are not known, probability distributions are used instead: the instantaneous cost distribution \n\n\n\n\nP\n(\n\nc\n\nt\n\n\n\n|\n\n\ns\n\nt\n\n\n)\n\n\n\n{\\displaystyle \\textstyle P(c_{t}|s_{t})}\n\n, the observation distribution \n\n\n\n\nP\n(\n\nx\n\nt\n\n\n\n|\n\n\ns\n\nt\n\n\n)\n\n\n\n{\\displaystyle \\textstyle P(x_{t}|s_{t})}\n\n and the transition distribution \n\n\n\n\nP\n(\n\ns\n\nt\n+\n1\n\n\n\n|\n\n\ns\n\nt\n\n\n,\n\na\n\nt\n\n\n)\n\n\n\n{\\displaystyle \\textstyle P(s_{t+1}|s_{t},a_{t})}\n\n, while a policy is defined as the conditional distribution over actions given the observations. Taken together, the two define a Markov chain (MC). The aim is to discover the lowest-cost MC.\n ANNs serve as the learning component in such applications.[132][133] Dynamic programming coupled with ANNs (giving neurodynamic programming)[134] has been applied to problems such as those involved in vehicle routing,[135] video games, natural resource management[136][137] and medicine[138] because of ANNs ability to mitigate losses of accuracy even when reducing the discretization grid density for numerically approximating the solution of control problems. Tasks that fall within the paradigm of reinforcement learning are control problems, games and other sequential decision making tasks.\n Self-learning in neural networks was introduced in 1982 along with a neural network capable of self-learning named crossbar adaptive array (CAA).[139] It is a system with only one input, situation s, and only one output, action (or behavior) a. It has neither external advice input nor external reinforcement input from the environment. The CAA computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about encountered situations. The system is driven by the interaction between cognition and emotion.[140] Given the memory matrix, W =||w(a,s)||, the crossbar self-learning algorithm in each iteration performs the following computation:\n The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, where from it initially and only once receives initial emotions about to be encountered situations in the behavioral environment. Having received the genome vector (species vector) from the genetic environment, the CAA will learn a goal-seeking behavior, in the behavioral environment that contains both desirable and undesirable situations.[141]\n Neuroevolution can create neural network topologies and weights using evolutionary computation. It is competitive with sophisticated gradient descent approaches.[142][143] One advantage of neuroevolution is that it may be less prone to get caught in \"dead ends\".[144]\n Stochastic neural networks originating from Sherrington\u2013Kirkpatrick models are a type of artificial neural network built by introducing random variations into the network, either by giving the network's artificial neurons stochastic transfer functions [citation needed], or by giving them stochastic weights. This makes them useful tools for optimization problems, since the random fluctuations help the network escape from local minima.[145] Stochastic neural networks trained using a Bayesian approach are known as Bayesian neural networks.[146]\n In a Bayesian framework, a distribution over the set of allowed models is chosen to minimize the cost. Evolutionary methods,[147] gene expression programming,[148] simulated annealing,[149] expectation\u2013maximization, non-parametric methods and particle swarm optimization[150] are other learning algorithms. Convergent recursion is a learning algorithm for cerebellar model articulation controller (CMAC) neural networks.[151][152]\n Two modes of learning are available: stochastic and batch. In stochastic learning, each input creates a weight adjustment. In batch learning weights are adjusted based on a batch of inputs, accumulating errors over the batch. Stochastic learning introduces \"noise\" into the process, using the local gradient calculated from one data point; this reduces the chance of the network getting stuck in local minima. However, batch learning typically yields a faster, more stable descent to a local minimum, since each update is performed in the direction of the batch's average error. A common compromise is to use \"mini-batches\", small batches with samples in each batch selected stochastically from the entire data set.\n ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains. The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter is much more complicated but can shorten learning periods and produce better results. Some types allow/require learning to be \"supervised\" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.\n Some of the main breakthroughs include: \n Using artificial neural networks requires an understanding of their characteristics.\n Neural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset, and use the results as feedback to teach the NAS network.[164] Available systems include AutoML and AutoKeras.[165] scikit-learn library provides functions to help with building a deep network from scratch. We can then implement a deep network with TensorFlow or Keras.\n \nHyperparameters must also be defined as part of the design (they are not learned), governing matters such as how many neurons are in each layer, learning rate, step, stride, depth, receptive field and padding (for CNNs), etc.[166]  [citation needed]\n Because of their ability to reproduce and model nonlinear processes, artificial neural networks have found applications in many disciplines. These include:\n ANNs have been used to diagnose several types of cancers[184][185] and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.[186][187]\n ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters[188][189] and to predict foundation settlements.[190] It can also be useful to mitigate flood by the use of ANNs for modelling rainfall-runoff.[191] ANNs have also been used for building black-box models in geoscience: hydrology,[192][193] ocean modelling and coastal engineering,[194][195] and geomorphology.[196] ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware,[197] for identifying domains belonging to threat actors and for detecting URLs posing a security risk.[198] Research is underway on ANN systems designed for penetration testing, for detecting botnets,[199] credit cards frauds[200] and network intrusions.\n ANNs have been proposed as a tool to solve partial differential equations in physics[201][202][203] and simulate the properties of many-body open quantum systems.[204][205][206][207] In brain research ANNs have studied short-term behavior of individual neurons,[208] the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level.\n It is possible to create a profile of a user's interests from pictures, using artificial neural networks trained for object recognition.[209]\n Beyond their traditional applications, artificial neural networks are increasingly being utilized in interdisciplinary research, such as materials science. For instance, graph neural networks (GNNs) have demonstrated their capability in scaling deep learning for the discovery of new stable materials by efficiently predicting the total energy of crystals. This application underscores the adaptability and potential of ANNs in tackling complex problems beyond the realms of predictive modeling and artificial intelligence, opening new pathways for scientific discovery and innovation.[210]\n The multilayer perceptron is a universal function approximator, as proven by the universal approximation theorem. However, the proof is not constructive regarding the number of neurons required, the network topology, the weights and the learning parameters.\n A specific recurrent architecture with rational-valued weights (as opposed to full precision real number-valued weights) has the power of a universal Turing machine,[211] using a finite number of neurons and standard linear connections. Further, the use of irrational values for weights results in a machine with super-Turing power.[212][213][failed verification]\n A model's \"capacity\" property corresponds to its ability to model any given function. It is related to the amount of information that can be stored in the network and to the notion of complexity.\nTwo notions of capacity are known by the community. The information capacity and the VC Dimension. The information capacity of a perceptron is intensively discussed in Sir David MacKay's book[214] which summarizes work by Thomas Cover.[215] The capacity of a network of standard neurons (not convolutional) can be derived by four rules[216] that derive from understanding a neuron as an electrical element. The information capacity captures the functions modelable by the network given any data as input. The second notion, is the VC dimension. VC Dimension uses the principles of measure theory and finds the maximum capacity under the best possible circumstances. This is, given input data in a specific form. As noted in,[214] the VC Dimension for arbitrary inputs is half the information capacity of a Perceptron. The VC Dimension for arbitrary points is sometimes referred to as Memory Capacity.[217]\n Models may not consistently converge on a single solution, firstly because local minima may exist, depending on the cost function and the model. Secondly, the optimization method used might not guarantee to converge when it begins far from any local minimum. Thirdly, for sufficiently large data or parameters, some methods become impractical.\n Another issue worthy to mention is that training may cross some Saddle point which may lead the convergence to the wrong direction.\n The convergence behavior of certain types of ANN architectures are more understood than others. When the width of network approaches to infinity, the ANN is well described by its first order Taylor expansion throughout training, and so inherits the convergence behavior of affine models.[218][219] Another example is when parameters are small, it is observed that ANNs often fits target functions from low to high frequencies. This behavior is referred to as the spectral bias, or frequency principle, of neural networks.[220][221][222][223] This phenomenon is the opposite to the behavior of some well studied iterative numerical schemes such as Jacobi method. Deeper neural networks have been observed to be more biased towards low frequency functions.[224]\n Applications whose goal is to create a system that generalizes well to unseen examples, face the possibility of over-training. This arises in convoluted or over-specified systems when the network capacity significantly exceeds the needed free parameters. Two approaches address over-training. The first is to use cross-validation and similar techniques to check for the presence of over-training and to select hyperparameters to minimize the generalization error.\n The second is to use some form of regularization. This concept emerges in a probabilistic (Bayesian) framework, where regularization can be performed by selecting a larger prior probability over simpler models; but also in statistical learning theory, where the goal is to minimize over two quantities: the 'empirical risk' and the 'structural risk', which roughly corresponds to the error over the training set and the predicted error in unseen data due to overfitting.\n Supervised neural networks that use a mean squared error (MSE) cost function can use formal statistical methods to determine the confidence of the trained model. The MSE on a validation set can be used as an estimate for variance. This value can then be used to calculate the confidence interval of network output, assuming a normal distribution. A confidence analysis made this way is statistically valid as long as the output probability distribution stays the same and the network is not modified.\n By assigning a softmax activation function, a generalization of the logistic function, on the output layer of the neural network (or a softmax component in a component-based network) for categorical target variables, the outputs can be interpreted as posterior probabilities. This is useful in classification as it gives a certainty measure on classifications.\n The softmax activation function is:\n \n A common criticism of neural networks, particularly in robotics, is that they require too many training samples for real-world operation.[225]\nAny learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Potential solutions include randomly shuffling training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, grouping examples in so-called mini-batches and/or introducing a recursive least squares algorithm for CMAC.[151]\nDean Pomerleau uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.), and a large amount of his research is devoted to extrapolating multiple training scenarios from a single training experience, and preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns\u2014it should not learn to always turn right).[226]\n A central claim[citation needed] of ANNs is that they embody new and powerful general principles for processing information. These principles are ill-defined. It is often claimed[by whom?] that they are emergent from the network itself. This allows simple statistical association (the basic function of artificial neural networks) to be described as learning or recognition. In 1997, Alexander Dewdney, a former Scientific American columnist, commented that as a result, artificial neural networks have a \"something-for-nothing quality, one that imparts a peculiar aura of laziness and a distinct lack of curiosity about just how good these computing systems are. No human hand (or mind) intervenes; solutions are found as if by magic; and no one, it seems, has learned anything\".[227] One response to Dewdney is that neural networks have been successfully used to handle many complex and diverse tasks, ranging from autonomously flying aircraft[228] to detecting credit card fraud to mastering the game of Go.\n Technology writer Roger Bridgman commented:\n Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be \"an opaque, unreadable table...valueless as a scientific resource\".\n In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.[229]\n Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.[230]\n Biological brains use both shallow and deep circuits as reported by brain anatomy,[231] displaying a wide variety of invariance. Weng[232] argued that the brain self-wires largely according to signal statistics and therefore, a serial cascade cannot catch all major statistical dependencies.\n Large and effective neural networks require considerable computing resources.[233] While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a simplified neuron on von Neumann architecture may consume vast amounts of memory and storage. Furthermore, the designer often needs to transmit signals through many of these connections and their associated neurons\u00a0\u2013  which require enormous CPU power and time.[citation needed]\n Some argue that the resurgence of neural networks in the twenty-first century is largely attributable to advances in hardware: from 1991 to 2015, computing power, especially as delivered by GPGPUs (on GPUs), has increased around a million-fold, making the standard backpropagation algorithm feasible for training networks that are several layers deeper than before.[38] The use of accelerators such as FPGAs and GPUs can reduce training times from months to days.[233][234]\n Neuromorphic engineering or a physical neural network addresses the hardware difficulty directly, by constructing non-von-Neumann chips to directly implement neural networks in circuitry. Another type of chip optimized for neural network processing is called a Tensor Processing Unit, or TPU.[235]\n Analyzing what has been learned by an ANN is much easier than analyzing what has been learned by a biological neural network. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering general principles that allow a learning machine to be successful. For example, local vs. non-local learning and shallow vs. deep architecture.[236]\n Advocates of hybrid models (combining neural networks and symbolic approaches) say that such a mixture can better capture the mechanisms of the human mind.[237][238]\n Neural networks are dependent on the quality of the data they are trained on, thus low quality data with imbalanced representativeness can lead to the model learning and perpetuating societal biases.[239][240] These inherited biases become especially critical when the ANNs are integrated into real-world scenarios where the training data may be imbalanced due to the scarcity of data for a specific race, gender or other attribute.[239] This imbalance can result in the model having inadequate representation and understanding of underrepresented groups, leading to discriminatory outcomes that exacerbate societal inequalities, especially in applications like facial recognition, hiring processes, and law enforcement.[240][241] For example, in 2018, Amazon had to scrap a recruiting tool because the model favored men over women for jobs in software engineering due to the higher number of male workers in the field.[241] The program would penalize any resume with the word \"woman\" or the name of any women's college. However, the use of synthetic data can help reduce dataset bias and increase representation in datasets.[242]\n Artificial neural networks (ANNs) have undergone significant advancements, particularly in their ability to model complex systems, handle large data sets, and adapt to various types of applications. Their evolution over the past few decades has been marked by a broad range of applications in fields such as image processing, speech recognition, natural language processing, finance, and medicine.[citation needed]\n In the realm of image processing, ANNs are employed in tasks such as image classification, object recognition, and image segmentation. For instance, deep convolutional neural networks (CNNs) have been important in handwritten digit recognition, achieving state-of-the-art performance.[243] This demonstrates the ability of ANNs to effectively process and interpret complex visual information, leading to advancements in fields ranging from automated surveillance to medical imaging.[243]\n By modeling speech signals, ANNs are used for tasks like speaker identification and speech-to-text conversion. Deep neural network architectures have introduced significant improvements in large vocabulary continuous speech recognition, outperforming traditional techniques.[243][244] These advancements have enabled the development of more accurate and efficient voice-activated systems, enhancing user interfaces in technology products.[citation needed]\n In natural language processing, ANNs are used for tasks such as text classification, sentiment analysis, and machine translation. They have enabled the development of models that can accurately translate between languages, understand the context and sentiment in textual data, and categorize text based on content.[243][244] This has implications for automated customer service, content moderation, and language understanding technologies.[citation needed]\n In the domain of control systems, ANNs are used to model dynamic systems for tasks such as system identification, control design, and optimization. For instance, deep feedforward neural networks are important in system identification and control applications.[citation needed]\n ANNs are used for stock market prediction and credit scoring: \n ANNs require high-quality data and careful tuning, and their \"black-box\" nature can pose challenges in interpretation. Nevertheless, ongoing advancements suggest that ANNs continue to play a role in finance, offering valuable insights and enhancing risk management strategies.[citation needed]\n ANNs are able to process and analyze vast medical datasets. They enhance diagnostic accuracy, especially by interpreting complex medical imaging for early disease detection, and by predicting patient outcomes for personalized treatment planning.[244] In drug discovery, ANNs speed up the identification of potential drug candidates and predict their efficacy and safety, significantly reducing development time and costs.[243] Additionally, their application in personalized medicine and healthcare data analysis allows tailored therapies and efficient patient care management.[244] Ongoing research is aimed at addressing remaining challenges such as data privacy and model interpretability, as well as expanding the scope of ANN applications in medicine.[citation needed]\n ANNs such as generative adversarial networks (GAN) and transformers are used for content creation across numerous industries.[245] This is because deep learning models are able to learn the style of an artist or musician from huge datasets and generate completely new artworks and music compositions. For instance, DALL-E is a deep neural network trained on 650 million pairs of images and texts across the internet that can create artworks based on text entered by the user.[246] In the field of music, transformers are used to create original music for commercials and documentaries through companies such as AIVA and Jukedeck.[247] In the marketing industry generative models are used to create personalized advertisements for consumers.[245] Additionally, major film companies are partnering with technology companies to analyze the financial success of a film, such as the partnership between Warner Bros and technology company Cinelytic established in 2020.[248] Furthermore, neural networks have found uses in video game creation, where Non Player Characters (NPCs) can make decisions based on all the characters currently in the game.[249]\n",
        "doc_number": 79
    },
    {
        "url": "https://en.wikipedia.org/wiki/Backpropagation",
        "content": "In machine learning, backpropagation is a gradient estimation method commonly used for training a neural network to compute its parameter updates.\n It is an efficient application of the chain rule to neural networks. Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single input\u2013output example, and does so efficiently, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this can be derived through dynamic programming.[1][2][3]\n Strictly speaking, the term backpropagation refers only to an algorithm for efficiently computing the gradient, not how the gradient is used; but the term is often used loosely to refer to the entire learning algorithm \u2013 including how the gradient is used, such as by stochastic gradient descent, or as an intermediate step in a more complicated optimizer, such as Adam.[4]\n Backpropagation had multiple discoveries and partial discoveries, with a tangled history and terminology. See the history section for details. Some other names for the technique include \"reverse mode of automatic differentiation\" or \"reverse accumulation\".[5]\n Backpropagation computes the gradient in weight space of a feedforward neural network, with respect to a loss function. Denote:\n In the derivation of backpropagation, other intermediate quantities are used by introducing them as needed below. Bias terms are not treated specially since they correspond to a weight with a fixed input of 1. For backpropagation the specific loss function and activation functions do not matter as long as they and their derivatives can be evaluated efficiently. Traditional activation functions include sigmoid, tanh, and ReLU. Swish,[6] mish,[7] and other activation functions have since been proposed as well.\n The overall network is a combination of function composition and matrix multiplication:\n For a training set there will be a set of input\u2013output pairs, \n\n\n\n\n{\n\n(\n\nx\n\ni\n\n\n,\n\ny\n\ni\n\n\n)\n\n}\n\n\n\n{\\displaystyle \\left\\{(x_{i},y_{i})\\right\\}}\n\n. For each input\u2013output pair \n\n\n\n(\n\nx\n\ni\n\n\n,\n\ny\n\ni\n\n\n)\n\n\n{\\displaystyle (x_{i},y_{i})}\n\n in the training set, the loss of the model on that pair is the cost of the difference between the predicted output \n\n\n\ng\n(\n\nx\n\ni\n\n\n)\n\n\n{\\displaystyle g(x_{i})}\n\n and the target output \n\n\n\n\ny\n\ni\n\n\n\n\n{\\displaystyle y_{i}}\n\n:\n Note the distinction: during model evaluation the weights are fixed while the inputs vary (and the target output may be unknown), and the network ends with the output layer (it does not include the loss function). During model training the input\u2013output pair is fixed while the weights vary, and the network ends with the loss function.\n Backpropagation computes the gradient for a fixed input\u2013output pair \n\n\n\n(\n\nx\n\ni\n\n\n,\n\ny\n\ni\n\n\n)\n\n\n{\\displaystyle (x_{i},y_{i})}\n\n, where the weights \n\n\n\n\nw\n\nj\nk\n\n\nl\n\n\n\n\n{\\displaystyle w_{jk}^{l}}\n\n can vary. Each individual component of the gradient, \n\n\n\n\u2202\nC\n\n/\n\n\u2202\n\nw\n\nj\nk\n\n\nl\n\n\n,\n\n\n{\\displaystyle \\partial C/\\partial w_{jk}^{l},}\n\n can be computed by the chain rule; but doing this separately for each weight is inefficient. Backpropagation efficiently computes the gradient by avoiding duplicate calculations and not computing unnecessary intermediate values, by computing the gradient of each layer \u2013 specifically the gradient of the weighted input of each layer, denoted by \n\n\n\n\n\u03b4\n\nl\n\n\n\n\n{\\displaystyle \\delta ^{l}}\n\n \u2013 from back to front.\n Informally, the key point is that since the only way a weight in \n\n\n\n\nW\n\nl\n\n\n\n\n{\\displaystyle W^{l}}\n\n affects the loss is through its effect on the next layer, and it does so linearly, \n\n\n\n\n\u03b4\n\nl\n\n\n\n\n{\\displaystyle \\delta ^{l}}\n\n are the only data you need to compute the gradients of the weights at layer \n\n\n\nl\n\n\n{\\displaystyle l}\n\n, and then the gradients of weights of previous layer can be computed by \n\n\n\n\n\u03b4\n\nl\n\u2212\n1\n\n\n\n\n{\\displaystyle \\delta ^{l-1}}\n\n and repeated recursively. This avoids inefficiency in two ways. First, it avoids duplication because when computing the gradient at layer \n\n\n\nl\n\n\n{\\displaystyle l}\n\n, it is unnecessary to recompute all derivatives on later layers \n\n\n\nl\n+\n1\n,\nl\n+\n2\n,\n\u2026\n\n\n{\\displaystyle l+1,l+2,\\ldots }\n\n each time. Second, it avoids unnecessary intermediate calculations, because at each stage it directly computes the gradient of the weights with respect to the ultimate output (the loss), rather than unnecessarily computing the derivatives of the values of hidden layers with respect to changes in weights \n\n\n\n\u2202\n\na\n\n\nj\n\u2032\n\n\n\n\nl\n\u2032\n\n\n\n\n/\n\n\u2202\n\nw\n\nj\nk\n\n\nl\n\n\n\n\n{\\displaystyle \\partial a_{j'}^{l'}/\\partial w_{jk}^{l}}\n\n.\n Backpropagation can be expressed for simple feedforward networks in terms of matrix multiplication, or more generally in terms of the adjoint graph.\n For the basic case of a feedforward network, where nodes in each layer are connected only to nodes in the immediate next layer (without skipping any layers), and there is a loss function that computes a scalar loss for the final output, backpropagation can be understood simply by matrix multiplication.[c] Essentially, backpropagation evaluates the expression for the derivative of the cost function as a product of derivatives between each layer from right to left \u2013 \"backwards\" \u2013 with the gradient of the weights between each layer being a simple modification of the partial products (the \"backwards propagated error\").\n Given an input\u2013output pair \n\n\n\n(\nx\n,\ny\n)\n\n\n{\\displaystyle (x,y)}\n\n, the loss is:\n To compute this, one starts with the input \n\n\n\nx\n\n\n{\\displaystyle x}\n\n and works forward; denote the weighted input of each hidden layer as \n\n\n\n\nz\n\nl\n\n\n\n\n{\\displaystyle z^{l}}\n\n and the output of hidden layer \n\n\n\nl\n\n\n{\\displaystyle l}\n\n as the activation \n\n\n\n\na\n\nl\n\n\n\n\n{\\displaystyle a^{l}}\n\n. For backpropagation, the activation \n\n\n\n\na\n\nl\n\n\n\n\n{\\displaystyle a^{l}}\n\n as well as the derivatives \n\n\n\n(\n\nf\n\nl\n\n\n\n)\n\u2032\n\n\n\n{\\displaystyle (f^{l})'}\n\n (evaluated at \n\n\n\n\nz\n\nl\n\n\n\n\n{\\displaystyle z^{l}}\n\n) must be cached for use during the backwards pass.\n The derivative of the loss in terms of the inputs is given by the chain rule; note that each term is a total derivative, evaluated at the value of the network (at each node) on the input \n\n\n\nx\n\n\n{\\displaystyle x}\n\n:\n where \n\n\n\n\n\n\nd\n\na\n\nL\n\n\n\n\nd\n\nz\n\nL\n\n\n\n\n\n\n\n{\\displaystyle {\\frac {da^{L}}{dz^{L}}}}\n\n is a diagonal matrix.\n These terms are: the derivative of the loss function;[d] the derivatives of the activation functions;[e] and the matrices of weights:[f]\n The gradient \n\n\n\n\u2207\n\n\n{\\displaystyle \\nabla }\n\n is the transpose of the derivative of the output in terms of the input, so the matrices are transposed and the order of multiplication is reversed, but the entries are the same:\n Backpropagation then consists essentially of evaluating this expression from right to left (equivalently, multiplying the previous expression for the derivative from left to right), computing the gradient at each layer on the way; there is an added step, because the gradient of the weights is not just a subexpression: there's an extra multiplication.\n Introducing the auxiliary quantity \n\n\n\n\n\u03b4\n\nl\n\n\n\n\n{\\displaystyle \\delta ^{l}}\n\n for the partial products (multiplying from right to left), interpreted as the \"error at level \n\n\n\nl\n\n\n{\\displaystyle l}\n\n\" and defined as the gradient of the input values at level \n\n\n\nl\n\n\n{\\displaystyle l}\n\n:\n Note that \n\n\n\n\n\u03b4\n\nl\n\n\n\n\n{\\displaystyle \\delta ^{l}}\n\n is a vector, of length equal to the number of nodes in level \n\n\n\nl\n\n\n{\\displaystyle l}\n\n; each component is interpreted as the \"cost attributable to (the value of) that node\".\n The gradient of the weights in layer \n\n\n\nl\n\n\n{\\displaystyle l}\n\n is then:\n The factor of \n\n\n\n\na\n\nl\n\u2212\n1\n\n\n\n\n{\\displaystyle a^{l-1}}\n\n is because the weights \n\n\n\n\nW\n\nl\n\n\n\n\n{\\displaystyle W^{l}}\n\n between level \n\n\n\nl\n\u2212\n1\n\n\n{\\displaystyle l-1}\n\n and \n\n\n\nl\n\n\n{\\displaystyle l}\n\n affect level \n\n\n\nl\n\n\n{\\displaystyle l}\n\n proportionally to the inputs (activations): the inputs are fixed, the weights vary.\n The \n\n\n\n\n\u03b4\n\nl\n\n\n\n\n{\\displaystyle \\delta ^{l}}\n\n can easily be computed recursively, going from right to left, as:\n The gradients of the weights can thus be computed using a few matrix multiplications for each level; this is backpropagation.\n Compared with naively computing forwards (using the \n\n\n\n\n\u03b4\n\nl\n\n\n\n\n{\\displaystyle \\delta ^{l}}\n\n for illustration):\n There are two key differences with backpropagation:\n For more general graphs, and other advanced variations, backpropagation can be understood in terms of automatic differentiation, where backpropagation is a special case of reverse accumulation (or \"reverse mode\").[5]\n The goal of any supervised learning algorithm is to find a function that best maps a set of inputs to their correct output. The motivation for backpropagation is to train a multi-layered neural network such that it can learn the appropriate internal representations to allow it to learn any arbitrary mapping of input to output.[8]\n \nTo understand the mathematical derivation of the backpropagation algorithm, it helps to first develop some intuition about the relationship between the actual output of a neuron and the correct output for a particular training example. Consider a simple neural network with two input units, one output unit and no hidden units, and in which each neuron uses a linear output (unlike most work on neural networks, in which mapping from inputs to outputs is non-linear)[g] that is the weighted sum of its input.  Initially, before training, the weights will be set randomly. Then the neuron learns from training examples, which in this case consist of a set of tuples \n\n\n\n(\n\nx\n\n1\n\n\n,\n\nx\n\n2\n\n\n,\nt\n)\n\n\n{\\displaystyle (x_{1},x_{2},t)}\n\n where \n\n\n\n\nx\n\n1\n\n\n\n\n{\\displaystyle x_{1}}\n\n and \n\n\n\n\nx\n\n2\n\n\n\n\n{\\displaystyle x_{2}}\n\n are the inputs to the network and t is the correct output (the output the network should produce given those inputs, when it has been trained). The initial network, given \n\n\n\n\nx\n\n1\n\n\n\n\n{\\displaystyle x_{1}}\n\n and \n\n\n\n\nx\n\n2\n\n\n\n\n{\\displaystyle x_{2}}\n\n, will compute an output y that likely differs from t (given random weights). A loss function \n\n\n\nL\n(\nt\n,\ny\n)\n\n\n{\\displaystyle L(t,y)}\n\n is used for measuring the discrepancy between the target output t and the computed output y. For regression analysis problems the squared error can be used as a loss function, for classification the categorical cross-entropy can be used.\n As an example consider a regression problem using the square error as a loss:\n where E is the discrepancy or error.\n \nConsider the network on a single training case: \n\n\n\n(\n1\n,\n1\n,\n0\n)\n\n\n{\\displaystyle (1,1,0)}\n\n. Thus, the input \n\n\n\n\nx\n\n1\n\n\n\n\n{\\displaystyle x_{1}}\n\n and \n\n\n\n\nx\n\n2\n\n\n\n\n{\\displaystyle x_{2}}\n\n are 1 and 1 respectively and the correct output, t is 0. Now if the relation is plotted between the network's output y on the horizontal axis and the error E on the vertical axis, the result is a parabola. The minimum of the parabola corresponds to the output y which minimizes the error E. For a single training case, the minimum also touches the horizontal axis, which means the error will be zero and the network can produce an output y that exactly matches the target output t. Therefore, the problem of mapping inputs to outputs can be reduced to an optimization problem of finding a function that will produce the minimal error.  However, the output of a neuron depends on the weighted sum of all its inputs:\n where \n\n\n\n\nw\n\n1\n\n\n\n\n{\\displaystyle w_{1}}\n\n and \n\n\n\n\nw\n\n2\n\n\n\n\n{\\displaystyle w_{2}}\n\n are the weights on the connection from the input units to the output unit. Therefore, the error also depends on the incoming weights to the neuron, which is ultimately what needs to be changed in the network to enable learning.\n In this example, upon injecting the training data \n\n\n\n(\n1\n,\n1\n,\n0\n)\n\n\n{\\displaystyle (1,1,0)}\n\n, the loss function becomes\n \n\n\n\nE\n=\n(\nt\n\u2212\ny\n\n)\n\n2\n\n\n=\n\ny\n\n2\n\n\n=\n(\n\nx\n\n1\n\n\n\nw\n\n1\n\n\n+\n\nx\n\n2\n\n\n\nw\n\n2\n\n\n\n)\n\n2\n\n\n=\n(\n\nw\n\n1\n\n\n+\n\nw\n\n2\n\n\n\n)\n\n2\n\n\n.\n\n\n{\\displaystyle E=(t-y)^{2}=y^{2}=(x_{1}w_{1}+x_{2}w_{2})^{2}=(w_{1}+w_{2})^{2}.}\n\n\n Then, the loss function \n\n\n\nE\n\n\n{\\displaystyle E}\n\n takes the form of a parabolic cylinder with its base directed along \n\n\n\n\nw\n\n1\n\n\n=\n\u2212\n\nw\n\n2\n\n\n\n\n{\\displaystyle w_{1}=-w_{2}}\n\n. Since all sets of weights that satisfy \n\n\n\n\nw\n\n1\n\n\n=\n\u2212\n\nw\n\n2\n\n\n\n\n{\\displaystyle w_{1}=-w_{2}}\n\n minimize the loss function, in this case additional constraints are required to converge to a unique solution. Additional constraints could either be generated by setting specific conditions to the weights, or by injecting additional training data.\n One commonly used algorithm to find the set of weights that minimizes the error is gradient descent. By backpropagation, the steepest descent direction is calculated of the loss function versus the present synaptic weights. Then, the weights can be modified along the steepest descent direction, and the error is minimized in an efficient way.\n The gradient descent method involves calculating the derivative of the loss function with respect to the weights of the network. This is normally done using backpropagation. Assuming one output neuron,[h] the squared error function is\n where\n For each neuron \n\n\n\nj\n\n\n{\\displaystyle j}\n\n, its output \n\n\n\n\no\n\nj\n\n\n\n\n{\\displaystyle o_{j}}\n\n is defined as\n where the activation function \n\n\n\n\u03c6\n\n\n{\\displaystyle \\varphi }\n\n is non-linear and differentiable over the activation region (the ReLU is not differentiable at one point). A historically used activation function is the logistic function:\n which has a convenient derivative of:\n The input \n\n\n\n\n\nnet\n\n\nj\n\n\n\n\n{\\displaystyle {\\text{net}}_{j}}\n\n to a neuron is the weighted sum of outputs \n\n\n\n\no\n\nk\n\n\n\n\n{\\displaystyle o_{k}}\n\n of previous neurons. If the neuron is in the first layer after the input layer, the \n\n\n\n\no\n\nk\n\n\n\n\n{\\displaystyle o_{k}}\n\n of the input layer are simply the inputs \n\n\n\n\nx\n\nk\n\n\n\n\n{\\displaystyle x_{k}}\n\n to the network. The number of input units to the neuron is \n\n\n\nn\n\n\n{\\displaystyle n}\n\n. The variable \n\n\n\n\nw\n\nk\nj\n\n\n\n\n{\\displaystyle w_{kj}}\n\n denotes the weight between neuron \n\n\n\nk\n\n\n{\\displaystyle k}\n\n of the previous layer and neuron \n\n\n\nj\n\n\n{\\displaystyle j}\n\n of the current layer.\n Calculating the partial derivative of the error with respect to a weight \n\n\n\n\nw\n\ni\nj\n\n\n\n\n{\\displaystyle w_{ij}}\n\n is done using the chain rule twice:\n In the last factor of the right-hand side of the above, only one term in the sum \n\n\n\n\n\nnet\n\n\nj\n\n\n\n\n{\\displaystyle {\\text{net}}_{j}}\n\n depends on \n\n\n\n\nw\n\ni\nj\n\n\n\n\n{\\displaystyle w_{ij}}\n\n, so that\n If the neuron is in the first layer after the input layer, \n\n\n\n\no\n\ni\n\n\n\n\n{\\displaystyle o_{i}}\n\n is just \n\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystyle x_{i}}\n\n.\n The derivative of the output of neuron \n\n\n\nj\n\n\n{\\displaystyle j}\n\n with respect to its input is simply the partial derivative of the activation function:\n which for the logistic activation function\n This is the reason why backpropagation requires that the activation function be differentiable. (Nevertheless, the ReLU activation function, which is non-differentiable at 0, has become quite popular, e.g. in AlexNet)\n The first factor is straightforward to evaluate if the neuron is in the output layer, because then \n\n\n\n\no\n\nj\n\n\n=\ny\n\n\n{\\displaystyle o_{j}=y}\n\n and\n If half of the square error is used as loss function we can rewrite it as\n However, if \n\n\n\nj\n\n\n{\\displaystyle j}\n\n is in an arbitrary inner layer of the network, finding the derivative \n\n\n\nE\n\n\n{\\displaystyle E}\n\n with respect to \n\n\n\n\no\n\nj\n\n\n\n\n{\\displaystyle o_{j}}\n\n is less obvious.\n Considering \n\n\n\nE\n\n\n{\\displaystyle E}\n\n as a function with the inputs being all neurons \n\n\n\nL\n=\n{\nu\n,\nv\n,\n\u2026\n,\nw\n}\n\n\n{\\displaystyle L=\\{u,v,\\dots ,w\\}}\n\n receiving input from neuron \n\n\n\nj\n\n\n{\\displaystyle j}\n\n,\n and taking the total derivative with respect to \n\n\n\n\no\n\nj\n\n\n\n\n{\\displaystyle o_{j}}\n\n, a recursive expression for the derivative is obtained:\n Therefore, the derivative with respect to \n\n\n\n\no\n\nj\n\n\n\n\n{\\displaystyle o_{j}}\n\n can be calculated if all the derivatives with respect to the outputs \n\n\n\n\no\n\n\u2113\n\n\n\n\n{\\displaystyle o_{\\ell }}\n\n of the next layer \u2013 the ones closer to the output neuron \u2013 are known. [Note, if any of the neurons in set \n\n\n\nL\n\n\n{\\displaystyle L}\n\n were not connected to neuron \n\n\n\nj\n\n\n{\\displaystyle j}\n\n, they would be independent of \n\n\n\n\nw\n\ni\nj\n\n\n\n\n{\\displaystyle w_{ij}}\n\n and the corresponding partial derivative under the summation would vanish to 0.]\n Substituting Eq. 2, Eq. 3 Eq.4 and Eq. 5 in Eq. 1 we obtain:\n with\n if \n\n\n\n\u03c6\n\n\n{\\displaystyle \\varphi }\n\n is the logistic function, and the error is the square error:\n To update the weight \n\n\n\n\nw\n\ni\nj\n\n\n\n\n{\\displaystyle w_{ij}}\n\n using gradient descent, one must choose a learning rate, \n\n\n\n\u03b7\n>\n0\n\n\n{\\displaystyle \\eta >0}\n\n. The change in weight needs to reflect the impact on \n\n\n\nE\n\n\n{\\displaystyle E}\n\n of an increase or decrease in \n\n\n\n\nw\n\ni\nj\n\n\n\n\n{\\displaystyle w_{ij}}\n\n. If \n\n\n\n\n\n\n\u2202\nE\n\n\n\u2202\n\nw\n\ni\nj\n\n\n\n\n\n>\n0\n\n\n{\\displaystyle {\\frac {\\partial E}{\\partial w_{ij}}}>0}\n\n, an increase in \n\n\n\n\nw\n\ni\nj\n\n\n\n\n{\\displaystyle w_{ij}}\n\n increases \n\n\n\nE\n\n\n{\\displaystyle E}\n\n; conversely, if \n\n\n\n\n\n\n\u2202\nE\n\n\n\u2202\n\nw\n\ni\nj\n\n\n\n\n\n<\n0\n\n\n{\\displaystyle {\\frac {\\partial E}{\\partial w_{ij}}}<0}\n\n, an increase in \n\n\n\n\nw\n\ni\nj\n\n\n\n\n{\\displaystyle w_{ij}}\n\n decreases \n\n\n\nE\n\n\n{\\displaystyle E}\n\n. The new \n\n\n\n\u0394\n\nw\n\ni\nj\n\n\n\n\n{\\displaystyle \\Delta w_{ij}}\n\n is added to the old weight, and the product of the learning rate and the gradient, multiplied by \n\n\n\n\u2212\n1\n\n\n{\\displaystyle -1}\n\n guarantees that \n\n\n\n\nw\n\ni\nj\n\n\n\n\n{\\displaystyle w_{ij}}\n\n changes in a way that always decreases \n\n\n\nE\n\n\n{\\displaystyle E}\n\n. In other words, in the equation immediately below, \n\n\n\n\u2212\n\u03b7\n\n\n\n\u2202\nE\n\n\n\u2202\n\nw\n\ni\nj\n\n\n\n\n\n\n\n{\\displaystyle -\\eta {\\frac {\\partial E}{\\partial w_{ij}}}}\n\n always changes \n\n\n\n\nw\n\ni\nj\n\n\n\n\n{\\displaystyle w_{ij}}\n\n in such a way that \n\n\n\nE\n\n\n{\\displaystyle E}\n\n is decreased:\n \n Using a Hessian matrix of second-order derivatives of the error function, the Levenberg\u2013Marquardt algorithm often converges faster than first-order gradient descent, especially when the topology of the error function is complicated.[9][10] It may also find solutions in smaller node counts for which other methods might not converge.[10] The Hessian can be approximated by the Fisher information matrix.[11]\n As an example, consider a simple feedforward network. At the \n\n\n\nl\n\n\n{\\displaystyle l}\n\n-th layer, we have\n\n\n\n\nx\n\ni\n\n\n(\nl\n)\n\n\n,\n\n\na\n\ni\n\n\n(\nl\n)\n\n\n=\nf\n(\n\nx\n\ni\n\n\n(\nl\n)\n\n\n)\n,\n\n\nx\n\ni\n\n\n(\nl\n+\n1\n)\n\n\n=\n\n\u2211\n\nj\n\n\n\nW\n\ni\nj\n\n\n\na\n\nj\n\n\n(\nl\n)\n\n\n\n\n{\\displaystyle x_{i}^{(l)},\\quad a_{i}^{(l)}=f(x_{i}^{(l)}),\\quad x_{i}^{(l+1)}=\\sum _{j}W_{ij}a_{j}^{(l)}}\n\nwhere \n\n\n\nx\n\n\n{\\displaystyle x}\n\n are the pre-activations, \n\n\n\na\n\n\n{\\displaystyle a}\n\n are the activations, and \n\n\n\nW\n\n\n{\\displaystyle W}\n\n is the weight matrix. Given a loss function \n\n\n\nL\n\n\n{\\displaystyle L}\n\n, the first-order backpropagation states that\n\n\n\n\n\n\n\u2202\nL\n\n\n\u2202\n\na\n\nj\n\n\n(\nl\n)\n\n\n\n\n\n=\n\n\u2211\n\nj\n\n\n\nW\n\ni\nj\n\n\n\n\n\n\u2202\nL\n\n\n\u2202\n\nx\n\ni\n\n\n(\nl\n+\n1\n)\n\n\n\n\n\n,\n\n\n\n\n\u2202\nL\n\n\n\u2202\n\nx\n\nj\n\n\n(\nl\n)\n\n\n\n\n\n=\n\nf\n\u2032\n\n(\n\nx\n\nj\n\n\n(\nl\n)\n\n\n)\n\n\n\n\u2202\nL\n\n\n\u2202\n\na\n\nj\n\n\n(\nl\n)\n\n\n\n\n\n\n\n{\\displaystyle {\\frac {\\partial L}{\\partial a_{j}^{(l)}}}=\\sum _{j}W_{ij}{\\frac {\\partial L}{\\partial x_{i}^{(l+1)}}},\\quad {\\frac {\\partial L}{\\partial x_{j}^{(l)}}}=f'(x_{j}^{(l)}){\\frac {\\partial L}{\\partial a_{j}^{(l)}}}}\n\nand the second-order backpropagation states that\n\n\n\n\n\n\n\n\u2202\n\n2\n\n\nL\n\n\n\u2202\n\na\n\n\nj\n\n1\n\n\n\n\n(\nl\n)\n\n\n\u2202\n\na\n\n\nj\n\n2\n\n\n\n\n(\nl\n)\n\n\n\n\n\n=\n\n\u2211\n\n\nj\n\n1\n\n\n\nj\n\n2\n\n\n\n\n\nW\n\n\ni\n\n1\n\n\n\nj\n\n1\n\n\n\n\n\nW\n\n\ni\n\n2\n\n\n\nj\n\n2\n\n\n\n\n\n\n\n\n\u2202\n\n2\n\n\nL\n\n\n\u2202\n\nx\n\n\ni\n\n1\n\n\n\n\n(\nl\n+\n1\n)\n\n\n\u2202\n\nx\n\n\ni\n\n2\n\n\n\n\n(\nl\n+\n1\n)\n\n\n\n\n\n,\n\n\n\n\n\n\u2202\n\n2\n\n\nL\n\n\n\u2202\n\nx\n\n\nj\n\n1\n\n\n\n\n(\nl\n)\n\n\n\u2202\n\nx\n\n\nj\n\n2\n\n\n\n\n(\nl\n)\n\n\n\n\n\n=\n\nf\n\u2032\n\n(\n\nx\n\n\nj\n\n1\n\n\n\n\n(\nl\n)\n\n\n)\n\nf\n\u2032\n\n(\n\nx\n\n\nj\n\n2\n\n\n\n\n(\nl\n)\n\n\n)\n\n\n\n\n\u2202\n\n2\n\n\nL\n\n\n\u2202\n\na\n\n\nj\n\n1\n\n\n\n\n(\nl\n)\n\n\n\u2202\n\na\n\n\nj\n\n2\n\n\n\n\n(\nl\n)\n\n\n\n\n\n+\n\n\u03b4\n\n\nj\n\n1\n\n\n\nj\n\n2\n\n\n\n\n\nf\n\u2033\n\n(\n\nx\n\n\nj\n\n1\n\n\n\n\n(\nl\n)\n\n\n)\n\n\n\n\u2202\nL\n\n\n\u2202\n\na\n\n\nj\n\n1\n\n\n\n\n(\nl\n)\n\n\n\n\n\n\n\n{\\displaystyle {\\frac {\\partial ^{2}L}{\\partial a_{j_{1}}^{(l)}\\partial a_{j_{2}}^{(l)}}}=\\sum _{j_{1}j_{2}}W_{i_{1}j_{1}}W_{i_{2}j_{2}}{\\frac {\\partial ^{2}L}{\\partial x_{i_{1}}^{(l+1)}\\partial x_{i_{2}}^{(l+1)}}},\\quad {\\frac {\\partial ^{2}L}{\\partial x_{j_{1}}^{(l)}\\partial x_{j_{2}}^{(l)}}}=f'(x_{j_{1}}^{(l)})f'(x_{j_{2}}^{(l)}){\\frac {\\partial ^{2}L}{\\partial a_{j_{1}}^{(l)}\\partial a_{j_{2}}^{(l)}}}+\\delta _{j_{1}j_{2}}f''(x_{j_{1}}^{(l)}){\\frac {\\partial L}{\\partial a_{j_{1}}^{(l)}}}}\n\nwhere \n\n\n\n\u03b4\n\n\n{\\displaystyle \\delta }\n\n is the Dirac delta symbol.\n Arbitrary-order derivatives in arbitrary computational graphs can be computed with backpropagation, but with more complex expressions for higher orders.\n The loss function is a function that maps values of one or more variables onto a real number intuitively representing some \"cost\" associated with those values. For backpropagation, the loss function calculates the difference between the network output and its expected output, after a training example has propagated through the network.\n The mathematical expression of the loss function must fulfill two conditions in order for it to be possibly used in backpropagation.[12] The first is that it can be written as an average \n\n\n\nE\n=\n\n\n1\nn\n\n\n\n\u2211\n\nx\n\n\n\nE\n\nx\n\n\n\n\n{\\textstyle E={\\frac {1}{n}}\\sum _{x}E_{x}}\n\n over error functions \n\n\n\n\nE\n\nx\n\n\n\n\n{\\textstyle E_{x}}\n\n, for \n\n\n\nn\n\n\n{\\textstyle n}\n\n individual training examples, \n\n\n\nx\n\n\n{\\textstyle x}\n\n. The reason for this assumption is that the backpropagation algorithm calculates the gradient of the error function for a single training example, which needs to be generalized to the overall error function. The second assumption is that it can be written as a function of the outputs from the neural network.\n Let \n\n\n\ny\n,\n\ny\n\u2032\n\n\n\n{\\displaystyle y,y'}\n\n be vectors in \n\n\n\n\n\nR\n\n\nn\n\n\n\n\n{\\displaystyle \\mathbb {R} ^{n}}\n\n.\n Select an error function \n\n\n\nE\n(\ny\n,\n\ny\n\u2032\n\n)\n\n\n{\\displaystyle E(y,y')}\n\n measuring the difference between two outputs. The standard choice is the square of the Euclidean distance between the vectors \n\n\n\ny\n\n\n{\\displaystyle y}\n\n and \n\n\n\n\ny\n\u2032\n\n\n\n{\\displaystyle y'}\n\n:\n\n\n\nE\n(\ny\n,\n\ny\n\u2032\n\n)\n=\n\n\n\n1\n2\n\n\n\n\u2016\ny\n\u2212\n\ny\n\u2032\n\n\n\u2016\n\n2\n\n\n\n\n{\\displaystyle E(y,y')={\\tfrac {1}{2}}\\lVert y-y'\\rVert ^{2}}\n\nThe error function over \n\n\n\nn\n\n\n{\\textstyle n}\n\n training examples can then be written as an average of losses over individual examples:\n\n\n\nE\n=\n\n\n1\n\n2\nn\n\n\n\n\n\u2211\n\nx\n\n\n\u2016\n(\ny\n(\nx\n)\n\u2212\n\ny\n\u2032\n\n(\nx\n)\n)\n\n\u2016\n\n2\n\n\n\n\n{\\displaystyle E={\\frac {1}{2n}}\\sum _{x}\\lVert (y(x)-y'(x))\\rVert ^{2}}\n\n\n Backpropagation had been derived repeatedly, as it is essentially an efficient application of the chain rule (first written down by Gottfried Wilhelm Leibniz in 1676)[15][16] to neural networks.\n The terminology \"back-propagating error correction\" was introduced in 1962 by Frank Rosenblatt, but he did not know how to implement this.[17] In any case, he only studied neurons whose outputs were discrete levels, which only had zero derivatives, making backpropagation impossible.\n Precursors to backpropagation appeared in optimal control theory since 1950s. Yann LeCun et al credits 1950s work by Pontryagin and others in optimal control theory, especially the adjoint state method, for being a continuous-time version of backpropagation.[18] Hecht-Nielsen[19] credits the Robbins\u2013Monro algorithm (1951)[20] and Arthur Bryson and Yu-Chi Ho's Applied Optimal Control (1969) as presages of backpropagation. Other precursors were Henry J. Kelley 1960,[1] and Arthur E. Bryson (1961).[2] In 1962, Stuart Dreyfus published a simpler derivation based only on the chain rule.[21][22][23] In 1973, he adapted parameters of controllers in proportion to error gradients.[24] Unlike modern backpropagation, these precursors used standard Jacobian matrix calculations from one stage to the previous one, neither addressing direct links across several stages nor potential additional efficiency gains due to network sparsity.[25]\n The ADALINE (1960) learning algorithm was gradient descent with a squared error loss for a single layer. The first multilayer perceptron (MLP) with more than one layer trained by stochastic gradient descent[20] was published in 1967 by Shun'ichi Amari.[26] The MLP had 5 layers, with 2 learnable layers, and it learned to classify patterns not linearly separable.[25]\n Modern backpropagation was first published by Seppo Linnainmaa as \"reverse mode of automatic differentiation\" (1970)[27] for discrete connected networks of nested differentiable functions.[28][29][30]\n In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard.[31][32] Werbos described how he developed backpropagation in an interview. In 1971, during his PhD work, he developed backpropagation to mathematicize Freud's \"flow of psychic energy\". He faced repeated difficulty in publishing the work, only managing in 1981.[33] He also claimed that \"the first practical application of back-propagation was for estimating a dynamic model to predict nationalism and social communications in 1974\" by him.[34]\n Around 1982,[33]:\u200a376\u200a David E. Rumelhart independently developed[35]:\u200a252\u200a backpropagation and taught the algorithm to others in his research circle. He did not cite previous work as he was unaware of them. He published the algorithm first in a 1985 paper, then in a 1986 Nature paper an experimental analysis of the technique.[36] These papers became highly cited, contributed to the popularization of backpropagation, and coincided with the resurging research interest in neural networks during the 1980s.[8][37][38]\n In 1985, the method was also described by David Parker.[39][40] Yann LeCun proposed an alternative form of backpropagation for neural networks in his PhD thesis in 1987.[41]\n Gradient descent took a considerable amount of time to reach acceptance. Some early objections were: there were no guarantees that gradient descent could reach a global minimum, only local minimum; neurons were \"known\" by physiologists as making discrete signals (0/1), not continuous ones, and with discrete signals, there is no gradient to take. See the interview with Geoffrey Hinton,[33] who was awarded the 2024 Nobel Prize in Physics for his contributions to the field.[42]\n Contributing to the acceptance were several applications in training neural networks via backpropagation, sometimes achieving popularity outside the research circles.\n In 1987, NETtalk learned to convert English text into pronunciation. Sejnowski tried training it with both backpropagation and Boltzmann machine, but found the backpropagation significantly faster, so he used it for the final NETtalk.[33]:\u200a324\u200a The NETtalk program became a popular success, appearing on the Today show.[43]\n In 1989, Dean A. Pomerleau published ALVINN, a neural network trained to drive autonomously using backpropagation.[44]\n The LeNet was published in 1989 to recognize handwritten zip codes.\n In 1992, TD-Gammon achieved top human level play in backgammon. It was a reinforcement learning agent with a neural network with two layers, trained by backpropagation.[45]\n In 1993, Eric Wan won an international pattern recognition contest through backpropagation.[46][47]\n During the 2000s it fell out of favour[citation needed], but returned in the 2010s, benefiting from cheap, powerful GPU-based computing systems. This has been especially so in speech recognition, machine vision, natural language processing, and language structure learning research (in which it has been used to explain a variety of phenomena related to first[48] and second language learning.[49])[50]\n Error backpropagation has been suggested to explain human brain event-related potential (ERP) components like the N400 and P600.[51]\n In 2023, a backpropagation algorithm was implemented on a photonic processor by a team at Stanford University.[52]\n",
        "doc_number": 80
    },
    {
        "url": "https://en.wikipedia.org/wiki/Perceptron",
        "content": "In machine learning, the perceptron (or McCulloch\u2013Pitts neuron) is an algorithm for supervised learning of binary classifiers.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.[1]  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.\n The artificial neuron network was invented in 1943 by Warren McCulloch and Walter Pitts in A logical calculus of the ideas immanent in nervous activity.[5]\n In 1957, Frank Rosenblatt was at the Cornell Aeronautical Laboratory. He simulated the perceptron on an IBM 704.[6][7] Later, he obtained funding by the Information Systems Branch of the United States Office of Naval Research and the Rome Air Development Center, to build a custom-made computer, the Mark I Perceptron. It was first publicly demonstrated on 23 June 1960.[8] The machine was \"part of a previously secret four-year NPIC [the US' National Photographic Interpretation Center] effort from 1963 through 1966 to develop this algorithm into a useful tool for photo-interpreters\".[9]\n Rosenblatt described the details of the perceptron in a 1958 paper.[10] His organization of a perceptron is constructed of three kinds of cells (\"units\"): AI, AII, R, which stand for \"projection\", \"association\" and \"response\". He presented at the first international symposium on AI, Mechanisation of Thought Processes, which took place in 1958 November.[11]\n Rosenblatt's project was funded under Contract Nonr-401(40) \"Cognitive Systems Research Program\", which lasted from 1959 to 1970,[12] and Contract Nonr-2381(00) \"Project PARA\" (\"PARA\" means \"Perceiving and Recognition Automata\"), which lasted from 1957[6] to 1963.[13]\n In 1959, the Institute for Defense Analysis awarded his group a $10,000 contract. By September 1961, the ONR awarded further $153,000 worth of contracts, with $108,000 committed for 1962.[14]\n The ONR research manager, Marvin Denicoff, stated that ONR, instead of ARPA, funded the Perceptron project, because the project was unlikely to produce technological results in the near or medium term. Funding from ARPA go up to the order of millions dollars, while from ONR are on the order of 10,000 dollars. Meanwhile, the head of IPTO at ARPA, J.C.R. Licklider, was interested in 'self-organizing', 'adaptive' and other biologically-inspired methods in the 1950s; but by the mid-1960s he was openly critical of these, including the perceptron. Instead he strongly favored the logical AI approach of Simon and Newell.[15]\n The perceptron was intended to be a machine, rather than a program, and while its first implementation was in software for the IBM 704, it was subsequently implemented in custom-built hardware as the Mark I Perceptron with the project name \"Project PARA\",[16] designed for image recognition. The machine is currently in Smithsonian National Museum of American History.[17]\n The Mark I Perceptron had 3 layers. One version was implemented as follows:\n Rosenblatt called this three-layered perceptron network the alpha-perceptron, to distinguish it from other perceptron models he experimented with.[8]\n The S-units are connected to the A-units randomly (according to a table of random numbers) via a plugboard (see photo), to \"eliminate any particular intentional bias in the perceptron\". The connection weights are fixed, not learned. Rosenblatt was adamant about the random connections, as he believed the retina was randomly connected to the visual cortex, and he wanted his perceptron machine to resemble human visual perception.[18]\n The A-units are connected to the R-units, with adjustable weights encoded in potentiometers, and weight updates during learning were performed by electric motors.[2]:\u200a193\u200aThe hardware details are in an operators' manual.[16]\n In a 1958 press conference organized by the US Navy, Rosenblatt made statements about the perceptron that caused a heated controversy among the fledgling AI community; based on Rosenblatt's statements, The New York Times reported the perceptron to be \"the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.\"[19]\n The Photo Division of Central Intelligence Agency, from 1960 to 1964, studied the use of Mark I Perceptron machine for recognizing militarily interesting silhouetted targets (such as planes and ships) in aerial photos.[20][21]\n Rosenblatt described his experiments with many variants of the Perceptron machine in a book Principles of Neurodynamics (1962). The book is a published version of the 1961 report.[22]\n Among the variants are:\n The machine was shipped from Cornell to Smithsonian in 1967, under a government transfer administered by the Office of Naval Research.[9]\n Although the perceptron initially seemed promising, it was quickly proved that perceptrons could not be trained to recognise many classes of patterns. This caused the field of neural network research to stagnate for many years, before it was recognised that a feedforward neural network with two or more layers (also called a multilayer perceptron) had greater processing power than perceptrons with one layer (also called a single-layer perceptron).\n Single-layer perceptrons are only capable of learning linearly separable patterns.[23] For a classification task with some step activation function, a single node will have a single line dividing the data points forming the patterns. More nodes can create more dividing lines, but those lines must somehow be combined to form more complex classifications. A second layer of perceptrons, or even linear nodes, are sufficient to solve many otherwise non-separable problems.\n In 1969, a famous book entitled Perceptrons by Marvin Minsky and Seymour Papert showed that it was impossible for these classes of network to learn an XOR function. It is often incorrectly believed that they also conjectured that a similar result would hold for a multi-layer perceptron network. However, this is not true, as both Minsky and Papert already knew that multi-layer perceptrons were capable of producing an XOR function. (See the page on Perceptrons (book) for more information.) Nevertheless, the often-miscited Minsky and Papert text caused a significant decline in interest and funding of neural network research. It took ten more years until neural network research experienced a resurgence in the 1980s.[23][verification needed] This text was reprinted in 1987 as \"Perceptrons - Expanded Edition\" where some errors in the original text are shown and corrected.\n Rosenblatt continued working on perceptrons despite diminishing funding. The last attempt was Tobermory, built between 1961 and 1967, built for speech recognition.[24] It occupied an entire room.[25] It had 4 layers with 12,000 weights implemented by toroidal magnetic cores. By the time of its completion, simulation on digital computers had become faster than purpose-built perceptron machines.[26] He died in a boating accident in 1971.\n The kernel perceptron algorithm was already introduced in 1964 by Aizerman et al.[27] Margin bounds guarantees were given for the Perceptron algorithm in the general non-separable case first by Freund and Schapire (1998),[1] and more recently by Mohri and Rostamizadeh (2013) who extend previous results and give new and more favorable L1 bounds.[28][29]\n The perceptron is a simplified model of a biological neuron. While the complexity of biological neuron models is often required to fully understand neural behavior, research suggests a perceptron-like linear model can produce some behavior seen in real neurons.[30]\n The solution spaces of decision boundaries for all binary functions and learning behaviors are studied in.[31]\n In the modern sense, the perceptron is an algorithm for learning a binary classifier called a threshold function: a function that maps its input \n\n\n\n\nx\n\n\n\n{\\displaystyle \\mathbf {x} }\n\n (a real-valued vector) to an output value \n\n\n\nf\n(\n\nx\n\n)\n\n\n{\\displaystyle f(\\mathbf {x} )}\n\n (a single binary value):\n \n\n\n\nf\n(\n\nx\n\n)\n=\nh\n(\n\nw\n\n\u22c5\n\nx\n\n+\nb\n)\n\n\n{\\displaystyle f(\\mathbf {x} )=h(\\mathbf {w} \\cdot \\mathbf {x} +b)}\n\n\n where \n\n\n\nh\n\n\n{\\displaystyle h}\n\n is the Heaviside step-function, \n\n\n\n\nw\n\n\n\n{\\displaystyle \\mathbf {w} }\n\n is a vector of real-valued weights, \n\n\n\n\nw\n\n\u22c5\n\nx\n\n\n\n{\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} }\n\n is the dot product \n\n\n\n\n\u2211\n\ni\n=\n1\n\n\nm\n\n\n\nw\n\ni\n\n\n\nx\n\ni\n\n\n\n\n{\\textstyle \\sum _{i=1}^{m}w_{i}x_{i}}\n\n, where m is the number of inputs to the perceptron, and b is the bias. The bias shifts the decision boundary away from the origin and does not depend on any input value.\n Equivalently, since \n\n\n\n\nw\n\n\u22c5\n\nx\n\n+\nb\n=\n(\n\nw\n\n,\nb\n)\n\u22c5\n(\n\nx\n\n,\n1\n)\n\n\n{\\displaystyle \\mathbf {w} \\cdot \\mathbf {x} +b=(\\mathbf {w} ,b)\\cdot (\\mathbf {x} ,1)}\n\n, we can add the bias term \n\n\n\nb\n\n\n{\\displaystyle b}\n\n as another weight \n\n\n\n\n\nw\n\n\nm\n+\n1\n\n\n\n\n{\\displaystyle \\mathbf {w} _{m+1}}\n\n and add a coordinate \n\n\n\n1\n\n\n{\\displaystyle 1}\n\n to each input \n\n\n\n\nx\n\n\n\n{\\displaystyle \\mathbf {x} }\n\n, and then write it as a linear classifier that passes the origin:\n\n\n\nf\n(\n\nx\n\n)\n=\nh\n(\n\nw\n\n\u22c5\n\nx\n\n)\n\n\n{\\displaystyle f(\\mathbf {x} )=h(\\mathbf {w} \\cdot \\mathbf {x} )}\n\n\n The binary value of \n\n\n\nf\n(\n\nx\n\n)\n\n\n{\\displaystyle f(\\mathbf {x} )}\n\n (0 or 1) is used to perform binary classification on \n\n\n\n\nx\n\n\n\n{\\displaystyle \\mathbf {x} }\n\n as either a positive or a negative instance. Spatially, the bias shifts the position (though not the orientation) of the planar decision boundary.\n In the context of neural networks, a perceptron is an artificial neuron using the Heaviside step function as the activation function. The perceptron algorithm is also termed the single-layer perceptron, to distinguish it from a multilayer perceptron, which is a misnomer for a more complicated neural network.  As a linear classifier, the single-layer perceptron is the simplest feedforward neural network.\n From an information theory point of view, a single perceptron with K inputs has a capacity of 2K bits of information.[32] This result is due to Thomas Cover.[33]\n Specifically let \n\n\n\nT\n(\nN\n,\nK\n)\n\n\n{\\displaystyle T(N,K)}\n\n be the number of ways to linearly separate N points in K dimensions, then\n\n\n\nT\n(\nN\n,\nK\n)\n=\n\n{\n\n\n\n\n\n2\n\nN\n\n\n\n\nK\n\u2265\nN\n\n\n\n\n2\n\n\u2211\n\nk\n=\n0\n\n\nK\n\u2212\n1\n\n\n\n(\n\n\n\n\nN\n\u2212\n1\n\n\n\n\nk\n\n\n\n\n)\n\n\n\nK\n<\nN\n\n\n\n\n\n\n\n\n{\\displaystyle T(N,K)=\\left\\{{\\begin{array}{cc}2^{N}&K\\geq N\\\\2\\sum _{k=0}^{K-1}\\left({\\begin{array}{c}N-1\\\\k\\end{array}}\\right)&K<N\\end{array}}\\right.}\n\nWhen K is large, \n\n\n\nT\n(\nN\n,\nK\n)\n\n/\n\n\n2\n\nN\n\n\n\n\n{\\displaystyle T(N,K)/2^{N}}\n\n is very close to one when \n\n\n\nN\n\u2264\n2\nK\n\n\n{\\displaystyle N\\leq 2K}\n\n, but very close to zero when \n\n\n\nN\n>\n2\nK\n\n\n{\\displaystyle N>2K}\n\n. In words, one perceptron unit can almost certainly memorize a random assignment of binary labels on N points when \n\n\n\nN\n\u2264\n2\nK\n\n\n{\\displaystyle N\\leq 2K}\n\n, but almost certainly not when \n\n\n\nN\n>\n2\nK\n\n\n{\\displaystyle N>2K}\n\n.\n When operating on only binary inputs, a perceptron is called a linearly separable Boolean function, or threshold Boolean function. The sequence of numbers of threshold Boolean functions on n inputs is OEIS A000609. The value is only known exactly up to \n\n\n\nn\n=\n9\n\n\n{\\displaystyle n=9}\n\n case, but the order of magnitude is known quite exactly: it has upper bound \n\n\n\n\n2\n\n\nn\n\n2\n\n\n\u2212\nn\n\nlog\n\n2\n\n\n\u2061\nn\n+\nO\n(\nn\n)\n\n\n\n\n{\\displaystyle 2^{n^{2}-n\\log _{2}n+O(n)}}\n\n and lower bound \n\n\n\n\n2\n\n\nn\n\n2\n\n\n\u2212\nn\n\nlog\n\n2\n\n\n\u2061\nn\n\u2212\nO\n(\nn\n)\n\n\n\n\n{\\displaystyle 2^{n^{2}-n\\log _{2}n-O(n)}}\n\n.[34]\n Any Boolean linear threshold function can be implemented with only integer weights. Furthermore, the number of bits necessary and sufficient for representing a single integer weight parameter is \n\n\n\n\u0398\n(\nn\nln\n\u2061\nn\n)\n\n\n{\\displaystyle \\Theta (n\\ln n)}\n\n.[34]\n A single perceptron can learn to classify any half-space. It cannot solve any linearly nonseparable vectors, such as the Boolean exclusive-or problem (the famous \"XOR problem\").\n A perceptron network with one hidden layer can learn to classify any compact subset arbitrarily closely. Similarly, it can also approximate any compactly-supported continuous function arbitrarily closely. This is essentially a special case of the theorems by George Cybenko and Kurt Hornik.\n Perceptrons (Minsky and Papert, 1969) studied the kind of perceptron networks necessary to learn various Boolean functions.\n Consider a perceptron network with \n\n\n\nn\n\n\n{\\displaystyle n}\n\n input units, one hidden layer, and one output, similar to the Mark I Perceptron machine. It computes a Boolean function of type \n\n\n\nf\n:\n\n2\n\nn\n\n\n\u2192\n2\n\n\n{\\displaystyle f:2^{n}\\to 2}\n\n. They call a function conjuctively local of order \n\n\n\nk\n\n\n{\\displaystyle k}\n\n, iff there exists a perceptron network such that each unit in the hidden layer connects to at most \n\n\n\nk\n\n\n{\\displaystyle k}\n\n input units.\n Theorem. (Theorem 3.1.1): The parity function is conjuctively local of order \n\n\n\nn\n\n\n{\\displaystyle n}\n\n.\n Theorem. (Section 5.5): The connectedness function is conjuctively local of order \n\n\n\n\u03a9\n(\n\nn\n\n1\n\n/\n\n2\n\n\n)\n\n\n{\\displaystyle \\Omega (n^{1/2})}\n\n.\n Below is an example of a learning algorithm for a single-layer perceptron with a single output unit. For a single-layer perceptron with multiple output units, since the weights of one output unit are completely separate from all the others', the same algorithm can be run for each output unit.\n For multilayer perceptrons, where a hidden layer exists, more sophisticated algorithms such as backpropagation must be used. If the activation function or the underlying process being modeled by the perceptron is nonlinear, alternative learning algorithms such as the delta rule can be used as long as the activation function is differentiable. Nonetheless, the learning algorithm described in the steps below will often work, even for multilayer perceptrons with nonlinear activation functions.\n When multiple perceptrons are combined in an artificial neural network, each output neuron operates independently of all the others; thus, learning each output can be considered in isolation.\n We first define some variables:\n We show the values of the features as follows:\n To represent the weights: \n To show the time-dependence of \n\n\n\n\nw\n\n\n\n{\\displaystyle \\mathbf {w} }\n\n, we use:\n The algorithm updates the weights after every training sample in step 2b.\n A single perceptron is a linear classifier. It can only reach a stable state if all input vectors are classified correctly. In case the training set D is not linearly separable, i.e. if the positive examples cannot be separated from the negative examples by a hyperplane, then the algorithm would not converge since there is no solution. Hence, if linear separability of the training set is not known a priori, one of the training variants below should be used. Detailed analysis and extensions to the convergence theorem are in Chapter 11 of Perceptrons (1969).\n Linear separability is testable in time \n\n\n\nmin\n(\nO\n(\n\nn\n\nd\n\n/\n\n2\n\n\n)\n,\nO\n(\n\nd\n\n2\nn\n\n\n)\n,\nO\n(\n\nn\n\nd\n\u2212\n1\n\n\nln\n\u2061\nn\n)\n)\n\n\n{\\displaystyle \\min(O(n^{d/2}),O(d^{2n}),O(n^{d-1}\\ln n))}\n\n, where \n\n\n\nn\n\n\n{\\displaystyle n}\n\n is the number of data points, and \n\n\n\nd\n\n\n{\\displaystyle d}\n\n is the dimension of each point.[35]\n If the training set is linearly separable, then the perceptron is guaranteed to converge after making finitely many mistakes.[36] The theorem is proved by Rosenblatt et al.\n Perceptron convergence theorem\u00a0\u2014\u00a0Given a dataset \n\n\n\nD\n\n\n{\\textstyle D}\n\n, such that \n\n\n\n\nmax\n\n(\nx\n,\ny\n)\n\u2208\nD\n\n\n\u2016\nx\n\n\u2016\n\n2\n\n\n=\nR\n\n\n{\\textstyle \\max _{(x,y)\\in D}\\|x\\|_{2}=R}\n\n, and it is linearly separable by some unit vector \n\n\n\n\nw\n\n\u2217\n\n\n\n\n{\\textstyle w^{*}}\n\n, with margin \n\n\n\n\u03b3\n\n\n{\\textstyle \\gamma }\n\n: \n\n\n\n\u03b3\n:=\n\nmin\n\n(\nx\n,\ny\n)\n\u2208\nD\n\n\ny\n(\n\nw\n\n\u2217\n\n\n\u22c5\nx\n)\n\n\n{\\displaystyle \\gamma :=\\min _{(x,y)\\in D}y(w^{*}\\cdot x)}\n\n\n Then the perceptron 0-1 learning algorithm converges after making at most \n\n\n\n(\nR\n\n/\n\n\u03b3\n\n)\n\n2\n\n\n\n\n{\\textstyle (R/\\gamma )^{2}}\n\n mistakes, for any learning rate, and any method of sampling from the dataset.\n The following simple proof is due to Novikoff (1962). The idea of the proof is that the weight vector is always adjusted by a bounded amount in a direction with which it has a negative dot product, and thus can be bounded above by O(\u221at), where t is the number of changes to the weight vector. However, it can also be bounded below by O(t) because if there exists an (unknown) satisfactory weight vector, then every change makes progress in this (unknown) direction by a positive amount that depends only on the input vector. Suppose at step \n\n\n\nt\n\n\n{\\textstyle t}\n\n, the perceptron with weight \n\n\n\n\nw\n\nt\n\n\n\n\n{\\textstyle w_{t}}\n\n makes a mistake on data point \n\n\n\n(\nx\n,\ny\n)\n\n\n{\\textstyle (x,y)}\n\n, then it updates to \n\n\n\n\nw\n\nt\n+\n1\n\n\n=\n\nw\n\nt\n\n\n+\nr\n(\ny\n\u2212\n\nf\n\n\nw\n\nt\n\n\n\n\n(\nx\n)\n)\nx\n\n\n{\\textstyle w_{t+1}=w_{t}+r(y-f_{w_{t}}(x))x}\n\n.\n If \n\n\n\ny\n=\n0\n\n\n{\\textstyle y=0}\n\n, the argument is symmetric, so we omit it.\n WLOG, \n\n\n\ny\n=\n1\n\n\n{\\textstyle y=1}\n\n, then \n\n\n\n\nf\n\n\nw\n\nt\n\n\n\n\n(\nx\n)\n=\n0\n\n\n{\\textstyle f_{w_{t}}(x)=0}\n\n, \n\n\n\n\nf\n\n\nw\n\n\u2217\n\n\n\n\n(\nx\n)\n=\n1\n\n\n{\\textstyle f_{w^{*}}(x)=1}\n\n, and \n\n\n\n\nw\n\nt\n+\n1\n\n\n=\n\nw\n\nt\n\n\n+\nr\nx\n\n\n{\\textstyle w_{t+1}=w_{t}+rx}\n\n.\n By assumption, we have separation with margins: \n\n\n\n\nw\n\n\u2217\n\n\n\u22c5\nx\n\u2265\n\u03b3\n\n\n{\\displaystyle w^{*}\\cdot x\\geq \\gamma }\n\n Thus,\n\n\n\n\n\nw\n\n\u2217\n\n\n\u22c5\n\nw\n\nt\n+\n1\n\n\n\u2212\n\nw\n\n\u2217\n\n\n\u22c5\n\nw\n\nt\n\n\n=\n\nw\n\n\u2217\n\n\n\u22c5\n(\nr\nx\n)\n\u2265\nr\n\u03b3\n\n\n{\\displaystyle w^{*}\\cdot w_{t+1}-w^{*}\\cdot w_{t}=w^{*}\\cdot (rx)\\geq r\\gamma }\n\n\n Also \n\n\n\n\u2016\n\nw\n\nt\n+\n1\n\n\n\n\u2016\n\n2\n\n\n2\n\n\n\u2212\n\u2016\n\nw\n\nt\n\n\n\n\u2016\n\n2\n\n\n2\n\n\n=\n\u2016\n\nw\n\nt\n\n\n+\nr\nx\n\n\u2016\n\n2\n\n\n2\n\n\n\u2212\n\u2016\n\nw\n\nt\n\n\n\n\u2016\n\n2\n\n\n2\n\n\n=\n2\nr\n(\n\nw\n\nt\n\n\n\u22c5\nx\n)\n+\n\nr\n\n2\n\n\n\u2016\nx\n\n\u2016\n\n2\n\n\n2\n\n\n\n\n{\\displaystyle \\|w_{t+1}\\|_{2}^{2}-\\|w_{t}\\|_{2}^{2}=\\|w_{t}+rx\\|_{2}^{2}-\\|w_{t}\\|_{2}^{2}=2r(w_{t}\\cdot x)+r^{2}\\|x\\|_{2}^{2}}\n\n and since the perceptron made a mistake, \n\n\n\n\nw\n\nt\n\n\n\u22c5\nx\n\u2264\n0\n\n\n{\\textstyle w_{t}\\cdot x\\leq 0}\n\n, and so\n\n\n\n\n\u2016\n\nw\n\nt\n+\n1\n\n\n\n\u2016\n\n2\n\n\n2\n\n\n\u2212\n\u2016\n\nw\n\nt\n\n\n\n\u2016\n\n2\n\n\n2\n\n\n\u2264\n\u2016\nx\n\n\u2016\n\n2\n\n\n2\n\n\n\u2264\n\nr\n\n2\n\n\n\nR\n\n2\n\n\n\n\n{\\displaystyle \\|w_{t+1}\\|_{2}^{2}-\\|w_{t}\\|_{2}^{2}\\leq \\|x\\|_{2}^{2}\\leq r^{2}R^{2}}\n\n\n Since we started with \n\n\n\n\nw\n\n0\n\n\n=\n0\n\n\n{\\textstyle w_{0}=0}\n\n, after making \n\n\n\nN\n\n\n{\\textstyle N}\n\n mistakes, \n\n\n\n\u2016\nw\n\n\u2016\n\n2\n\n\n\u2264\n\n\nN\n\nr\n\n2\n\n\n\nR\n\n2\n\n\n\n\n\n\n{\\displaystyle \\|w\\|_{2}\\leq {\\sqrt {Nr^{2}R^{2}}}}\n\n but also\n\n\n\n\n\u2016\nw\n\n\u2016\n\n2\n\n\n\u2265\nw\n\u22c5\n\nw\n\n\u2217\n\n\n\u2265\nN\nr\n\u03b3\n\n\n{\\displaystyle \\|w\\|_{2}\\geq w\\cdot w^{*}\\geq Nr\\gamma }\n\n\n Combining the two, we have \n\n\n\nN\n\u2264\n(\nR\n\n/\n\n\u03b3\n\n)\n\n2\n\n\n\n\n{\\textstyle N\\leq (R/\\gamma )^{2}}\n\n\n While the perceptron algorithm is guaranteed to converge on some solution in the case of a linearly separable training set, it may still pick any solution and problems may admit many solutions of varying quality.[37] The perceptron of optimal stability, nowadays better known as the linear support-vector machine, was designed to solve this problem (Krauth and Mezard, 1987).[38]\n When the dataset is not linearly separable, then there is no way for a single perceptron to converge. However, we still have[39]\n Perceptron cycling theorem\u00a0\u2014\u00a0If the dataset \n\n\n\nD\n\n\n{\\displaystyle D}\n\n has only finitely many points, then there exists an upper bound number \n\n\n\nM\n\n\n{\\displaystyle M}\n\n, such that for any starting weight vector \n\n\n\n\nw\n\n0\n\n\n\n\n{\\displaystyle w_{0}}\n\n all weight vector \n\n\n\n\nw\n\nt\n\n\n\n\n{\\displaystyle w_{t}}\n\n has norm bounded by \n\n\n\n\u2016\n\nw\n\nt\n\n\n\u2016\n\u2264\n\u2016\n\nw\n\n0\n\n\n\u2016\n+\nM\n\n\n{\\displaystyle \\|w_{t}\\|\\leq \\|w_{0}\\|+M}\n\n\n This is proved first by Bradley Efron.[40]\n Consider a dataset where the \n\n\n\nx\n\n\n{\\displaystyle x}\n\n are from \n\n\n\n{\n\u2212\n1\n,\n+\n1\n\n}\n\nn\n\n\n\n\n{\\displaystyle \\{-1,+1\\}^{n}}\n\n, that is, the vertices of an n-dimensional hypercube centered at origin, and \n\n\n\ny\n=\n\u03b8\n(\n\nx\n\ni\n\n\n)\n\n\n{\\displaystyle y=\\theta (x_{i})}\n\n. That is, all data points with positive \n\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystyle x_{i}}\n\n have \n\n\n\ny\n=\n1\n\n\n{\\displaystyle y=1}\n\n, and vice versa. By the perceptron convergence theorem, a perceptron would converge after making at most \n\n\n\nn\n\n\n{\\displaystyle n}\n\n mistakes.\n If we were to write a logical program to perform the same task, each positive example shows that one of the coordinates is the right one, and each negative example shows that its complement is a positive example. By collecting all the known positive examples, we eventually eliminate all but one coordinate, at which point the dataset is learned.[41]\n This bound is asymptotically tight in terms of the worst-case. In the worst-case, the first presented example is entirely new, and gives \n\n\n\nn\n\n\n{\\displaystyle n}\n\n bits of information, but each subsequent example would differ minimally from previous examples, and gives 1 bit each. After \n\n\n\nn\n+\n1\n\n\n{\\displaystyle n+1}\n\n examples, there are \n\n\n\n2\nn\n\n\n{\\displaystyle 2n}\n\n bits of information, which is sufficient for the perceptron (with \n\n\n\n2\nn\n\n\n{\\displaystyle 2n}\n\n bits of information).[32]\n However, it is not tight in terms of expectation if the examples are presented uniformly at random, since the first would give \n\n\n\nn\n\n\n{\\displaystyle n}\n\n bits, the second \n\n\n\nn\n\n/\n\n2\n\n\n{\\displaystyle n/2}\n\n bits, and so on, taking \n\n\n\nO\n(\nln\n\u2061\nn\n)\n\n\n{\\displaystyle O(\\ln n)}\n\n examples in total.[41]\n The pocket algorithm with ratchet (Gallant, 1990) solves the stability problem of perceptron learning by keeping the best solution seen so far \"in its pocket\". The pocket algorithm then returns the solution in the pocket, rather than the last solution. It can be used also for non-separable data sets, where the aim is to find a perceptron with a small number of misclassifications. However, these solutions appear purely stochastically and hence the pocket algorithm neither approaches them gradually in the course of learning, nor are they guaranteed to show up within a given number of learning steps.\n The Maxover algorithm (Wendemuth, 1995) is \"robust\" in the sense that it will converge regardless of (prior) knowledge of linear separability of the data set.[42] In the linearly separable case, it will solve the training problem \u2013 if desired, even with optimal stability (maximum margin between the classes). For non-separable data sets, it will return a solution with a small number of misclassifications. In all cases, the algorithm gradually approaches the solution in the course of learning, without memorizing previous states and without stochastic jumps. Convergence is to global optimality for separable data sets and to local optimality for non-separable data sets.\n The Voted Perceptron (Freund and Schapire, 1999), is a variant using multiple weighted perceptrons. The algorithm starts a new perceptron every time an example is wrongly classified, initializing the weights vector with the final weights of the last perceptron. Each perceptron will also be given another weight corresponding to how many examples do they correctly classify before wrongly classifying one, and at the end the output will be a weighted vote on all perceptrons.\n In separable problems, perceptron training can also aim at finding the largest separating margin between the classes. The so-called perceptron of optimal stability can be determined by means of iterative training and optimization schemes, such as the Min-Over algorithm (Krauth and Mezard, 1987)[38]  or the AdaTron (Anlauf and Biehl, 1989)).[43] AdaTron uses the fact that the corresponding quadratic optimization problem is convex. The perceptron of optimal stability, together with the kernel trick, are the conceptual foundations of the support-vector machine.\n The \n\n\n\n\u03b1\n\n\n{\\displaystyle \\alpha }\n\n-perceptron further used a pre-processing layer of fixed random weights, with thresholded output units. This enabled the perceptron to classify analogue patterns, by projecting them into a binary space. In fact, for a projection space of sufficiently high dimension, patterns can become linearly separable.\n Another way to solve nonlinear problems without using multiple layers is to use higher order networks (sigma-pi unit). In this type of network, each element in the input vector is extended with each pairwise combination of multiplied inputs (second order). This can be extended to an n-order network.\n It should be kept in mind, however, that the best classifier is not necessarily that which classifies all the training data perfectly. Indeed, if we had the prior constraint that the data come from equi-variant Gaussian distributions, the linear separation in the input space is optimal, and the nonlinear solution is overfitted.\n Other linear classification algorithms include Winnow, support-vector machine, and logistic regression.\n Like most other techniques for training linear classifiers, the perceptron generalizes naturally to multiclass classification.  Here, the input \n\n\n\nx\n\n\n{\\displaystyle x}\n\n and the output \n\n\n\ny\n\n\n{\\displaystyle y}\n\n are drawn from arbitrary sets. A feature representation function \n\n\n\nf\n(\nx\n,\ny\n)\n\n\n{\\displaystyle f(x,y)}\n\n maps each possible input/output pair to a finite-dimensional real-valued feature vector.  As before, the feature vector is multiplied by a weight vector \n\n\n\nw\n\n\n{\\displaystyle w}\n\n, but now the resulting score is used to choose among many possible outputs:\n Learning again iterates over the examples, predicting an output for each, leaving the weights unchanged when the predicted output matches the target, and changing them when it does not.  The update becomes:\n This multiclass feedback formulation reduces to the original perceptron when \n\n\n\nx\n\n\n{\\displaystyle x}\n\n is a real-valued vector, \n\n\n\ny\n\n\n{\\displaystyle y}\n\n is chosen from \n\n\n\n{\n0\n,\n1\n}\n\n\n{\\displaystyle \\{0,1\\}}\n\n, and \n\n\n\nf\n(\nx\n,\ny\n)\n=\ny\nx\n\n\n{\\displaystyle f(x,y)=yx}\n\n.\n For certain problems, input/output representations and features can be chosen so that \n\n\n\n\n\na\nr\ng\nm\na\nx\n\n\ny\n\n\nf\n(\nx\n,\ny\n)\n\u22c5\nw\n\n\n{\\displaystyle \\mathrm {argmax} _{y}f(x,y)\\cdot w}\n\n can be found efficiently even though \n\n\n\ny\n\n\n{\\displaystyle y}\n\n is chosen from a very large or even infinite set.\n Since 2002, perceptron training has become popular in the field of natural language processing for such tasks as part-of-speech tagging and syntactic parsing (Collins, 2002). It has also been applied to large-scale machine learning problems in a distributed computing setting.[44]\n",
        "doc_number": 81
    },
    {
        "url": "https://en.wikipedia.org/wiki/Support-vector_machine",
        "content": "In machine learning, support vector machines (SVMs, also support vector networks[1]) are supervised max-margin models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories,[1][2] SVMs are one of the most studied models, being based on statistical learning frameworks of VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974).\n In addition to performing linear classification, SVMs can efficiently perform non-linear classification using the kernel trick, representing the data only through a set of pairwise similarity comparisons between the original data points using a kernel function, which transforms them into coordinates in a higher-dimensional feature space. Thus, SVMs use the kernel trick to implicitly map their inputs into high-dimensional feature spaces, where linear classification can be performed.[3]  Being max-margin models, SVMs are resilient to noisy data (e.g., misclassified examples). SVMs can also be used for regression tasks, where the objective becomes \n\n\n\n\u03f5\n\n\n{\\displaystyle \\epsilon }\n\n-sensitive.\n The support vector clustering[4] algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data.[citation needed] These data sets require unsupervised learning approaches, which attempt to find natural clustering of the data into groups, and then to map new data according to these clusters.\n The popularity of SVMs is likely due to their amenability to theoretical analysis, and their flexibility in being applied to a wide variety of tasks, including structured prediction problems. It is not clear that SVMs have better predictive performance than other linear models, such as logistic regression and linear regression.[5]\n Classifying data is a common task in machine learning.\nSuppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In the case of support vector machines, a data point is viewed as a \n\n\n\np\n\n\n{\\displaystyle p}\n\n-dimensional vector (a list of \n\n\n\np\n\n\n{\\displaystyle p}\n\n numbers), and we want to know whether we can separate such points with a \n\n\n\n(\np\n\u2212\n1\n)\n\n\n{\\displaystyle (p-1)}\n\n-dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum-margin classifier; or equivalently, the perceptron of optimal stability. [6]\n More formally, a support vector machine constructs a hyperplane or set of hyperplanes in a high or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection.[7] Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin, the lower the generalization error of the classifier.[8] A lower generalization error means that the implementer is less likely to experience overfitting.\n Whereas the original problem may be stated in a finite-dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed[9] that the original finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space. To keep the computational load reasonable, the mappings used by SVM schemes are designed to ensure that dot products of pairs of input data vectors may be computed easily in terms of the variables in the original space, by defining them in terms of a kernel function \n\n\n\nk\n(\nx\n,\ny\n)\n\n\n{\\displaystyle k(x,y)}\n\n selected to suit the problem.[10] The hyperplanes in the higher-dimensional space are defined as the set of points whose dot product with a vector in that space is constant, where such a set of vectors is an orthogonal (and thus minimal) set of vectors that defines a hyperplane. The vectors defining the hyperplanes can be chosen to be linear combinations with parameters \n\n\n\n\n\u03b1\n\ni\n\n\n\n\n{\\displaystyle \\alpha _{i}}\n\n of images of feature vectors \n\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystyle x_{i}}\n\n that occur in the data base. With this choice of a hyperplane, the points \n\n\n\nx\n\n\n{\\displaystyle x}\n\n in the feature space that are mapped into the hyperplane are defined by the relation \n\n\n\n\n\n\u2211\n\ni\n\n\n\n\u03b1\n\ni\n\n\nk\n(\n\nx\n\ni\n\n\n,\nx\n)\n=\n\nconstant\n\n.\n\n\n\n{\\displaystyle \\textstyle \\sum _{i}\\alpha _{i}k(x_{i},x)={\\text{constant}}.}\n\n  Note that if \n\n\n\nk\n(\nx\n,\ny\n)\n\n\n{\\displaystyle k(x,y)}\n\n becomes small as \n\n\n\ny\n\n\n{\\displaystyle y}\n\n grows further away from \n\n\n\nx\n\n\n{\\displaystyle x}\n\n, each term in the sum measures the degree of closeness of the test point \n\n\n\nx\n\n\n{\\displaystyle x}\n\n to the corresponding data base point \n\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystyle x_{i}}\n\n. In this way, the sum of kernels above can be used to measure the relative nearness of each test point to the data points originating in one or the other of the sets to be discriminated. Note the fact that the set of points \n\n\n\nx\n\n\n{\\displaystyle x}\n\n mapped into any hyperplane can be quite convoluted as a result, allowing much more complex discrimination between sets that are not convex at all in the original space.\n SVMs can be used to solve various real-world problems:\n The original SVM algorithm was invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1964.[citation needed] In 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick to maximum-margin hyperplanes.[9] The \"soft margin\" incarnation, as is commonly used in software packages, was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995.[1]\n We are given a training dataset of \n\n\n\nn\n\n\n{\\displaystyle n}\n\n points of the form\n\n\n\n\n(\n\n\nx\n\n\n1\n\n\n,\n\ny\n\n1\n\n\n)\n,\n\u2026\n,\n(\n\n\nx\n\n\nn\n\n\n,\n\ny\n\nn\n\n\n)\n,\n\n\n{\\displaystyle (\\mathbf {x} _{1},y_{1}),\\ldots ,(\\mathbf {x} _{n},y_{n}),}\n\n\nwhere the \n\n\n\n\ny\n\ni\n\n\n\n\n{\\displaystyle y_{i}}\n\n are either 1 or \u22121, each indicating the class to which the point \n\n\n\n\n\nx\n\n\ni\n\n\n\n\n{\\displaystyle \\mathbf {x} _{i}}\n\n belongs. Each \n\n\n\n\n\nx\n\n\ni\n\n\n\n\n{\\displaystyle \\mathbf {x} _{i}}\n\n is a \n\n\n\np\n\n\n{\\displaystyle p}\n\n-dimensional real vector. We want to find the \"maximum-margin hyperplane\" that divides the group of points \n\n\n\n\n\nx\n\n\ni\n\n\n\n\n{\\displaystyle \\mathbf {x} _{i}}\n\n for which \n\n\n\n\ny\n\ni\n\n\n=\n1\n\n\n{\\displaystyle y_{i}=1}\n\n from the group of points for which \n\n\n\n\ny\n\ni\n\n\n=\n\u2212\n1\n\n\n{\\displaystyle y_{i}=-1}\n\n, which is defined so that the distance between the hyperplane and the nearest point \n\n\n\n\n\nx\n\n\ni\n\n\n\n\n{\\displaystyle \\mathbf {x} _{i}}\n\n from either group is maximized.\n Any hyperplane can be written as the set of points \n\n\n\n\nx\n\n\n\n{\\displaystyle \\mathbf {x} }\n\n satisfying\n\n\n\n\n\n\nw\n\n\n\nT\n\n\n\n\nx\n\n\u2212\nb\n=\n0\n,\n\n\n{\\displaystyle \\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} -b=0,}\n\n\nwhere \n\n\n\n\nw\n\n\n\n{\\displaystyle \\mathbf {w} }\n\n is the (not necessarily normalized) normal vector to the hyperplane. This is much like Hesse normal form, except that \n\n\n\n\nw\n\n\n\n{\\displaystyle \\mathbf {w} }\n\n is not necessarily a unit vector. The parameter \n\n\n\n\n\n\nb\n\n\u2016\n\nw\n\n\u2016\n\n\n\n\n\n\n{\\displaystyle {\\tfrac {b}{\\|\\mathbf {w} \\|}}}\n\n determines the offset of the hyperplane from the origin along the normal vector \n\n\n\n\nw\n\n\n\n{\\displaystyle \\mathbf {w} }\n\n.\n Warning: most of the literature on the subject defines the bias so that\n\n\n\n\n\n\nw\n\n\n\nT\n\n\n\n\nx\n\n+\nb\n=\n0.\n\n\n{\\displaystyle \\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} +b=0.}\n\n\n If the training data is linearly separable, we can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible. The region bounded by these two hyperplanes is called the \"margin\", and the maximum-margin hyperplane is the hyperplane that lies halfway between them. With a normalized or standardized dataset, these hyperplanes can be described by the equations\n and\n Geometrically, the distance between these two hyperplanes is \n\n\n\n\n\n\n2\n\n\u2016\n\nw\n\n\u2016\n\n\n\n\n\n\n{\\displaystyle {\\tfrac {2}{\\|\\mathbf {w} \\|}}}\n\n,[21] so to maximize the distance between the planes we want to minimize \n\n\n\n\u2016\n\nw\n\n\u2016\n\n\n{\\displaystyle \\|\\mathbf {w} \\|}\n\n. The distance is computed using the distance from a point to a plane equation. We also have to prevent data points from falling into the margin, we add the following constraint: for each \n\n\n\ni\n\n\n{\\displaystyle i}\n\n either\n\n\n\n\n\n\nw\n\n\n\nT\n\n\n\n\n\nx\n\n\ni\n\n\n\u2212\nb\n\u2265\n1\n\n,\n\n\u00a0if\u00a0\n\n\ny\n\ni\n\n\n=\n1\n,\n\n\n{\\displaystyle \\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} _{i}-b\\geq 1\\,,{\\text{ if }}y_{i}=1,}\n\n\nor\n\n\n\n\n\n\nw\n\n\n\nT\n\n\n\n\n\nx\n\n\ni\n\n\n\u2212\nb\n\u2264\n\u2212\n1\n\n,\n\n\u00a0if\u00a0\n\n\ny\n\ni\n\n\n=\n\u2212\n1.\n\n\n{\\displaystyle \\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} _{i}-b\\leq -1\\,,{\\text{ if }}y_{i}=-1.}\n\n\nThese constraints state that each data point must lie on the correct side of the margin.\n This can be rewritten as\n We can put this together to get the optimization problem:\n \n\n\n\n\n\n\n\n\n\n\nminimize\n\n\nw\n\n,\n\nb\n\n\n\n\n\n\n\n\n1\n2\n\n\n\u2016\n\nw\n\n\n\u2016\n\n2\n\n\n\n\n\n\n\n\nsubject to\n\n\n\n\n\ny\n\ni\n\n\n(\n\n\nw\n\n\n\u22a4\n\n\n\n\nx\n\n\ni\n\n\n\u2212\nb\n)\n\u2265\n1\n\n\u2200\ni\n\u2208\n{\n1\n,\n\u2026\n,\nn\n}\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}&{\\underset {\\mathbf {w} ,\\;b}{\\operatorname {minimize} }}&&{\\frac {1}{2}}\\|\\mathbf {w} \\|^{2}\\\\&{\\text{subject to}}&&y_{i}(\\mathbf {w} ^{\\top }\\mathbf {x} _{i}-b)\\geq 1\\quad \\forall i\\in \\{1,\\dots ,n\\}\\end{aligned}}}\n\n\n The \n\n\n\n\nw\n\n\n\n{\\displaystyle \\mathbf {w} }\n\n and \n\n\n\nb\n\n\n{\\displaystyle b}\n\n that solve this problem determine the final classifier, \n\n\n\n\nx\n\n\u21a6\nsgn\n\u2061\n(\n\n\nw\n\n\n\nT\n\n\n\n\nx\n\n\u2212\nb\n)\n\n\n{\\displaystyle \\mathbf {x} \\mapsto \\operatorname {sgn}(\\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} -b)}\n\n, where \n\n\n\nsgn\n\u2061\n(\n\u22c5\n)\n\n\n{\\displaystyle \\operatorname {sgn}(\\cdot )}\n\n is the sign function.\n An important consequence of this geometric description is that the max-margin hyperplane is completely determined by those \n\n\n\n\n\nx\n\n\ni\n\n\n\n\n{\\displaystyle \\mathbf {x} _{i}}\n\n that lie nearest to it (explained below). These \n\n\n\n\n\nx\n\n\ni\n\n\n\n\n{\\displaystyle \\mathbf {x} _{i}}\n\n are called support vectors.\n To extend SVM to cases in which the data are not linearly separable, the hinge loss function is helpful\n\n\n\n\nmax\n\n(\n\n0\n,\n1\n\u2212\n\ny\n\ni\n\n\n(\n\n\nw\n\n\n\nT\n\n\n\n\n\nx\n\n\ni\n\n\n\u2212\nb\n)\n\n)\n\n.\n\n\n{\\displaystyle \\max \\left(0,1-y_{i}(\\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} _{i}-b)\\right).}\n\n\n Note that \n\n\n\n\ny\n\ni\n\n\n\n\n{\\displaystyle y_{i}}\n\n is the i-th target (i.e., in this case, 1 or \u22121), and \n\n\n\n\n\nw\n\n\n\nT\n\n\n\n\n\nx\n\n\ni\n\n\n\u2212\nb\n\n\n{\\displaystyle \\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} _{i}-b}\n\n is the i-th output.\n This function is zero if the constraint in (1) is satisfied, in other words, if \n\n\n\n\n\nx\n\n\ni\n\n\n\n\n{\\displaystyle \\mathbf {x} _{i}}\n\n lies on the correct side of the margin. For data on the wrong side of the margin, the function's value is proportional to the distance from the margin.\n The goal of the optimization then is to minimize:\n \n\n\n\n\u2016\n\nw\n\n\n\u2016\n\n2\n\n\n+\nC\n\n[\n\n\n\n1\nn\n\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\nmax\n\n(\n\n0\n,\n1\n\u2212\n\ny\n\ni\n\n\n(\n\n\nw\n\n\n\nT\n\n\n\n\n\nx\n\n\ni\n\n\n\u2212\nb\n)\n\n)\n\n\n]\n\n,\n\n\n{\\displaystyle \\lVert \\mathbf {w} \\rVert ^{2}+C\\left[{\\frac {1}{n}}\\sum _{i=1}^{n}\\max \\left(0,1-y_{i}(\\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} _{i}-b)\\right)\\right],}\n\n\n where the parameter \n\n\n\nC\n>\n0\n\n\n{\\displaystyle C>0}\n\n determines the trade-off between increasing the margin size and ensuring that the \n\n\n\n\n\nx\n\n\ni\n\n\n\n\n{\\displaystyle \\mathbf {x} _{i}}\n\n lie on the correct side of the margin (Note we can add a weight to either term in the equation above). By deconstructing the hinge loss, this optimization problem can be massaged into the following:\n \n\n\n\n\n\n\n\n\n\n\nminimize\n\n\nw\n\n,\n\nb\n,\n\n\n\u03b6\n\n\n\n\n\n\n\n\n\u2016\n\nw\n\n\n\u2016\n\n2\n\n\n2\n\n\n+\nC\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\n\u03b6\n\ni\n\n\n\n\n\n\n\n\nsubject to\n\n\n\n\n\ny\n\ni\n\n\n(\n\n\nw\n\n\n\u22a4\n\n\n\n\nx\n\n\ni\n\n\n\u2212\nb\n)\n\u2265\n1\n\u2212\n\n\u03b6\n\ni\n\n\n,\n\n\n\u03b6\n\ni\n\n\n\u2265\n0\n\n\u2200\ni\n\u2208\n{\n1\n,\n\u2026\n,\nn\n}\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}&{\\underset {\\mathbf {w} ,\\;b,\\;\\mathbf {\\zeta } }{\\operatorname {minimize} }}&&\\|\\mathbf {w} \\|_{2}^{2}+C\\sum _{i=1}^{n}\\zeta _{i}\\\\&{\\text{subject to}}&&y_{i}(\\mathbf {w} ^{\\top }\\mathbf {x} _{i}-b)\\geq 1-\\zeta _{i},\\quad \\zeta _{i}\\geq 0\\quad \\forall i\\in \\{1,\\dots ,n\\}\\end{aligned}}}\n\n\n Thus, for large values of \n\n\n\nC\n\n\n{\\displaystyle C}\n\n, it will behave similar to the hard-margin SVM, if the input data are linearly classifiable, but will still learn if a classification rule is viable or not.\n The original maximum-margin hyperplane algorithm proposed by Vapnik in 1963 constructed a linear classifier. However, in 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick (originally proposed by Aizerman et al.[22]) to maximum-margin hyperplanes.[9] The kernel trick, where dot products are replaced by kernels, is easily derived in the dual representation of the SVM problem. This allows the algorithm to fit the maximum-margin hyperplane in a transformed feature space. The transformation may be nonlinear and the transformed space high-dimensional; although the classifier is a hyperplane in the transformed feature space, it may be nonlinear in the original input space.\n It is noteworthy that working in a higher-dimensional feature space increases the generalization error of support vector machines, although given enough samples the algorithm still performs well.[23]\n Some common kernels include:\n The kernel is related to the transform \n\n\n\n\u03c6\n(\n\n\nx\n\n\ni\n\n\n)\n\n\n{\\displaystyle \\varphi (\\mathbf {x} _{i})}\n\n by the equation \n\n\n\nk\n(\n\n\nx\n\n\ni\n\n\n,\n\n\nx\n\n\nj\n\n\n)\n=\n\u03c6\n(\n\n\nx\n\n\ni\n\n\n)\n\u22c5\n\u03c6\n(\n\n\nx\n\n\nj\n\n\n)\n\n\n{\\displaystyle k(\\mathbf {x} _{i},\\mathbf {x} _{j})=\\varphi (\\mathbf {x} _{i})\\cdot \\varphi (\\mathbf {x} _{j})}\n\n. The value w is also in the transformed space, with \n\n\n\n\nw\n\n=\n\n\u2211\n\ni\n\n\n\n\u03b1\n\ni\n\n\n\ny\n\ni\n\n\n\u03c6\n(\n\n\nx\n\n\ni\n\n\n)\n\n\n{\\textstyle \\mathbf {w} =\\sum _{i}\\alpha _{i}y_{i}\\varphi (\\mathbf {x} _{i})}\n\n. Dot products with w for classification can again be computed by the kernel trick, i.e. \n\n\n\n\nw\n\n\u22c5\n\u03c6\n(\n\nx\n\n)\n=\n\n\u2211\n\ni\n\n\n\n\u03b1\n\ni\n\n\n\ny\n\ni\n\n\nk\n(\n\n\nx\n\n\ni\n\n\n,\n\nx\n\n)\n\n\n{\\textstyle \\mathbf {w} \\cdot \\varphi (\\mathbf {x} )=\\sum _{i}\\alpha _{i}y_{i}k(\\mathbf {x} _{i},\\mathbf {x} )}\n\n.\n Computing the (soft-margin) SVM classifier amounts to minimizing an expression of the form\n We focus on the soft-margin classifier since, as noted above, choosing a sufficiently small value for \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n yields the hard-margin classifier for linearly classifiable input data. The classical approach, which involves reducing (2) to a quadratic programming problem, is detailed below. Then, more recent approaches such as sub-gradient descent and coordinate descent will be discussed.\n Minimizing (2) can be rewritten as a constrained optimization problem with a differentiable objective function in the following way.\n For each \n\n\n\ni\n\u2208\n{\n1\n,\n\n\u2026\n,\n\nn\n}\n\n\n{\\displaystyle i\\in \\{1,\\,\\ldots ,\\,n\\}}\n\n we introduce a variable \n\n\n\n\n\u03b6\n\ni\n\n\n=\nmax\n\n(\n\n0\n,\n1\n\u2212\n\ny\n\ni\n\n\n(\n\n\nw\n\n\n\nT\n\n\n\n\n\nx\n\n\ni\n\n\n\u2212\nb\n)\n\n)\n\n\n\n{\\displaystyle \\zeta _{i}=\\max \\left(0,1-y_{i}(\\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} _{i}-b)\\right)}\n\n. Note that \n\n\n\n\n\u03b6\n\ni\n\n\n\n\n{\\displaystyle \\zeta _{i}}\n\n is the smallest nonnegative number satisfying \n\n\n\n\ny\n\ni\n\n\n(\n\n\nw\n\n\n\nT\n\n\n\n\n\nx\n\n\ni\n\n\n\u2212\nb\n)\n\u2265\n1\n\u2212\n\n\u03b6\n\ni\n\n\n.\n\n\n{\\displaystyle y_{i}(\\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} _{i}-b)\\geq 1-\\zeta _{i}.}\n\n\n Thus we can rewrite the optimization problem as follows\n \n\n\n\n\n\n\n\n\n\nminimize\u00a0\n\n\n\n1\nn\n\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\n\u03b6\n\ni\n\n\n+\n\u03bb\n\u2016\n\nw\n\n\n\u2016\n\n2\n\n\n\n\n\n\n\n\nsubject to\u00a0\n\n\ny\n\ni\n\n\n\n(\n\n\n\nw\n\n\n\nT\n\n\n\n\n\nx\n\n\ni\n\n\n\u2212\nb\n\n)\n\n\u2265\n1\n\u2212\n\n\u03b6\n\ni\n\n\n\n\n\u00a0and\u00a0\n\n\n\n\u03b6\n\ni\n\n\n\u2265\n0\n,\n\n\nfor all\u00a0\n\ni\n.\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}&{\\text{minimize }}{\\frac {1}{n}}\\sum _{i=1}^{n}\\zeta _{i}+\\lambda \\|\\mathbf {w} \\|^{2}\\\\[0.5ex]&{\\text{subject to }}y_{i}\\left(\\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} _{i}-b\\right)\\geq 1-\\zeta _{i}\\,{\\text{ and }}\\,\\zeta _{i}\\geq 0,\\,{\\text{for all }}i.\\end{aligned}}}\n\n\n This is called the primal problem.\n By solving for the Lagrangian dual of the above problem, one obtains the simplified problem\n \n\n\n\n\n\n\n\n\n\nmaximize\n\n\n\nf\n(\n\nc\n\n1\n\n\n\u2026\n\nc\n\nn\n\n\n)\n=\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\nc\n\ni\n\n\n\u2212\n\n\n1\n2\n\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\n\u2211\n\nj\n=\n1\n\n\nn\n\n\n\ny\n\ni\n\n\n\nc\n\ni\n\n\n(\n\n\nx\n\n\ni\n\n\n\nT\n\n\n\n\n\nx\n\n\nj\n\n\n)\n\ny\n\nj\n\n\n\nc\n\nj\n\n\n,\n\n\n\n\n\n\nsubject to\u00a0\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\nc\n\ni\n\n\n\ny\n\ni\n\n\n=\n0\n,\n\n\nand\u00a0\n\n0\n\u2264\n\nc\n\ni\n\n\n\u2264\n\n\n1\n\n2\nn\n\u03bb\n\n\n\n\n\nfor all\u00a0\n\ni\n.\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}&{\\text{maximize}}\\,\\,f(c_{1}\\ldots c_{n})=\\sum _{i=1}^{n}c_{i}-{\\frac {1}{2}}\\sum _{i=1}^{n}\\sum _{j=1}^{n}y_{i}c_{i}(\\mathbf {x} _{i}^{\\mathsf {T}}\\mathbf {x} _{j})y_{j}c_{j},\\\\&{\\text{subject to }}\\sum _{i=1}^{n}c_{i}y_{i}=0,\\,{\\text{and }}0\\leq c_{i}\\leq {\\frac {1}{2n\\lambda }}\\;{\\text{for all }}i.\\end{aligned}}}\n\n\n This is called the dual problem. Since the dual maximization problem is a quadratic function of the \n\n\n\n\nc\n\ni\n\n\n\n\n{\\displaystyle c_{i}}\n\n subject to linear constraints, it is efficiently solvable by quadratic programming algorithms.\n Here, the variables \n\n\n\n\nc\n\ni\n\n\n\n\n{\\displaystyle c_{i}}\n\n are defined such that\n \n\n\n\n\nw\n\n=\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\nc\n\ni\n\n\n\ny\n\ni\n\n\n\n\nx\n\n\ni\n\n\n.\n\n\n{\\displaystyle \\mathbf {w} =\\sum _{i=1}^{n}c_{i}y_{i}\\mathbf {x} _{i}.}\n\n\n Moreover, \n\n\n\n\nc\n\ni\n\n\n=\n0\n\n\n{\\displaystyle c_{i}=0}\n\n exactly when \n\n\n\n\n\nx\n\n\ni\n\n\n\n\n{\\displaystyle \\mathbf {x} _{i}}\n\n lies on the correct side of the margin, and \n\n\n\n0\n<\n\nc\n\ni\n\n\n<\n(\n2\nn\n\u03bb\n\n)\n\n\u2212\n1\n\n\n\n\n{\\displaystyle 0<c_{i}<(2n\\lambda )^{-1}}\n\n  when \n\n\n\n\n\nx\n\n\ni\n\n\n\n\n{\\displaystyle \\mathbf {x} _{i}}\n\n lies on the margin's boundary. It follows that \n\n\n\n\nw\n\n\n\n{\\displaystyle \\mathbf {w} }\n\n can be written as a linear combination of the support vectors.\n The offset, \n\n\n\nb\n\n\n{\\displaystyle b}\n\n, can be recovered by finding an \n\n\n\n\n\nx\n\n\ni\n\n\n\n\n{\\displaystyle \\mathbf {x} _{i}}\n\n on the margin's boundary and solving\n\n\n\n\n\ny\n\ni\n\n\n(\n\n\nw\n\n\n\nT\n\n\n\n\n\nx\n\n\ni\n\n\n\u2212\nb\n)\n=\n1\n\n\u27fa\n\nb\n=\n\n\nw\n\n\n\nT\n\n\n\n\n\nx\n\n\ni\n\n\n\u2212\n\ny\n\ni\n\n\n.\n\n\n{\\displaystyle y_{i}(\\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} _{i}-b)=1\\iff b=\\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} _{i}-y_{i}.}\n\n\n (Note that \n\n\n\n\ny\n\ni\n\n\n\u2212\n1\n\n\n=\n\ny\n\ni\n\n\n\n\n{\\displaystyle y_{i}^{-1}=y_{i}}\n\n since \n\n\n\n\ny\n\ni\n\n\n=\n\u00b1\n1\n\n\n{\\displaystyle y_{i}=\\pm 1}\n\n.)\n Suppose now that we would like to learn a nonlinear classification rule which corresponds to a linear classification rule for the transformed data points \n\n\n\n\u03c6\n(\n\n\nx\n\n\ni\n\n\n)\n.\n\n\n{\\displaystyle \\varphi (\\mathbf {x} _{i}).}\n\n Moreover, we are given a kernel function \n\n\n\nk\n\n\n{\\displaystyle k}\n\n which satisfies \n\n\n\nk\n(\n\n\nx\n\n\ni\n\n\n,\n\n\nx\n\n\nj\n\n\n)\n=\n\u03c6\n(\n\n\nx\n\n\ni\n\n\n)\n\u22c5\n\u03c6\n(\n\n\nx\n\n\nj\n\n\n)\n\n\n{\\displaystyle k(\\mathbf {x} _{i},\\mathbf {x} _{j})=\\varphi (\\mathbf {x} _{i})\\cdot \\varphi (\\mathbf {x} _{j})}\n\n.\n We know the classification vector \n\n\n\n\nw\n\n\n\n{\\displaystyle \\mathbf {w} }\n\n in the transformed space satisfies\n \n\n\n\n\nw\n\n=\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\nc\n\ni\n\n\n\ny\n\ni\n\n\n\u03c6\n(\n\n\nx\n\n\ni\n\n\n)\n,\n\n\n{\\displaystyle \\mathbf {w} =\\sum _{i=1}^{n}c_{i}y_{i}\\varphi (\\mathbf {x} _{i}),}\n\n\n where, the \n\n\n\n\nc\n\ni\n\n\n\n\n{\\displaystyle c_{i}}\n\n are obtained by solving the optimization problem\n \n\n\n\n\n\n\n\n\nmaximize\n\n\n\nf\n(\n\nc\n\n1\n\n\n\u2026\n\nc\n\nn\n\n\n)\n\n\n\n=\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\nc\n\ni\n\n\n\u2212\n\n\n1\n2\n\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\n\u2211\n\nj\n=\n1\n\n\nn\n\n\n\ny\n\ni\n\n\n\nc\n\ni\n\n\n(\n\u03c6\n(\n\n\nx\n\n\ni\n\n\n)\n\u22c5\n\u03c6\n(\n\n\nx\n\n\nj\n\n\n)\n)\n\ny\n\nj\n\n\n\nc\n\nj\n\n\n\n\n\n\n\n\n=\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\nc\n\ni\n\n\n\u2212\n\n\n1\n2\n\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\n\u2211\n\nj\n=\n1\n\n\nn\n\n\n\ny\n\ni\n\n\n\nc\n\ni\n\n\nk\n(\n\n\nx\n\n\ni\n\n\n,\n\n\nx\n\n\nj\n\n\n)\n\ny\n\nj\n\n\n\nc\n\nj\n\n\n\n\n\n\n\nsubject to\u00a0\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\nc\n\ni\n\n\n\ny\n\ni\n\n\n\n\n\n=\n0\n,\n\n\nand\u00a0\n\n0\n\u2264\n\nc\n\ni\n\n\n\u2264\n\n\n1\n\n2\nn\n\u03bb\n\n\n\n\n\nfor all\u00a0\n\ni\n.\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}{\\text{maximize}}\\,\\,f(c_{1}\\ldots c_{n})&=\\sum _{i=1}^{n}c_{i}-{\\frac {1}{2}}\\sum _{i=1}^{n}\\sum _{j=1}^{n}y_{i}c_{i}(\\varphi (\\mathbf {x} _{i})\\cdot \\varphi (\\mathbf {x} _{j}))y_{j}c_{j}\\\\&=\\sum _{i=1}^{n}c_{i}-{\\frac {1}{2}}\\sum _{i=1}^{n}\\sum _{j=1}^{n}y_{i}c_{i}k(\\mathbf {x} _{i},\\mathbf {x} _{j})y_{j}c_{j}\\\\{\\text{subject to }}\\sum _{i=1}^{n}c_{i}y_{i}&=0,\\,{\\text{and }}0\\leq c_{i}\\leq {\\frac {1}{2n\\lambda }}\\;{\\text{for all }}i.\\end{aligned}}}\n\n\n The coefficients \n\n\n\n\nc\n\ni\n\n\n\n\n{\\displaystyle c_{i}}\n\n can be solved for using quadratic programming, as before. Again, we can find some index \n\n\n\ni\n\n\n{\\displaystyle i}\n\n such that \n\n\n\n0\n<\n\nc\n\ni\n\n\n<\n(\n2\nn\n\u03bb\n\n)\n\n\u2212\n1\n\n\n\n\n{\\displaystyle 0<c_{i}<(2n\\lambda )^{-1}}\n\n, so that \n\n\n\n\u03c6\n(\n\n\nx\n\n\ni\n\n\n)\n\n\n{\\displaystyle \\varphi (\\mathbf {x} _{i})}\n\n lies on the boundary of the margin in the transformed space, and then solve\n \n\n\n\n\n\n\n\nb\n=\n\n\nw\n\n\n\nT\n\n\n\n\u03c6\n(\n\n\nx\n\n\ni\n\n\n)\n\u2212\n\ny\n\ni\n\n\n\n\n\n=\n\n[\n\n\n\u2211\n\nj\n=\n1\n\n\nn\n\n\n\nc\n\nj\n\n\n\ny\n\nj\n\n\n\u03c6\n(\n\n\nx\n\n\nj\n\n\n)\n\u22c5\n\u03c6\n(\n\n\nx\n\n\ni\n\n\n)\n\n]\n\n\u2212\n\ny\n\ni\n\n\n\n\n\n\n\n\n=\n\n[\n\n\n\u2211\n\nj\n=\n1\n\n\nn\n\n\n\nc\n\nj\n\n\n\ny\n\nj\n\n\nk\n(\n\n\nx\n\n\nj\n\n\n,\n\n\nx\n\n\ni\n\n\n)\n\n]\n\n\u2212\n\ny\n\ni\n\n\n.\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}b=\\mathbf {w} ^{\\mathsf {T}}\\varphi (\\mathbf {x} _{i})-y_{i}&=\\left[\\sum _{j=1}^{n}c_{j}y_{j}\\varphi (\\mathbf {x} _{j})\\cdot \\varphi (\\mathbf {x} _{i})\\right]-y_{i}\\\\&=\\left[\\sum _{j=1}^{n}c_{j}y_{j}k(\\mathbf {x} _{j},\\mathbf {x} _{i})\\right]-y_{i}.\\end{aligned}}}\n\n\n Finally,\n \n\n\n\n\nz\n\n\u21a6\nsgn\n\u2061\n(\n\n\nw\n\n\n\nT\n\n\n\n\u03c6\n(\n\nz\n\n)\n\u2212\nb\n)\n=\nsgn\n\u2061\n\n(\n\n\n[\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\nc\n\ni\n\n\n\ny\n\ni\n\n\nk\n(\n\n\nx\n\n\ni\n\n\n,\n\nz\n\n)\n\n]\n\n\u2212\nb\n\n)\n\n.\n\n\n{\\displaystyle \\mathbf {z} \\mapsto \\operatorname {sgn}(\\mathbf {w} ^{\\mathsf {T}}\\varphi (\\mathbf {z} )-b)=\\operatorname {sgn} \\left(\\left[\\sum _{i=1}^{n}c_{i}y_{i}k(\\mathbf {x} _{i},\\mathbf {z} )\\right]-b\\right).}\n\n\n Recent algorithms for finding the SVM classifier include sub-gradient descent and coordinate descent. Both techniques have proven to offer significant advantages over the traditional approach when dealing with large, sparse datasets\u2014sub-gradient methods are especially efficient when there are many training examples, and coordinate descent when the dimension of the feature space is high.\n Sub-gradient descent algorithms for the SVM work directly with the expression\n \n\n\n\nf\n(\n\nw\n\n,\nb\n)\n=\n\n[\n\n\n\n1\nn\n\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\nmax\n\n(\n\n0\n,\n1\n\u2212\n\ny\n\ni\n\n\n(\n\n\nw\n\n\n\nT\n\n\n\n\n\nx\n\n\ni\n\n\n\u2212\nb\n)\n\n)\n\n\n]\n\n+\n\u03bb\n\u2016\n\nw\n\n\n\u2016\n\n2\n\n\n.\n\n\n{\\displaystyle f(\\mathbf {w} ,b)=\\left[{\\frac {1}{n}}\\sum _{i=1}^{n}\\max \\left(0,1-y_{i}(\\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} _{i}-b)\\right)\\right]+\\lambda \\|\\mathbf {w} \\|^{2}.}\n\n\n Note that \n\n\n\nf\n\n\n{\\displaystyle f}\n\n is a convex function of \n\n\n\n\nw\n\n\n\n{\\displaystyle \\mathbf {w} }\n\n and \n\n\n\nb\n\n\n{\\displaystyle b}\n\n. As such, traditional gradient descent (or SGD) methods can be adapted, where instead of taking a step in the direction of the function's gradient, a step is taken in the direction of a vector selected from the function's sub-gradient. This approach has the advantage that, for certain implementations, the number of iterations does not scale with \n\n\n\nn\n\n\n{\\displaystyle n}\n\n, the number of data points.[24]\n Coordinate descent algorithms for the SVM work from the dual problem\n \n\n\n\n\n\n\n\n\n\nmaximize\n\n\n\nf\n(\n\nc\n\n1\n\n\n\u2026\n\nc\n\nn\n\n\n)\n=\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\nc\n\ni\n\n\n\u2212\n\n\n1\n2\n\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\n\u2211\n\nj\n=\n1\n\n\nn\n\n\n\ny\n\ni\n\n\n\nc\n\ni\n\n\n(\n\nx\n\ni\n\n\n\u22c5\n\nx\n\nj\n\n\n)\n\ny\n\nj\n\n\n\nc\n\nj\n\n\n,\n\n\n\n\n\n\nsubject to\u00a0\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\nc\n\ni\n\n\n\ny\n\ni\n\n\n=\n0\n,\n\n\nand\u00a0\n\n0\n\u2264\n\nc\n\ni\n\n\n\u2264\n\n\n1\n\n2\nn\n\u03bb\n\n\n\n\n\nfor all\u00a0\n\ni\n.\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}&{\\text{maximize}}\\,\\,f(c_{1}\\ldots c_{n})=\\sum _{i=1}^{n}c_{i}-{\\frac {1}{2}}\\sum _{i=1}^{n}\\sum _{j=1}^{n}y_{i}c_{i}(x_{i}\\cdot x_{j})y_{j}c_{j},\\\\&{\\text{subject to }}\\sum _{i=1}^{n}c_{i}y_{i}=0,\\,{\\text{and }}0\\leq c_{i}\\leq {\\frac {1}{2n\\lambda }}\\;{\\text{for all }}i.\\end{aligned}}}\n\n\n For each \n\n\n\ni\n\u2208\n{\n1\n,\n\n\u2026\n,\n\nn\n}\n\n\n{\\displaystyle i\\in \\{1,\\,\\ldots ,\\,n\\}}\n\n, iteratively, the coefficient \n\n\n\n\nc\n\ni\n\n\n\n\n{\\displaystyle c_{i}}\n\n is adjusted in the direction of \n\n\n\n\u2202\nf\n\n/\n\n\u2202\n\nc\n\ni\n\n\n\n\n{\\displaystyle \\partial f/\\partial c_{i}}\n\n. Then, the resulting vector of coefficients \n\n\n\n(\n\nc\n\n1\n\n\u2032\n\n,\n\n\u2026\n,\n\n\nc\n\nn\n\n\u2032\n\n)\n\n\n{\\displaystyle (c_{1}',\\,\\ldots ,\\,c_{n}')}\n\n is projected onto the nearest vector of coefficients that satisfies the given constraints. (Typically Euclidean distances are used.) The process is then repeated until a near-optimal vector of coefficients is obtained. The resulting algorithm is extremely fast in practice, although few performance guarantees have been proven.[25]\n The soft-margin support vector machine described above is an example of an empirical risk minimization (ERM) algorithm for the hinge loss. Seen this way, support vector machines belong to a natural class of algorithms for statistical inference, and many of its unique features are due to the behavior of the hinge loss. This perspective can provide further insight into how and why SVMs work, and allow us to better analyze their statistical properties.\n In supervised learning, one is given a set of training examples \n\n\n\n\nX\n\n1\n\n\n\u2026\n\nX\n\nn\n\n\n\n\n{\\displaystyle X_{1}\\ldots X_{n}}\n\n with labels \n\n\n\n\ny\n\n1\n\n\n\u2026\n\ny\n\nn\n\n\n\n\n{\\displaystyle y_{1}\\ldots y_{n}}\n\n, and wishes to predict \n\n\n\n\ny\n\nn\n+\n1\n\n\n\n\n{\\displaystyle y_{n+1}}\n\n given \n\n\n\n\nX\n\nn\n+\n1\n\n\n\n\n{\\displaystyle X_{n+1}}\n\n. To do so one forms a hypothesis, \n\n\n\nf\n\n\n{\\displaystyle f}\n\n, such that \n\n\n\nf\n(\n\nX\n\nn\n+\n1\n\n\n)\n\n\n{\\displaystyle f(X_{n+1})}\n\n is a \"good\" approximation of \n\n\n\n\ny\n\nn\n+\n1\n\n\n\n\n{\\displaystyle y_{n+1}}\n\n. A \"good\" approximation is usually defined with the help of a loss function, \n\n\n\n\u2113\n(\ny\n,\nz\n)\n\n\n{\\displaystyle \\ell (y,z)}\n\n, which characterizes how bad \n\n\n\nz\n\n\n{\\displaystyle z}\n\n is as a prediction of \n\n\n\ny\n\n\n{\\displaystyle y}\n\n. We would then like to choose a hypothesis that minimizes the expected risk:\n \n\n\n\n\u03b5\n(\nf\n)\n=\n\nE\n\n\n[\n\n\u2113\n(\n\ny\n\nn\n+\n1\n\n\n,\nf\n(\n\nX\n\nn\n+\n1\n\n\n)\n)\n\n]\n\n.\n\n\n{\\displaystyle \\varepsilon (f)=\\mathbb {E} \\left[\\ell (y_{n+1},f(X_{n+1}))\\right].}\n\n\n In most cases, we don't know the joint distribution of \n\n\n\n\nX\n\nn\n+\n1\n\n\n,\n\n\ny\n\nn\n+\n1\n\n\n\n\n{\\displaystyle X_{n+1},\\,y_{n+1}}\n\n outright. In these cases, a common strategy is to choose the hypothesis that minimizes the empirical risk:\n \n\n\n\n\n\n\n\u03b5\n^\n\n\n\n(\nf\n)\n=\n\n\n1\nn\n\n\n\n\u2211\n\nk\n=\n1\n\n\nn\n\n\n\u2113\n(\n\ny\n\nk\n\n\n,\nf\n(\n\nX\n\nk\n\n\n)\n)\n.\n\n\n{\\displaystyle {\\hat {\\varepsilon }}(f)={\\frac {1}{n}}\\sum _{k=1}^{n}\\ell (y_{k},f(X_{k})).}\n\n\n Under certain assumptions about the sequence of random variables \n\n\n\n\nX\n\nk\n\n\n,\n\n\ny\n\nk\n\n\n\n\n{\\displaystyle X_{k},\\,y_{k}}\n\n (for example, that they are generated by a finite Markov process), if the set of hypotheses being considered is small enough, the minimizer of the empirical risk will closely approximate the minimizer of the expected risk as \n\n\n\nn\n\n\n{\\displaystyle n}\n\n grows large. This approach is called empirical risk minimization, or ERM.\n In order for the minimization problem to have a well-defined solution, we have to place constraints on the set \n\n\n\n\n\nH\n\n\n\n\n{\\displaystyle {\\mathcal {H}}}\n\n of hypotheses being considered. If \n\n\n\n\n\nH\n\n\n\n\n{\\displaystyle {\\mathcal {H}}}\n\n is a normed space (as is the case for SVM), a particularly effective technique is to consider only those hypotheses \n\n\n\nf\n\n\n{\\displaystyle f}\n\n for which \n\n\n\n\u2016\nf\n\n\u2016\n\n\nH\n\n\n\n<\nk\n\n\n{\\displaystyle \\lVert f\\rVert _{\\mathcal {H}}<k}\n\n . This is equivalent to imposing a regularization penalty \n\n\n\n\n\nR\n\n\n(\nf\n)\n=\n\n\u03bb\n\nk\n\n\n\u2016\nf\n\n\u2016\n\n\nH\n\n\n\n\n\n{\\displaystyle {\\mathcal {R}}(f)=\\lambda _{k}\\lVert f\\rVert _{\\mathcal {H}}}\n\n, and solving the new optimization problem\n \n\n\n\n\n\n\nf\n^\n\n\n\n=\n\na\nr\ng\n\n\nmin\n\nf\n\u2208\n\n\nH\n\n\n\n\n\n\n\n\u03b5\n^\n\n\n\n(\nf\n)\n+\n\n\nR\n\n\n(\nf\n)\n.\n\n\n{\\displaystyle {\\hat {f}}=\\mathrm {arg} \\min _{f\\in {\\mathcal {H}}}{\\hat {\\varepsilon }}(f)+{\\mathcal {R}}(f).}\n\n\n This approach is called Tikhonov regularization.\n More generally, \n\n\n\n\n\nR\n\n\n(\nf\n)\n\n\n{\\displaystyle {\\mathcal {R}}(f)}\n\n can be some measure of the complexity of the hypothesis \n\n\n\nf\n\n\n{\\displaystyle f}\n\n, so that simpler hypotheses are preferred.\n Recall that the (soft-margin) SVM classifier \n\n\n\n\n\n\n\nw\n\n^\n\n\n\n,\nb\n:\n\nx\n\n\u21a6\nsgn\n\u2061\n(\n\n\n\n\n\nw\n\n^\n\n\n\n\n\nT\n\n\n\n\nx\n\n\u2212\nb\n)\n\n\n{\\displaystyle {\\hat {\\mathbf {w} }},b:\\mathbf {x} \\mapsto \\operatorname {sgn}({\\hat {\\mathbf {w} }}^{\\mathsf {T}}\\mathbf {x} -b)}\n\n is chosen to minimize the following expression:\n \n\n\n\n\n[\n\n\n\n1\nn\n\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\nmax\n\n(\n\n0\n,\n1\n\u2212\n\ny\n\ni\n\n\n(\n\n\nw\n\n\n\nT\n\n\n\n\nx\n\n\u2212\nb\n)\n\n)\n\n\n]\n\n+\n\u03bb\n\u2016\n\nw\n\n\n\u2016\n\n2\n\n\n.\n\n\n{\\displaystyle \\left[{\\frac {1}{n}}\\sum _{i=1}^{n}\\max \\left(0,1-y_{i}(\\mathbf {w} ^{\\mathsf {T}}\\mathbf {x} -b)\\right)\\right]+\\lambda \\|\\mathbf {w} \\|^{2}.}\n\n\n In light of the above discussion, we see that the SVM technique is equivalent to empirical risk minimization with Tikhonov regularization, where in this case the loss function is the hinge loss\n \n\n\n\n\u2113\n(\ny\n,\nz\n)\n=\nmax\n\n(\n\n0\n,\n1\n\u2212\ny\nz\n\n)\n\n.\n\n\n{\\displaystyle \\ell (y,z)=\\max \\left(0,1-yz\\right).}\n\n\n From this perspective, SVM is closely related to other fundamental classification algorithms such as regularized least-squares and logistic regression. The difference between the three lies in the choice of loss function: regularized least-squares amounts to empirical risk minimization with the square-loss,  \n\n\n\n\n\u2113\n\ns\nq\n\n\n(\ny\n,\nz\n)\n=\n(\ny\n\u2212\nz\n\n)\n\n2\n\n\n\n\n{\\displaystyle \\ell _{sq}(y,z)=(y-z)^{2}}\n\n; logistic regression employs the log-loss,\n \n\n\n\n\n\u2113\n\nlog\n\n\n(\ny\n,\nz\n)\n=\nln\n\u2061\n(\n1\n+\n\ne\n\n\u2212\ny\nz\n\n\n)\n.\n\n\n{\\displaystyle \\ell _{\\log }(y,z)=\\ln(1+e^{-yz}).}\n\n\n The difference between the hinge loss and these other loss functions is best stated in terms of target functions - the function that minimizes expected risk for a given pair of random variables \n\n\n\nX\n,\n\ny\n\n\n{\\displaystyle X,\\,y}\n\n.\n In particular, let \n\n\n\n\ny\n\nx\n\n\n\n\n{\\displaystyle y_{x}}\n\n denote \n\n\n\ny\n\n\n{\\displaystyle y}\n\n conditional on the event that \n\n\n\nX\n=\nx\n\n\n{\\displaystyle X=x}\n\n.  In the classification setting, we have:\n \n\n\n\n\ny\n\nx\n\n\n=\n\n\n{\n\n\n\n1\n\n\n\nwith probability\u00a0\n\n\np\n\nx\n\n\n\n\n\n\n\u2212\n1\n\n\n\nwith probability\u00a0\n\n1\n\u2212\n\np\n\nx\n\n\n\n\n\n\n\n\n\n\n{\\displaystyle y_{x}={\\begin{cases}1&{\\text{with probability }}p_{x}\\\\-1&{\\text{with probability }}1-p_{x}\\end{cases}}}\n\n\n The optimal classifier is therefore:\n \n\n\n\n\nf\n\n\u2217\n\n\n(\nx\n)\n=\n\n\n{\n\n\n\n1\n\n\n\nif\u00a0\n\n\np\n\nx\n\n\n\u2265\n1\n\n/\n\n2\n\n\n\n\n\u2212\n1\n\n\n\notherwise\n\n\n\n\n\n\n\n\n\n{\\displaystyle f^{*}(x)={\\begin{cases}1&{\\text{if }}p_{x}\\geq 1/2\\\\-1&{\\text{otherwise}}\\end{cases}}}\n\n\n For the square-loss, the target function is the conditional expectation function, \n\n\n\n\nf\n\ns\nq\n\n\n(\nx\n)\n=\n\nE\n\n\n[\n\ny\n\nx\n\n\n]\n\n\n\n{\\displaystyle f_{sq}(x)=\\mathbb {E} \\left[y_{x}\\right]}\n\n; For the logistic loss, it's the logit function, \n\n\n\n\nf\n\nlog\n\n\n(\nx\n)\n=\nln\n\u2061\n\n(\n\n\np\n\nx\n\n\n\n/\n\n(\n\n1\n\u2212\n\np\n\nx\n\n\n\n)\n\n)\n\n\n\n{\\displaystyle f_{\\log }(x)=\\ln \\left(p_{x}/({1-p_{x}})\\right)}\n\n. While both of these target functions yield the correct classifier, as \n\n\n\nsgn\n\u2061\n(\n\nf\n\ns\nq\n\n\n)\n=\nsgn\n\u2061\n(\n\nf\n\nlog\n\n\n)\n=\n\nf\n\n\u2217\n\n\n\n\n{\\displaystyle \\operatorname {sgn}(f_{sq})=\\operatorname {sgn}(f_{\\log })=f^{*}}\n\n, they give us more information than we need. In fact, they give us enough information to completely describe the distribution of \n\n\n\n\ny\n\nx\n\n\n\n\n{\\displaystyle y_{x}}\n\n.\n On the other hand, one can check that the target function for the hinge loss is exactly \n\n\n\n\nf\n\n\u2217\n\n\n\n\n{\\displaystyle f^{*}}\n\n. Thus, in a sufficiently rich hypothesis space\u2014or equivalently, for an appropriately chosen kernel\u2014the SVM classifier will converge to the simplest function (in terms of \n\n\n\n\n\nR\n\n\n\n\n{\\displaystyle {\\mathcal {R}}}\n\n) that correctly classifies the data. This extends the geometric interpretation of SVM\u2014for linear classification, the empirical risk is minimized by any function whose margins lie between the support vectors, and the simplest of these is the max-margin classifier.[26]\n SVMs belong to a family of generalized linear classifiers and can be interpreted as an extension of the perceptron.[27] They can also be considered a special case of Tikhonov regularization. A special property is that they simultaneously minimize the empirical classification error and maximize the geometric margin; hence they are also known as maximum margin classifiers.\n A comparison of the SVM to other classifiers has been made by Meyer, Leisch and Hornik.[28]\n The effectiveness of SVM depends on the selection of kernel, the kernel's parameters, and soft margin parameter \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n.\nA common choice is a Gaussian kernel, which has a single parameter \n\n\n\n\u03b3\n\n\n{\\displaystyle \\gamma }\n\n. The best combination of \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n and \n\n\n\n\u03b3\n\n\n{\\displaystyle \\gamma }\n\n is often selected by a grid search with exponentially growing sequences of \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n and \n\n\n\n\u03b3\n\n\n{\\displaystyle \\gamma }\n\n, for example, \n\n\n\n\u03bb\n\u2208\n{\n\n2\n\n\u2212\n5\n\n\n,\n\n2\n\n\u2212\n3\n\n\n,\n\u2026\n,\n\n2\n\n13\n\n\n,\n\n2\n\n15\n\n\n}\n\n\n{\\displaystyle \\lambda \\in \\{2^{-5},2^{-3},\\dots ,2^{13},2^{15}\\}}\n\n; \n\n\n\n\u03b3\n\u2208\n{\n\n2\n\n\u2212\n15\n\n\n,\n\n2\n\n\u2212\n13\n\n\n,\n\u2026\n,\n\n2\n\n1\n\n\n,\n\n2\n\n3\n\n\n}\n\n\n{\\displaystyle \\gamma \\in \\{2^{-15},2^{-13},\\dots ,2^{1},2^{3}\\}}\n\n. Typically, each combination of parameter choices is checked using cross validation, and the parameters with best cross-validation accuracy are picked. Alternatively, recent work in Bayesian optimization can be used to select \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n and \n\n\n\n\u03b3\n\n\n{\\displaystyle \\gamma }\n\n , often requiring the evaluation of far fewer parameter combinations than grid search. The final model, which is used for testing and for classifying new data, is then trained on the whole training set using the selected parameters.[29]\n Potential drawbacks of the SVM include the following aspects:\n Multiclass SVM aims to assign labels to instances by using support vector machines, where the labels are drawn from a finite set of several elements.\n The dominant approach for doing so is to reduce the single multiclass problem into multiple binary classification problems.[30] Common methods for such reduction include:[30][31]\n Crammer and Singer proposed a multiclass SVM method which casts the multiclass classification problem into a single optimization problem, rather than decomposing it into multiple binary classification problems.[34] See also Lee, Lin and Wahba[35][36] and Van den Burg and Groenen.[37]\n Transductive support vector machines extend SVMs in that they could also treat partially labeled data in semi-supervised learning by following the principles of transduction. Here, in addition to the training set \n\n\n\n\n\nD\n\n\n\n\n{\\displaystyle {\\mathcal {D}}}\n\n, the learner is also given a set\n \n\n\n\n\n\n\nD\n\n\n\n\u22c6\n\n\n=\n{\n\n\nx\n\n\ni\n\n\n\u22c6\n\n\n\u2223\n\n\nx\n\n\ni\n\n\n\u22c6\n\n\n\u2208\n\n\nR\n\n\np\n\n\n\n}\n\ni\n=\n1\n\n\nk\n\n\n\n\n{\\displaystyle {\\mathcal {D}}^{\\star }=\\{\\mathbf {x} _{i}^{\\star }\\mid \\mathbf {x} _{i}^{\\star }\\in \\mathbb {R} ^{p}\\}_{i=1}^{k}}\n\n\n of test examples to be classified. Formally, a transductive support vector machine is defined by the following primal optimization problem:[38]\n Minimize (in \n\n\n\n\nw\n\n,\nb\n,\n\n\ny\n\n\n\u22c6\n\n\n\n\n{\\displaystyle \\mathbf {w} ,b,\\mathbf {y} ^{\\star }}\n\n)\n \n\n\n\n\n\n1\n2\n\n\n\u2016\n\nw\n\n\n\u2016\n\n2\n\n\n\n\n{\\displaystyle {\\frac {1}{2}}\\|\\mathbf {w} \\|^{2}}\n\n\n subject to (for any \n\n\n\ni\n=\n1\n,\n\u2026\n,\nn\n\n\n{\\displaystyle i=1,\\dots ,n}\n\n and any \n\n\n\nj\n=\n1\n,\n\u2026\n,\nk\n\n\n{\\displaystyle j=1,\\dots ,k}\n\n)\n \n\n\n\n\n\n\n\n\n\ny\n\ni\n\n\n(\n\nw\n\n\u22c5\n\n\nx\n\n\ni\n\n\n\u2212\nb\n)\n\u2265\n1\n,\n\n\n\n\n\n\ny\n\nj\n\n\n\u22c6\n\n\n(\n\nw\n\n\u22c5\n\n\nx\n\n\nj\n\n\n\u22c6\n\n\n\u2212\nb\n)\n\u2265\n1\n,\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}&y_{i}(\\mathbf {w} \\cdot \\mathbf {x} _{i}-b)\\geq 1,\\\\&y_{j}^{\\star }(\\mathbf {w} \\cdot \\mathbf {x} _{j}^{\\star }-b)\\geq 1,\\end{aligned}}}\n\n\n and\n \n\n\n\n\ny\n\nj\n\n\n\u22c6\n\n\n\u2208\n{\n\u2212\n1\n,\n1\n}\n.\n\n\n{\\displaystyle y_{j}^{\\star }\\in \\{-1,1\\}.}\n\n\n Transductive support vector machines were introduced by Vladimir N. Vapnik in 1998.\n Structured support-vector machine is an extension of the traditional SVM model. While the SVM model is primarily designed for binary classification, multiclass classification, and regression tasks, structured SVM broadens its application to handle general structured output labels, for example parse trees, classification with taxonomies, sequence alignment and many more.[39]\n A version of SVM for regression was proposed in 1996 by Vladimir N. Vapnik, Harris Drucker, Christopher J. C. Burges, Linda Kaufman and Alexander J. Smola.[40] This method is called support vector regression (SVR). The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by SVR depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction. Another SVM version known as least-squares support vector machine (LS-SVM) has been proposed by Suykens and Vandewalle.[41]\n Training the original SVR means solving[42]\n where \n\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystyle x_{i}}\n\n is a training sample with target value \n\n\n\n\ny\n\ni\n\n\n\n\n{\\displaystyle y_{i}}\n\n. The inner product plus intercept \n\n\n\n\u27e8\nw\n,\n\nx\n\ni\n\n\n\u27e9\n+\nb\n\n\n{\\displaystyle \\langle w,x_{i}\\rangle +b}\n\n is the prediction for that sample, and \n\n\n\n\u03b5\n\n\n{\\displaystyle \\varepsilon }\n\n is a free parameter that serves as a threshold: all predictions have to be within an \n\n\n\n\u03b5\n\n\n{\\displaystyle \\varepsilon }\n\n range of the true predictions. Slack variables are usually added into the above to allow for errors and to allow approximation in the case the above problem is infeasible.\n In 2011 it was shown by Polson and Scott that the SVM admits a Bayesian interpretation through the technique of data augmentation.[43] In this approach the SVM is viewed as a graphical model (where the parameters are connected via probability distributions). This extended view allows the application of Bayesian techniques to SVMs, such as flexible feature modeling, automatic hyperparameter tuning, and predictive uncertainty quantification. Recently, a scalable version of the Bayesian SVM was developed by Florian Wenzel, enabling the application of Bayesian SVMs to big data.[44] Florian Wenzel developed two different versions, a variational inference (VI) scheme for the Bayesian kernel support vector machine (SVM) and a stochastic version (SVI) for the linear Bayesian SVM.[45]\n The parameters of the maximum-margin hyperplane are derived by solving the optimization. There exist several specialized algorithms for quickly solving the quadratic programming (QP) problem that arises from SVMs, mostly relying on heuristics for breaking the problem down into smaller, more manageable chunks.\n Another approach is to use an interior-point method that uses Newton-like iterations to find a solution of the Karush\u2013Kuhn\u2013Tucker conditions of the primal and dual problems.[46]\nInstead of solving a sequence of broken-down problems, this approach directly solves the problem altogether. To avoid solving a linear system involving the large kernel matrix, a low-rank approximation to the matrix is often used in the kernel trick.\n Another common method is Platt's sequential minimal optimization (SMO) algorithm, which breaks the problem down into 2-dimensional sub-problems that are solved analytically, eliminating the need for a numerical optimization algorithm and matrix storage. This algorithm is conceptually simple, easy to implement, generally faster, and has better scaling properties for difficult SVM problems.[47]\n The special case of linear support vector machines can be solved more efficiently by the same kind of algorithms used to optimize its close cousin, logistic regression; this class of algorithms includes sub-gradient descent (e.g., PEGASOS[48]) and coordinate descent (e.g., LIBLINEAR[49]). LIBLINEAR has some attractive training-time properties. Each convergence iteration takes time linear in the time taken to read the train data, and the iterations also have a Q-linear convergence property, making the algorithm extremely fast.\n The general kernel SVMs can also be solved more efficiently using sub-gradient descent (e.g. P-packSVM[50]), especially when parallelization is allowed.\n Kernel SVMs are available in many machine-learning toolkits, including LIBSVM, MATLAB, SAS, SVMlight, kernlab, scikit-learn, Shogun, Weka, Shark, JKernelMachines, OpenCV and others.\n Preprocessing of data (standardization) is highly recommended to enhance accuracy of classification.[51] There are a few methods of standardization, such as min-max, normalization by decimal scaling, Z-score.[52] Subtraction of mean and division by variance of each feature is usually used for SVM.[53]\n",
        "doc_number": 82
    },
    {
        "url": "https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm",
        "content": "In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method. It was first developed by Evelyn Fix and Joseph Hodges in 1951,[1] and later expanded by Thomas Cover.[2] \nMost often, it is used for classification, as a k-NN classifier, the output of which is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k\u00a0=\u00a01, then the object is simply assigned to the class of that single nearest neighbor.\n The k-NN algorithm can also be generalized for regression. In k-NN regression, also known as nearest neighbor smoothing, the output is the property value for the object. This value is the average of the values of k nearest neighbors. If k\u00a0=\u00a01, then the output is simply assigned to the value of that single nearest neighbor, also known as nearest neighbor interpolation.\n For both classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that nearer neighbors contribute more to the average than distant ones. For example, a common weighting scheme consists of giving each neighbor a weight of 1/d, where d is the distance to the neighbor.[3]\n The input consists of the k closest training examples in a data set. \nThe neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.\n A peculiarity (sometimes even a disadvantage) of the k-NN algorithm is its sensitivity to the local structure of the data.\nIn k-NN classification the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance, if the features represent different physical units or come in vastly different scales, then feature-wise normalizing of the training data can greatly improve its accuracy.[4]\n Suppose we have pairs \n\n\n\n(\n\nX\n\n1\n\n\n,\n\nY\n\n1\n\n\n)\n,\n(\n\nX\n\n2\n\n\n,\n\nY\n\n2\n\n\n)\n,\n\u2026\n,\n(\n\nX\n\nn\n\n\n,\n\nY\n\nn\n\n\n)\n\n\n{\\displaystyle (X_{1},Y_{1}),(X_{2},Y_{2}),\\dots ,(X_{n},Y_{n})}\n\n taking values in \n\n\n\n\n\nR\n\n\nd\n\n\n\u00d7\n{\n1\n,\n2\n}\n\n\n{\\displaystyle \\mathbb {R} ^{d}\\times \\{1,2\\}}\n\n, where Y is the class label of X, so that \n\n\n\nX\n\n|\n\nY\n=\nr\n\u223c\n\nP\n\nr\n\n\n\n\n{\\displaystyle X|Y=r\\sim P_{r}}\n\n for \n\n\n\nr\n=\n1\n,\n2\n\n\n{\\displaystyle r=1,2}\n\n (and probability distributions \n\n\n\n\nP\n\nr\n\n\n\n\n{\\displaystyle P_{r}}\n\n). Given some norm \n\n\n\n\u2016\n\u22c5\n\u2016\n\n\n{\\displaystyle \\|\\cdot \\|}\n\n on \n\n\n\n\n\nR\n\n\nd\n\n\n\n\n{\\displaystyle \\mathbb {R} ^{d}}\n\n and a point \n\n\n\nx\n\u2208\n\n\nR\n\n\nd\n\n\n\n\n{\\displaystyle x\\in \\mathbb {R} ^{d}}\n\n, let \n\n\n\n(\n\nX\n\n(\n1\n)\n\n\n,\n\nY\n\n(\n1\n)\n\n\n)\n,\n\u2026\n,\n(\n\nX\n\n(\nn\n)\n\n\n,\n\nY\n\n(\nn\n)\n\n\n)\n\n\n{\\displaystyle (X_{(1)},Y_{(1)}),\\dots ,(X_{(n)},Y_{(n)})}\n\n be a reordering of the training data such that \n\n\n\n\u2016\n\nX\n\n(\n1\n)\n\n\n\u2212\nx\n\u2016\n\u2264\n\u22ef\n\u2264\n\u2016\n\nX\n\n(\nn\n)\n\n\n\u2212\nx\n\u2016\n\n\n{\\displaystyle \\|X_{(1)}-x\\|\\leq \\dots \\leq \\|X_{(n)}-x\\|}\n\n.\n The training examples are vectors in a multidimensional feature space, each with a class label. The training phase of the algorithm consists only of storing the feature vectors and class labels of the training samples.\n In the classification phase, k is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the k training samples nearest to that query point.\n A commonly used distance metric for continuous variables is Euclidean distance. For discrete variables, such as for text classification, another metric can be used, such as the overlap metric (or Hamming distance). In the context of gene expression microarray data, for example, k-NN has been employed with correlation coefficients, such as Pearson and Spearman, as a metric.[5] Often, the classification accuracy of k-NN can be improved significantly if the distance metric is learned with specialized algorithms such as Large Margin Nearest Neighbor or Neighbourhood components analysis.\n A drawback of the basic \"majority voting\" classification occurs when the class distribution is skewed. That is, examples of a more frequent class tend to dominate the prediction of the new example, because they tend to be common among the k nearest neighbors due to their large number.[6] One way to overcome this problem is to weight the classification, taking into account the distance from the test point to each of its k nearest neighbors. The class (or value, in regression problems) of each of the k nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point. Another way to overcome skew is by abstraction in data representation. For example, in a self-organizing map (SOM), each node is a representative (a center) of a cluster of similar points, regardless of their density in the original training data. K-NN can then be applied to the SOM.\n The best choice of k depends upon the data; generally, larger values of k reduces effect of the noise on the classification,[7] but make boundaries between classes less distinct. A good k can be selected by various heuristic techniques (see hyperparameter optimization). The special case where the class is predicted to be the class of the closest training sample (i.e. when k = 1) is called the nearest neighbor algorithm.\n The accuracy of the k-NN algorithm can be severely degraded by the presence of noisy or irrelevant features, or if the feature scales are not consistent with their importance. Much research effort has been put into selecting or scaling features to improve classification. A particularly popular[citation needed] approach is the use of evolutionary algorithms to optimize feature scaling.[8] Another popular approach is to scale features by the mutual information of the training data with the training classes.[citation needed]\n In binary (two class) classification problems, it is helpful to choose k to be an odd number as this avoids tied votes. One popular way of choosing the empirically optimal k in this setting is via bootstrap method.[9]\n The most intuitive nearest neighbour type classifier is the one nearest neighbour classifier that assigns a point x to the class of its closest neighbour in the feature space, that is \n\n\n\n\nC\n\nn\n\n\n1\nn\nn\n\n\n(\nx\n)\n=\n\nY\n\n(\n1\n)\n\n\n\n\n{\\displaystyle C_{n}^{1nn}(x)=Y_{(1)}}\n\n.\n As the size of training data set approaches infinity, the one nearest neighbour classifier guarantees an error rate of no worse than twice the Bayes error rate (the minimum achievable error rate given the distribution of the data).\n The k-nearest neighbour classifier can be viewed as assigning the k nearest neighbours a weight \n\n\n\n1\n\n/\n\nk\n\n\n{\\displaystyle 1/k}\n\n and all others 0 weight. This can be generalised to weighted nearest neighbour classifiers. That is, where the ith nearest neighbour is assigned a weight \n\n\n\n\nw\n\nn\ni\n\n\n\n\n{\\displaystyle w_{ni}}\n\n, with \n\n\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\nw\n\nn\ni\n\n\n=\n1\n\n\n{\\textstyle \\sum _{i=1}^{n}w_{ni}=1}\n\n. An analogous result on the strong consistency of weighted nearest neighbour classifiers also holds.[10]\n Let \n\n\n\n\nC\n\nn\n\n\nw\nn\nn\n\n\n\n\n{\\displaystyle C_{n}^{wnn}}\n\n denote the weighted nearest classifier with weights \n\n\n\n{\n\nw\n\nn\ni\n\n\n\n}\n\ni\n=\n1\n\n\nn\n\n\n\n\n{\\displaystyle \\{w_{ni}\\}_{i=1}^{n}}\n\n. Subject to regularity conditions, which in asymptotic theory are conditional variables which require assumptions to differentiate among parameters with some criteria. On the class distributions the excess risk has the following asymptotic expansion[11]\n\n\n\n\n\n\n\nR\n\n\n\n\nR\n\n\n\n(\n\nC\n\nn\n\n\nw\nn\nn\n\n\n)\n\u2212\n\n\n\nR\n\n\n\n\nR\n\n\n\n(\n\nC\n\nBayes\n\n\n)\n=\n\n(\n\n\nB\n\n1\n\n\n\ns\n\nn\n\n\n2\n\n\n+\n\nB\n\n2\n\n\n\nt\n\nn\n\n\n2\n\n\n\n)\n\n{\n1\n+\no\n(\n1\n)\n}\n,\n\n\n{\\displaystyle {\\mathcal {R}}_{\\mathcal {R}}(C_{n}^{wnn})-{\\mathcal {R}}_{\\mathcal {R}}(C^{\\text{Bayes}})=\\left(B_{1}s_{n}^{2}+B_{2}t_{n}^{2}\\right)\\{1+o(1)\\},}\n\n\nfor constants \n\n\n\n\nB\n\n1\n\n\n\n\n{\\displaystyle B_{1}}\n\n and \n\n\n\n\nB\n\n2\n\n\n\n\n{\\displaystyle B_{2}}\n\n where \n\n\n\n\ns\n\nn\n\n\n2\n\n\n=\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\nw\n\nn\ni\n\n\n2\n\n\n\n\n{\\displaystyle s_{n}^{2}=\\sum _{i=1}^{n}w_{ni}^{2}}\n\n and \n\n\n\n\nt\n\nn\n\n\n=\n\nn\n\n\u2212\n2\n\n/\n\nd\n\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\nw\n\nn\ni\n\n\n\n{\n\n\ni\n\n1\n+\n2\n\n/\n\nd\n\n\n\u2212\n(\ni\n\u2212\n1\n\n)\n\n1\n+\n2\n\n/\n\nd\n\n\n\n}\n\n\n\n{\\displaystyle t_{n}=n^{-2/d}\\sum _{i=1}^{n}w_{ni}\\left\\{i^{1+2/d}-(i-1)^{1+2/d}\\right\\}}\n\n.\n The optimal weighting scheme \n\n\n\n{\n\nw\n\nn\ni\n\n\n\u2217\n\n\n\n}\n\ni\n=\n1\n\n\nn\n\n\n\n\n{\\displaystyle \\{w_{ni}^{*}\\}_{i=1}^{n}}\n\n, that balances the two terms in the display above, is given as follows: set \n\n\n\n\nk\n\n\u2217\n\n\n=\n\u230a\nB\n\nn\n\n\n4\n\nd\n+\n4\n\n\n\n\n\u230b\n\n\n{\\displaystyle k^{*}=\\lfloor Bn^{\\frac {4}{d+4}}\\rfloor }\n\n, \n\n\n\n\n\nw\n\nn\ni\n\n\n\u2217\n\n\n=\n\n\n1\n\nk\n\n\u2217\n\n\n\n\n\n[\n\n1\n+\n\n\nd\n2\n\n\n\u2212\n\n\nd\n\n2\n\n\n\nk\n\n\u2217\n\n\n\n\n2\n\n/\n\nd\n\n\n\n\n\n{\n\ni\n\n1\n+\n2\n\n/\n\nd\n\n\n\u2212\n(\ni\n\u2212\n1\n\n)\n\n1\n+\n2\n\n/\n\nd\n\n\n}\n\n]\n\n\n\n{\\displaystyle w_{ni}^{*}={\\frac {1}{k^{*}}}\\left[1+{\\frac {d}{2}}-{\\frac {d}{2{k^{*}}^{2/d}}}\\{i^{1+2/d}-(i-1)^{1+2/d}\\}\\right]}\n\n for \n\n\n\ni\n=\n1\n,\n2\n,\n\u2026\n,\n\nk\n\n\u2217\n\n\n\n\n{\\displaystyle i=1,2,\\dots ,k^{*}}\n\n and \n\n\n\n\n\nw\n\nn\ni\n\n\n\u2217\n\n\n=\n0\n\n\n{\\displaystyle w_{ni}^{*}=0}\n\n for \n\n\n\ni\n=\n\nk\n\n\u2217\n\n\n+\n1\n,\n\u2026\n,\nn\n\n\n{\\displaystyle i=k^{*}+1,\\dots ,n}\n\n.\n With optimal weights the dominant term in the asymptotic expansion of the excess risk is \n\n\n\n\n\nO\n\n\n(\n\nn\n\n\u2212\n\n\n4\n\nd\n+\n4\n\n\n\n\n\n)\n\n\n{\\displaystyle {\\mathcal {O}}(n^{-{\\frac {4}{d+4}}})}\n\n. Similar results are true when using a bagged nearest neighbour classifier.\n k-NN is a special case of a variable-bandwidth, kernel density \"balloon\" estimator with a uniform kernel.[12][13]\n The naive version of the algorithm is easy to implement by computing the distances from the test example to all stored examples, but it is computationally intensive for large training sets. Using an approximate nearest neighbor search algorithm makes k-NN computationally tractable even for large data sets. Many nearest neighbor search algorithms have been proposed over the years; these generally seek to reduce the number of distance evaluations actually performed.\n k-NN has some strong consistency results. As the amount of data approaches infinity, the two-class k-NN algorithm is guaranteed to yield an error rate no worse than twice the Bayes error rate (the minimum achievable error rate given the distribution of the data).[2] Various improvements to the k-NN speed are possible by using proximity graphs.[14]\n For multi-class k-NN classification, Cover and Hart (1967) prove an upper bound error rate of\n\n\n\n\n\nR\n\n\u2217\n\n\n\u00a0\n\u2264\n\u00a0\n\nR\n\nk\n\nN\nN\n\n\n\n\u00a0\n\u2264\n\u00a0\n\nR\n\n\u2217\n\n\n\n(\n\n2\n\u2212\n\n\n\nM\n\nR\n\n\u2217\n\n\n\n\nM\n\u2212\n1\n\n\n\n\n)\n\n\n\n{\\displaystyle R^{*}\\ \\leq \\ R_{k\\mathrm {NN} }\\ \\leq \\ R^{*}\\left(2-{\\frac {MR^{*}}{M-1}}\\right)}\n\n\nwhere \n\n\n\n\nR\n\n\u2217\n\n\n\n\n{\\displaystyle R^{*}}\n\n is the Bayes error rate (which is the minimal error rate possible), \n\n\n\n\nR\n\nk\nN\nN\n\n\n\n\n{\\displaystyle R_{kNN}}\n\n is the asymptotic k-NN error rate, and M is the number of classes in the problem. This bound is tight in the sense that both the lower and upper bounds are achievable by some distribution.[15] For \n\n\n\nM\n=\n2\n\n\n{\\displaystyle M=2}\n\n and as the Bayesian error rate \n\n\n\n\nR\n\n\u2217\n\n\n\n\n{\\displaystyle R^{*}}\n\n approaches zero, this limit reduces to \"not more than twice the Bayesian error rate\".\n There are many results on the error rate of the k nearest neighbour classifiers.[16] The k-nearest neighbour classifier is strongly (that is for any joint distribution on \n\n\n\n(\nX\n,\nY\n)\n\n\n{\\displaystyle (X,Y)}\n\n) consistent provided \n\n\n\nk\n:=\n\nk\n\nn\n\n\n\n\n{\\displaystyle k:=k_{n}}\n\n diverges and \n\n\n\n\nk\n\nn\n\n\n\n/\n\nn\n\n\n{\\displaystyle k_{n}/n}\n\n converges to zero as \n\n\n\nn\n\u2192\n\u221e\n\n\n{\\displaystyle n\\to \\infty }\n\n.\n Let \n\n\n\n\nC\n\nn\n\n\nk\nn\nn\n\n\n\n\n{\\displaystyle C_{n}^{knn}}\n\n denote the k nearest neighbour classifier based on a training set of size n. Under certain regularity conditions, the excess risk yields the following asymptotic expansion[11]\n\n\n\n\n\n\n\nR\n\n\n\n\nR\n\n\n\n(\n\nC\n\nn\n\n\nk\nn\nn\n\n\n)\n\u2212\n\n\n\nR\n\n\n\n\nR\n\n\n\n(\n\nC\n\nBayes\n\n\n)\n=\n\n{\n\n\nB\n\n1\n\n\n\n\n1\nk\n\n\n+\n\nB\n\n2\n\n\n\n\n(\n\n\nk\nn\n\n\n)\n\n\n4\n\n/\n\nd\n\n\n\n}\n\n{\n1\n+\no\n(\n1\n)\n}\n,\n\n\n{\\displaystyle {\\mathcal {R}}_{\\mathcal {R}}(C_{n}^{knn})-{\\mathcal {R}}_{\\mathcal {R}}(C^{\\text{Bayes}})=\\left\\{B_{1}{\\frac {1}{k}}+B_{2}\\left({\\frac {k}{n}}\\right)^{4/d}\\right\\}\\{1+o(1)\\},}\n\n\nfor some constants \n\n\n\n\nB\n\n1\n\n\n\n\n{\\displaystyle B_{1}}\n\n and \n\n\n\n\nB\n\n2\n\n\n\n\n{\\displaystyle B_{2}}\n\n.\n The choice \n\n\n\n\nk\n\n\u2217\n\n\n=\n\n\u230a\n\nB\n\nn\n\n\n4\n\nd\n+\n4\n\n\n\n\n\n\u230b\n\n\n\n{\\displaystyle k^{*}=\\left\\lfloor Bn^{\\frac {4}{d+4}}\\right\\rfloor }\n\n offers a trade off between the two terms in the above display, for which the \n\n\n\n\nk\n\n\u2217\n\n\n\n\n{\\displaystyle k^{*}}\n\n-nearest neighbour error converges to the Bayes error at the optimal (minimax) rate \n\n\n\n\n\nO\n\n\n\n(\n\nn\n\n\u2212\n\n\n4\n\nd\n+\n4\n\n\n\n\n\n)\n\n\n\n{\\displaystyle {\\mathcal {O}}\\left(n^{-{\\frac {4}{d+4}}}\\right)}\n\n.\n The K-nearest neighbor classification performance can often be significantly improved through (supervised) metric learning. Popular algorithms are neighbourhood components analysis and large margin nearest neighbor. Supervised metric learning algorithms use the label information to learn a new metric or pseudo-metric.\n When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters) then the input data will be transformed into a reduced representation set of features (also named features vector). Transforming the input data into the set of features is called feature extraction. If the features extracted are carefully chosen it is expected that the features set will extract the relevant information from the input data in order to perform the desired task using this reduced representation instead of the full size input. Feature extraction is performed on raw data prior to applying k-NN algorithm on the transformed data in feature space.\n An example of a typical computer vision computation pipeline for face recognition using k-NN including feature extraction and dimension reduction pre-processing steps (usually implemented with OpenCV):\n For high-dimensional data (e.g., with number of dimensions more than 10) dimension reduction is usually performed prior to applying the k-NN algorithm in order to avoid the effects of the curse of dimensionality.[17]\n The curse of dimensionality in the k-NN context basically means that Euclidean distance is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector (imagine multiple points lying more or less on a circle with the query point at the center; the distance from the query to all data points in the search space is almost the same).\n Feature extraction and dimension reduction can be combined in one step using principal component analysis (PCA),  linear discriminant analysis (LDA), or canonical correlation analysis (CCA) techniques as a pre-processing step, followed by clustering by k-NN on feature vectors in reduced-dimension space. This process is also called low-dimensional embedding.[18]\n For very-high-dimensional datasets (e.g. when performing a similarity search on live video streams, DNA data or high-dimensional time series) running a fast approximate k-NN search using locality sensitive hashing, \"random projections\",[19] \"sketches\"[20] or other high-dimensional similarity search techniques from the VLDB toolbox might be the only feasible option.\n Nearest neighbor rules in effect implicitly compute the decision boundary. It is also possible to compute the decision boundary explicitly, and to do so efficiently, so that the computational complexity is a function of the boundary complexity.[21]\n Data reduction is one of the most important problems for work with huge data sets. Usually, only some of the data points are needed for accurate classification. Those data are called the prototypes and can be found as follows:\n A training example surrounded by examples of other classes is called a class outlier. Causes of class outliers include:\n Class outliers with k-NN produce noise. They can be detected and separated for future analysis. Given two natural numbers, k>r>0, a training example is called a (k,r)NN class-outlier if its k nearest neighbors include more than r examples of other classes.\n Condensed nearest neighbor (CNN, the Hart algorithm) is an algorithm designed to reduce the data set for k-NN classification.[22] It selects the set of prototypes U from the training data, such that 1NN with U can classify the examples almost as accurately as 1NN does with the whole data set.\n Given a training set X, CNN works iteratively:\n Use U instead of X for classification. The examples that are not prototypes are called \"absorbed\" points.\n It is efficient to scan the training examples in order of decreasing border ratio.[23] The border ratio of a training example x is defined as \n where \u2016x-y\u2016 is the distance to the closest example y having a different color than x, and \u2016x'-y\u2016 is the distance from y to its closest example x'  with the same label as x.\n The border ratio is in the interval [0,1] because \u2016x'-y\u2016 never exceeds \u2016x-y\u2016. This ordering gives preference to the borders of the classes for inclusion in the set of prototypes U. A point of a different label than x is called external to x. The calculation of the border ratio is illustrated by the figure on the right. The data points are labeled by colors: the initial point is x and its label is red. External points are blue and green. The closest to x external point is y. The closest to y red point is x' . The border ratio a(x) = \u2016x'-y\u2016 / \u2016x-y\u2016is the attribute of the initial point x.\n Below is an illustration of CNN in a series of figures. There are three classes (red, green and blue). Fig. 1: initially there are 60 points in each class. Fig. 2 shows the 1NN classification map: each pixel is classified by 1NN using all the data. Fig. 3 shows the 5NN classification map. White areas correspond to the unclassified regions, where 5NN voting is tied (for example, if there are two green, two red and one blue points among 5 nearest neighbors). Fig. 4 shows the reduced data set. The crosses are the class-outliers selected by the (3,2)NN rule (all the three nearest neighbors of these instances belong to other classes); the squares are the prototypes, and the empty circles are the absorbed points. The left bottom corner shows the numbers of the class-outliers, prototypes and absorbed points for all three classes. The number of prototypes varies from 15% to 20% for different classes in this example. Fig. 5 shows that the 1NN classification map with the prototypes is very similar to that with the initial data set. The figures were produced using the Mirkes applet.[23]\n In k-NN regression, also known as k-NN smoothing, the k-NN algorithm is used for estimating continuous variables.[citation needed] One such algorithm uses a weighted average of the k nearest neighbors, weighted by the inverse of their distance. This algorithm works as follows:\n The distance to the kth nearest neighbor can also be seen as a local density estimate and thus is also a popular outlier score in anomaly detection. The larger the distance to the k-NN, the lower the local density, the more likely the query point is an outlier.[24] Although quite simple, this outlier model, along with another classic data mining method, local outlier factor, works quite well also in comparison to more recent and more complex approaches, according to a large scale experimental analysis.[25]\n A confusion matrix or \"matching matrix\" is often used as a tool to validate the accuracy of k-NN classification. More robust statistical methods such as likelihood-ratio test can also be applied.[how?]\n",
        "doc_number": 83
    },
    {
        "url": "https://en.wikipedia.org/wiki/Decision_tree_learning",
        "content": "Decision tree learning is a supervised learning approach used in statistics, data mining and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations.\n Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. More generally, the concept of regression tree can be extended to any kind of object equipped with pairwise dissimilarities such as categorical sequences.[1]\n Decision trees are among the most popular machine learning algorithms given their intelligibility and simplicity.[2]\n In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data (but the resulting classification tree can be an input for decision making).\n Decision tree learning is a method commonly used in data mining.[3] The goal is to create a model that predicts the value of a target variable based on several input variables.\n A decision tree is a simple representation for classifying examples. For this section, assume that all of the input features have finite discrete domains, and there is a single target feature called the \"classification\". Each element of the domain of the classification is called a class.\nA decision tree or a classification tree is a tree in which each internal (non-leaf) node is labeled with an input feature. The arcs coming from a node labeled with an input feature are labeled with each of the possible values of the target feature or the arc leads to a subordinate decision node on a different input feature. Each leaf of the tree is labeled with a class or a probability distribution over the classes, signifying that the data set has been classified by the tree into either a specific class, or into a particular probability distribution (which, if the decision tree is well-constructed, is skewed towards certain subsets of classes).\n A tree is built by splitting the source set, constituting the root node of the tree, into subsets\u2014which constitute the successor children. The splitting is based on a set of splitting rules based on classification features.[4]  This process is repeated on each derived subset in a recursive manner called recursive partitioning.\nThe recursion is completed when the subset at a node has all the same values of the target variable, or when splitting no longer adds value to the predictions. This process of top-down induction of decision trees (TDIDT)[5] is an example of a greedy algorithm, and it is by far the most common strategy for learning decision trees from data.[6]\n In data mining, decision trees can be described also as the combination of mathematical and computational techniques to aid the description, categorization and generalization of a given set of data.\n Data comes in records of the form:\n The dependent variable, \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n, is the target variable that we are trying to understand, classify or generalize. The vector \n\n\n\n\n\nx\n\n\n\n\n{\\displaystyle {\\textbf {x}}}\n\n is composed of the features, \n\n\n\n\nx\n\n1\n\n\n,\n\nx\n\n2\n\n\n,\n\nx\n\n3\n\n\n\n\n{\\displaystyle x_{1},x_{2},x_{3}}\n\n etc., that are used for that task.\n Decision trees used in data mining are of two main types:\n The term classification and regression tree (CART) analysis is an umbrella term used to refer to either of the above procedures, first introduced by Breiman et al. in 1984.[7] Trees used for regression and trees used for classification have some similarities \u2013 but also some differences, such as the procedure used to determine where to split.[7]\n Some techniques, often called ensemble methods, construct more than one decision tree:\n A special case of a decision tree is a decision list,[14] which is a one-sided decision tree, so that every internal node has exactly 1 leaf node and exactly 1 internal node as a child (except for the bottommost node, whose only child is a single leaf node).  While less expressive, decision lists are arguably easier to understand than general decision trees due to their added sparsity[citation needed], permit non-greedy learning methods[15] and monotonic constraints to be imposed.[16]\n Notable decision tree algorithms include:\n ID3 and CART were invented independently at around the same time (between 1970 and 1980)[citation needed], yet follow a similar approach for learning a decision tree from training tuples.\n It has also been proposed to leverage concepts of fuzzy set theory for the definition of a special version of decision tree, known as Fuzzy Decision Tree (FDT).[23]\nIn this type of fuzzy classification, generally, an input vector \n\n\n\n\n\nx\n\n\n\n\n{\\displaystyle {\\textbf {x}}}\n\n is associated with multiple classes, each with a different confidence value.\nBoosted ensembles of FDTs have been recently investigated as well, and they have shown performances comparable to those of other very efficient fuzzy classifiers.[24]\n Algorithms for constructing decision trees usually work top-down, by choosing a variable at each step that best splits the set of items.[6] Different algorithms use different metrics for measuring \"best\".  These generally measure the homogeneity of the target variable within the subsets. Some examples are given below. These metrics are applied to each candidate subset, and the resulting values are combined (e.g., averaged) to provide a measure of the quality of the split. Depending on the underlying metric, the performance of various heuristic algorithms for decision tree learning may vary significantly.[25]\n A simple and effective metric can be used to identify the degree to which true positives outweigh false positives (see Confusion matrix). This metric, \"Estimate of Positive Correctness\" is defined below:\n \n\n\n\n\nE\n\nP\n\n\n=\nT\nP\n\u2212\nF\nP\n\n\n{\\displaystyle E_{P}=TP-FP}\n\n\n In this equation, the total false positives (FP) are subtracted from the total true positives (TP). The resulting number gives an estimate on how many positive examples the feature could correctly identify within the data, with higher numbers meaning that the feature could correctly classify more positive samples. Below is an example of how to use the metric when the full confusion matrix of a certain feature is given:\n Feature A Confusion Matrix\n Here we can see that the TP value would be 8 and the FP value would be 2 (the underlined numbers in the table). When we plug these numbers in the equation we are able to calculate the estimate: \n\n\n\n\nE\n\np\n\n\n=\nT\nP\n\u2212\nF\nP\n=\n8\n\u2212\n2\n=\n6\n\n\n{\\displaystyle E_{p}=TP-FP=8-2=6}\n\n. This means that using the estimate on this feature would have it receive a score of 6.\n However, it should be worth noting that this number is only an estimate. For example, if two features both had a FP value of 2 while one of the features had a higher TP value, that feature would be ranked higher than the other because the resulting estimate when using the equation would give a higher value. This could lead to some inaccuracies when using the metric if some features have more positive samples than others. To combat this, one could use a more powerful metric known as Sensitivity that takes into account the proportions of the values from the confusion matrix to give the actual true positive rate (TPR). The difference between these metrics is shown in the example below:\n \n\n\n\nT\nP\nR\n=\nT\nP\n\n/\n\n(\nT\nP\n+\nF\nN\n)\n=\n8\n\n/\n\n(\n8\n+\n3\n)\n\u2248\n0.73\n\n\n{\\displaystyle TPR=TP/(TP+FN)=8/(8+3)\\approx 0.73}\n\n\n \n\n\n\nT\nP\nR\n=\nT\nP\n\n/\n\n(\nT\nP\n+\nF\nN\n)\n=\n6\n\n/\n\n(\n6\n+\n2\n)\n=\n0.75\n\n\n{\\displaystyle TPR=TP/(TP+FN)=6/(6+2)=0.75}\n\n\n In this example, Feature A had an estimate of 6 and a TPR of approximately 0.73 while Feature B had an estimate of 4 and a TPR of 0.75. This shows that although the positive estimate for some feature may be higher, the more accurate TPR value for that feature may be lower when compared to other features that have a lower positive estimate. Depending on the situation and knowledge of the data and decision trees, one may opt to use the positive estimate for a quick and easy solution to their problem. On the other hand, a more experienced user would most likely prefer to use the TPR value to rank the features because it takes into account the proportions of the data and all the samples that should have been classified as positive.\n Gini impurity, Gini's diversity index,[26] or Gini-Simpson Index in biodiversity research, is named after Italian mathematician Corrado Gini and used by the CART (classification and regression tree) algorithm for classification trees. Gini impurity measures how often a randomly chosen element of a set would be incorrectly labeled if it were labeled randomly and independently according to the distribution of labels in the set. It reaches its minimum (zero) when all cases in the node fall into a single target category.\n For a set of items with \n\n\n\nJ\n\n\n{\\displaystyle J}\n\n classes and relative frequencies \n\n\n\n\np\n\ni\n\n\n\n\n{\\displaystyle p_{i}}\n\n, \n\n\n\ni\n\u2208\n{\n1\n,\n2\n,\n.\n.\n.\n,\nJ\n}\n\n\n{\\displaystyle i\\in \\{1,2,...,J\\}}\n\n, the probability of choosing an item with label \n\n\n\ni\n\n\n{\\displaystyle i}\n\n is \n\n\n\n\np\n\ni\n\n\n\n\n{\\displaystyle p_{i}}\n\n, and the probability of miscategorizing that item is \n\n\n\n\n\u2211\n\nk\n\u2260\ni\n\n\n\np\n\nk\n\n\n=\n1\n\u2212\n\np\n\ni\n\n\n\n\n{\\displaystyle \\sum _{k\\neq i}p_{k}=1-p_{i}}\n\n. The Gini impurity is computed by summing pairwise products of these probabilities for each class label:\n The Gini impurity is also an information theoretic measure and corresponds to Tsallis Entropy with deformation coefficient \n\n\n\nq\n=\n2\n\n\n{\\displaystyle q=2}\n\n, which in physics is associated with the lack of information in out-of-equilibrium, non-extensive, dissipative and quantum systems. For the limit \n\n\n\nq\n\u2192\n1\n\n\n{\\displaystyle q\\to 1}\n\n one recovers the usual Boltzmann-Gibbs or Shannon entropy. In this sense, the Gini impurity is nothing but a variation of the usual entropy measure for decision trees.\n Used by the ID3, C4.5 and C5.0 tree-generation algorithms. Information gain is based on the concept of entropy and information content from information theory.\n Entropy is defined as below\n where \n\n\n\n\np\n\n1\n\n\n,\n\np\n\n2\n\n\n,\n\u2026\n\n\n{\\displaystyle p_{1},p_{2},\\ldots }\n\n are fractions that add up to 1 and represent the percentage of each class present in the child node that results from a split in the tree.[27]\n Averaging over the possible values of \n\n\n\nA\n\n\n{\\displaystyle A}\n\n,\n That is, the expected information gain is the mutual information, meaning that on average, the reduction in the entropy of T is the mutual information.\n Information gain is used to decide which feature to split on at each step in building the tree. Simplicity is best, so we want to keep our tree small. To do so, at each step we should choose the split that results in the most consistent child nodes. A commonly used measure of consistency is called information which is measured in bits. For each node of the tree, the information value \"represents the expected amount of information that would be needed to specify whether a new instance should be classified yes or no, given that the example reached that node\".[27]\n Consider an example data set with four attributes: outlook (sunny, overcast, rainy), temperature (hot, mild, cool), humidity (high, normal), and windy (true, false), with a binary (yes or no) target variable, play, and 14 data points. To construct a decision tree on this data, we need to compare the information gain of each of four trees, each split on one of the four features. The split with the highest information gain will be taken as the first split and the process will continue until all children nodes each have consistent data, or until the information gain is 0.\n To find the information gain of the split using windy, we must first calculate the information in the data before the split. The original data contained nine yes's and five no's.\n The split using the feature windy results in two children nodes, one for a windy value of true and one for a windy value of false. In this data set, there are six data points with a true windy value, three of which have a play (where play is the target variable) value of yes and three with a play value of no. The eight remaining data points with a windy value of false contain two no's and six yes's. The information of the windy=true node is calculated using the entropy equation above. Since there is an equal number of yes's and no's in this node, we have\n For the node where windy=false there were eight data points, six yes's and two no's. Thus we have\n To find the information of the split, we take the weighted average of these two numbers based on how many observations fell into which node.\n Now we can calculate the information gain achieved by splitting on the windy feature.\n To build the tree, the information gain of each possible first split would need to be calculated. The best first split is the one that provides the most information gain. This process is repeated for each impure node until the tree is complete. This example is adapted from the example appearing in Witten et al.[27]\n Information gain is also known as Shannon index in bio diversity research.\n Introduced in CART,[7] variance reduction is often employed in cases where the target variable is continuous (regression tree), meaning that use of many other metrics would first require discretization before being applied. The variance reduction of a node N is defined as the total reduction of the variance of the target variable Y due to the split at this node:\n where \n\n\n\nS\n\n\n{\\displaystyle S}\n\n, \n\n\n\n\nS\n\nt\n\n\n\n\n{\\displaystyle S_{t}}\n\n, and \n\n\n\n\nS\n\nf\n\n\n\n\n{\\displaystyle S_{f}}\n\n are the set of presplit sample indices, set of sample indices for which the split test is true, and set of sample indices for which the split test is false, respectively. Each of the above summands are indeed variance estimates, though, written in a form without directly referring to the mean.\n By replacing \n\n\n\n(\n\ny\n\ni\n\n\n\u2212\n\ny\n\nj\n\n\n\n)\n\n2\n\n\n\n\n{\\displaystyle (y_{i}-y_{j})^{2}}\n\n in the formula above with the dissimilarity \n\n\n\n\nd\n\ni\nj\n\n\n\n\n{\\displaystyle d_{ij}}\n\n between two objects \n\n\n\ni\n\n\n{\\displaystyle i}\n\n and \n\n\n\nj\n\n\n{\\displaystyle j}\n\n, the variance reduction criterion applies to any kind of object for which pairwise dissimilarities can be computed.[1]\n Used by CART in 1984,[28] the measure of \"goodness\" is a function that seeks to optimize the balance of a candidate split's capacity to create pure children with its capacity to create equally-sized children. This process is repeated for each impure node until the tree is complete. The function \n\n\n\n\u03c6\n(\ns\n\u2223\nt\n)\n\n\n{\\displaystyle \\varphi (s\\mid t)}\n\n, where \n\n\n\ns\n\n\n{\\displaystyle s}\n\n is a candidate split at node \n\n\n\nt\n\n\n{\\displaystyle t}\n\n, is defined as below\n where \n\n\n\n\nt\n\nL\n\n\n\n\n{\\displaystyle t_{L}}\n\n and \n\n\n\n\nt\n\nR\n\n\n\n\n{\\displaystyle t_{R}}\n\n are the left and right children of node \n\n\n\nt\n\n\n{\\displaystyle t}\n\n using split \n\n\n\ns\n\n\n{\\displaystyle s}\n\n, respectively; \n\n\n\n\nP\n\nL\n\n\n\n\n{\\displaystyle P_{L}}\n\n and \n\n\n\n\nP\n\nR\n\n\n\n\n{\\displaystyle P_{R}}\n\n are the proportions of records in \n\n\n\nt\n\n\n{\\displaystyle t}\n\n in \n\n\n\n\nt\n\nL\n\n\n\n\n{\\displaystyle t_{L}}\n\n and \n\n\n\n\nt\n\nR\n\n\n\n\n{\\displaystyle t_{R}}\n\n, respectively; and \n\n\n\nP\n(\nj\n\u2223\n\nt\n\nL\n\n\n)\n\n\n{\\displaystyle P(j\\mid t_{L})}\n\n and \n\n\n\nP\n(\nj\n\u2223\n\nt\n\nR\n\n\n)\n\n\n{\\displaystyle P(j\\mid t_{R})}\n\n are the proportions of class \n\n\n\nj\n\n\n{\\displaystyle j}\n\n records in \n\n\n\n\nt\n\nL\n\n\n\n\n{\\displaystyle t_{L}}\n\n and \n\n\n\n\nt\n\nR\n\n\n\n\n{\\displaystyle t_{R}}\n\n, respectively.\n Consider an example data set with three attributes: savings(low, medium, high), assets(low, medium, high), income(numerical value), and a binary target variable credit risk(good, bad) and 8 data points.[28] The full data is presented in the table below. To start a decision tree, we will calculate the maximum value of \n\n\n\n\u03c6\n(\ns\n\u2223\nt\n)\n\n\n{\\displaystyle \\varphi (s\\mid t)}\n\n using each feature to find which one will split the root node. This process will continue until all children are pure or all \n\n\n\n\u03c6\n(\ns\n\u2223\nt\n)\n\n\n{\\displaystyle \\varphi (s\\mid t)}\n\n values are below a set threshold.\n To find \n\n\n\n\u03c6\n(\ns\n\u2223\nt\n)\n\n\n{\\displaystyle \\varphi (s\\mid t)}\n\n of the feature savings, we need to note the quantity of each value. The original data contained three low's, three medium's, and two high's. Out of the low's, one had a good credit risk while out of the medium's and high's, 4 had a good credit risk. Assume a candidate split \n\n\n\ns\n\n\n{\\displaystyle s}\n\n such that records with a low savings will be put in the left child and all other records will be put into the right child.\n To build the tree, the \"goodness\" of all candidate splits for the root node need to be calculated. The candidate with the maximum value will split the root node, and the process will continue for each impure node until the tree is complete.\n Compared to other metrics such as information gain, the measure of \"goodness\" will attempt to create a more balanced tree, leading to more-consistent decision time. However, it sacrifices some priority for creating pure children which can lead to additional splits that are not present with other metrics.\n Amongst other data mining methods, decision trees have various advantages:\n Many data mining software packages provide implementations of one or more decision tree algorithms (e.g. random forest).\n Open source examples include:\n Notable commercial software:\n In a decision tree, all paths from the root node to the leaf node proceed by way of conjunction, or AND. In a decision graph, it is possible to use disjunctions (ORs) to join two more paths together using minimum message length (MML).[43]  Decision graphs have been further extended to allow for previously unstated new attributes to be learnt dynamically and used at different places within the graph.[44]  The more general coding scheme results in better predictive accuracy and log-loss probabilistic scoring.[citation needed]  In general, decision graphs infer models with fewer leaves than decision trees.\n Evolutionary algorithms have been used to avoid local optimal decisions and search the decision tree space with little a priori bias.[45][46]\n It is also possible for a tree to be sampled using MCMC.[47]\n The tree can be searched for in a bottom-up fashion.[48] Or several trees can be constructed parallelly to reduce the expected number of tests till classification.[38]\n",
        "doc_number": 84
    },
    {
        "url": "https://en.wikipedia.org/wiki/Random_forest",
        "content": "Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that works by creating a multitude of decision trees during training. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the output is the average of the predictions of the trees.[1][2] Random forests correct for decision trees' habit of overfitting to their training set.[3]:\u200a587\u2013588\u200a\n The first algorithm for random decision forests was created in 1995 by Tin Kam Ho[1] using the random subspace method,[2] which, in Ho's formulation, is a way to implement the \"stochastic discrimination\" approach to classification proposed by Eugene Kleinberg.[4][5][6]\n An extension of the algorithm was developed by Leo Breiman[7] and Adele Cutler,[8] who registered[9] \"Random Forests\" as a trademark in 2006 (as of 2019[update], owned by Minitab, Inc.).[10] The extension combines Breiman's \"bagging\" idea and random selection of features, introduced first by Ho[1] and later independently by Amit and Geman[11] in order to construct a collection of decision trees with controlled variance.\n The general method of random decision forests was first proposed by Salzberg and Heath in 1993,[12] with a method that used a randomized decision tree algorithm to create multiple trees and then combine them using majority voting.  This idea was developed further by Ho in 1995.[1]  Ho established that forests of trees splitting with oblique hyperplanes can gain accuracy as they grow without suffering from overtraining, as long as the forests are randomly restricted to be sensitive to only selected feature dimensions.  A subsequent work along the same lines[2] concluded that other splitting methods behave similarly, as long as they are randomly forced to be insensitive to some feature dimensions.  This observation that a more complex classifier (a larger forest) gets more accurate nearly monotonically is in sharp contrast to the common belief that the complexity of a classifier can only grow to a certain level of accuracy before being hurt by overfitting.  The explanation of the forest method's resistance to overtraining can be found in Kleinberg's theory of stochastic discrimination.[4][5][6]\n The early development of Breiman's notion of random forests was influenced by the work of Amit and Geman[11] who introduced the idea of searching over a random subset of the available decisions when splitting a node, in the context of growing a single tree.  The idea of random subspace selection from Ho[2] was also influential in the design of random forests.  This method grows a forest of trees, and introduces variation among the trees by projecting the training data into a randomly chosen subspace before fitting each tree or each node.  Finally, the idea of randomized node optimization, where the decision at each node is selected by a randomized procedure, rather than a deterministic optimization was first introduced by Thomas G. Dietterich.[13]\n The proper introduction of random forests was made in a paper by Leo Breiman.[7]  This paper describes a method of building a forest of uncorrelated trees using a CART like procedure, combined with randomized node optimization and bagging.  In addition, this paper combines several ingredients, some previously known and some novel, which form the basis of the modern practice of random forests, in particular:\n The report also offers the first theoretical result for random forests in the form of a bound on the generalization error which depends on the strength of the trees in the forest and their correlation.\n Decision trees are a popular method for various machine learning tasks. Tree learning is almost \"an off-the-shelf procedure for data mining\", say Hastie et al., \"because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate\".[3]:\u200a352\u200a\n In particular, trees that are grown very deep tend to learn highly irregular patterns: they overfit their training sets, i.e. have low bias, but very high variance. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance.[3]:\u200a587\u2013588\u200a This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance in the final model.\n The training algorithm for random forests applies the general technique of bootstrap aggregating, or bagging, to tree learners. Given a training set X = x1, ..., xn with responses Y = y1, ..., yn, bagging repeatedly (B times) selects a random sample with replacement of the training set and fits trees to these samples:\n After training, predictions for unseen samples x' can be made by averaging the predictions from all the individual regression trees on x':\n \n\n\n\n\n\n\nf\n^\n\n\n\n=\n\n\n1\nB\n\n\n\n\u2211\n\nb\n=\n1\n\n\nB\n\n\n\nf\n\nb\n\n\n(\n\nx\n\u2032\n\n)\n\n\n{\\displaystyle {\\hat {f}}={\\frac {1}{B}}\\sum _{b=1}^{B}f_{b}(x')}\n\n\n or by taking the plurality vote in the case of classification trees.\n This bootstrapping procedure leads to better model performance because it decreases the variance of the model, without increasing the bias. This means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated. Simply training many trees on a single training set would give strongly correlated trees (or even the same tree many times, if the training algorithm is deterministic); bootstrap sampling is a way of de-correlating the trees by showing them different training sets.\n Additionally, an estimate of the uncertainty of the prediction can be made as the standard deviation of the predictions from all the individual regression trees on x\u2032:\n\n\n\n\n\u03c3\n=\n\n\n\n\n\n\u2211\n\nb\n=\n1\n\n\nB\n\n\n(\n\nf\n\nb\n\n\n(\n\nx\n\u2032\n\n)\n\u2212\n\n\n\nf\n^\n\n\n\n\n)\n\n2\n\n\n\n\nB\n\u2212\n1\n\n\n\n\n.\n\n\n{\\displaystyle \\sigma ={\\sqrt {\\frac {\\sum _{b=1}^{B}(f_{b}(x')-{\\hat {f}})^{2}}{B-1}}}.}\n\n\n The number B of samples (equivalently, of trees) is a free parameter. Typically, a few hundred to several thousand trees are used, depending on the size and nature of the training set. B can be optimized using cross-validation, or by observing the out-of-bag error: the mean prediction error on each training sample xi, using only the trees that did not have xi in their bootstrap sample.[14]\n The training and test error tend to level off after some number of trees have been fit.\n The above procedure describes the original bagging algorithm for trees. Random forests also include another type of bagging scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. This process is sometimes called \"feature bagging\". The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the B trees, causing them to become correlated. An analysis of how bagging and random subspace projection contribute to accuracy gains under different conditions is given by Ho.[15]\n Typically, for a classification problem with p features, \u221ap (rounded down) features are used in each split.[3]:\u200a592\u200a  For regression problems the inventors recommend p/3 (rounded down) with a minimum node size of 5 as the default.[3]:\u200a592\u200a In practice, the best values for these parameters should be tuned on a case-to-case basis for every problem.[3]:\u200a592\u200a\n Adding one further step of randomization yields extremely randomized trees, or ExtraTrees. As with ordinary random forests, they are an ensemble of individual trees, but there are two main differences: (1) each tree is trained using the whole learning sample (rather than a bootstrap sample), and (2) the top-down splitting is randomized: for each feature under consideration, a number of random cut-points are selected, instead of computing the locally optimal cut-point (based on, e.g., information gain or the Gini impurity). The values are chosen from a uniform distribution within the feature's empirical range (in the tree's training set). Then, of all the randomly chosen splits, the split that yields the highest score is chosen to split the node.\n Similar to ordinary random forests, the number of randomly selected features to be considered at each node can be specified. Default values for this parameter are \n\n\n\n\n\np\n\n\n\n\n{\\displaystyle {\\sqrt {p}}}\n\n for classification and \n\n\n\np\n\n\n{\\displaystyle p}\n\n for regression, where \n\n\n\np\n\n\n{\\displaystyle p}\n\n is the number of features in the model.[16]\n The basic random forest procedure may not work well in situations where there are a large number of features but only a small proportion of these features are informative with respect to sample classification. This can be addressed by encouraging the procedure to focus mainly on features and trees that are informative. Some methods for accomplishing this are:\n Random forests can be used to rank the importance of variables in a regression or classification problem in a natural way.  The following technique was described in Breiman's original paper[7] and is implemented in the R package randomForest.[8]\n To measure a feature's importance in a data set \n\n\n\n\n\n\nD\n\n\n\nn\n\n\n=\n{\n(\n\nX\n\ni\n\n\n,\n\nY\n\ni\n\n\n)\n\n}\n\ni\n=\n1\n\n\nn\n\n\n\n\n{\\displaystyle {\\mathcal {D}}_{n}=\\{(X_{i},Y_{i})\\}_{i=1}^{n}}\n\n, first a random forest is trained on the data.  During training, the out-of-bag error for each data point is recorded and averaged over the forest. (If bagging is not used during training, we can instead compute errors on an independent test set.)\n After training, the values of the feature are permuted in the out-of-bag samples and the out-of-bag error is again computed on this perturbed data set. The importance for the feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees.  The score is normalized by the standard deviation of these differences.\n Features which produce large values for this score are ranked as more important than features which produce small values. The statistical definition of the variable importance measure was given and analyzed by Zhu et al.[23]\n This method of determining variable importance has some drawbacks:\n This approach to feature importance for random forests considers as important the variables which decrease a lot the impurity during splitting.[31] It is described in the book Classification and Regression Trees by Leo Breiman[32] and is the default implementation in sci-kit learn and R. The definition is:\n\n\n\n\nunormalized average importance\n\n(\nx\n)\n=\n\n\n1\n\nn\n\nT\n\n\n\n\n\n\u2211\n\ni\n=\n1\n\n\n\nn\n\nT\n\n\n\n\n\n\u2211\n\n\nnode\u00a0\n\nj\n\u2208\n\nT\n\ni\n\n\n\n|\n\n\nsplit variable\n\n(\nj\n)\n=\nx\n\n\n\np\n\n\nT\n\ni\n\n\n\n\n(\nj\n)\n\u0394\n\ni\n\n\nT\n\ni\n\n\n\n\n(\nj\n)\n,\n\n\n{\\displaystyle {\\text{unormalized average importance}}(x)={\\frac {1}{n_{T}}}\\sum _{i=1}^{n_{T}}\\sum _{{\\text{node }}j\\in T_{i}|{\\text{split variable}}(j)=x}p_{T_{i}}(j)\\Delta i_{T_{i}}(j),}\n\nwhere \n As impurity measure for samples falling in a node e.g. the following statistics can be used:\n The normalized importance is then obtained by normalizing over all features, so that the sum of normalized feature importances is 1.\n The sci-kit learn default implementation can report misleading feature importance:[30]\n A relationship between random forests and the k-nearest neighbor algorithm (k-NN) was pointed out by Lin and Jeon in 2002.[34] Both can be viewed as so-called weighted neighborhoods schemes. These are models built from a training set \n\n\n\n{\n(\n\nx\n\ni\n\n\n,\n\ny\n\ni\n\n\n)\n\n}\n\ni\n=\n1\n\n\nn\n\n\n\n\n{\\displaystyle \\{(x_{i},y_{i})\\}_{i=1}^{n}}\n\n that make predictions \n\n\n\n\n\n\ny\n^\n\n\n\n\n\n{\\displaystyle {\\hat {y}}}\n\n for new points x' by looking at the \"neighborhood\" of the point, formalized by a weight function W:\n\n\n\n\n\n\ny\n^\n\n\n\n=\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\nW\n(\n\nx\n\ni\n\n\n,\n\nx\n\u2032\n\n)\n\n\ny\n\ni\n\n\n.\n\n\n{\\displaystyle {\\hat {y}}=\\sum _{i=1}^{n}W(x_{i},x')\\,y_{i}.}\n\nHere, \n\n\n\nW\n(\n\nx\n\ni\n\n\n,\n\nx\n\u2032\n\n)\n\n\n{\\displaystyle W(x_{i},x')}\n\n is the non-negative weight of the i'th training point relative to the new point x' in the same tree. For any x', the weights for points \n\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystyle x_{i}}\n\n must sum to 1. Weight functions are as follows:\n Since a forest averages the predictions of a set of m trees with individual weight functions \n\n\n\n\nW\n\nj\n\n\n\n\n{\\displaystyle W_{j}}\n\n, its predictions are\n\n\n\n\n\n\ny\n^\n\n\n\n=\n\n\n1\nm\n\n\n\n\u2211\n\nj\n=\n1\n\n\nm\n\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\nW\n\nj\n\n\n(\n\nx\n\ni\n\n\n,\n\nx\n\u2032\n\n)\n\n\ny\n\ni\n\n\n=\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\n(\n\n\n\n1\nm\n\n\n\n\u2211\n\nj\n=\n1\n\n\nm\n\n\n\nW\n\nj\n\n\n(\n\nx\n\ni\n\n\n,\n\nx\n\u2032\n\n)\n\n)\n\n\n\ny\n\ni\n\n\n.\n\n\n{\\displaystyle {\\hat {y}}={\\frac {1}{m}}\\sum _{j=1}^{m}\\sum _{i=1}^{n}W_{j}(x_{i},x')\\,y_{i}=\\sum _{i=1}^{n}\\left({\\frac {1}{m}}\\sum _{j=1}^{m}W_{j}(x_{i},x')\\right)\\,y_{i}.}\n\n\n This shows that the whole forest is again a weighted neighborhood scheme, with weights that average those of the individual trees. The neighbors of x' in this interpretation are the points \n\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystyle x_{i}}\n\n sharing the same leaf in any tree \n\n\n\nj\n\n\n{\\displaystyle j}\n\n. In this way, the neighborhood of x' depends in a complex way on the structure of the trees, and thus on the structure of the training set. Lin and Jeon show that the shape of the neighborhood used by a random forest adapts to the local importance of each feature.[34]\n As part of their construction, random forest predictors naturally lead to a dissimilarity measure among observations. One can analogously define dissimilarity between unlabeled data, by training a forest to distinguish original \"observed\" data from suitably generated synthetic data drawn from a reference distribution.[7][35] A random forest dissimilarity is attractive because it handles mixed variable types very well, is invariant to monotonic transformations of the input variables, and is robust to outlying observations. Random forest dissimilarity easily deals with a large number of semi-continuous variables due to its intrinsic variable selection; for example, the \"Addcl 1\" random forest dissimilarity weighs the contribution of each variable according to how dependent it is on other variables. Random forest dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data.[36]\n Instead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive Bayes classifiers.[37][38][39] In cases that the relationship between the predictors and the target variable is linear, the base learners may have an equally high accuracy as the ensemble learner.[40][37]\n In machine learning, kernel random forests (KeRF) establish the connection between random forests and kernel methods. By slightly modifying their definition, random forests can be rewritten as kernel methods, which are more interpretable and easier to analyze.[41]\n Leo Breiman[42] was the first person to notice the link between random forest and kernel methods. He pointed out that random forests trained using i.i.d. random vectors in the tree construction are equivalent to a kernel acting on the true margin. Lin and Jeon[43] established the connection between random forests and adaptive nearest neighbor, implying that random forests can be seen as adaptive kernel estimates. Davies and Ghahramani[44] proposed Kernel Random Forest (KeRF) and showed that it can empirically outperform state-of-art kernel methods. Scornet[41] first defined KeRF estimates and gave the explicit link between KeRF estimates and random forest. He also gave explicit expressions for kernels based on centered random forest[45] and uniform random forest,[46] two simplified models of random forest. He named these two KeRFs Centered KeRF and Uniform KeRF, and proved upper bounds on their rates of consistency.\n Centered forest[45] is a simplified model for Breiman's original random forest, which uniformly selects an attribute among all attributes and performs splits at the center of the cell along the pre-chosen attribute. The algorithm stops when a fully binary tree of level \n\n\n\nk\n\n\n{\\displaystyle k}\n\n is built, where \n\n\n\nk\n\u2208\n\nN\n\n\n\n{\\displaystyle k\\in \\mathbb {N} }\n\n is a parameter of the algorithm.\n Uniform forest[46] is another simplified model for Breiman's original random forest, which uniformly selects a feature among all features and performs splits at a point uniformly drawn on the side of the cell, along the preselected feature.\n Given a training sample  \n\n\n\n\n\n\nD\n\n\n\nn\n\n\n=\n{\n(\n\n\nX\n\n\ni\n\n\n,\n\nY\n\ni\n\n\n)\n\n}\n\ni\n=\n1\n\n\nn\n\n\n\n\n{\\displaystyle {\\mathcal {D}}_{n}=\\{(\\mathbf {X} _{i},Y_{i})\\}_{i=1}^{n}}\n\n of \n\n\n\n[\n0\n,\n1\n\n]\n\np\n\n\n\u00d7\n\nR\n\n\n\n{\\displaystyle [0,1]^{p}\\times \\mathbb {R} }\n\n-valued independent random variables distributed as the independent prototype pair \n\n\n\n(\n\nX\n\n,\nY\n)\n\n\n{\\displaystyle (\\mathbf {X} ,Y)}\n\n, where \n\n\n\nE\n\u2061\n[\n\nY\n\n2\n\n\n]\n<\n\u221e\n\n\n{\\displaystyle \\operatorname {E} [Y^{2}]<\\infty }\n\n. We aim at predicting the response \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n, associated with the random variable \n\n\n\n\nX\n\n\n\n{\\displaystyle \\mathbf {X} }\n\n, by estimating the regression function \n\n\n\nm\n(\n\nx\n\n)\n=\nE\n\u2061\n[\nY\n\u2223\n\nX\n\n=\n\nx\n\n]\n\n\n{\\displaystyle m(\\mathbf {x} )=\\operatorname {E} [Y\\mid \\mathbf {X} =\\mathbf {x} ]}\n\n. A random regression forest is an ensemble of \n\n\n\nM\n\n\n{\\displaystyle M}\n\n randomized regression trees. Denote \n\n\n\n\nm\n\nn\n\n\n(\n\nx\n\n,\n\n\n\u0398\n\n\nj\n\n\n)\n\n\n{\\displaystyle m_{n}(\\mathbf {x} ,\\mathbf {\\Theta } _{j})}\n\n the predicted value at point \n\n\n\n\nx\n\n\n\n{\\displaystyle \\mathbf {x} }\n\n by the \n\n\n\nj\n\n\n{\\displaystyle j}\n\n-th tree, where \n\n\n\n\n\n\u0398\n\n\n1\n\n\n,\n\u2026\n,\n\n\n\u0398\n\n\nM\n\n\n\n\n{\\displaystyle \\mathbf {\\Theta } _{1},\\ldots ,\\mathbf {\\Theta } _{M}}\n\n are independent random variables, distributed as a generic random variable \n\n\n\n\n\u0398\n\n\n\n{\\displaystyle \\mathbf {\\Theta } }\n\n, independent of the sample \n\n\n\n\n\n\nD\n\n\n\nn\n\n\n\n\n{\\displaystyle {\\mathcal {D}}_{n}}\n\n. This random variable can be used to describe the randomness induced by node splitting and the sampling procedure for tree construction. The trees are combined to form the finite forest estimate \n\n\n\n\nm\n\nM\n,\nn\n\n\n(\n\nx\n\n,\n\n\u0398\n\n1\n\n\n,\n\u2026\n,\n\n\u0398\n\nM\n\n\n)\n=\n\n\n1\nM\n\n\n\n\u2211\n\nj\n=\n1\n\n\nM\n\n\n\nm\n\nn\n\n\n(\n\nx\n\n,\n\n\u0398\n\nj\n\n\n)\n\n\n{\\displaystyle m_{M,n}(\\mathbf {x} ,\\Theta _{1},\\ldots ,\\Theta _{M})={\\frac {1}{M}}\\sum _{j=1}^{M}m_{n}(\\mathbf {x} ,\\Theta _{j})}\n\n.\nFor regression trees, we have \n\n\n\n\nm\n\nn\n\n\n=\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\n\n\n\nY\n\ni\n\n\n\n\n1\n\n\n\n\nX\n\n\ni\n\n\n\u2208\n\nA\n\nn\n\n\n(\n\nx\n\n,\n\n\u0398\n\nj\n\n\n)\n\n\n\n\n\nN\n\nn\n\n\n(\n\nx\n\n,\n\n\u0398\n\nj\n\n\n)\n\n\n\n\n\n{\\displaystyle m_{n}=\\sum _{i=1}^{n}{\\frac {Y_{i}\\mathbf {1} _{\\mathbf {X} _{i}\\in A_{n}(\\mathbf {x} ,\\Theta _{j})}}{N_{n}(\\mathbf {x} ,\\Theta _{j})}}}\n\n, where \n\n\n\n\nA\n\nn\n\n\n(\n\nx\n\n,\n\n\u0398\n\nj\n\n\n)\n\n\n{\\displaystyle A_{n}(\\mathbf {x} ,\\Theta _{j})}\n\n is the cell containing \n\n\n\n\nx\n\n\n\n{\\displaystyle \\mathbf {x} }\n\n, designed with randomness \n\n\n\n\n\u0398\n\nj\n\n\n\n\n{\\displaystyle \\Theta _{j}}\n\n and dataset \n\n\n\n\n\n\nD\n\n\n\nn\n\n\n\n\n{\\displaystyle {\\mathcal {D}}_{n}}\n\n, and \n\n\n\n\nN\n\nn\n\n\n(\n\nx\n\n,\n\n\u0398\n\nj\n\n\n)\n=\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\n\n1\n\n\n\n\nX\n\n\ni\n\n\n\u2208\n\nA\n\nn\n\n\n(\n\nx\n\n,\n\n\u0398\n\nj\n\n\n)\n\n\n\n\n{\\displaystyle N_{n}(\\mathbf {x} ,\\Theta _{j})=\\sum _{i=1}^{n}\\mathbf {1} _{\\mathbf {X} _{i}\\in A_{n}(\\mathbf {x} ,\\Theta _{j})}}\n\n.\n Thus random forest estimates satisfy, for all \n\n\n\n\nx\n\n\u2208\n[\n0\n,\n1\n\n]\n\nd\n\n\n\n\n{\\displaystyle \\mathbf {x} \\in [0,1]^{d}}\n\n, \n\n\n\n\nm\n\nM\n,\nn\n\n\n(\n\nx\n\n,\n\n\u0398\n\n1\n\n\n,\n\u2026\n,\n\n\u0398\n\nM\n\n\n)\n=\n\n\n1\nM\n\n\n\n\u2211\n\nj\n=\n1\n\n\nM\n\n\n\n(\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\n\n\n\nY\n\ni\n\n\n\n\n1\n\n\n\n\nX\n\n\ni\n\n\n\u2208\n\nA\n\nn\n\n\n(\n\nx\n\n,\n\n\u0398\n\nj\n\n\n)\n\n\n\n\n\nN\n\nn\n\n\n(\n\nx\n\n,\n\n\u0398\n\nj\n\n\n)\n\n\n\n\n)\n\n\n\n{\\displaystyle m_{M,n}(\\mathbf {x} ,\\Theta _{1},\\ldots ,\\Theta _{M})={\\frac {1}{M}}\\sum _{j=1}^{M}\\left(\\sum _{i=1}^{n}{\\frac {Y_{i}\\mathbf {1} _{\\mathbf {X} _{i}\\in A_{n}(\\mathbf {x} ,\\Theta _{j})}}{N_{n}(\\mathbf {x} ,\\Theta _{j})}}\\right)}\n\n. Random regression forest has two levels of averaging, first over the samples in the target cell of a tree, then over all trees. Thus the contributions of observations that are in cells with a high density of data points are smaller than that of observations which belong to less populated cells. In order to improve the random forest methods and compensate the misestimation, Scornet[41] defined KeRF by\n\n\n\n\n\n\n\n\nm\n~\n\n\n\n\nM\n,\nn\n\n\n(\n\nx\n\n,\n\n\u0398\n\n1\n\n\n,\n\u2026\n,\n\n\u0398\n\nM\n\n\n)\n=\n\n\n1\n\n\n\u2211\n\nj\n=\n1\n\n\nM\n\n\n\nN\n\nn\n\n\n(\n\nx\n\n,\n\n\u0398\n\nj\n\n\n)\n\n\n\n\n\u2211\n\nj\n=\n1\n\n\nM\n\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\nY\n\ni\n\n\n\n\n1\n\n\n\n\nX\n\n\ni\n\n\n\u2208\n\nA\n\nn\n\n\n(\n\nx\n\n,\n\n\u0398\n\nj\n\n\n)\n\n\n,\n\n\n{\\displaystyle {\\tilde {m}}_{M,n}(\\mathbf {x} ,\\Theta _{1},\\ldots ,\\Theta _{M})={\\frac {1}{\\sum _{j=1}^{M}N_{n}(\\mathbf {x} ,\\Theta _{j})}}\\sum _{j=1}^{M}\\sum _{i=1}^{n}Y_{i}\\mathbf {1} _{\\mathbf {X} _{i}\\in A_{n}(\\mathbf {x} ,\\Theta _{j})},}\n\n\nwhich is equal to the mean of the \n\n\n\n\nY\n\ni\n\n\n\n\n{\\displaystyle Y_{i}}\n\n's falling in the cells containing \n\n\n\n\nx\n\n\n\n{\\displaystyle \\mathbf {x} }\n\n in the forest. If we define the connection function of the \n\n\n\nM\n\n\n{\\displaystyle M}\n\n finite forest as \n\n\n\n\nK\n\nM\n,\nn\n\n\n(\n\nx\n\n,\n\nz\n\n)\n=\n\n\n1\nM\n\n\n\n\u2211\n\nj\n=\n1\n\n\nM\n\n\n\n\n1\n\n\n\nz\n\n\u2208\n\nA\n\nn\n\n\n(\n\nx\n\n,\n\n\u0398\n\nj\n\n\n)\n\n\n\n\n{\\displaystyle K_{M,n}(\\mathbf {x} ,\\mathbf {z} )={\\frac {1}{M}}\\sum _{j=1}^{M}\\mathbf {1} _{\\mathbf {z} \\in A_{n}(\\mathbf {x} ,\\Theta _{j})}}\n\n, i.e. the proportion of cells shared between \n\n\n\n\nx\n\n\n\n{\\displaystyle \\mathbf {x} }\n\n and \n\n\n\n\nz\n\n\n\n{\\displaystyle \\mathbf {z} }\n\n, then almost surely we have \n\n\n\n\n\n\n\nm\n~\n\n\n\n\nM\n,\nn\n\n\n(\n\nx\n\n,\n\n\u0398\n\n1\n\n\n,\n\u2026\n,\n\n\u0398\n\nM\n\n\n)\n=\n\n\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\nY\n\ni\n\n\n\nK\n\nM\n,\nn\n\n\n(\n\nx\n\n,\n\n\nx\n\n\ni\n\n\n)\n\n\n\n\u2211\n\n\u2113\n=\n1\n\n\nn\n\n\n\nK\n\nM\n,\nn\n\n\n(\n\nx\n\n,\n\n\nx\n\n\n\u2113\n\n\n)\n\n\n\n\n\n{\\displaystyle {\\tilde {m}}_{M,n}(\\mathbf {x} ,\\Theta _{1},\\ldots ,\\Theta _{M})={\\frac {\\sum _{i=1}^{n}Y_{i}K_{M,n}(\\mathbf {x} ,\\mathbf {x} _{i})}{\\sum _{\\ell =1}^{n}K_{M,n}(\\mathbf {x} ,\\mathbf {x} _{\\ell })}}}\n\n, which defines the KeRF.\n The construction of Centered KeRF of level \n\n\n\nk\n\n\n{\\displaystyle k}\n\n is the same as for centered forest, except that predictions are made by \n\n\n\n\n\n\n\nm\n~\n\n\n\n\nM\n,\nn\n\n\n(\n\nx\n\n,\n\n\u0398\n\n1\n\n\n,\n\u2026\n,\n\n\u0398\n\nM\n\n\n)\n\n\n{\\displaystyle {\\tilde {m}}_{M,n}(\\mathbf {x} ,\\Theta _{1},\\ldots ,\\Theta _{M})}\n\n, the corresponding kernel function, or connection function is\n\n\n\n\n\nK\n\nk\n\n\nc\nc\n\n\n(\n\nx\n\n,\n\nz\n\n)\n=\n\n\u2211\n\n\nk\n\n1\n\n\n,\n\u2026\n,\n\nk\n\nd\n\n\n,\n\n\u2211\n\nj\n=\n1\n\n\nd\n\n\n\nk\n\nj\n\n\n=\nk\n\n\n\n\n\nk\n!\n\n\n\nk\n\n1\n\n\n!\n\u22ef\n\nk\n\nd\n\n\n!\n\n\n\n\n\n(\n\n\n1\nd\n\n\n)\n\n\nk\n\n\n\n\u220f\n\nj\n=\n1\n\n\nd\n\n\n\n\n1\n\n\n\u2308\n\n2\n\n\nk\n\nj\n\n\n\n\n\nx\n\nj\n\n\n\u2309\n=\n\u2308\n\n2\n\n\nk\n\nj\n\n\n\n\n\nz\n\nj\n\n\n\u2309\n\n\n,\n\n\n\u00a0for all\u00a0\n\n\nx\n\n,\n\nz\n\n\u2208\n[\n0\n,\n1\n\n]\n\nd\n\n\n.\n\n\n{\\displaystyle K_{k}^{cc}(\\mathbf {x} ,\\mathbf {z} )=\\sum _{k_{1},\\ldots ,k_{d},\\sum _{j=1}^{d}k_{j}=k}{\\frac {k!}{k_{1}!\\cdots k_{d}!}}\\left({\\frac {1}{d}}\\right)^{k}\\prod _{j=1}^{d}\\mathbf {1} _{\\lceil 2^{k_{j}}x_{j}\\rceil =\\lceil 2^{k_{j}}z_{j}\\rceil },\\qquad {\\text{ for all }}\\mathbf {x} ,\\mathbf {z} \\in [0,1]^{d}.}\n\n\n Uniform KeRF is built in the same way as uniform forest, except that predictions are made by \n\n\n\n\n\n\n\nm\n~\n\n\n\n\nM\n,\nn\n\n\n(\n\nx\n\n,\n\n\u0398\n\n1\n\n\n,\n\u2026\n,\n\n\u0398\n\nM\n\n\n)\n\n\n{\\displaystyle {\\tilde {m}}_{M,n}(\\mathbf {x} ,\\Theta _{1},\\ldots ,\\Theta _{M})}\n\n, the corresponding kernel function, or connection function is\n\n\n\n\n\nK\n\nk\n\n\nu\nf\n\n\n(\n\n0\n\n,\n\nx\n\n)\n=\n\n\u2211\n\n\nk\n\n1\n\n\n,\n\u2026\n,\n\nk\n\nd\n\n\n,\n\n\u2211\n\nj\n=\n1\n\n\nd\n\n\n\nk\n\nj\n\n\n=\nk\n\n\n\n\n\nk\n!\n\n\n\nk\n\n1\n\n\n!\n\u2026\n\nk\n\nd\n\n\n!\n\n\n\n\n\n(\n\n\n1\nd\n\n\n)\n\n\nk\n\n\n\n\u220f\n\nm\n=\n1\n\n\nd\n\n\n\n(\n\n1\n\u2212\n\n|\n\n\nx\n\nm\n\n\n\n|\n\n\n\u2211\n\nj\n=\n0\n\n\n\nk\n\nm\n\n\n\u2212\n1\n\n\n\n\n\n\n(\n\n\u2212\nln\n\u2061\n\n|\n\n\nx\n\nm\n\n\n\n|\n\n\n)\n\n\nj\n\n\n\nj\n!\n\n\n\n\n)\n\n\n\u00a0for all\u00a0\n\n\nx\n\n\u2208\n[\n0\n,\n1\n\n]\n\nd\n\n\n.\n\n\n{\\displaystyle K_{k}^{uf}(\\mathbf {0} ,\\mathbf {x} )=\\sum _{k_{1},\\ldots ,k_{d},\\sum _{j=1}^{d}k_{j}=k}{\\frac {k!}{k_{1}!\\ldots k_{d}!}}\\left({\\frac {1}{d}}\\right)^{k}\\prod _{m=1}^{d}\\left(1-|x_{m}|\\sum _{j=0}^{k_{m}-1}{\\frac {\\left(-\\ln |x_{m}|\\right)^{j}}{j!}}\\right){\\text{ for all }}\\mathbf {x} \\in [0,1]^{d}.}\n\n\n Predictions given by KeRF and random forests are close if the number of points in each cell is controlled:\n Assume that there exist sequences \n\n\n\n(\n\na\n\nn\n\n\n)\n,\n(\n\nb\n\nn\n\n\n)\n\n\n{\\displaystyle (a_{n}),(b_{n})}\n\n such that, almost surely,\n\n\n\n\n\na\n\nn\n\n\n\u2264\n\nN\n\nn\n\n\n(\n\nx\n\n,\n\u0398\n)\n\u2264\n\nb\n\nn\n\n\n\n\u00a0and\u00a0\n\n\na\n\nn\n\n\n\u2264\n\n\n1\nM\n\n\n\n\u2211\n\nm\n=\n1\n\n\nM\n\n\n\nN\n\nn\n\n\n\n\nx\n\n,\n\n\u0398\n\nm\n\n\n\n\u2264\n\nb\n\nn\n\n\n.\n\n\n{\\displaystyle a_{n}\\leq N_{n}(\\mathbf {x} ,\\Theta )\\leq b_{n}{\\text{ and }}a_{n}\\leq {\\frac {1}{M}}\\sum _{m=1}^{M}N_{n}{\\mathbf {x} ,\\Theta _{m}}\\leq b_{n}.}\n\n\nThen almost surely,\n\n\n\n\n\n|\n\n\nm\n\nM\n,\nn\n\n\n(\n\nx\n\n)\n\u2212\n\n\n\n\nm\n~\n\n\n\n\nM\n,\nn\n\n\n(\n\nx\n\n)\n\n|\n\n\u2264\n\n\n\n\nb\n\nn\n\n\n\u2212\n\na\n\nn\n\n\n\n\na\n\nn\n\n\n\n\n\n\n\n\nm\n~\n\n\n\n\nM\n,\nn\n\n\n(\n\nx\n\n)\n.\n\n\n{\\displaystyle |m_{M,n}(\\mathbf {x} )-{\\tilde {m}}_{M,n}(\\mathbf {x} )|\\leq {\\frac {b_{n}-a_{n}}{a_{n}}}{\\tilde {m}}_{M,n}(\\mathbf {x} ).}\n\n\n When the number of trees \n\n\n\nM\n\n\n{\\displaystyle M}\n\n goes to infinity, then we have infinite random forest and infinite KeRF. Their estimates are close if the number of observations in each cell is bounded:\n Assume that there exist sequences \n\n\n\n(\n\n\u03b5\n\nn\n\n\n)\n,\n(\n\na\n\nn\n\n\n)\n,\n(\n\nb\n\nn\n\n\n)\n\n\n{\\displaystyle (\\varepsilon _{n}),(a_{n}),(b_{n})}\n\n such that, almost surely\n Then almost surely,\n\n\n\n\n\n|\n\n\nm\n\n\u221e\n,\nn\n\n\n(\n\nx\n\n)\n\u2212\n\n\n\n\nm\n~\n\n\n\n\n\u221e\n,\nn\n\n\n(\n\nx\n\n)\n\n|\n\n\u2264\n\n\n\n\nb\n\nn\n\n\n\u2212\n\na\n\nn\n\n\n\n\na\n\nn\n\n\n\n\n\n\n\n\nm\n~\n\n\n\n\n\u221e\n,\nn\n\n\n(\n\nx\n\n)\n+\nn\n\n\u03b5\n\nn\n\n\n\n(\n\n\nmax\n\n1\n\u2264\ni\n\u2264\nn\n\n\n\nY\n\ni\n\n\n\n)\n\n.\n\n\n{\\displaystyle |m_{\\infty ,n}(\\mathbf {x} )-{\\tilde {m}}_{\\infty ,n}(\\mathbf {x} )|\\leq {\\frac {b_{n}-a_{n}}{a_{n}}}{\\tilde {m}}_{\\infty ,n}(\\mathbf {x} )+n\\varepsilon _{n}\\left(\\max _{1\\leq i\\leq n}Y_{i}\\right).}\n\n\n Assume that \n\n\n\nY\n=\nm\n(\n\nX\n\n)\n+\n\u03b5\n\n\n{\\displaystyle Y=m(\\mathbf {X} )+\\varepsilon }\n\n, where \n\n\n\n\u03b5\n\n\n{\\displaystyle \\varepsilon }\n\n is a centered Gaussian noise, independent of \n\n\n\n\nX\n\n\n\n{\\displaystyle \\mathbf {X} }\n\n, with finite variance \n\n\n\n\n\u03c3\n\n2\n\n\n<\n\u221e\n\n\n{\\displaystyle \\sigma ^{2}<\\infty }\n\n. Moreover, \n\n\n\n\nX\n\n\n\n{\\displaystyle \\mathbf {X} }\n\n is uniformly distributed on \n\n\n\n[\n0\n,\n1\n\n]\n\nd\n\n\n\n\n{\\displaystyle [0,1]^{d}}\n\n and \n\n\n\nm\n\n\n{\\displaystyle m}\n\n is Lipschitz. Scornet[41] proved upper bounds on the rates of consistency for centered KeRF and uniform KeRF.\n Providing \n\n\n\nk\n\u2192\n\u221e\n\n\n{\\displaystyle k\\rightarrow \\infty }\n\n and \n\n\n\nn\n\n/\n\n\n2\n\nk\n\n\n\u2192\n\u221e\n\n\n{\\displaystyle n/2^{k}\\rightarrow \\infty }\n\n, there exists a constant \n\n\n\n\nC\n\n1\n\n\n>\n0\n\n\n{\\displaystyle C_{1}>0}\n\n such that, for all \n\n\n\nn\n\n\n{\\displaystyle n}\n\n,\n\n\n\n\n\nE\n\n[\n\n\n\n\nm\n~\n\n\n\n\nn\n\n\nc\nc\n\n\n(\n\nX\n\n)\n\u2212\nm\n(\n\nX\n\n)\n\n]\n\n2\n\n\n\u2264\n\nC\n\n1\n\n\n\nn\n\n\u2212\n1\n\n/\n\n(\n3\n+\nd\nlog\n\u2061\n2\n)\n\n\n(\nlog\n\u2061\nn\n\n)\n\n2\n\n\n\n\n{\\displaystyle \\mathbb {E} [{\\tilde {m}}_{n}^{cc}(\\mathbf {X} )-m(\\mathbf {X} )]^{2}\\leq C_{1}n^{-1/(3+d\\log 2)}(\\log n)^{2}}\n\n.\n Providing \n\n\n\nk\n\u2192\n\u221e\n\n\n{\\displaystyle k\\rightarrow \\infty }\n\n and \n\n\n\nn\n\n/\n\n\n2\n\nk\n\n\n\u2192\n\u221e\n\n\n{\\displaystyle n/2^{k}\\rightarrow \\infty }\n\n, there exists a constant \n\n\n\nC\n>\n0\n\n\n{\\displaystyle C>0}\n\n such that,\n\n\n\n\n\nE\n\n[\n\n\n\n\nm\n~\n\n\n\n\nn\n\n\nu\nf\n\n\n(\n\nX\n\n)\n\u2212\nm\n(\n\nX\n\n)\n\n]\n\n2\n\n\n\u2264\nC\n\nn\n\n\u2212\n2\n\n/\n\n(\n6\n+\n3\nd\nlog\n\u2061\n2\n)\n\n\n(\nlog\n\u2061\nn\n\n)\n\n2\n\n\n\n\n{\\displaystyle \\mathbb {E} [{\\tilde {m}}_{n}^{uf}(\\mathbf {X} )-m(\\mathbf {X} )]^{2}\\leq Cn^{-2/(6+3d\\log 2)}(\\log n)^{2}}\n\n.\n While random forests often achieve higher accuracy than a single decision tree, they sacrifice the intrinsic interpretability of decision trees. Decision trees are among a fairly small family of machine learning models that are easily interpretable along with linear models, rule-based models, and attention-based models. This interpretability is one of the main advantages of decision trees. It allows developers to confirm that the model has learned realistic information from the data and allows end-users to have trust and confidence in the decisions made by the model.[37][3] For example, following the path that a decision tree takes to make its decision is quite trivial, but following the paths of tens or hundreds of trees is much harder. To achieve both performance and interpretability, some model compression techniques allow transforming a random forest into a minimal \"born-again\" decision tree that faithfully reproduces the same decision function.[37][47][48]\n Another limitation of random forests is that if features are linearly correlated with the target, random forest may not enhance the accuracy of the base learner.[37][40] Likewise in problems with multiple categorical variables.[49]\n",
        "doc_number": 85
    },
    {
        "url": "https://en.wikipedia.org/wiki/Naive_Bayes_classifier",
        "content": "In statistics, naive Bayes classifiers are a family of linear \"probabilistic classifiers\" which assumes that the features are conditionally independent, given the target class. The strength (naivety) of this assumption is what gives the classifier its name. These classifiers are among the simplest Bayesian network models.[1]\n Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression,[2]:\u200a718\u200a which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.\n In the statistics literature, naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes.[3] All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method.[2][3]\n Naive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. There is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10\u00a0cm in diameter.  A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of any possible correlations between the color, roundness, and diameter features.\n In many practical applications, parameter estimation for naive Bayes models uses the method of maximum likelihood; in other words, one can work with the naive Bayes model without accepting Bayesian probability or using any Bayesian methods.\n Despite their naive design and apparently oversimplified assumptions, naive Bayes classifiers have worked quite well in many complex real-world situations. In 2004, an analysis of the Bayesian classification problem showed that there are sound theoretical reasons for the apparently implausible efficacy of naive Bayes classifiers.[4] Still, a comprehensive comparison with other classification algorithms in 2006 showed that Bayes classification is outperformed by other approaches, such as boosted trees or random forests.[5]\n An advantage of naive Bayes is that it only requires a small amount of training data to estimate the parameters necessary for classification.[6]\n Abstractly, naive Bayes is a conditional probability model: it assigns probabilities \n\n\n\np\n(\n\nC\n\nk\n\n\n\u2223\n\nx\n\n1\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n)\n\n\n{\\displaystyle p(C_{k}\\mid x_{1},\\ldots ,x_{n})}\n\n for each of the K possible outcomes or classes \n\n\n\n\nC\n\nk\n\n\n\n\n{\\displaystyle C_{k}}\n\n given a problem instance to be classified, represented by a vector \n\n\n\n\nx\n\n=\n(\n\nx\n\n1\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n)\n\n\n{\\displaystyle \\mathbf {x} =(x_{1},\\ldots ,x_{n})}\n\n encoding some n features (independent variables).[7]\n The problem with the above formulation is that if the number of features n is large or if a feature can take on a large number of values, then basing such a model on probability tables is infeasible. The model must therefore be reformulated to make it more tractable. Using Bayes' theorem, the conditional probability can be decomposed as:\n\n\n\n\np\n(\n\nC\n\nk\n\n\n\u2223\n\nx\n\n)\n=\n\n\n\np\n(\n\nC\n\nk\n\n\n)\n\u00a0\np\n(\n\nx\n\n\u2223\n\nC\n\nk\n\n\n)\n\n\np\n(\n\nx\n\n)\n\n\n\n\n\n\n{\\displaystyle p(C_{k}\\mid \\mathbf {x} )={\\frac {p(C_{k})\\ p(\\mathbf {x} \\mid C_{k})}{p(\\mathbf {x} )}}\\,}\n\n\n In plain English, using Bayesian probability terminology, the above equation can be written as\n\n\n\n\n\nposterior\n\n=\n\n\n\n\nprior\n\n\u00d7\n\nlikelihood\n\n\nevidence\n\n\n\n\n\n{\\displaystyle {\\text{posterior}}={\\frac {{\\text{prior}}\\times {\\text{likelihood}}}{\\text{evidence}}}\\,}\n\n\n In practice, there is interest only in the numerator of that fraction, because the denominator does not depend on \n\n\n\nC\n\n\n{\\displaystyle C}\n\n and the values of the features \n\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystyle x_{i}}\n\n are given, so that the denominator is effectively constant.\nThe numerator is equivalent to the joint probability model\n\n\n\n\np\n(\n\nC\n\nk\n\n\n,\n\nx\n\n1\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n)\n\n\n\n{\\displaystyle p(C_{k},x_{1},\\ldots ,x_{n})\\,}\n\n\nwhich can be rewritten as follows, using the chain rule for repeated applications of the definition of conditional probability:\n\n\n\n\n\n\n\n\np\n(\n\nC\n\nk\n\n\n,\n\nx\n\n1\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n)\n\n\n\n=\np\n(\n\nx\n\n1\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n,\n\nC\n\nk\n\n\n)\n\n\n\n\n\n\n=\np\n(\n\nx\n\n1\n\n\n\u2223\n\nx\n\n2\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n,\n\nC\n\nk\n\n\n)\n\u00a0\np\n(\n\nx\n\n2\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n,\n\nC\n\nk\n\n\n)\n\n\n\n\n\n\n=\np\n(\n\nx\n\n1\n\n\n\u2223\n\nx\n\n2\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n,\n\nC\n\nk\n\n\n)\n\u00a0\np\n(\n\nx\n\n2\n\n\n\u2223\n\nx\n\n3\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n,\n\nC\n\nk\n\n\n)\n\u00a0\np\n(\n\nx\n\n3\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n,\n\nC\n\nk\n\n\n)\n\n\n\n\n\n\n=\n\u22ef\n\n\n\n\n\n\n=\np\n(\n\nx\n\n1\n\n\n\u2223\n\nx\n\n2\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n,\n\nC\n\nk\n\n\n)\n\u00a0\np\n(\n\nx\n\n2\n\n\n\u2223\n\nx\n\n3\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n,\n\nC\n\nk\n\n\n)\n\u22ef\np\n(\n\nx\n\nn\n\u2212\n1\n\n\n\u2223\n\nx\n\nn\n\n\n,\n\nC\n\nk\n\n\n)\n\u00a0\np\n(\n\nx\n\nn\n\n\n\u2223\n\nC\n\nk\n\n\n)\n\u00a0\np\n(\n\nC\n\nk\n\n\n)\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}p(C_{k},x_{1},\\ldots ,x_{n})&=p(x_{1},\\ldots ,x_{n},C_{k})\\\\&=p(x_{1}\\mid x_{2},\\ldots ,x_{n},C_{k})\\ p(x_{2},\\ldots ,x_{n},C_{k})\\\\&=p(x_{1}\\mid x_{2},\\ldots ,x_{n},C_{k})\\ p(x_{2}\\mid x_{3},\\ldots ,x_{n},C_{k})\\ p(x_{3},\\ldots ,x_{n},C_{k})\\\\&=\\cdots \\\\&=p(x_{1}\\mid x_{2},\\ldots ,x_{n},C_{k})\\ p(x_{2}\\mid x_{3},\\ldots ,x_{n},C_{k})\\cdots p(x_{n-1}\\mid x_{n},C_{k})\\ p(x_{n}\\mid C_{k})\\ p(C_{k})\\\\\\end{aligned}}}\n\n\n Now the \"naive\" conditional independence assumptions come into play: assume that all features in \n\n\n\n\nx\n\n\n\n{\\displaystyle \\mathbf {x} }\n\n are mutually independent, conditional on the category \n\n\n\n\nC\n\nk\n\n\n\n\n{\\displaystyle C_{k}}\n\n. Under this assumption,\n\n\n\n\np\n(\n\nx\n\ni\n\n\n\u2223\n\nx\n\ni\n+\n1\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n,\n\nC\n\nk\n\n\n)\n=\np\n(\n\nx\n\ni\n\n\n\u2223\n\nC\n\nk\n\n\n)\n\n.\n\n\n{\\displaystyle p(x_{i}\\mid x_{i+1},\\ldots ,x_{n},C_{k})=p(x_{i}\\mid C_{k})\\,.}\n\n\n Thus, the joint model can be expressed as\n\n\n\n\n\n\n\n\np\n(\n\nC\n\nk\n\n\n\u2223\n\nx\n\n1\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n)\n\u221d\n\u00a0\n\n\np\n(\n\nC\n\nk\n\n\n,\n\nx\n\n1\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n)\n\n\n\n\n\n\n=\np\n(\n\nC\n\nk\n\n\n)\n\u00a0\np\n(\n\nx\n\n1\n\n\n\u2223\n\nC\n\nk\n\n\n)\n\u00a0\np\n(\n\nx\n\n2\n\n\n\u2223\n\nC\n\nk\n\n\n)\n\u00a0\np\n(\n\nx\n\n3\n\n\n\u2223\n\nC\n\nk\n\n\n)\n\u00a0\n\u22ef\n\n\n\n\n\n\n=\np\n(\n\nC\n\nk\n\n\n)\n\n\u220f\n\ni\n=\n1\n\n\nn\n\n\np\n(\n\nx\n\ni\n\n\n\u2223\n\nC\n\nk\n\n\n)\n\n,\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}p(C_{k}\\mid x_{1},\\ldots ,x_{n})\\varpropto \\ &p(C_{k},x_{1},\\ldots ,x_{n})\\\\&=p(C_{k})\\ p(x_{1}\\mid C_{k})\\ p(x_{2}\\mid C_{k})\\ p(x_{3}\\mid C_{k})\\ \\cdots \\\\&=p(C_{k})\\prod _{i=1}^{n}p(x_{i}\\mid C_{k})\\,,\\end{aligned}}}\n\n\nwhere \n\n\n\n\u221d\n\n\n{\\displaystyle \\varpropto }\n\n denotes proportionality since the denominator \n\n\n\np\n(\n\nx\n\n)\n\n\n{\\displaystyle p(\\mathbf {x} )}\n\n is omitted.\n This means that under the above independence assumptions, the conditional distribution over the class variable \n\n\n\nC\n\n\n{\\displaystyle C}\n\n is:\n\n\n\n\np\n(\n\nC\n\nk\n\n\n\u2223\n\nx\n\n1\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n)\n=\n\n\n1\nZ\n\n\n\u00a0\np\n(\n\nC\n\nk\n\n\n)\n\n\u220f\n\ni\n=\n1\n\n\nn\n\n\np\n(\n\nx\n\ni\n\n\n\u2223\n\nC\n\nk\n\n\n)\n\n\n{\\displaystyle p(C_{k}\\mid x_{1},\\ldots ,x_{n})={\\frac {1}{Z}}\\ p(C_{k})\\prod _{i=1}^{n}p(x_{i}\\mid C_{k})}\n\n\nwhere the evidence \n\n\n\nZ\n=\np\n(\n\nx\n\n)\n=\n\n\u2211\n\nk\n\n\np\n(\n\nC\n\nk\n\n\n)\n\u00a0\np\n(\n\nx\n\n\u2223\n\nC\n\nk\n\n\n)\n\n\n{\\displaystyle Z=p(\\mathbf {x} )=\\sum _{k}p(C_{k})\\ p(\\mathbf {x} \\mid C_{k})}\n\n is a scaling factor dependent only on \n\n\n\n\nx\n\n1\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n\n\n{\\displaystyle x_{1},\\ldots ,x_{n}}\n\n, that is, a constant if the values of the feature variables are known.\n The discussion so far has derived the independent feature model, that is, the naive Bayes probability model. The naive Bayes classifier combines this model with a decision rule. One common rule is to pick the hypothesis that is most probable so as to minimize the probability of misclassification; this is known as the maximum a posteriori or MAP decision rule. The corresponding classifier, a Bayes classifier, is the function that assigns a class label \n\n\n\n\n\n\ny\n^\n\n\n\n=\n\nC\n\nk\n\n\n\n\n{\\displaystyle {\\hat {y}}=C_{k}}\n\n for some k as follows:\n\n\n\n\n\n\n\ny\n^\n\n\n\n=\n\n\nargmax\n\nk\n\u2208\n{\n1\n,\n\u2026\n,\nK\n}\n\n\n\n\u00a0\np\n(\n\nC\n\nk\n\n\n)\n\n\n\u220f\n\ni\n=\n1\n\n\nn\n\n\np\n(\n\nx\n\ni\n\n\n\u2223\n\nC\n\nk\n\n\n)\n.\n\n\n\n{\\displaystyle {\\hat {y}}={\\underset {k\\in \\{1,\\ldots ,K\\}}{\\operatorname {argmax} }}\\ p(C_{k})\\displaystyle \\prod _{i=1}^{n}p(x_{i}\\mid C_{k}).}\n\n\n A class's prior may be calculated by assuming equiprobable classes, i.e., \n\n\n\np\n(\n\nC\n\nk\n\n\n)\n=\n\n\n1\nK\n\n\n\n\n{\\displaystyle p(C_{k})={\\frac {1}{K}}}\n\n, or by calculating an estimate for the class probability from the training set:\n\n\n\n\n\nprior for a given class\n\n=\n\n\nno. of samples in that class\ntotal no. of samples\n\n\n\n\n\n{\\displaystyle {\\text{prior for a given class}}={\\frac {\\text{no. of samples in that class}}{\\text{total no. of samples}}}\\,}\n\n\nTo estimate the parameters for a feature's distribution, one must assume a distribution or generate nonparametric models for the features from the training set.[8]\n The assumptions on distributions of features are called the \"event model\" of the naive Bayes classifier. For discrete features like the ones encountered in document classification (include spam filtering), multinomial and Bernoulli distributions are popular. These assumptions lead to two distinct models, which are often confused.[9][10]\n When dealing with continuous data, a typical assumption is that the continuous values associated with each class are distributed according to a normal (or Gaussian) distribution. For example, suppose the training data contains a continuous attribute, \n\n\n\nx\n\n\n{\\displaystyle x}\n\n. The data is first segmented by the class, and then the mean and variance of \n\n\n\nx\n\n\n{\\displaystyle x}\n\n is computed in each class. Let \n\n\n\n\n\u03bc\n\nk\n\n\n\n\n{\\displaystyle \\mu _{k}}\n\n be the mean of the values in \n\n\n\nx\n\n\n{\\displaystyle x}\n\n associated with class \n\n\n\n\nC\n\nk\n\n\n\n\n{\\displaystyle C_{k}}\n\n, and let \n\n\n\n\n\u03c3\n\nk\n\n\n2\n\n\n\n\n{\\displaystyle \\sigma _{k}^{2}}\n\n be the Bessel corrected variance of the values in \n\n\n\nx\n\n\n{\\displaystyle x}\n\n associated with class \n\n\n\n\nC\n\nk\n\n\n\n\n{\\displaystyle C_{k}}\n\n. Suppose one has collected some observation value \n\n\n\nv\n\n\n{\\displaystyle v}\n\n. Then, the probability density of \n\n\n\nv\n\n\n{\\displaystyle v}\n\n given a class \n\n\n\n\nC\n\nk\n\n\n\n\n{\\displaystyle C_{k}}\n\n, i.e., \n\n\n\np\n(\nx\n=\nv\n\u2223\n\nC\n\nk\n\n\n)\n\n\n{\\displaystyle p(x=v\\mid C_{k})}\n\n, can be computed by plugging \n\n\n\nv\n\n\n{\\displaystyle v}\n\n into the equation for a normal distribution parameterized by \n\n\n\n\n\u03bc\n\nk\n\n\n\n\n{\\displaystyle \\mu _{k}}\n\n and \n\n\n\n\n\u03c3\n\nk\n\n\n2\n\n\n\n\n{\\displaystyle \\sigma _{k}^{2}}\n\n. Formally,\n\n\n\n\np\n(\nx\n=\nv\n\u2223\n\nC\n\nk\n\n\n)\n=\n\n\n1\n\n2\n\u03c0\n\n\u03c3\n\nk\n\n\n2\n\n\n\n\n\n\n\ne\n\n\u2212\n\n\n\n(\nv\n\u2212\n\n\u03bc\n\nk\n\n\n\n)\n\n2\n\n\n\n\n2\n\n\u03c3\n\nk\n\n\n2\n\n\n\n\n\n\n\n\n\n{\\displaystyle p(x=v\\mid C_{k})={\\frac {1}{\\sqrt {2\\pi \\sigma _{k}^{2}}}}\\,e^{-{\\frac {(v-\\mu _{k})^{2}}{2\\sigma _{k}^{2}}}}}\n\n\n Another common technique for handling continuous values is to use binning to discretize the feature values and obtain a new set of Bernoulli-distributed features. Some literature suggests that this is required in order to use naive Bayes, but it is not true, as the discretization may throw away discriminative information.[3]\n Sometimes the distribution of class-conditional marginal densities is far from normal. In these cases, kernel density estimation can be used for a more realistic estimate of the marginal densities of each class. This method, which was introduced by John and Langley,[8] can boost the accuracy of the classifier considerably.[11][12]\n With a multinomial event model, samples (feature vectors) represent the frequencies with which certain events have been generated by a multinomial \n\n\n\n(\n\np\n\n1\n\n\n,\n\u2026\n,\n\np\n\nn\n\n\n)\n\n\n{\\displaystyle (p_{1},\\dots ,p_{n})}\n\n where \n\n\n\n\np\n\ni\n\n\n\n\n{\\displaystyle p_{i}}\n\n is the probability that event i occurs (or K such multinomials in the multiclass case). A feature vector \n\n\n\n\nx\n\n=\n(\n\nx\n\n1\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n)\n\n\n{\\displaystyle \\mathbf {x} =(x_{1},\\dots ,x_{n})}\n\n is then a histogram, with \n\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystyle x_{i}}\n\n counting the number of times event i was observed in a particular instance. This is the event model typically used for document classification, with events representing the occurrence of a word in a single document (see bag of words assumption).[13] The likelihood of observing a histogram x is given by:\n\n\n\n\np\n(\n\nx\n\n\u2223\n\nC\n\nk\n\n\n)\n=\n\n\n\n(\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\nx\n\ni\n\n\n)\n!\n\n\n\n\u220f\n\ni\n=\n1\n\n\nn\n\n\n\nx\n\ni\n\n\n!\n\n\n\n\n\u220f\n\ni\n=\n1\n\n\nn\n\n\n\n\n\np\n\nk\ni\n\n\n\n\n\nx\n\ni\n\n\n\n\n\n\n{\\displaystyle p(\\mathbf {x} \\mid C_{k})={\\frac {(\\sum _{i=1}^{n}x_{i})!}{\\prod _{i=1}^{n}x_{i}!}}\\prod _{i=1}^{n}{p_{ki}}^{x_{i}}}\n\n\nwhere \n\n\n\n\np\n\nk\ni\n\n\n:=\np\n(\ni\n\u2223\n\nC\n\nk\n\n\n)\n\n\n{\\displaystyle p_{ki}:=p(i\\mid C_{k})}\n\n. \n The multinomial naive Bayes classifier becomes a linear classifier when expressed in log-space:[14]\n\n\n\n\n\n\n\n\nlog\n\u2061\np\n(\n\nC\n\nk\n\n\n\u2223\n\nx\n\n)\n\n\n\n\u221d\nlog\n\u2061\n\n(\n\np\n(\n\nC\n\nk\n\n\n)\n\n\u220f\n\ni\n=\n1\n\n\nn\n\n\n\n\n\np\n\nk\ni\n\n\n\n\n\nx\n\ni\n\n\n\n\n\n)\n\n\n\n\n\n\n\n=\nlog\n\u2061\np\n(\n\nC\n\nk\n\n\n)\n+\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\nx\n\ni\n\n\n\u22c5\nlog\n\u2061\n\np\n\nk\ni\n\n\n\n\n\n\n\n\n=\nb\n+\n\n\nw\n\n\nk\n\n\n\u22a4\n\n\n\nx\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}\\log p(C_{k}\\mid \\mathbf {x} )&\\varpropto \\log \\left(p(C_{k})\\prod _{i=1}^{n}{p_{ki}}^{x_{i}}\\right)\\\\&=\\log p(C_{k})+\\sum _{i=1}^{n}x_{i}\\cdot \\log p_{ki}\\\\&=b+\\mathbf {w} _{k}^{\\top }\\mathbf {x} \\end{aligned}}}\n\n\nwhere \n\n\n\nb\n=\nlog\n\u2061\np\n(\n\nC\n\nk\n\n\n)\n\n\n{\\displaystyle b=\\log p(C_{k})}\n\n and \n\n\n\n\nw\n\nk\ni\n\n\n=\nlog\n\u2061\n\np\n\nk\ni\n\n\n\n\n{\\displaystyle w_{ki}=\\log p_{ki}}\n\n. Estimating the parameters in log space is advantageous since multiplying a large number of small values can lead to significant rounding error. Applying a log transform reduces the effect of this rounding error.\n If a given class and feature value never occur together in the training data, then the frequency-based probability estimate will be zero, because the probability estimate is directly proportional to the number of occurrences of a feature's value. This is problematic because it will wipe out all information in the other probabilities when they are multiplied. Therefore, it is often desirable to incorporate a small-sample correction, called pseudocount, in all probability estimates such that no probability is ever set to be exactly zero. This way of regularizing naive Bayes is called Laplace smoothing when the pseudocount is one, and Lidstone smoothing in the general case.\n Rennie et al. discuss problems with the multinomial assumption in the context of document classification and possible ways to alleviate those problems, including the use of tf\u2013idf weights instead of raw term frequencies and document length normalization, to produce a naive Bayes classifier that is competitive with support vector machines.[14]\n In the multivariate Bernoulli event model, features are independent Boolean variables (binary variables) describing inputs. Like the multinomial model, this model is popular for document classification tasks,[9] where binary term occurrence features are used rather than term frequencies. If \n\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystyle x_{i}}\n\n is a Boolean expressing the occurrence or absence of the i'th term from the vocabulary, then the likelihood of a document given a class \n\n\n\n\nC\n\nk\n\n\n\n\n{\\displaystyle C_{k}}\n\n is given by:[9]\n\n\n\n\np\n(\n\nx\n\n\u2223\n\nC\n\nk\n\n\n)\n=\n\n\u220f\n\ni\n=\n1\n\n\nn\n\n\n\np\n\nk\ni\n\n\n\nx\n\ni\n\n\n\n\n(\n1\n\u2212\n\np\n\nk\ni\n\n\n\n)\n\n(\n1\n\u2212\n\nx\n\ni\n\n\n)\n\n\n\n\n{\\displaystyle p(\\mathbf {x} \\mid C_{k})=\\prod _{i=1}^{n}p_{ki}^{x_{i}}(1-p_{ki})^{(1-x_{i})}}\n\n\nwhere \n\n\n\n\np\n\nk\ni\n\n\n\n\n{\\displaystyle p_{ki}}\n\n is the probability of class \n\n\n\n\nC\n\nk\n\n\n\n\n{\\displaystyle C_{k}}\n\n generating the term \n\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystyle x_{i}}\n\n. This event model is especially popular for classifying short texts. It has the benefit of explicitly modelling the absence of terms. Note that a naive Bayes classifier with a Bernoulli event model is not the same as a multinomial NB classifier with frequency counts truncated to one.\n Given a way to train a naive Bayes classifier from labeled data, it's possible to construct a semi-supervised training algorithm that can learn from a combination of labeled and unlabeled data by running the supervised learning algorithm in a loop:[15]\n Convergence is determined based on improvement to the model likelihood \n\n\n\nP\n(\nD\n\u2223\n\u03b8\n)\n\n\n{\\displaystyle P(D\\mid \\theta )}\n\n, where \n\n\n\n\u03b8\n\n\n{\\displaystyle \\theta }\n\n denotes the parameters of the naive Bayes model.\n This training algorithm is an instance of the more general expectation\u2013maximization algorithm (EM): the prediction step inside the loop is the E-step of EM, while the re-training of naive Bayes is the M-step. The algorithm is formally justified by the assumption that the data are generated by a mixture model, and the components of this mixture model are exactly the classes of the classification problem.[15]\n Despite the fact that the far-reaching independence assumptions are often inaccurate, the naive Bayes classifier has several properties that make it surprisingly useful in practice. In particular, the decoupling of the class conditional feature distributions means that each distribution can be independently estimated as a one-dimensional distribution. This helps alleviate problems stemming from the curse of dimensionality, such as the need for data sets that scale exponentially with the number of features. While naive Bayes often fails to produce a good estimate for the correct class probabilities,[16] this may not be a requirement for many applications. For example, the naive Bayes classifier will make the correct MAP decision rule classification so long as the correct class is predicted as more probable than any other class. This is true regardless of whether the probability estimate is slightly, or even grossly inaccurate. In this manner, the overall classifier can be robust enough to ignore serious deficiencies in its underlying naive probability model.[17] Other reasons for the observed success of the naive Bayes classifier are discussed in the literature cited below.\n In the case of discrete inputs (indicator or frequency features for discrete events), naive Bayes classifiers form a generative-discriminative pair with multinomial logistic regression classifiers: each naive Bayes classifier can be considered a way of fitting a probability model that optimizes the joint likelihood \n\n\n\np\n(\nC\n,\n\nx\n\n)\n\n\n{\\displaystyle p(C,\\mathbf {x} )}\n\n, while logistic regression fits the same probability model to optimize the conditional \n\n\n\np\n(\nC\n\u2223\n\nx\n\n)\n\n\n{\\displaystyle p(C\\mid \\mathbf {x} )}\n\n.[18]\n More formally, we have the following:\n Theorem\u00a0\u2014\u00a0Naive Bayes classifiers on binary features are subsumed by logistic regression classifiers.\n Consider a generic multiclass classification problem, with possible classes \n\n\n\nY\n\u2208\n{\n1\n,\n.\n.\n.\n,\nn\n}\n\n\n{\\displaystyle Y\\in \\{1,...,n\\}}\n\n, then the (non-naive) Bayes classifier gives, by Bayes theorem:\n\n\n\n\np\n(\nY\n\u2223\nX\n=\nx\n)\n=\n\nsoftmax\n\n(\n{\nln\n\u2061\np\n(\nY\n=\nk\n)\n+\nln\n\u2061\np\n(\nX\n=\nx\n\u2223\nY\n=\nk\n)\n\n}\n\nk\n\n\n)\n\n\n{\\displaystyle p(Y\\mid X=x)={\\text{softmax}}(\\{\\ln p(Y=k)+\\ln p(X=x\\mid Y=k)\\}_{k})}\n\n\n The naive Bayes classifier gives  \n\n\n\n\n\nsoftmax\n\n\n(\n\n\n{\n\nln\n\u2061\np\n(\nY\n=\nk\n)\n+\n\n\n1\n2\n\n\n\n\u2211\n\ni\n\n\n(\n\na\n\ni\n,\nk\n\n\n+\n\n\n\u2212\n\na\n\ni\n,\nk\n\n\n\u2212\n\n\n)\n\nx\n\ni\n\n\n+\n(\n\na\n\ni\n,\nk\n\n\n+\n\n\n+\n\na\n\ni\n,\nk\n\n\n\u2212\n\n\n)\n\n}\n\n\nk\n\n\n)\n\n\n\n{\\displaystyle {\\text{softmax}}\\left(\\left\\{\\ln p(Y=k)+{\\frac {1}{2}}\\sum _{i}(a_{i,k}^{+}-a_{i,k}^{-})x_{i}+(a_{i,k}^{+}+a_{i,k}^{-})\\right\\}_{k}\\right)}\n\n\nwhere   \n\n\n\n\n\na\n\ni\n,\ns\n\n\n+\n\n\n=\nln\n\u2061\np\n(\n\nX\n\ni\n\n\n=\n+\n1\n\u2223\nY\n=\ns\n)\n;\n\n\na\n\ni\n,\ns\n\n\n\u2212\n\n\n=\nln\n\u2061\np\n(\n\nX\n\ni\n\n\n=\n\u2212\n1\n\u2223\nY\n=\ns\n)\n\n\n{\\displaystyle a_{i,s}^{+}=\\ln p(X_{i}=+1\\mid Y=s);\\quad a_{i,s}^{-}=\\ln p(X_{i}=-1\\mid Y=s)}\n\n\n This is exactly a logistic regression classifier.\n The link between the two can be seen by observing that the decision function for naive Bayes (in the binary case) can be rewritten as \"predict class \n\n\n\n\nC\n\n1\n\n\n\n\n{\\displaystyle C_{1}}\n\n if the odds of \n\n\n\np\n(\n\nC\n\n1\n\n\n\u2223\n\nx\n\n)\n\n\n{\\displaystyle p(C_{1}\\mid \\mathbf {x} )}\n\n exceed those of \n\n\n\np\n(\n\nC\n\n2\n\n\n\u2223\n\nx\n\n)\n\n\n{\\displaystyle p(C_{2}\\mid \\mathbf {x} )}\n\n\". Expressing this in log-space gives:\n\n\n\n\nlog\n\u2061\n\n\n\np\n(\n\nC\n\n1\n\n\n\u2223\n\nx\n\n)\n\n\np\n(\n\nC\n\n2\n\n\n\u2223\n\nx\n\n)\n\n\n\n=\nlog\n\u2061\np\n(\n\nC\n\n1\n\n\n\u2223\n\nx\n\n)\n\u2212\nlog\n\u2061\np\n(\n\nC\n\n2\n\n\n\u2223\n\nx\n\n)\n>\n0\n\n\n{\\displaystyle \\log {\\frac {p(C_{1}\\mid \\mathbf {x} )}{p(C_{2}\\mid \\mathbf {x} )}}=\\log p(C_{1}\\mid \\mathbf {x} )-\\log p(C_{2}\\mid \\mathbf {x} )>0}\n\n\n The left-hand side of this equation is the log-odds, or logit, the quantity predicted by the linear model that underlies logistic regression. Since naive Bayes is also a linear model for the two \"discrete\" event models, it can be reparametrised as a linear function \n\n\n\nb\n+\n\n\nw\n\n\n\u22a4\n\n\nx\n>\n0\n\n\n{\\displaystyle b+\\mathbf {w} ^{\\top }x>0}\n\n. Obtaining the probabilities is then a matter of applying the logistic function to \n\n\n\nb\n+\n\n\nw\n\n\n\u22a4\n\n\nx\n\n\n{\\displaystyle b+\\mathbf {w} ^{\\top }x}\n\n, or in the multiclass case, the softmax function.\n Discriminative classifiers have lower asymptotic error than generative ones; however, research by Ng and Jordan has shown that in some practical cases naive Bayes can outperform logistic regression because it reaches its asymptotic error faster.[18]\n Problem: classify whether a given person is a male or a female based on the measured features.\nThe features include height, weight, and foot size. Although with NB classifier we treat them as independent, they are not in reality.\n Example training set below.\n The classifier created from the training set using a Gaussian distribution assumption would be (given variances are unbiased sample variances):\n The following example assumes equiprobable classes so that P(male)= P(female) = 0.5. This prior probability distribution might be based on prior knowledge of frequencies in the larger population or in the training set.\n Below is a sample to be classified as male or female.\n In order to classify the sample, one has to determine which posterior is greater, male or female. For the classification as male the posterior is given by\n\n\n\n\n\nposterior (male)\n\n=\n\n\n\nP\n(\n\nmale\n\n)\n\np\n(\n\nheight\n\n\u2223\n\nmale\n\n)\n\np\n(\n\nweight\n\n\u2223\n\nmale\n\n)\n\np\n(\n\nfoot size\n\n\u2223\n\nmale\n\n)\n\nevidence\n\n\n\n\n{\\displaystyle {\\text{posterior (male)}}={\\frac {P({\\text{male}})\\,p({\\text{height}}\\mid {\\text{male}})\\,p({\\text{weight}}\\mid {\\text{male}})\\,p({\\text{foot size}}\\mid {\\text{male}})}{\\text{evidence}}}}\n\n\n For the classification as female the posterior is given by\n\n\n\n\n\nposterior (female)\n\n=\n\n\n\nP\n(\n\nfemale\n\n)\n\np\n(\n\nheight\n\n\u2223\n\nfemale\n\n)\n\np\n(\n\nweight\n\n\u2223\n\nfemale\n\n)\n\np\n(\n\nfoot size\n\n\u2223\n\nfemale\n\n)\n\nevidence\n\n\n\n\n{\\displaystyle {\\text{posterior (female)}}={\\frac {P({\\text{female}})\\,p({\\text{height}}\\mid {\\text{female}})\\,p({\\text{weight}}\\mid {\\text{female}})\\,p({\\text{foot size}}\\mid {\\text{female}})}{\\text{evidence}}}}\n\n\n The evidence (also termed normalizing constant) may be calculated:\n\n\n\n\n\n\n\n\n\nevidence\n\n=\nP\n(\n\nmale\n\n)\n\np\n(\n\nheight\n\n\u2223\n\nmale\n\n)\n\np\n(\n\nweight\n\n\u2223\n\nmale\n\n)\n\np\n(\n\nfoot size\n\n\u2223\n\nmale\n\n)\n\n\n\n\n+\nP\n(\n\nfemale\n\n)\n\np\n(\n\nheight\n\n\u2223\n\nfemale\n\n)\n\np\n(\n\nweight\n\n\u2223\n\nfemale\n\n)\n\np\n(\n\nfoot size\n\n\u2223\n\nfemale\n\n)\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}{\\text{evidence}}=P({\\text{male}})\\,p({\\text{height}}\\mid {\\text{male}})\\,p({\\text{weight}}\\mid {\\text{male}})\\,p({\\text{foot size}}\\mid {\\text{male}})\\\\+P({\\text{female}})\\,p({\\text{height}}\\mid {\\text{female}})\\,p({\\text{weight}}\\mid {\\text{female}})\\,p({\\text{foot size}}\\mid {\\text{female}})\\end{aligned}}}\n\n\n However, given the sample, the evidence is a constant and thus scales both posteriors equally. It therefore does not affect classification and can be ignored.  The probability distribution for the sex of the sample can now be determined:\n\n\n\n\nP\n(\n\nmale\n\n)\n=\n0.5\n\n\n{\\displaystyle P({\\text{male}})=0.5}\n\n\n\n\n\n\np\n(\n\nheight\n\n\u2223\n\nmale\n\n)\n=\n\n\n1\n\n2\n\u03c0\n\n\u03c3\n\n2\n\n\n\n\n\nexp\n\u2061\n\n(\n\n\n\n\u2212\n(\n6\n\u2212\n\u03bc\n\n)\n\n2\n\n\n\n\n2\n\n\u03c3\n\n2\n\n\n\n\n\n)\n\n\u2248\n1.5789\n,\n\n\n{\\displaystyle p({\\text{height}}\\mid {\\text{male}})={\\frac {1}{\\sqrt {2\\pi \\sigma ^{2}}}}\\exp \\left({\\frac {-(6-\\mu )^{2}}{2\\sigma ^{2}}}\\right)\\approx 1.5789,}\n\n\nwhere \n\n\n\n\u03bc\n=\n5.855\n\n\n{\\displaystyle \\mu =5.855}\n\n and \n\n\n\n\n\u03c3\n\n2\n\n\n=\n3.5033\n\u22c5\n\n10\n\n\u2212\n2\n\n\n\n\n{\\displaystyle \\sigma ^{2}=3.5033\\cdot 10^{-2}}\n\n are the parameters of normal distribution which have been previously determined from the training set. Note that a value greater than 1 is OK here \u2013 it is a probability density rather than a probability, because height is a continuous variable.\n \n\n\n\np\n(\n\nweight\n\n\u2223\n\nmale\n\n)\n=\n\n\n1\n\n2\n\u03c0\n\n\u03c3\n\n2\n\n\n\n\n\nexp\n\u2061\n\n(\n\n\n\n\u2212\n(\n130\n\u2212\n\u03bc\n\n)\n\n2\n\n\n\n\n2\n\n\u03c3\n\n2\n\n\n\n\n\n)\n\n=\n5.9881\n\u22c5\n\n10\n\n\u2212\n6\n\n\n\n\n{\\displaystyle p({\\text{weight}}\\mid {\\text{male}})={\\frac {1}{\\sqrt {2\\pi \\sigma ^{2}}}}\\exp \\left({\\frac {-(130-\\mu )^{2}}{2\\sigma ^{2}}}\\right)=5.9881\\cdot 10^{-6}}\n\n\n\n\n\n\np\n(\n\nfoot size\n\n\u2223\n\nmale\n\n)\n=\n\n\n1\n\n2\n\u03c0\n\n\u03c3\n\n2\n\n\n\n\n\nexp\n\u2061\n\n(\n\n\n\n\u2212\n(\n8\n\u2212\n\u03bc\n\n)\n\n2\n\n\n\n\n2\n\n\u03c3\n\n2\n\n\n\n\n\n)\n\n=\n1.3112\n\u22c5\n\n10\n\n\u2212\n3\n\n\n\n\n{\\displaystyle p({\\text{foot size}}\\mid {\\text{male}})={\\frac {1}{\\sqrt {2\\pi \\sigma ^{2}}}}\\exp \\left({\\frac {-(8-\\mu )^{2}}{2\\sigma ^{2}}}\\right)=1.3112\\cdot 10^{-3}}\n\n\n\n\n\n\n\nposterior numerator (male)\n\n=\n\ntheir product\n\n=\n6.1984\n\u22c5\n\n10\n\n\u2212\n9\n\n\n\n\n{\\displaystyle {\\text{posterior numerator (male)}}={\\text{their product}}=6.1984\\cdot 10^{-9}}\n\n\n \n\n\n\nP\n(\n\nfemale\n\n)\n=\n0.5\n\n\n{\\displaystyle P({\\text{female}})=0.5}\n\n\n\n\n\n\np\n(\n\nheight\n\n\u2223\n\nfemale\n\n)\n=\n2.23\n\u22c5\n\n10\n\n\u2212\n1\n\n\n\n\n{\\displaystyle p({\\text{height}}\\mid {\\text{female}})=2.23\\cdot 10^{-1}}\n\n\n\n\n\n\np\n(\n\nweight\n\n\u2223\n\nfemale\n\n)\n=\n1.6789\n\u22c5\n\n10\n\n\u2212\n2\n\n\n\n\n{\\displaystyle p({\\text{weight}}\\mid {\\text{female}})=1.6789\\cdot 10^{-2}}\n\n\n\n\n\n\np\n(\n\nfoot size\n\n\u2223\n\nfemale\n\n)\n=\n2.8669\n\u22c5\n\n10\n\n\u2212\n1\n\n\n\n\n{\\displaystyle p({\\text{foot size}}\\mid {\\text{female}})=2.8669\\cdot 10^{-1}}\n\n\n\n\n\n\n\nposterior numerator (female)\n\n=\n\ntheir product\n\n=\n5.3778\n\u22c5\n\n10\n\n\u2212\n4\n\n\n\n\n{\\displaystyle {\\text{posterior numerator (female)}}={\\text{their product}}=5.3778\\cdot 10^{-4}}\n\n\n Since posterior numerator is greater in the female case, the prediction is that the sample is female.\n Here is a worked example of naive Bayesian classification to the document classification problem.\nConsider the problem of classifying documents by their content, for example into spam and non-spam e-mails. Imagine that documents are drawn from a number of classes of documents which can be modeled as sets of words where the (independent) probability that the i-th word of a given document occurs in a document from class C can be written as\n\n\n\n\np\n(\n\nw\n\ni\n\n\n\u2223\nC\n)\n\n\n\n{\\displaystyle p(w_{i}\\mid C)\\,}\n\n\n (For this treatment, things are further simplified by assuming that words are randomly distributed in the document - that is, words are not dependent on the length of the document, position within the document with relation to other words, or other document-context.)\n Then the probability that a given document D contains all of the words \n\n\n\n\nw\n\ni\n\n\n\n\n{\\displaystyle w_{i}}\n\n, given a class C, is\n\n\n\n\np\n(\nD\n\u2223\nC\n)\n=\n\n\u220f\n\ni\n\n\np\n(\n\nw\n\ni\n\n\n\u2223\nC\n)\n\n\n\n{\\displaystyle p(D\\mid C)=\\prod _{i}p(w_{i}\\mid C)\\,}\n\n\n The question that has to be answered is: \"what is the probability that a given document D belongs to a given class C?\" In other words, what is \n\n\n\np\n(\nC\n\u2223\nD\n)\n\n\n\n{\\displaystyle p(C\\mid D)\\,}\n\n?\n Now by definition\n\n\n\n\np\n(\nD\n\u2223\nC\n)\n=\n\n\n\np\n(\nD\n\u2229\nC\n)\n\n\np\n(\nC\n)\n\n\n\n\n\n{\\displaystyle p(D\\mid C)={p(D\\cap C) \\over p(C)}}\n\n\nand\n\n\n\n\np\n(\nC\n\u2223\nD\n)\n=\n\n\n\np\n(\nD\n\u2229\nC\n)\n\n\np\n(\nD\n)\n\n\n\n\n\n{\\displaystyle p(C\\mid D)={p(D\\cap C) \\over p(D)}}\n\n\n Bayes' theorem manipulates these into a statement of probability in terms of likelihood.\n\n\n\n\np\n(\nC\n\u2223\nD\n)\n=\n\n\n\np\n(\nC\n)\n\np\n(\nD\n\u2223\nC\n)\n\n\np\n(\nD\n)\n\n\n\n\n\n{\\displaystyle p(C\\mid D)={\\frac {p(C)\\,p(D\\mid C)}{p(D)}}}\n\n\n Assume for the moment that there are only two mutually exclusive classes, S and \u00acS (e.g. spam and not spam), such that every element (email) is in either one or the other;\n\n\n\n\np\n(\nD\n\u2223\nS\n)\n=\n\n\u220f\n\ni\n\n\np\n(\n\nw\n\ni\n\n\n\u2223\nS\n)\n\n\n\n{\\displaystyle p(D\\mid S)=\\prod _{i}p(w_{i}\\mid S)\\,}\n\n\nand\n\n\n\n\np\n(\nD\n\u2223\n\u00ac\nS\n)\n=\n\n\u220f\n\ni\n\n\np\n(\n\nw\n\ni\n\n\n\u2223\n\u00ac\nS\n)\n\n\n\n{\\displaystyle p(D\\mid \\neg S)=\\prod _{i}p(w_{i}\\mid \\neg S)\\,}\n\n\n Using the Bayesian result above, one can write:\n\n\n\n\np\n(\nS\n\u2223\nD\n)\n=\n\n\n\np\n(\nS\n)\n\n\np\n(\nD\n)\n\n\n\n\n\n\u220f\n\ni\n\n\np\n(\n\nw\n\ni\n\n\n\u2223\nS\n)\n\n\n{\\displaystyle p(S\\mid D)={p(S) \\over p(D)}\\,\\prod _{i}p(w_{i}\\mid S)}\n\n\n\n\n\n\np\n(\n\u00ac\nS\n\u2223\nD\n)\n=\n\n\n\np\n(\n\u00ac\nS\n)\n\n\np\n(\nD\n)\n\n\n\n\n\n\u220f\n\ni\n\n\np\n(\n\nw\n\ni\n\n\n\u2223\n\u00ac\nS\n)\n\n\n{\\displaystyle p(\\neg S\\mid D)={p(\\neg S) \\over p(D)}\\,\\prod _{i}p(w_{i}\\mid \\neg S)}\n\n\n Dividing one by the other gives:\n\n\n\n\n\n\n\np\n(\nS\n\u2223\nD\n)\n\n\np\n(\n\u00ac\nS\n\u2223\nD\n)\n\n\n\n=\n\n\n\np\n(\nS\n)\n\n\n\u220f\n\ni\n\n\np\n(\n\nw\n\ni\n\n\n\u2223\nS\n)\n\n\np\n(\n\u00ac\nS\n)\n\n\n\u220f\n\ni\n\n\np\n(\n\nw\n\ni\n\n\n\u2223\n\u00ac\nS\n)\n\n\n\n\n\n{\\displaystyle {p(S\\mid D) \\over p(\\neg S\\mid D)}={p(S)\\,\\prod _{i}p(w_{i}\\mid S) \\over p(\\neg S)\\,\\prod _{i}p(w_{i}\\mid \\neg S)}}\n\n\n Which can be re-factored as:\n\n\n\n\n\n\n\np\n(\nS\n\u2223\nD\n)\n\n\np\n(\n\u00ac\nS\n\u2223\nD\n)\n\n\n\n=\n\n\n\np\n(\nS\n)\n\n\np\n(\n\u00ac\nS\n)\n\n\n\n\n\n\u220f\n\ni\n\n\n\n\n\np\n(\n\nw\n\ni\n\n\n\u2223\nS\n)\n\n\np\n(\n\nw\n\ni\n\n\n\u2223\n\u00ac\nS\n)\n\n\n\n\n\n{\\displaystyle {p(S\\mid D) \\over p(\\neg S\\mid D)}={p(S) \\over p(\\neg S)}\\,\\prod _{i}{p(w_{i}\\mid S) \\over p(w_{i}\\mid \\neg S)}}\n\n\n Thus, the probability ratio p(S | D) / p(\u00acS | D) can be expressed in terms of a series of likelihood ratios.\nThe actual probability p(S | D) can be easily computed from log (p(S | D) / p(\u00acS | D)) based on the observation that p(S | D) + p(\u00acS | D) = 1.\n Taking the logarithm of all these ratios, one obtains:\n\n\n\n\nln\n\u2061\n\n\n\np\n(\nS\n\u2223\nD\n)\n\n\np\n(\n\u00ac\nS\n\u2223\nD\n)\n\n\n\n=\nln\n\u2061\n\n\n\np\n(\nS\n)\n\n\np\n(\n\u00ac\nS\n)\n\n\n\n+\n\n\u2211\n\ni\n\n\nln\n\u2061\n\n\n\np\n(\n\nw\n\ni\n\n\n\u2223\nS\n)\n\n\np\n(\n\nw\n\ni\n\n\n\u2223\n\u00ac\nS\n)\n\n\n\n\n\n{\\displaystyle \\ln {p(S\\mid D) \\over p(\\neg S\\mid D)}=\\ln {p(S) \\over p(\\neg S)}+\\sum _{i}\\ln {p(w_{i}\\mid S) \\over p(w_{i}\\mid \\neg S)}}\n\n\n (This technique of \"log-likelihood ratios\" is a common technique in statistics.\nIn the case of two mutually exclusive alternatives (such as this example), the conversion of a log-likelihood ratio to a probability takes the form of a sigmoid curve: see logit for details.)\n Finally, the document can be classified as follows.  It is spam if \n\n\n\np\n(\nS\n\u2223\nD\n)\n>\np\n(\n\u00ac\nS\n\u2223\nD\n)\n\n\n{\\displaystyle p(S\\mid D)>p(\\neg S\\mid D)}\n\n (i. e., \n\n\n\nln\n\u2061\n\n\n\np\n(\nS\n\u2223\nD\n)\n\n\np\n(\n\u00ac\nS\n\u2223\nD\n)\n\n\n\n>\n0\n\n\n{\\displaystyle \\ln {p(S\\mid D) \\over p(\\neg S\\mid D)}>0}\n\n), otherwise it is not spam.\n",
        "doc_number": 86
    },
    {
        "url": "https://en.wikipedia.org/wiki/Logistic_regression",
        "content": "In statistics, the logistic model (or logit model) is a statistical model that models the log-odds of an event as a linear combination of one or more independent variables. In regression analysis, logistic regression[1] (or logit regression) estimates the parameters of a logistic model (the coefficients in the linear or non linear combinations). In binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled \"0\" and \"1\", while the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled \"1\" can vary between 0 (certainly the value \"0\") and 1 (certainly the value \"1\"), hence the labeling;[2] the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. See \u00a7\u00a0Background and \u00a7\u00a0Definition for formal mathematics, and \u00a7\u00a0Example for a worked example.\n Binary variables are widely used in statistics to model the probability of a certain class or event taking place, such as the probability of a team winning, of a patient being healthy, etc. (see \u00a7\u00a0Applications), and the logistic model has been the most commonly used model for binary regression since about 1970.[3] Binary variables can be generalized to categorical variables when there are more than two possible values (e.g. whether an image is of a cat, dog, lion, etc.), and the binary logistic regression generalized to multinomial logistic regression. If the multiple categories are ordered, one can use the ordinal logistic regression (for example the proportional odds ordinal logistic model[4]). See \u00a7\u00a0Extensions for further extensions. The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier.\n Analogous linear models for binary variables with a different sigmoid function instead of the logistic function (to convert the linear combination to a probability) can also be used, most notably the probit model; see \u00a7\u00a0Alternatives. The defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio. More abstractly, the logistic function is the natural parameter for the Bernoulli distribution, and in this sense is the \"simplest\" way to convert a real number to a probability. In particular, it maximizes entropy (minimizes added information), and in this sense makes the fewest assumptions of the data being modeled; see \u00a7\u00a0Maximum entropy.\n The parameters of a logistic regression are most commonly estimated by maximum-likelihood estimation (MLE). This does not have a closed-form expression, unlike linear least squares; see \u00a7\u00a0Model fitting. Logistic regression by MLE plays a similarly basic role for binary or categorical responses as linear regression by ordinary least squares (OLS) plays for scalar responses: it is a simple, well-analyzed baseline model; see \u00a7\u00a0Comparison with linear regression for discussion. The logistic regression as a general statistical model was originally developed and popularized primarily by Joseph Berkson,[5] beginning in Berkson (1944), where he coined \"logit\"; see \u00a7\u00a0History.\n Logistic regression is used in various fields, including machine learning, most medical fields, and social sciences. For example, the Trauma and Injury Severity Score (TRISS), which is widely used to predict mortality in injured patients, was originally developed by Boyd et al. using logistic regression.[6]  Many other medical scales used to assess severity of a patient have been developed using logistic regression.[7][8][9][10] Logistic regression may be used to predict the risk of developing a given disease (e.g. diabetes; coronary heart disease), based on observed characteristics of the patient (age, sex, body mass index, results of various blood tests, etc.).[11][12]  Another example might be to predict whether a Nepalese voter will vote Nepali Congress or Communist Party of Nepal or Any Other Party, based on age, income, sex, race, state of residence, votes in previous elections, etc.[13] The technique can also be used in engineering, especially for predicting the probability of failure of a given process, system or product.[14][15] It is also used in marketing applications such as prediction of a customer's propensity to purchase a product or halt a subscription, etc.[16] In economics, it can be used to predict the likelihood of a person ending up in the labor force, and a business application would be to predict the likelihood of a homeowner defaulting on a mortgage. Conditional random fields, an extension of logistic regression to sequential data, are used in natural language processing. Disaster planners and engineers  rely on these models to predict decision take by householders or building occupants in small-scale and large-scales evacuations, such as building fires, wildfires, hurricanes among others.[17][18][19] These models help in the development of reliable disaster managing plans and safer design for the built environment.\n Logistic regression is a supervised machine learning algorithm widely used for binary classification tasks, such as identifying whether an email is spam or not and diagnosing diseases by assessing the presence or absence of specific conditions based on patient test results. This approach utilizes the logistic (or sigmoid) function to transform a linear combination of input features into a probability value ranging between 0 and 1. This probability indicates the likelihood that a given input corresponds to one of two predefined categories. The essential mechanism of logistic regression is grounded in the logistic function's ability to model the probability of binary outcomes accurately. With its distinctive S-shaped curve, the logistic function effectively maps any real-valued number to a value within the 0 to 1 interval. This feature renders it particularly suitable for binary classification tasks, such as sorting emails into \"spam\" or \"not spam\". By calculating the probability that the dependent variable will be categorized into a specific group, logistic regression provides a probabilistic framework that supports informed decision-making.[20]\n As a simple example, we can use a logistic regression with one explanatory variable and two categories to answer the following question:\n A group of 20 students spends between 0 and 6 hours studying for an exam. How does the number of hours spent studying affect the probability of the student passing the exam?\n The reason for using logistic regression for this problem is that the values of the dependent variable, pass and fail, while represented by \"1\" and \"0\", are not cardinal numbers. If the problem was changed so that pass/fail was replaced with the grade 0\u2013100 (cardinal numbers), then simple regression analysis could be used.\n The table shows the number of hours each student spent studying, and whether they passed (1) or failed (0).\n We wish to fit a logistic function to the data consisting of the hours studied (xk) and the outcome of the test (yk\u00a0=1 for pass, 0 for fail). The data points are indexed by the subscript k which runs from \n\n\n\nk\n=\n1\n\n\n{\\displaystyle k=1}\n\n to \n\n\n\nk\n=\nK\n=\n20\n\n\n{\\displaystyle k=K=20}\n\n. The x variable is called the \"explanatory variable\", and the y variable is called the \"categorical variable\" consisting of two categories: \"pass\" or \"fail\" corresponding to the categorical values 1 and 0 respectively.\n The logistic function is of the form:\n where \u03bc is a location parameter (the midpoint of the curve, where \n\n\n\np\n(\n\u03bc\n)\n=\n1\n\n/\n\n2\n\n\n{\\displaystyle p(\\mu )=1/2}\n\n) and s is a scale parameter. This expression may be rewritten as:\n where \n\n\n\n\n\u03b2\n\n0\n\n\n=\n\u2212\n\u03bc\n\n/\n\ns\n\n\n{\\displaystyle \\beta _{0}=-\\mu /s}\n\n and is known as the intercept (it is the vertical intercept or y-intercept of the line \n\n\n\ny\n=\n\n\u03b2\n\n0\n\n\n+\n\n\u03b2\n\n1\n\n\nx\n\n\n{\\displaystyle y=\\beta _{0}+\\beta _{1}x}\n\n), and \n\n\n\n\n\u03b2\n\n1\n\n\n=\n1\n\n/\n\ns\n\n\n{\\displaystyle \\beta _{1}=1/s}\n\n (inverse scale parameter or rate parameter): these are the y-intercept and slope of the log-odds as a function of x. Conversely, \n\n\n\n\u03bc\n=\n\u2212\n\n\u03b2\n\n0\n\n\n\n/\n\n\n\u03b2\n\n1\n\n\n\n\n{\\displaystyle \\mu =-\\beta _{0}/\\beta _{1}}\n\n and \n\n\n\ns\n=\n1\n\n/\n\n\n\u03b2\n\n1\n\n\n\n\n{\\displaystyle s=1/\\beta _{1}}\n\n.\n Remark: This model is actually an oversimplification, since it assumes everybody will pass if they learn long enough (limit = 1). The limit value should be a variable parameter too, if you want to make it more realistic.\n The usual measure of goodness of fit for a logistic regression uses logistic loss (or log loss), the negative log-likelihood. For a given xk and yk, write \n\n\n\n\np\n\nk\n\n\n=\np\n(\n\nx\n\nk\n\n\n)\n\n\n{\\displaystyle p_{k}=p(x_{k})}\n\n. The \u2060\n\n\n\n\np\n\nk\n\n\n\n\n{\\displaystyle p_{k}}\n\n\u2060 are the probabilities that the corresponding \u2060\n\n\n\n\ny\n\nk\n\n\n\n\n{\\displaystyle y_{k}}\n\n\u2060 will equal one and \u2060\n\n\n\n1\n\u2212\n\np\n\nk\n\n\n\n\n{\\displaystyle 1-p_{k}}\n\n\u2060 are the probabilities that they will be zero (see Bernoulli distribution). We wish to find the values of \u2060\n\n\n\n\n\u03b2\n\n0\n\n\n\n\n{\\displaystyle \\beta _{0}}\n\n\u2060 and \u2060\n\n\n\n\n\u03b2\n\n1\n\n\n\n\n{\\displaystyle \\beta _{1}}\n\n\u2060 which give the \"best fit\" to the data. In the case of linear regression, the sum of the squared deviations of the fit from the data points (yk), the squared error loss, is taken as a measure of the goodness of fit, and the best fit is obtained when that function is minimized.\n The log loss for the k-th point \u2060\n\n\n\n\n\u2113\n\nk\n\n\n\n\n{\\displaystyle \\ell _{k}}\n\n\u2060 is:\n The log loss can be interpreted as the \"surprisal\" of the actual outcome \u2060\n\n\n\n\ny\n\nk\n\n\n\n\n{\\displaystyle y_{k}}\n\n\u2060 relative to the prediction \u2060\n\n\n\n\np\n\nk\n\n\n\n\n{\\displaystyle p_{k}}\n\n\u2060, and is a measure of information content. Log loss is always greater than or equal to 0, equals 0 only in case of a perfect prediction (i.e., when \n\n\n\n\np\n\nk\n\n\n=\n1\n\n\n{\\displaystyle p_{k}=1}\n\n and \n\n\n\n\ny\n\nk\n\n\n=\n1\n\n\n{\\displaystyle y_{k}=1}\n\n, or \n\n\n\n\np\n\nk\n\n\n=\n0\n\n\n{\\displaystyle p_{k}=0}\n\n and \n\n\n\n\ny\n\nk\n\n\n=\n0\n\n\n{\\displaystyle y_{k}=0}\n\n), and approaches infinity as the prediction gets worse (i.e., when \n\n\n\n\ny\n\nk\n\n\n=\n1\n\n\n{\\displaystyle y_{k}=1}\n\n and \n\n\n\n\np\n\nk\n\n\n\u2192\n0\n\n\n{\\displaystyle p_{k}\\to 0}\n\n or \n\n\n\n\ny\n\nk\n\n\n=\n0\n\n\n{\\displaystyle y_{k}=0}\n\n and \n\n\n\n\np\n\nk\n\n\n\u2192\n1\n\n\n{\\displaystyle p_{k}\\to 1}\n\n), meaning the actual outcome is \"more surprising\". Since the value of the logistic function is always strictly between zero and one, the log loss is always greater than zero and less than infinity. Unlike in a linear regression, where the model can have zero loss at a point by passing through a data point (and zero loss overall if all points are on a line), in a logistic regression it is not possible to have zero loss at any points, since \u2060\n\n\n\n\ny\n\nk\n\n\n\n\n{\\displaystyle y_{k}}\n\n\u2060 is either 0 or 1, but \u2060\n\n\n\n0\n<\n\np\n\nk\n\n\n<\n1\n\n\n{\\displaystyle 0<p_{k}<1}\n\n\u2060.\n These can be combined into a single expression:\n This expression is more formally known as the cross-entropy of the predicted distribution \n\n\n\n\n\n(\n\n\n\np\n\nk\n\n\n,\n(\n1\n\u2212\n\np\n\nk\n\n\n)\n\n\n)\n\n\n\n\n{\\displaystyle {\\big (}p_{k},(1-p_{k}){\\big )}}\n\n from the actual distribution \n\n\n\n\n\n(\n\n\n\ny\n\nk\n\n\n,\n(\n1\n\u2212\n\ny\n\nk\n\n\n)\n\n\n)\n\n\n\n\n{\\displaystyle {\\big (}y_{k},(1-y_{k}){\\big )}}\n\n, as probability distributions on the two-element space of (pass, fail).\n The sum of these, the total loss, is the overall negative log-likelihood \u2060\n\n\n\n\u2212\n\u2113\n\n\n{\\displaystyle -\\ell }\n\n\u2060, and the best fit is obtained for those choices of \u2060\n\n\n\n\n\u03b2\n\n0\n\n\n\n\n{\\displaystyle \\beta _{0}}\n\n\u2060 and \u2060\n\n\n\n\n\u03b2\n\n1\n\n\n\n\n{\\displaystyle \\beta _{1}}\n\n\u2060 for which \u2060\n\n\n\n\u2212\n\u2113\n\n\n{\\displaystyle -\\ell }\n\n\u2060 is minimized.\n Alternatively, instead of minimizing the loss, one can maximize its inverse, the (positive) log-likelihood:\n or equivalently maximize the likelihood function itself, which is the probability that the given data set is produced by a particular logistic function:\n This method is known as maximum likelihood estimation.\n Since \u2113 is nonlinear in \u2060\n\n\n\n\n\u03b2\n\n0\n\n\n\n\n{\\displaystyle \\beta _{0}}\n\n\u2060 and \u2060\n\n\n\n\n\u03b2\n\n1\n\n\n\n\n{\\displaystyle \\beta _{1}}\n\n\u2060, determining their optimum values will require numerical methods. One method of maximizing  \u2113 is to require the derivatives of \u2113 with respect to \u2060\n\n\n\n\n\u03b2\n\n0\n\n\n\n\n{\\displaystyle \\beta _{0}}\n\n\u2060 and \u2060\n\n\n\n\n\u03b2\n\n1\n\n\n\n\n{\\displaystyle \\beta _{1}}\n\n\u2060 to be zero:\n and the maximization procedure can be accomplished by solving the above two equations for \u2060\n\n\n\n\n\u03b2\n\n0\n\n\n\n\n{\\displaystyle \\beta _{0}}\n\n\u2060 and \u2060\n\n\n\n\n\u03b2\n\n1\n\n\n\n\n{\\displaystyle \\beta _{1}}\n\n\u2060, which, again, will generally require the use of numerical methods.\n The values of \u2060\n\n\n\n\n\u03b2\n\n0\n\n\n\n\n{\\displaystyle \\beta _{0}}\n\n\u2060 and \u2060\n\n\n\n\n\u03b2\n\n1\n\n\n\n\n{\\displaystyle \\beta _{1}}\n\n\u2060 which maximize \u2113 and L using the above data are found to be:\n which yields a value for \u03bc and s of:\n The \u2060\n\n\n\n\n\u03b2\n\n0\n\n\n\n\n{\\displaystyle \\beta _{0}}\n\n\u2060 and \u2060\n\n\n\n\n\u03b2\n\n1\n\n\n\n\n{\\displaystyle \\beta _{1}}\n\n\u2060 coefficients may be entered into the logistic regression equation to estimate the probability of passing the exam.\n For example, for a student who studies 2 hours, entering the value \n\n\n\nx\n=\n2\n\n\n{\\displaystyle x=2}\n\n into the equation gives the estimated probability of passing the exam of 0.25:\n Similarly, for a student who studies 4 hours, the estimated probability of passing the exam is 0.87:\n This table shows the estimated probability of passing the exam for several values of hours studying.\n The logistic regression analysis gives the following output.\n By the Wald test, the output indicates that hours studying is significantly associated with the probability of passing the exam (\n\n\n\np\n=\n0.017\n\n\n{\\displaystyle p=0.017}\n\n). Rather than the Wald method, the recommended method[21] to calculate the p-value for logistic regression is the likelihood-ratio test (LRT), which for these data give \n\n\n\np\n\u2248\n0.00064\n\n\n{\\displaystyle p\\approx 0.00064}\n\n (see \u00a7\u00a0Deviance and likelihood ratio tests below).\n This simple model is an example of binary logistic regression, and has one explanatory variable and a binary categorical variable which can assume one of two categorical values. Multinomial logistic regression is the generalization of binary logistic regression to include any number of explanatory variables and any number of categories.\n An explanation of logistic regression can begin with an explanation of the standard logistic function. The logistic function is a sigmoid function, which takes any real input \n\n\n\nt\n\n\n{\\displaystyle t}\n\n, and outputs a value between zero and one.[2] For the logit, this is interpreted as taking input log-odds and having output probability. The standard logistic function \n\n\n\n\u03c3\n:\n\nR\n\n\u2192\n(\n0\n,\n1\n)\n\n\n{\\displaystyle \\sigma :\\mathbb {R} \\rightarrow (0,1)}\n\n is defined as follows:\n A graph of the logistic function on the t-interval (\u22126,6) is shown in Figure 1.\n Let us assume that \n\n\n\nt\n\n\n{\\displaystyle t}\n\n is a linear function of a single explanatory variable \n\n\n\nx\n\n\n{\\displaystyle x}\n\n (the case where \n\n\n\nt\n\n\n{\\displaystyle t}\n\n is a linear combination of multiple explanatory variables is treated similarly). We can then express \n\n\n\nt\n\n\n{\\displaystyle t}\n\n as follows:\n And the general logistic function \n\n\n\np\n:\n\nR\n\n\u2192\n(\n0\n,\n1\n)\n\n\n{\\displaystyle p:\\mathbb {R} \\rightarrow (0,1)}\n\n can now be written as:\n In the logistic model, \n\n\n\np\n(\nx\n)\n\n\n{\\displaystyle p(x)}\n\n is interpreted as the probability of the dependent variable \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n equaling a success/case rather than a failure/non-case. It is clear that the response variables \n\n\n\n\nY\n\ni\n\n\n\n\n{\\displaystyle Y_{i}}\n\n are not identically distributed: \n\n\n\nP\n(\n\nY\n\ni\n\n\n=\n1\n\u2223\nX\n)\n\n\n{\\displaystyle P(Y_{i}=1\\mid X)}\n\n differs from one data point \n\n\n\n\nX\n\ni\n\n\n\n\n{\\displaystyle X_{i}}\n\n to another, though they are independent given design matrix \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and shared parameters \n\n\n\n\u03b2\n\n\n{\\displaystyle \\beta }\n\n.[11]\n We can now define the logit (log odds) function as the inverse \n\n\n\ng\n=\n\n\u03c3\n\n\u2212\n1\n\n\n\n\n{\\displaystyle g=\\sigma ^{-1}}\n\n of the standard logistic function. It is easy to see that it satisfies:\n and equivalently, after exponentiating both sides we have the odds:\n In the above equations, the terms are as follows:\n The odds of the dependent variable equaling a case (given some linear combination \n\n\n\nx\n\n\n{\\displaystyle x}\n\n of the predictors) is equivalent to the exponential function of the linear regression expression. This illustrates how the logit serves as a link function between the probability and the linear regression expression. Given that the logit ranges between negative and positive infinity, it provides an adequate criterion upon which to conduct linear regression and the logit is easily converted back into the odds.[2]\n So we define odds of the dependent variable equaling a case (given some linear combination \n\n\n\nx\n\n\n{\\displaystyle x}\n\n of the predictors) as follows:\n For a continuous independent variable the odds ratio can be defined as:\n This exponential relationship provides an interpretation for \n\n\n\n\n\u03b2\n\n1\n\n\n\n\n{\\displaystyle \\beta _{1}}\n\n: The odds multiply by \n\n\n\n\ne\n\n\n\u03b2\n\n1\n\n\n\n\n\n\n{\\displaystyle e^{\\beta _{1}}}\n\n for every 1-unit increase in x.[22]\n For a binary independent variable the odds ratio is defined as \n\n\n\n\n\n\na\nd\n\n\nb\nc\n\n\n\n\n\n{\\displaystyle {\\frac {ad}{bc}}}\n\n where a, b, c and d are cells in a 2\u00d72 contingency table.[23]\n If there are multiple explanatory variables, the above expression \n\n\n\n\n\u03b2\n\n0\n\n\n+\n\n\u03b2\n\n1\n\n\nx\n\n\n{\\displaystyle \\beta _{0}+\\beta _{1}x}\n\n can be revised to \n\n\n\n\n\u03b2\n\n0\n\n\n+\n\n\u03b2\n\n1\n\n\n\nx\n\n1\n\n\n+\n\n\u03b2\n\n2\n\n\n\nx\n\n2\n\n\n+\n\u22ef\n+\n\n\u03b2\n\nm\n\n\n\nx\n\nm\n\n\n=\n\n\u03b2\n\n0\n\n\n+\n\n\u2211\n\ni\n=\n1\n\n\nm\n\n\n\n\u03b2\n\ni\n\n\n\nx\n\ni\n\n\n\n\n{\\displaystyle \\beta _{0}+\\beta _{1}x_{1}+\\beta _{2}x_{2}+\\cdots +\\beta _{m}x_{m}=\\beta _{0}+\\sum _{i=1}^{m}\\beta _{i}x_{i}}\n\n. Then when this is used in the equation relating the log odds of a success to the values of the predictors, the linear regression will be a multiple regression with m explanators; the parameters \n\n\n\n\n\u03b2\n\ni\n\n\n\n\n{\\displaystyle \\beta _{i}}\n\n for all \n\n\n\ni\n=\n0\n,\n1\n,\n2\n,\n\u2026\n,\nm\n\n\n{\\displaystyle i=0,1,2,\\dots ,m}\n\n are all estimated.\n Again, the more traditional equations are:\n and\n where usually \n\n\n\nb\n=\ne\n\n\n{\\displaystyle b=e}\n\n.\n A dataset contains N points. Each point i consists of a set of m input variables x1,i ... xm,i (also called independent variables, explanatory variables, predictor variables, features, or attributes), and a binary outcome variable Yi (also known as a dependent variable, response variable, output variable, or class), i.e. it can assume only the two possible values 0 (often meaning \"no\" or \"failure\") or 1 (often meaning \"yes\" or \"success\"). The goal of logistic regression is to use the dataset to create a predictive model of the outcome variable.\n As in linear regression, the outcome variables Yi are assumed to depend on the explanatory variables x1,i ... xm,i.\n The explanatory variables may be of any type: real-valued, binary, categorical, etc.  The main distinction is between continuous variables and discrete variables.\n (Discrete variables referring to more than two possible choices are typically coded using dummy variables (or indicator variables), that is, separate explanatory variables taking the value 0 or 1 are created for each possible value of the discrete variable, with a 1 meaning \"variable does have the given value\" and a 0 meaning \"variable does not have that value\".)\n Formally, the outcomes Yi are described as being Bernoulli-distributed data, where each outcome is determined by an unobserved probability pi that is specific to the outcome at hand, but related to the explanatory variables.  This can be expressed in any of the following equivalent forms:\n The meanings of these four lines are:\n The basic idea of logistic regression is to use the mechanism already developed for linear regression by modeling the probability pi using a linear predictor function, i.e. a linear combination of the explanatory variables and a set of regression coefficients that are specific to the model at hand but the same for all trials.  The linear predictor function \n\n\n\nf\n(\ni\n)\n\n\n{\\displaystyle f(i)}\n\n for a particular data point i is written as:\n where \n\n\n\n\n\u03b2\n\n0\n\n\n,\n\u2026\n,\n\n\u03b2\n\nm\n\n\n\n\n{\\displaystyle \\beta _{0},\\ldots ,\\beta _{m}}\n\n are regression coefficients indicating the relative effect of a particular explanatory variable on the outcome.\n The model is usually put into a more compact form as follows:\n This makes it possible to write the linear predictor function as follows:\n using the notation for a dot product between two vectors.\n The above example of binary logistic regression on one explanatory variable can be generalized to binary logistic regression on any number of explanatory variables x1, x2,... and any number of categorical values \n\n\n\ny\n=\n0\n,\n1\n,\n2\n,\n\u2026\n\n\n{\\displaystyle y=0,1,2,\\dots }\n\n.\n To begin with, we may consider a logistic model with M explanatory variables, x1, x2 ... xM and, as in the example above, two categorical values (y = 0 and 1). For the simple binary logistic regression model, we assumed a linear relationship between the predictor variable and the log-odds (also called logit) of the event that \n\n\n\ny\n=\n1\n\n\n{\\displaystyle y=1}\n\n. This linear relationship may be extended to the case of M explanatory variables:\n where t is the log-odds and \n\n\n\n\n\u03b2\n\ni\n\n\n\n\n{\\displaystyle \\beta _{i}}\n\n are parameters of the model. An additional generalization has been introduced in which the base of the model (b) is not restricted to Euler's number e. In most applications, the base \n\n\n\nb\n\n\n{\\displaystyle b}\n\n of the logarithm is usually taken to be e. However, in some cases it can be easier to communicate results by working in base 2 or base 10.\n For a more compact notation, we will specify the explanatory variables and the \u03b2 coefficients as \u2060\n\n\n\n(\nM\n+\n1\n)\n\n\n{\\displaystyle (M+1)}\n\n\u2060-dimensional vectors:\n with an added explanatory variable x0 =1. The logit may now be written as:\n Solving for the probability p that \n\n\n\ny\n=\n1\n\n\n{\\displaystyle y=1}\n\n yields:\n where \n\n\n\n\nS\n\nb\n\n\n\n\n{\\displaystyle S_{b}}\n\n is the sigmoid function with base \n\n\n\nb\n\n\n{\\displaystyle b}\n\n. The above formula shows that once the \n\n\n\n\n\u03b2\n\nm\n\n\n\n\n{\\displaystyle \\beta _{m}}\n\n are fixed, we can easily compute either the log-odds that \n\n\n\ny\n=\n1\n\n\n{\\displaystyle y=1}\n\n for a given observation, or the probability that \n\n\n\ny\n=\n1\n\n\n{\\displaystyle y=1}\n\n for a given observation. The main use-case of a logistic model is to be given an observation \n\n\n\n\nx\n\n\n\n{\\displaystyle {\\boldsymbol {x}}}\n\n, and estimate the probability \n\n\n\np\n(\n\nx\n\n)\n\n\n{\\displaystyle p({\\boldsymbol {x}})}\n\n that \n\n\n\ny\n=\n1\n\n\n{\\displaystyle y=1}\n\n. The optimum beta coefficients may again be found by maximizing the log-likelihood. For K measurements, defining \n\n\n\n\n\nx\n\n\nk\n\n\n\n\n{\\displaystyle {\\boldsymbol {x}}_{k}}\n\n as the explanatory vector of the k-th measurement, and \n\n\n\n\ny\n\nk\n\n\n\n\n{\\displaystyle y_{k}}\n\n as the categorical outcome of that measurement, the log likelihood may be written in a form very similar to the simple \n\n\n\nM\n=\n1\n\n\n{\\displaystyle M=1}\n\n case above:\n As in the simple example above, finding the optimum \u03b2 parameters will require numerical methods. One useful technique is to equate the derivatives of the log likelihood with respect to each of the \u03b2 parameters to zero yielding a set of equations which will hold at the maximum of the log likelihood:\n where xmk is the value of the xm explanatory variable from the k-th measurement.\n Consider an example with \n\n\n\nM\n=\n2\n\n\n{\\displaystyle M=2}\n\n explanatory variables, \n\n\n\nb\n=\n10\n\n\n{\\displaystyle b=10}\n\n, and coefficients \n\n\n\n\n\u03b2\n\n0\n\n\n=\n\u2212\n3\n\n\n{\\displaystyle \\beta _{0}=-3}\n\n, \n\n\n\n\n\u03b2\n\n1\n\n\n=\n1\n\n\n{\\displaystyle \\beta _{1}=1}\n\n, and \n\n\n\n\n\u03b2\n\n2\n\n\n=\n2\n\n\n{\\displaystyle \\beta _{2}=2}\n\n which have been determined by the above method. To be concrete, the model is:\n where p is the probability of the event that \n\n\n\ny\n=\n1\n\n\n{\\displaystyle y=1}\n\n. This can be interpreted as follows:\n In the above cases of two categories (binomial logistic regression), the categories were indexed by \"0\" and \"1\", and we had two probabilities: The probability that the outcome was in category 1 was given by \n\n\n\np\n(\n\nx\n\n)\n\n\n{\\displaystyle p({\\boldsymbol {x}})}\n\nand the probability that the outcome was in category 0 was given by \n\n\n\n1\n\u2212\np\n(\n\nx\n\n)\n\n\n{\\displaystyle 1-p({\\boldsymbol {x}})}\n\n. The sum of these probabilities equals 1, which must be true, since \"0\" and \"1\" are the only possible categories in this setup.\n In general, if we have \u2060\n\n\n\nM\n+\n1\n\n\n{\\displaystyle M+1}\n\n\u2060 explanatory variables (including x0) and \u2060\n\n\n\nN\n+\n1\n\n\n{\\displaystyle N+1}\n\n\u2060 categories, we will need \u2060\n\n\n\nN\n+\n1\n\n\n{\\displaystyle N+1}\n\n\u2060 separate probabilities,  one for each category, indexed by n, which describe the probability that the categorical outcome y will be in category y=n, conditional on the vector of covariates x. The sum of these probabilities over all categories must equal 1. Using the mathematically convenient base e, these probabilities are:\n Each of the probabilities except \n\n\n\n\np\n\n0\n\n\n(\n\nx\n\n)\n\n\n{\\displaystyle p_{0}({\\boldsymbol {x}})}\n\n will have their own set of regression coefficients \n\n\n\n\n\n\u03b2\n\n\nn\n\n\n\n\n{\\displaystyle {\\boldsymbol {\\beta }}_{n}}\n\n.  It can be seen that, as required, the sum of the \n\n\n\n\np\n\nn\n\n\n(\n\nx\n\n)\n\n\n{\\displaystyle p_{n}({\\boldsymbol {x}})}\n\n over all categories n is 1. The selection of \n\n\n\n\np\n\n0\n\n\n(\n\nx\n\n)\n\n\n{\\displaystyle p_{0}({\\boldsymbol {x}})}\n\n to be defined in terms of the other probabilities is artificial. Any of the probabilities could have been selected to be so defined. This special value of n is termed the \"pivot index\", and the log-odds (tn) are expressed in terms of the pivot probability and are again expressed as a linear combination of the explanatory variables:\n Note also that for the simple case of \n\n\n\nN\n=\n1\n\n\n{\\displaystyle N=1}\n\n, the two-category case is recovered, with \n\n\n\np\n(\n\nx\n\n)\n=\n\np\n\n1\n\n\n(\n\nx\n\n)\n\n\n{\\displaystyle p({\\boldsymbol {x}})=p_{1}({\\boldsymbol {x}})}\n\n and \n\n\n\n\np\n\n0\n\n\n(\n\nx\n\n)\n=\n1\n\u2212\n\np\n\n1\n\n\n(\n\nx\n\n)\n\n\n{\\displaystyle p_{0}({\\boldsymbol {x}})=1-p_{1}({\\boldsymbol {x}})}\n\n.\n The log-likelihood that a particular set of K measurements or data points will be generated by the above probabilities can now be calculated. Indexing each measurement by k, let the k-th set of measured explanatory variables be denoted by \n\n\n\n\n\nx\n\n\nk\n\n\n\n\n{\\displaystyle {\\boldsymbol {x}}_{k}}\n\n and their categorical outcomes be denoted by \n\n\n\n\ny\n\nk\n\n\n\n\n{\\displaystyle y_{k}}\n\n which can be equal to any integer in [0,N]. The log-likelihood is then:\n where \n\n\n\n\u0394\n(\nn\n,\n\ny\n\nk\n\n\n)\n\n\n{\\displaystyle \\Delta (n,y_{k})}\n\n is an indicator function which equals 1 if yk = n and zero otherwise. In the case of two explanatory variables, this indicator function was defined as yk when n = 1 and 1-yk when n = 0. This was convenient, but not necessary.[24] Again, the optimum beta coefficients may be found by maximizing the log-likelihood function generally using numerical methods. A possible method of solution is to set the derivatives of the log-likelihood with respect to each beta coefficient equal to zero and solve for the beta coefficients:\n where \n\n\n\n\n\u03b2\n\nn\nm\n\n\n\n\n{\\displaystyle \\beta _{nm}}\n\n is the m-th coefficient of the \n\n\n\n\n\n\u03b2\n\n\nn\n\n\n\n\n{\\displaystyle {\\boldsymbol {\\beta }}_{n}}\n\n vector and \n\n\n\n\nx\n\nm\nk\n\n\n\n\n{\\displaystyle x_{mk}}\n\n is the m-th explanatory variable of the k-th measurement. Once the beta coefficients have been estimated from the data, we will be able to estimate the probability that any subsequent set of explanatory variables will result in any of the possible outcome categories.\n There are various equivalent specifications and interpretations of logistic regression, which fit into different types of more general models, and allow different generalizations.\n The particular model used by logistic regression, which distinguishes it from standard linear regression and from other types of regression analysis used for binary-valued outcomes, is the way the probability of a particular outcome is linked to the linear predictor function:\n Written using the more compact notation described above, this is:\n This formulation expresses logistic regression as a type of generalized linear model, which predicts variables with various types of probability distributions by fitting a linear predictor function of the above form to some sort of arbitrary transformation of the expected value of the variable.\n The intuition for transforming using the logit function (the natural log of the odds) was explained above[clarification needed].  It also has the practical effect of converting the probability (which is bounded to be between 0 and 1) to a variable that ranges over \n\n\n\n(\n\u2212\n\u221e\n,\n+\n\u221e\n)\n\n\n{\\displaystyle (-\\infty ,+\\infty )}\n\n \u2014 thereby matching the potential range of the linear prediction function on the right side of the equation.\n Both the probabilities pi and the regression coefficients are unobserved, and the means of determining them is not part of the model itself.  They are typically determined by some sort of optimization procedure, e.g. maximum likelihood estimation, that finds values that best fit the observed data (i.e. that give the most accurate predictions for the data already observed), usually subject to regularization conditions that seek to exclude unlikely values, e.g. extremely large values for any of the regression coefficients.  The use of a regularization condition is equivalent to doing maximum a posteriori (MAP) estimation, an extension of maximum likelihood.  (Regularization is most commonly done using a squared regularizing function, which is equivalent to placing a zero-mean Gaussian prior distribution on the coefficients, but other regularizers are also possible.)  Whether or not regularization is used, it is usually not possible to find a closed-form solution; instead, an iterative numerical method must be used, such as iteratively reweighted least squares (IRLS) or, more commonly these days, a quasi-Newton method such as the L-BFGS method.[25]\n The interpretation of the \u03b2j parameter estimates is as the additive effect on the log of the odds for a unit change in the j the explanatory variable.  In the case of a dichotomous explanatory variable, for instance, gender \n\n\n\n\ne\n\n\u03b2\n\n\n\n\n{\\displaystyle e^{\\beta }}\n\n is the estimate of the odds of having the outcome for, say, males compared with females.\n An equivalent formula uses the inverse of the logit function, which is the logistic function, i.e.:\n The formula can also be written as a probability distribution (specifically, using a probability mass function):\n The logistic model has an equivalent formulation as a latent-variable model.  This formulation is common in the theory of discrete choice models and makes it easier to extend to certain more complicated models with multiple, correlated choices, as well as to compare logistic regression to the closely related probit model.\n Imagine that, for each trial i, there is a continuous latent variable Yi* (i.e. an unobserved random variable) that is distributed as follows:\n where\n i.e. the latent variable can be written directly in terms of the linear predictor function and an additive random error variable that is distributed according to a standard logistic distribution.\n Then Yi can be viewed as an indicator for whether this latent variable is positive:\n The choice of modeling the error variable specifically with a standard logistic distribution, rather than a general logistic distribution with the location and scale set to arbitrary values, seems restrictive, but in fact, it is not.  It must be kept in mind that we can choose the regression coefficients ourselves, and very often can use them to offset changes in the parameters of the error variable's distribution.  For example, a logistic error-variable distribution with a non-zero location parameter \u03bc (which sets the mean) is equivalent to a distribution with a zero location parameter, where \u03bc has been added to the intercept coefficient.  Both situations produce the same value for Yi* regardless of settings of explanatory variables.  Similarly, an arbitrary scale parameter s is equivalent to setting the scale parameter to 1 and then dividing all regression coefficients by s.  In the latter case, the resulting value of Yi* will be smaller by a factor of s than in the former case, for all sets of explanatory variables \u2014 but critically, it will always remain on the same side of 0, and hence lead to the same Yi choice.\n (This predicts that the irrelevancy of the scale parameter may not carry over into more complex models where more than two choices are available.)\n It turns out that this formulation is exactly equivalent to the preceding one, phrased in terms of the generalized linear model and without any latent variables.  This can be shown as follows, using the fact that the cumulative distribution function (CDF) of the standard logistic distribution is the logistic function, which is the inverse of the logit function, i.e.\n Then:\n This formulation\u2014which is standard in discrete choice models\u2014makes clear the relationship between logistic regression (the \"logit model\") and the probit model, which uses an error variable distributed according to a standard normal distribution instead of a standard logistic distribution.  Both the logistic and normal distributions are symmetric with a basic unimodal, \"bell curve\" shape.  The only difference is that the logistic distribution has somewhat heavier tails, which means that it is less sensitive to outlying data (and hence somewhat more robust to model mis-specifications or erroneous data).\n Yet another formulation uses two separate latent variables:\n where\n where EV1(0,1) is a standard type-1 extreme value distribution: i.e.\n Then\n This model has a separate latent variable and a separate set of regression coefficients for each possible outcome of the dependent variable.  The reason for this separation is that it makes it easy to extend logistic regression to multi-outcome categorical variables, as in the multinomial logit model. In such a model, it is natural to model each possible outcome using a different set of regression coefficients.  It is also possible to motivate each of the separate latent variables as the theoretical utility associated with making the associated choice, and thus motivate logistic regression in terms of utility theory. (In terms of utility theory, a rational actor always chooses the choice with the greatest associated utility.) This is the approach taken by economists when formulating discrete choice models, because it both provides a theoretically strong foundation and facilitates intuitions about the model, which in turn makes it easy to consider various sorts of extensions. (See the example below.)\n The choice of the type-1 extreme value distribution seems fairly arbitrary, but it makes the mathematics work out, and it may be possible to justify its use through rational choice theory.\n It turns out that this model is equivalent to the previous model, although this seems non-obvious, since there are now two sets of regression coefficients and error variables, and the error variables have a different distribution.  In fact, this model reduces directly to the previous one with the following substitutions:\n An intuition for this comes from the fact that, since we choose based on the maximum of two values, only their difference matters, not the exact values \u2014 and this effectively removes one degree of freedom. Another critical fact is that the difference of two type-1 extreme-value-distributed variables is a logistic distribution, i.e. \n\n\n\n\u03b5\n=\n\n\u03b5\n\n1\n\n\n\u2212\n\n\u03b5\n\n0\n\n\n\u223c\nLogistic\n\u2061\n(\n0\n,\n1\n)\n.\n\n\n{\\displaystyle \\varepsilon =\\varepsilon _{1}-\\varepsilon _{0}\\sim \\operatorname {Logistic} (0,1).}\n\n We can demonstrate the equivalent as follows:\n As an example, consider a province-level election where the choice is between a right-of-center party, a left-of-center party, and a secessionist party (e.g. the Parti Qu\u00e9b\u00e9cois, which wants Quebec to secede from Canada).  We would then use three latent variables, one for each choice.  Then, in accordance with utility theory, we can then interpret the latent variables as expressing the utility that results from making each of the choices.  We can also interpret the regression coefficients as indicating the strength that the associated factor (i.e. explanatory variable) has in contributing to the utility \u2014 or more correctly, the amount by which a unit change in an explanatory variable changes the utility of a given choice.  A voter might expect that the right-of-center party would lower taxes, especially on rich people.  This would give low-income people no benefit, i.e. no change in utility (since they usually don't pay taxes); would cause moderate benefit (i.e. somewhat more money, or moderate utility increase) for middle-incoming people; would cause significant benefits for high-income people.  On the other hand, the left-of-center party might be expected to raise taxes and offset it with increased welfare and other assistance for the lower and middle classes.  This would cause significant positive benefit to low-income people, perhaps a weak benefit to middle-income people, and significant negative benefit to high-income people.  Finally, the secessionist party would take no direct actions on the economy, but simply secede. A low-income or middle-income voter might expect basically no clear utility gain or loss from this, but a high-income voter might expect negative utility since he/she is likely to own companies, which will have a harder time doing business in such an environment and probably lose money.\n These intuitions can be expressed as follows:\n This clearly shows that\n Yet another formulation combines the two-way latent variable formulation above with the original formulation higher up without latent variables, and in the process provides a link to one of the standard formulations of the multinomial logit.\n Here, instead of writing the logit of the probabilities pi as a linear predictor, we separate the linear predictor into two, one for each of the two outcomes:\n Two separate sets of regression coefficients have been introduced, just as in the two-way latent variable model, and the two equations appear a form that writes the logarithm of the associated probability as a linear predictor, with an extra term \n\n\n\n\u2212\nln\n\u2061\nZ\n\n\n{\\displaystyle -\\ln Z}\n\n at the end.  This term, as it turns out, serves as the normalizing factor ensuring that the result is a distribution.  This can be seen by exponentiating both sides:\n In this form it is clear that the purpose of Z is to ensure that the resulting distribution over Yi is in fact a probability distribution, i.e. it sums to 1.  This means that Z is simply the sum of all un-normalized probabilities, and by dividing each probability by Z, the probabilities become \"normalized\".  That is:\n and the resulting equations are\n Or generally:\n This shows clearly how to generalize this formulation to more than two outcomes, as in multinomial logit.\nThis general formulation is exactly the softmax function as in\n In order to prove that this is equivalent to the previous model, the above model is overspecified, in that \n\n\n\nPr\n(\n\nY\n\ni\n\n\n=\n0\n)\n\n\n{\\displaystyle \\Pr(Y_{i}=0)}\n\n and \n\n\n\nPr\n(\n\nY\n\ni\n\n\n=\n1\n)\n\n\n{\\displaystyle \\Pr(Y_{i}=1)}\n\n cannot be independently specified: rather \n\n\n\nPr\n(\n\nY\n\ni\n\n\n=\n0\n)\n+\nPr\n(\n\nY\n\ni\n\n\n=\n1\n)\n=\n1\n\n\n{\\displaystyle \\Pr(Y_{i}=0)+\\Pr(Y_{i}=1)=1}\n\n so knowing one automatically determines the other.  As a result, the model is nonidentifiable, in that multiple combinations of \u03b20 and \u03b21 will produce the same probabilities for all possible explanatory variables.  In fact, it can be seen that adding any constant vector to both of them will produce the same probabilities:\n As a result, we can simplify matters, and restore identifiability, by picking an arbitrary value for one of the two vectors.  We choose to set \n\n\n\n\n\n\u03b2\n\n\n0\n\n\n=\n\n0\n\n.\n\n\n{\\displaystyle {\\boldsymbol {\\beta }}_{0}=\\mathbf {0} .}\n\n  Then,\n and so\n which shows that this formulation is indeed equivalent to the previous formulation. (As in the two-way latent variable formulation, any settings where \n\n\n\n\n\u03b2\n\n=\n\n\n\u03b2\n\n\n1\n\n\n\u2212\n\n\n\u03b2\n\n\n0\n\n\n\n\n{\\displaystyle {\\boldsymbol {\\beta }}={\\boldsymbol {\\beta }}_{1}-{\\boldsymbol {\\beta }}_{0}}\n\n will produce equivalent results.)\n Most treatments of the multinomial logit model start out either by extending the \"log-linear\" formulation presented here or the two-way latent variable formulation presented above, since both clearly show the way that the model could be extended to multi-way outcomes.  In general, the presentation with latent variables is more common in econometrics and political science, where discrete choice models and utility theory reign, while the \"log-linear\" formulation here is more common in computer science, e.g. machine learning and natural language processing.\n The model has an equivalent formulation\n This functional form is commonly called a single-layer perceptron or single-layer artificial neural network. A single-layer neural network computes a continuous output instead of a step function. The derivative of pi with respect to  X\u00a0=\u00a0(x1, ..., xk) is computed from the general form:\n where f(X) is an analytic function in X. With this choice, the single-layer neural network is identical to the logistic regression model. This function has a continuous derivative, which allows it to be used in backpropagation. This function is also preferred because its derivative is easily calculated:\n A closely related model assumes that each i is associated not with a single Bernoulli trial but with ni independent identically distributed trials, where the observation Yi is the number of successes observed (the sum of the individual Bernoulli-distributed random variables), and hence follows a binomial distribution:\n An example of this distribution is the fraction of seeds (pi) that germinate after ni are planted.\n In terms of expected values, this model is expressed as follows:\n so that\n Or equivalently:\n This model can be fit using the same sorts of methods as the above more basic model.\n The regression coefficients are usually estimated using maximum likelihood estimation.[26][27] Unlike linear regression with normally distributed residuals, it is not possible to find a closed-form expression for the coefficient values that maximize the likelihood function so an iterative process must be used instead; for example Newton's method. This process begins with a tentative solution, revises it slightly to see if it can be improved, and repeats this revision until no more improvement is made, at which point the process is said to have converged.[26]\n In some instances, the model may not reach convergence. Non-convergence of a model indicates that the coefficients are not meaningful because the iterative process was unable to find appropriate solutions. A failure to converge may occur for a number of reasons: having a large ratio of predictors to cases, multicollinearity, sparseness, or complete separation.\n Binary logistic regression (\n\n\n\ny\n=\n0\n\n\n{\\displaystyle y=0}\n\n or \n\n\n\ny\n=\n1\n\n\n{\\displaystyle y=1}\n\n) can, for example, be calculated using iteratively reweighted least squares (IRLS), which is equivalent to maximizing the log-likelihood of a Bernoulli distributed  process using Newton's method. If the problem is written in vector matrix form, with parameters \n\n\n\n\n\nw\n\n\nT\n\n\n=\n[\n\n\u03b2\n\n0\n\n\n,\n\n\u03b2\n\n1\n\n\n,\n\n\u03b2\n\n2\n\n\n,\n\u2026\n]\n\n\n{\\displaystyle \\mathbf {w} ^{T}=[\\beta _{0},\\beta _{1},\\beta _{2},\\ldots ]}\n\n, explanatory variables \n\n\n\n\nx\n\n(\ni\n)\n=\n[\n1\n,\n\nx\n\n1\n\n\n(\ni\n)\n,\n\nx\n\n2\n\n\n(\ni\n)\n,\n\u2026\n\n]\n\nT\n\n\n\n\n{\\displaystyle \\mathbf {x} (i)=[1,x_{1}(i),x_{2}(i),\\ldots ]^{T}}\n\n and expected value of the Bernoulli distribution \n\n\n\n\u03bc\n(\ni\n)\n=\n\n\n1\n\n1\n+\n\ne\n\n\u2212\n\n\nw\n\n\nT\n\n\n\nx\n\n(\ni\n)\n\n\n\n\n\n\n\n{\\displaystyle \\mu (i)={\\frac {1}{1+e^{-\\mathbf {w} ^{T}\\mathbf {x} (i)}}}}\n\n, the parameters \n\n\n\n\nw\n\n\n\n{\\displaystyle \\mathbf {w} }\n\n can be found using the following iterative algorithm:\n where \n\n\n\n\nS\n\n=\ndiag\n\u2061\n(\n\u03bc\n(\ni\n)\n(\n1\n\u2212\n\u03bc\n(\ni\n)\n)\n)\n\n\n{\\displaystyle \\mathbf {S} =\\operatorname {diag} (\\mu (i)(1-\\mu (i)))}\n\n is a diagonal weighting matrix, \n\n\n\n\n\u03bc\n\n=\n[\n\u03bc\n(\n1\n)\n,\n\u03bc\n(\n2\n)\n,\n\u2026\n]\n\n\n{\\displaystyle {\\boldsymbol {\\mu }}=[\\mu (1),\\mu (2),\\ldots ]}\n\n the vector of expected values,\n The regressor matrix and \n\n\n\n\ny\n\n(\ni\n)\n=\n[\ny\n(\n1\n)\n,\ny\n(\n2\n)\n,\n\u2026\n\n]\n\nT\n\n\n\n\n{\\displaystyle \\mathbf {y} (i)=[y(1),y(2),\\ldots ]^{T}}\n\n the vector of response variables. More details can be found in the literature.[29]\n In a Bayesian statistics context, prior distributions are normally placed on the regression coefficients, for example in the form of Gaussian distributions.  There is no conjugate prior of the likelihood function in logistic regression.  When Bayesian inference was performed analytically, this made the posterior distribution difficult to calculate except in very low dimensions.  Now, though, automatic software such as OpenBUGS, JAGS, PyMC, Stan or Turing.jl allows these posteriors to be computed using simulation, so lack of conjugacy is not a concern.  However, when the sample size or the number of parameters is large, full Bayesian simulation can be slow, and people often use approximate methods such as variational Bayesian methods and expectation propagation.\n Widely used, the \"one in ten rule\", states that logistic regression models give stable values for the explanatory variables if based on a minimum of about 10 events per explanatory variable (EPV); where event denotes the cases belonging to the less frequent category in the dependent variable. Thus a study designed to use \n\n\n\nk\n\n\n{\\displaystyle k}\n\n explanatory variables for an event (e.g. myocardial infarction) expected to occur in a proportion \n\n\n\np\n\n\n{\\displaystyle p}\n\n of participants in the study will require a total of \n\n\n\n10\nk\n\n/\n\np\n\n\n{\\displaystyle 10k/p}\n\n participants. However, there is considerable debate about the reliability of this rule, which is based on simulation studies and lacks a secure theoretical underpinning.[30] According to some authors[31] the rule is overly conservative in some circumstances, with the authors stating, \"If we (somewhat subjectively) regard confidence interval coverage less than 93 percent, type I error greater than 7 percent, or relative bias greater than 15 percent as problematic, our results indicate that problems are fairly frequent with 2\u20134 EPV, uncommon with 5\u20139 EPV, and still observed with 10\u201316 EPV. The worst instances of each problem were not severe with 5\u20139 EPV and usually comparable to those with 10\u201316 EPV\".[32]\n Others have found results that are not consistent with the above, using different criteria.  A useful criterion is whether the fitted model will be expected to achieve the same predictive discrimination in a new sample as it appeared to achieve in the model development sample.  For that criterion, 20 events per candidate variable may be required.[33]  Also, one can argue that 96 observations are needed only to estimate the model's intercept precisely enough that the margin of error in predicted probabilities is \u00b10.1 with a 0.95 confidence level.[13]\n In any fitting procedure, the addition of another fitting parameter to a model (e.g. the beta parameters in a logistic regression model) will almost always improve the ability of the model to predict the measured outcomes. This will be true even if the additional term has no predictive value, since the model will simply be \"overfitting\" to the noise in the data.  The question arises as to whether the improvement gained by the addition of another fitting parameter is significant enough to recommend the inclusion of the additional term, or whether the improvement is simply that which may be expected from overfitting.\n In short, for logistic regression, a statistic known as the deviance is defined which is a measure of the error between the logistic model fit and the outcome data. In the limit of a large number of data points, the deviance is chi-squared distributed, which allows a chi-squared test to be implemented in order to determine the significance of the explanatory variables.\n Linear regression and logistic regression have many similarities. For example, in simple linear regression, a set of K data points (xk, yk) are fitted to a proposed model function of the form \n\n\n\ny\n=\n\nb\n\n0\n\n\n+\n\nb\n\n1\n\n\nx\n\n\n{\\displaystyle y=b_{0}+b_{1}x}\n\n. The fit is obtained by choosing the b parameters which minimize the sum of the squares of the residuals (the squared error term) for each data point:\n The minimum value which constitutes the fit will be denoted by \n\n\n\n\n\n\n\n\u03b5\n^\n\n\n\n\n2\n\n\n\n\n{\\displaystyle {\\hat {\\varepsilon }}^{2}}\n\n\n The idea of a null model may be introduced, in which it is assumed that the x variable is of no use in predicting the yk outcomes: The data points are fitted to a null model function of the form y\u00a0=\u00a0b0 with a squared error term:\n The fitting process consists of choosing a value of b0 which minimizes \n\n\n\n\n\u03b5\n\n2\n\n\n\n\n{\\displaystyle \\varepsilon ^{2}}\n\n of the fit to the null model, denoted by  \n\n\n\n\n\u03b5\n\n\u03c6\n\n\n2\n\n\n\n\n{\\displaystyle \\varepsilon _{\\varphi }^{2}}\n\n where the \n\n\n\n\u03c6\n\n\n{\\displaystyle \\varphi }\n\n subscript denotes the null model. It is seen that the null model is optimized by \n\n\n\n\nb\n\n0\n\n\n=\n\n\ny\n\u00af\n\n\n\n\n{\\displaystyle b_{0}={\\overline {y}}}\n\n where \n\n\n\n\n\ny\n\u00af\n\n\n\n\n{\\displaystyle {\\overline {y}}}\n\n is the mean of the yk values, and the optimized \n\n\n\n\n\u03b5\n\n\u03c6\n\n\n2\n\n\n\n\n{\\displaystyle \\varepsilon _{\\varphi }^{2}}\n\n is:\n which is proportional to the square of the (uncorrected) sample standard deviation of the yk data points.\n We can imagine a case where the yk data points are randomly assigned to the various xk, and then fitted using the proposed model. Specifically, we can consider the fits of the proposed model to every permutation of the yk outcomes. It can be shown that the optimized error of any of these fits will never be less than the optimum error of the null model, and that the difference between these minimum error will follow a chi-squared distribution, with degrees of freedom equal those of the proposed model minus those of the null model which, in this case, will be \n\n\n\n2\n\u2212\n1\n=\n1\n\n\n{\\displaystyle 2-1=1}\n\n. Using the chi-squared test, we may then estimate how many of these permuted sets of yk will yield an minimum error less than or equal to the minimum  error using the original yk, and so we can estimate how significant an improvement is given by the inclusion of the x variable in the proposed model.\n For logistic regression, the measure of goodness-of-fit is the likelihood function L, or its logarithm, the log-likelihood \u2113. The likelihood function L is analogous to the \n\n\n\n\n\u03b5\n\n2\n\n\n\n\n{\\displaystyle \\varepsilon ^{2}}\n\n in the linear regression case, except that the likelihood is maximized rather than minimized. Denote the maximized log-likelihood of the proposed model by \n\n\n\n\n\n\n\u2113\n^\n\n\n\n\n\n{\\displaystyle {\\hat {\\ell }}}\n\n.\n In the case of simple binary logistic regression, the set of K data points are fitted in a probabilistic sense to a function of the form:\n where \u2060\n\n\n\np\n(\nx\n)\n\n\n{\\displaystyle p(x)}\n\n\u2060 is the probability that \n\n\n\ny\n=\n1\n\n\n{\\displaystyle y=1}\n\n. The log-odds are given by:\n and the log-likelihood is:\n For the null model, the probability that \n\n\n\ny\n=\n1\n\n\n{\\displaystyle y=1}\n\n is given by:\n The log-odds for the null model are given by:\n and the log-likelihood is:\n Since we have \n\n\n\n\np\n\n\u03c6\n\n\n=\n\n\ny\n\u00af\n\n\n\n\n{\\displaystyle p_{\\varphi }={\\overline {y}}}\n\n at the maximum of L, the maximum log-likelihood for the null model is\n The optimum \n\n\n\n\n\u03b2\n\n0\n\n\n\n\n{\\displaystyle \\beta _{0}}\n\n is:\n where \n\n\n\n\n\ny\n\u00af\n\n\n\n\n{\\displaystyle {\\overline {y}}}\n\n is again the mean of the yk values. Again, we can conceptually consider the fit of the proposed model to every permutation of the yk and it can be shown that the maximum log-likelihood of these permutation fits will never be smaller than that of the null model:\n Also, as an analog to the error of the linear regression case, we may define the deviance of a logistic regression fit as:\n which will always be positive or zero. The reason for this choice is that not only is the deviance a good measure of the goodness of fit, it is also approximately chi-squared distributed, with the approximation improving as the number of data points (K) increases, becoming exactly chi-square distributed in the limit of an infinite number of data points. As in the case of linear regression, we may use this fact to estimate the probability that a random set of data points will give a better fit than the fit obtained by the proposed model, and so have an estimate how significantly the model is improved by including the xk data points in the proposed model.\n For the simple model of student test scores described above, the maximum value of the log-likelihood of the null model is \n\n\n\n\n\n\n\n\u2113\n^\n\n\n\n\n\u03c6\n\n\n=\n\u2212\n13.8629\n\u2026\n\n\n{\\displaystyle {\\hat {\\ell }}_{\\varphi }=-13.8629\\ldots }\n\n The maximum value of the log-likelihood for the simple model is \n\n\n\n\n\n\n\u2113\n^\n\n\n\n=\n\u2212\n8.02988\n\u2026\n\n\n{\\displaystyle {\\hat {\\ell }}=-8.02988\\ldots }\n\n so that the deviance is \n\n\n\nD\n=\n2\n(\n\n\n\n\u2113\n^\n\n\n\n\u2212\n\n\n\n\n\u2113\n^\n\n\n\n\n\u03c6\n\n\n)\n=\n11.6661\n\u2026\n\n\n{\\displaystyle D=2({\\hat {\\ell }}-{\\hat {\\ell }}_{\\varphi })=11.6661\\ldots }\n\n\n Using the chi-squared test of significance, the integral of the chi-squared distribution with one degree of freedom from 11.6661... to infinity is equal to 0.00063649...\n This effectively means that about 6 out of a 10,000 fits to random yk can be expected to have a better fit (smaller deviance) than the given yk and so we can conclude that the inclusion of the x variable and data in the proposed model is a very significant improvement over the null model. In other words, we reject the null hypothesis with \n\n\n\n1\n\u2212\nD\n\u2248\n99.94\n%\n\n\n{\\displaystyle 1-D\\approx 99.94\\%}\n\n confidence.\n Goodness of fit in linear regression models is generally measured using R2. Since this has no direct analog in logistic regression, various methods[34]:\u200ach.21\u200a including the following can be used instead.\n In linear regression analysis, one is concerned with partitioning variance via the sum of squares calculations \u2013 variance in the criterion is essentially divided into variance accounted for by the predictors and residual variance. In logistic regression analysis, deviance is used in lieu of a sum of squares calculations.[35] Deviance is analogous to the sum of squares calculations in linear regression[2]  and is a measure of the lack of fit to the data in a logistic regression model.[35] When a \"saturated\" model is available (a model with a theoretically perfect fit), deviance is calculated by comparing a given model with the saturated model.[2]  This computation gives the likelihood-ratio test:[2]\n In the above equation, D represents the deviance and ln represents the natural logarithm. The log of this likelihood ratio (the ratio of the fitted model to the saturated model) will produce a negative value, hence the need for a negative sign. D can be shown to follow an approximate chi-squared distribution.[2]  Smaller values indicate better fit as the fitted model deviates less from the saturated model. When assessed upon a chi-square distribution, nonsignificant chi-square values indicate very little unexplained variance and thus, good model fit. Conversely, a significant chi-square value indicates that a significant amount of the variance is unexplained.\n When the saturated model is not available (a common case), deviance is calculated simply as \u22122\u00b7(log likelihood of the fitted model), and the reference to the saturated model's log likelihood can be removed from all that follows without harm.\n Two measures of deviance are particularly important in logistic regression: null deviance and model deviance. The null deviance represents the difference between a model with only the intercept (which means \"no predictors\") and the saturated model. The model deviance represents the difference between a model with at least one predictor and the saturated model.[35] In this respect, the null model provides a baseline upon which to compare predictor models. Given that deviance is a measure of the difference between a given model and the saturated model, smaller values indicate better fit. Thus, to assess the contribution of a predictor or set of predictors, one can subtract the model deviance from the null deviance and assess the difference on a \n\n\n\n\n\u03c7\n\ns\n\u2212\np\n\n\n2\n\n\n,\n\n\n{\\displaystyle \\chi _{s-p}^{2},}\n\n  chi-square distribution with degrees of freedom[2] equal to the difference in the number of parameters estimated.\n Let\n Then the difference of both is:\n If the model deviance is significantly smaller than the null deviance then one can conclude that the predictor or set of predictors significantly improve the model's fit. This is analogous to the F-test used in linear regression analysis to assess the significance of prediction.[35]\n In linear regression the squared multiple correlation, R2 is used to assess goodness of fit as it represents the proportion of variance in the criterion that is explained by the predictors.[35] In logistic regression analysis, there is no agreed upon analogous measure, but there are several competing measures each with limitations.[35][36]\n Four of the most commonly used indices and one less commonly used one are examined on this page:\n The Hosmer\u2013Lemeshow test uses a test statistic that asymptotically follows a \n\n\n\n\n\u03c7\n\n2\n\n\n\n\n{\\displaystyle \\chi ^{2}}\n\n distribution to assess whether or not the observed event rates match expected event rates in subgroups of the model population.  This test is considered to be obsolete by some statisticians because of its dependence on arbitrary binning of predicted probabilities and relative low power.[37]\n After fitting the model, it is likely that researchers will want to examine the contribution of individual predictors. To do so, they will want to examine the regression coefficients. In linear regression, the regression coefficients represent the change in the criterion for each unit change in the predictor.[35] In logistic regression, however, the regression coefficients represent the change in the logit for each unit change in the predictor. Given that the logit is not intuitive, researchers are likely to focus on a predictor's effect on the exponential function of the regression coefficient \u2013 the odds ratio (see definition). In linear regression, the significance of a regression coefficient is assessed by computing a t test. In logistic regression, there are several different tests designed to assess the significance of an individual predictor, most notably the likelihood ratio test and the Wald statistic.\n The likelihood-ratio test discussed above to assess model fit is also the recommended procedure to assess the contribution of individual \"predictors\" to a given model.[2][26][35] In the case of a single predictor model, one simply compares the deviance of the predictor model with that of the null model on a chi-square distribution with a single degree of freedom. If the predictor model has significantly smaller deviance (c.f. chi-square using the difference in degrees of freedom of the two models), then one can conclude that there is a significant association between the \"predictor\" and the outcome. Although some common statistical packages (e.g. SPSS) do provide likelihood ratio test statistics, without this computationally intensive test it would be more difficult to assess the contribution of individual predictors in the multiple logistic regression case.[citation needed] To assess the contribution of individual predictors one can enter the predictors hierarchically, comparing each new model with the previous to determine the contribution of each predictor.[35] There is some debate among statisticians about the appropriateness of so-called \"stepwise\" procedures.[weasel\u00a0words] The fear is that they may not preserve nominal statistical properties and may become misleading.[38]\n Alternatively, when assessing the contribution of individual predictors in a given model, one may examine the significance of the Wald statistic. The Wald statistic, analogous to the t-test in linear regression, is used to assess the significance of coefficients. The Wald statistic is the ratio of the square of the regression coefficient to the square of the standard error of the coefficient and is asymptotically distributed as a chi-square distribution.[26]\n Although several statistical packages (e.g., SPSS, SAS) report the Wald statistic to assess the contribution of individual predictors, the Wald statistic has limitations. When the regression coefficient is large, the standard error of the regression coefficient also tends to be larger increasing the probability of Type-II error. The Wald statistic also tends to be biased when data are sparse.[35]\n Suppose cases are rare. Then we might wish to sample them more frequently than their prevalence in the population. For example, suppose there is a disease that affects 1 person in 10,000 and to collect our data we need to do a complete physical. It may be too expensive to do thousands of physicals of healthy people in order to obtain data for only a few diseased individuals. Thus, we may evaluate more diseased individuals, perhaps all of the rare outcomes. This is also retrospective sampling, or equivalently it is called unbalanced data. As a rule of thumb, sampling controls at a rate of five times the number of cases will produce sufficient control data.[39]\n Logistic regression is unique in that it may be estimated on unbalanced data, rather than randomly sampled data, and still yield correct coefficient estimates of the effects of each independent variable on the outcome.  That is to say, if we form a logistic model from such data, if the model is correct in the general population, the \n\n\n\n\n\u03b2\n\nj\n\n\n\n\n{\\displaystyle \\beta _{j}}\n\n parameters are all correct except for \n\n\n\n\n\u03b2\n\n0\n\n\n\n\n{\\displaystyle \\beta _{0}}\n\n. We can correct \n\n\n\n\n\u03b2\n\n0\n\n\n\n\n{\\displaystyle \\beta _{0}}\n\n if we know the true prevalence as follows:[39]\n where \n\n\n\n\u03c0\n\n\n{\\displaystyle \\pi }\n\n is the true prevalence and \n\n\n\n\n\n\n\u03c0\n~\n\n\n\n\n\n{\\displaystyle {\\tilde {\\pi }}}\n\n is the prevalence in the sample.\n Like other forms of regression analysis, logistic regression makes use of one or more predictor variables that may be either continuous or categorical. Unlike ordinary linear regression, however, logistic regression is used for predicting dependent variables that take membership in one of a limited number of categories (treating the dependent variable in the binomial case as the outcome of a Bernoulli trial) rather than a continuous outcome. Given this difference, the assumptions of linear regression are violated. In particular, the residuals cannot be normally distributed. In addition, linear regression may make nonsensical predictions for a binary dependent variable. What is needed is a way to convert a binary variable into a continuous one that can take on any real value (negative or positive). To do that, binomial logistic regression first calculates the odds of the event happening for different levels of each independent variable, and then takes its logarithm to create a continuous criterion as a transformed version of the dependent variable. The logarithm of the odds is the logit of the probability, the logit is defined as follows:\n\n\n\n\nlogit\n\u2061\np\n=\nln\n\u2061\n\n\np\n\n1\n\u2212\np\n\n\n\n\n\nfor\u00a0\n\n0\n<\np\n<\n1\n\n.\n\n\n{\\displaystyle \\operatorname {logit} p=\\ln {\\frac {p}{1-p}}\\quad {\\text{for }}0<p<1\\,.}\n\n\n Although the dependent variable in logistic regression is Bernoulli, the logit is on an unrestricted scale.[2] The logit function is the link function in this kind of generalized linear model, i.e.\n\n\n\n\nlogit\n\u2061\n\n\nE\n\n\n\u2061\n(\nY\n)\n=\n\n\u03b2\n\n0\n\n\n+\n\n\u03b2\n\n1\n\n\nx\n\n\n{\\displaystyle \\operatorname {logit} \\operatorname {\\mathcal {E}} (Y)=\\beta _{0}+\\beta _{1}x}\n\n\n Y is the Bernoulli-distributed response variable and x is the predictor variable; the \u03b2 values are the linear parameters.\n The logit of the probability of success is then fitted to the predictors. The predicted value of the logit is converted back into predicted odds, via the inverse of the natural logarithm \u2013 the exponential function. Thus, although the observed dependent variable in binary logistic regression is a 0-or-1 variable, the logistic regression estimates the odds, as a continuous variable, that the dependent variable is a 'success'. In some applications, the odds are all that is needed. In others, a specific yes-or-no prediction is needed for whether the dependent variable is or is not a 'success'; this categorical prediction can be based on the computed odds of success, with predicted odds above some chosen cutoff value being translated into a prediction of success.\n Of all the functional forms used for estimating the probabilities of a particular categorical outcome which optimize the fit by maximizing the likelihood function (e.g. probit regression, Poisson regression, etc.), the logistic regression solution is unique in that it is a maximum entropy solution.[40] This is a case of a general property: an exponential family of distributions maximizes entropy, given an expected value. In the case of the logistic model, the logistic function is the natural parameter of the Bernoulli distribution (it is in \"canonical form\", and the logistic function is the canonical link function), while other sigmoid functions are non-canonical link functions; this underlies its mathematical elegance and ease of optimization. See Exponential family \u00a7\u00a0Maximum entropy derivation for details.\n In order to show this, we use the method of Lagrange multipliers. The Lagrangian is equal to the entropy plus the sum of the products of Lagrange multipliers times various constraint expressions. The general multinomial case will be considered, since the proof is not made that much simpler by considering simpler cases. Equating the derivative of the Lagrangian with respect to the various probabilities to zero yields a functional form for those probabilities which corresponds to those used in logistic regression.[40]\n As in the above section on multinomial logistic regression, we will consider \u2060\n\n\n\nM\n+\n1\n\n\n{\\displaystyle M+1}\n\n\u2060 explanatory variables denoted \u2060\n\n\n\n\nx\n\nm\n\n\n\n\n{\\displaystyle x_{m}}\n\n\u2060 and which include \n\n\n\n\nx\n\n0\n\n\n=\n1\n\n\n{\\displaystyle x_{0}=1}\n\n. There will be a total of K data points, indexed by \n\n\n\nk\n=\n{\n1\n,\n2\n,\n\u2026\n,\nK\n}\n\n\n{\\displaystyle k=\\{1,2,\\dots ,K\\}}\n\n, and the data points are given by \n\n\n\n\nx\n\nm\nk\n\n\n\n\n{\\displaystyle x_{mk}}\n\n and \u2060\n\n\n\n\ny\n\nk\n\n\n\n\n{\\displaystyle y_{k}}\n\n\u2060. The xmk will also be represented as an \u2060\n\n\n\n(\nM\n+\n1\n)\n\n\n{\\displaystyle (M+1)}\n\n\u2060-dimensional vector  \n\n\n\n\n\nx\n\n\nk\n\n\n=\n{\n\nx\n\n0\nk\n\n\n,\n\nx\n\n1\nk\n\n\n,\n\u2026\n,\n\nx\n\nM\nk\n\n\n}\n\n\n{\\displaystyle {\\boldsymbol {x}}_{k}=\\{x_{0k},x_{1k},\\dots ,x_{Mk}\\}}\n\n. There will be \u2060\n\n\n\nN\n+\n1\n\n\n{\\displaystyle N+1}\n\n\u2060 possible values of the categorical variable y ranging from 0 to N.\n Let pn(x) be the probability, given explanatory variable vector x, that the outcome will be \n\n\n\ny\n=\nn\n\n\n{\\displaystyle y=n}\n\n. Define \n\n\n\n\np\n\nn\nk\n\n\n=\n\np\n\nn\n\n\n(\n\n\nx\n\n\nk\n\n\n)\n\n\n{\\displaystyle p_{nk}=p_{n}({\\boldsymbol {x}}_{k})}\n\n which is the probability that for the k-th measurement, the categorical outcome is n.\n The Lagrangian will be expressed as a function of the probabilities pnk and will minimized by equating the derivatives of the Lagrangian with respect to these probabilities to zero. An important point is that the probabilities are treated equally and the fact that they sum to 1 is part of the Lagrangian formulation, rather than being assumed from the beginning.\n The first contribution to the Lagrangian is the entropy:\n The log-likelihood is:\n Assuming the multinomial logistic function, the derivative of the log-likelihood with respect the beta coefficients was found to be:\n A very important point here is that this expression is (remarkably) not an explicit function of the beta coefficients. It is only a function of the probabilities pnk and the data. Rather than being specific to the assumed multinomial logistic case, it is taken to be a general statement of the condition at which the log-likelihood is maximized and makes no reference to the functional form of pnk. There are then (M+1)(N+1) fitting constraints and the fitting constraint term in the Lagrangian is then:\n where the \u03bbnm are the appropriate Lagrange multipliers. There are K normalization constraints which may be written:\n so that the normalization term in the Lagrangian is:\n where the \u03b1k are the appropriate Lagrange multipliers. The Lagrangian is then the sum of the above three terms:\n Setting the derivative of the Lagrangian with respect to one of the probabilities to zero yields:\n Using the more condensed vector notation:\n and dropping the primes on the n and k indices, and then solving for \n\n\n\n\np\n\nn\nk\n\n\n\n\n{\\displaystyle p_{nk}}\n\n yields:\n where:\n Imposing the normalization constraint, we can solve for the Zk and write the probabilities as:\n The \n\n\n\n\n\n\u03bb\n\n\nn\n\n\n\n\n{\\displaystyle {\\boldsymbol {\\lambda }}_{n}}\n\n are not all independent. We can add any constant \u2060\n\n\n\n(\nM\n+\n1\n)\n\n\n{\\displaystyle (M+1)}\n\n\u2060-dimensional vector to each of the \n\n\n\n\n\n\u03bb\n\n\nn\n\n\n\n\n{\\displaystyle {\\boldsymbol {\\lambda }}_{n}}\n\n without changing the value of the \n\n\n\n\np\n\nn\nk\n\n\n\n\n{\\displaystyle p_{nk}}\n\n probabilities so that there are only N rather than \u2060\n\n\n\nN\n+\n1\n\n\n{\\displaystyle N+1}\n\n\u2060 independent \n\n\n\n\n\n\u03bb\n\n\nn\n\n\n\n\n{\\displaystyle {\\boldsymbol {\\lambda }}_{n}}\n\n. In the multinomial logistic regression section above, the \n\n\n\n\n\n\u03bb\n\n\n0\n\n\n\n\n{\\displaystyle {\\boldsymbol {\\lambda }}_{0}}\n\n was subtracted from each \n\n\n\n\n\n\u03bb\n\n\nn\n\n\n\n\n{\\displaystyle {\\boldsymbol {\\lambda }}_{n}}\n\n which set the exponential term involving \n\n\n\n\n\n\u03bb\n\n\n0\n\n\n\n\n{\\displaystyle {\\boldsymbol {\\lambda }}_{0}}\n\n to 1, and the beta coefficients were given by \n\n\n\n\n\n\u03b2\n\n\nn\n\n\n=\n\n\n\u03bb\n\n\nn\n\n\n\u2212\n\n\n\u03bb\n\n\n0\n\n\n\n\n{\\displaystyle {\\boldsymbol {\\beta }}_{n}={\\boldsymbol {\\lambda }}_{n}-{\\boldsymbol {\\lambda }}_{0}}\n\n.\n In machine learning applications where logistic regression is used for binary classification, the MLE minimises the cross-entropy loss function.\n Logistic regression is an important machine learning algorithm. The goal is to model the probability of a random variable \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n being 0 or 1 given experimental data.[41]\n Consider a generalized linear model function parameterized by \n\n\n\n\u03b8\n\n\n{\\displaystyle \\theta }\n\n,\n Therefore,\n and since \n\n\n\nY\n\u2208\n{\n0\n,\n1\n}\n\n\n{\\displaystyle Y\\in \\{0,1\\}}\n\n, we see that \n\n\n\nPr\n(\ny\n\u2223\nX\n;\n\u03b8\n)\n\n\n{\\displaystyle \\Pr(y\\mid X;\\theta )}\n\n is given by \n\n\n\nPr\n(\ny\n\u2223\nX\n;\n\u03b8\n)\n=\n\nh\n\n\u03b8\n\n\n(\nX\n\n)\n\ny\n\n\n(\n1\n\u2212\n\nh\n\n\u03b8\n\n\n(\nX\n)\n\n)\n\n(\n1\n\u2212\ny\n)\n\n\n.\n\n\n{\\displaystyle \\Pr(y\\mid X;\\theta )=h_{\\theta }(X)^{y}(1-h_{\\theta }(X))^{(1-y)}.}\n\n We now calculate the likelihood function assuming that all the observations in the sample are independently Bernoulli distributed,\n Typically, the log likelihood is maximized,\n which is maximized using optimization techniques such as gradient descent.\n Assuming the \n\n\n\n(\nx\n,\ny\n)\n\n\n{\\displaystyle (x,y)}\n\n pairs are drawn uniformly from the underlying distribution, then in the limit of large\u00a0N,\n where \n\n\n\nH\n(\nY\n\u2223\nX\n)\n\n\n{\\displaystyle H(Y\\mid X)}\n\n is the conditional entropy and \n\n\n\n\nD\n\nKL\n\n\n\n\n{\\displaystyle D_{\\text{KL}}}\n\n is the Kullback\u2013Leibler divergence. This leads to the intuition that by maximizing the log-likelihood of a model, you are minimizing the KL divergence of your model from the maximal entropy distribution. Intuitively searching for the model that makes the fewest assumptions in its parameters.\n Logistic regression can be seen as a special case of the generalized linear model and thus analogous to linear regression. The model of logistic regression, however, is based on quite different assumptions (about the relationship between the dependent and independent variables) from those of linear regression. In particular, the key differences between these two models can be seen in the following two features of logistic regression. First, the conditional distribution \n\n\n\ny\n\u2223\nx\n\n\n{\\displaystyle y\\mid x}\n\n is a Bernoulli distribution rather than a Gaussian distribution, because the dependent variable is binary. Second, the predicted values are probabilities and are therefore restricted to (0,1) through the logistic distribution function because logistic regression predicts the probability of particular outcomes rather than the outcomes themselves.\n A common alternative to the logistic model (logit model) is the probit model, as the related names suggest. From the perspective of generalized linear models, these differ in the choice of link function: the logistic model uses the logit function (inverse logistic function), while the probit model uses the probit function (inverse error function). Equivalently, in the latent variable interpretations of these two methods, the first assumes a standard logistic distribution of errors and the second a standard normal distribution of errors.[42] Other sigmoid functions or error distributions can be used instead.\n Logistic regression is an alternative to Fisher's 1936 method, linear discriminant analysis.[43]  If the assumptions of linear discriminant analysis hold, the conditioning can be reversed to produce logistic regression.  The converse is not true, however, because logistic regression does not require the multivariate normal assumption of discriminant analysis.[44]\n The assumption of linear predictor effects can easily be relaxed using techniques such as spline functions.[13]\n A detailed history of the logistic regression is given in Cramer (2002). The logistic function was developed as a model of population growth and named \"logistic\" by Pierre Fran\u00e7ois Verhulst in the 1830s and 1840s, under the guidance of Adolphe Quetelet; see Logistic function \u00a7\u00a0History for details.[45] In his earliest paper (1838), Verhulst did not specify how he fit the curves to the data.[46][47] In his more detailed paper (1845), Verhulst determined the three parameters of the model by making the curve pass through three observed points, which yielded poor predictions.[48][49]\n The logistic function was independently developed in chemistry as a model of autocatalysis (Wilhelm Ostwald, 1883).[50] An autocatalytic reaction is one in which one of the products is itself a catalyst for the same reaction, while the supply of one of the reactants is fixed. This naturally gives rise to the logistic equation for the same reason as population growth: the reaction is self-reinforcing but constrained.\n The logistic function was independently rediscovered as a model of population growth in 1920 by Raymond Pearl and Lowell Reed, published as Pearl & Reed (1920), which led to its use in modern statistics. They were initially unaware of Verhulst's work and presumably learned about it from L. Gustave du Pasquier, but they gave him little credit and did not adopt his terminology.[51] Verhulst's priority was acknowledged and the term \"logistic\" revived by Udny Yule in 1925 and has been followed since.[52] Pearl and Reed first applied the model to the population of the United States, and also initially fitted the curve by making it pass through three points; as with Verhulst, this again yielded poor results.[53]\n In the 1930s, the probit model was developed and systematized by Chester Ittner Bliss, who coined the term \"probit\" in Bliss (1934), and by John Gaddum in Gaddum (1933), and the model fit by maximum likelihood estimation by Ronald A. Fisher in Fisher (1935), as an addendum to Bliss's work. The probit model was principally used in bioassay, and had been preceded by earlier work dating to 1860; see Probit model \u00a7\u00a0History. The probit model influenced the subsequent development of the logit model and these models competed with each other.[54]\n The logistic model was likely first used as an alternative to the probit model in bioassay by Edwin Bidwell Wilson and his student Jane Worcester in Wilson & Worcester (1943).[55] However, the development of the logistic model as a general alternative to the probit model was principally due to the work of Joseph Berkson over many decades, beginning in Berkson (1944), where he coined \"logit\", by analogy with \"probit\", and continuing through Berkson (1951) and following years.[56] The logit model was initially dismissed as inferior to the probit model, but \"gradually achieved an equal footing with the probit\",[57] particularly between 1960 and 1970. By 1970, the logit model achieved parity with the probit model in use in statistics journals and thereafter surpassed it. This relative popularity was due to the adoption of the logit outside of bioassay, rather than displacing the probit within bioassay, and its informal use in practice; the logit's popularity is credited to the logit model's computational simplicity, mathematical properties, and generality, allowing its use in varied fields.[3]\n Various refinements occurred during that time, notably by David Cox, as in Cox (1958).[4]\n The multinomial logit model was introduced independently in Cox (1966) and Theil (1969), which greatly increased the scope of application and the popularity of the logit model.[58] In 1973 Daniel McFadden linked the multinomial logit to the theory of discrete choice, specifically Luce's choice axiom, showing that the multinomial logit followed from the assumption of independence of irrelevant alternatives and interpreting odds of alternatives as relative preferences;[59] this gave a theoretical foundation for the logistic regression.[58]\n There are large numbers of extensions:\n",
        "doc_number": 87
    },
    {
        "url": "https://en.wikipedia.org/wiki/Linear_regression",
        "content": "In statistics, linear regression is a model that estimates the linear relationship between a scalar response (dependent variable) and one or more explanatory variables (regressor or independent variable). A model with exactly one explanatory variable is a simple linear regression; a model with two or more explanatory variables is a multiple linear regression.[1] This term is distinct from multivariate linear regression, which predicts multiple correlated dependent variables rather than a single dependent variable.[2]\n In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.\n Linear regression is also a type of machine learning algorithm, more specifically a supervised algorithm, that learns from the labelled datasets and maps the data points to the most optimized linear functions that can be used for prediction on new datasets. [3]\n Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications.[4] This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.\n Linear regression has many practical uses. Most applications fall into one of the following two broad categories:\n Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the \"lack of fit\" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Use of the Mean Squared Error (MSE) as the cost on a dataset that has many large outliers, can result in a model that fits the outliers more than the true data due to the higher importance assigned by MSE to large errors. So, cost functions that are robust to outliers should be used if the dataset has many large outliers. Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms \"least squares\" and \"linear model\" are closely linked, they are not synonymous.\n Given a data set \n\n\n\n{\n\ny\n\ni\n\n\n,\n\n\nx\n\ni\n1\n\n\n,\n\u2026\n,\n\nx\n\ni\np\n\n\n\n}\n\ni\n=\n1\n\n\nn\n\n\n\n\n{\\displaystyle \\{y_{i},\\,x_{i1},\\ldots ,x_{ip}\\}_{i=1}^{n}}\n\n of n statistical units, a linear regression model assumes that the relationship between the dependent variable y and the vector of regressors x is linear. This relationship is modeled through a disturbance term or error variable \u03b5\u2014an unobserved random variable that adds \"noise\" to the linear relationship between the dependent variable and regressors. Thus the model takes the form\n\n\n\n\ny\n\ni\n\n\n=\n\n\u03b2\n\n0\n\n\n+\n\n\u03b2\n\n1\n\n\n\nx\n\ni\n1\n\n\n+\n\u22ef\n+\n\n\u03b2\n\np\n\n\n\nx\n\ni\np\n\n\n+\n\n\u03b5\n\ni\n\n\n=\n\n\nx\n\n\ni\n\n\n\nT\n\n\n\n\n\u03b2\n\n+\n\n\u03b5\n\ni\n\n\n,\n\ni\n=\n1\n,\n\u2026\n,\nn\n,\n\n\n{\\displaystyle y_{i}=\\beta _{0}+\\beta _{1}x_{i1}+\\cdots +\\beta _{p}x_{ip}+\\varepsilon _{i}=\\mathbf {x} _{i}^{\\mathsf {T}}{\\boldsymbol {\\beta }}+\\varepsilon _{i},\\qquad i=1,\\ldots ,n,}\n\nwhere T denotes the transpose, so that xiT\u03b2 is the inner product between vectors xi and \u03b2.\n Often these n equations are stacked together and written in matrix notation as\n where\n Fitting a linear model to a given data set usually requires estimating the regression coefficients \n\n\n\n\n\u03b2\n\n\n\n{\\displaystyle {\\boldsymbol {\\beta }}}\n\n such that the error term \n\n\n\n\n\u03b5\n\n=\n\ny\n\n\u2212\n\nX\n\n\n\u03b2\n\n\n\n{\\displaystyle {\\boldsymbol {\\varepsilon }}=\\mathbf {y} -\\mathbf {X} {\\boldsymbol {\\beta }}}\n\n is minimized. For example, it is common to use the sum of squared errors \n\n\n\n\u2016\n\n\u03b5\n\n\n\u2016\n\n2\n\n\n2\n\n\n\n\n{\\displaystyle \\|{\\boldsymbol {\\varepsilon }}\\|_{2}^{2}}\n\n as a measure of \n\n\n\n\n\u03b5\n\n\n\n{\\displaystyle {\\boldsymbol {\\varepsilon }}}\n\n for minimization.\n Consider a situation where a small ball is being tossed up in the air and then we measure its heights of ascent hi at various moments in time ti. Physics tells us that, ignoring the drag, the relationship can be modeled as\n where \u03b21 determines the initial velocity of the ball, \u03b22 is proportional to the standard gravity, and \u03b5i is due to measurement errors. Linear regression can be used to estimate the values of \u03b21 and \u03b22 from the measured data. This model is non-linear in the time variable, but it is linear in the parameters \u03b21 and \u03b22; if we take regressors xi\u00a0=\u00a0(xi1, xi2) \u00a0=\u00a0(ti, ti2), the model takes on the standard form\n Standard linear regression models with standard estimation techniques make a number of assumptions about the predictor variables, the response variable and their relationship. Numerous extensions have been developed that allow each of these assumptions to be relaxed (i.e. reduced to a weaker form), and in some cases eliminated entirely. Generally these extensions make the estimation procedure more complex and time-consuming, and may also require more data in order to produce an equally precise model.[citation needed]\n The following are the major assumptions made by standard linear regression models with standard estimation techniques (e.g. ordinary least squares):\n Violations of these assumptions can result in biased estimations of \u03b2, biased standard errors, untrustworthy confidence intervals and significance tests. Beyond these assumptions, several other statistical properties of the data strongly influence the performance of different estimation methods:\n A fitted linear regression model can be used to identify the relationship between a single predictor variable xj and the response variable y when all the other predictor variables in the model are \"held fixed\". Specifically, the interpretation of \u03b2j is the expected change in y for a one-unit change in xj when the other covariates are held fixed\u2014that is, the expected value of the partial derivative of y with respect to xj. This is sometimes called the unique effect of xj on y. In contrast, the marginal effect of xj on y can be assessed using a correlation coefficient or simple linear regression model relating only xj to y; this effect is the total derivative of y with respect to xj.\n Care must be taken when interpreting regression results, as some of the regressors may not allow for marginal changes (such as dummy variables, or the intercept term), while others cannot be held fixed (recall the example from the introduction: it would be impossible to \"hold ti fixed\" and at the same time change the value of ti2).\n It is possible that the unique effect be nearly zero even when the marginal effect is large. This may imply that some other covariate captures all the information in xj, so that once that variable is in the model, there is no contribution of xj to the variation in y. Conversely, the unique effect of xj can be large while its marginal effect is nearly zero. This would happen if the other covariates explained a great deal of the variation of y, but they mainly explain variation in a way that is complementary to what is captured by xj. In this case, including the other variables in the model reduces the part of the variability of y that is unrelated to xj, thereby strengthening the apparent relationship with xj.\n The meaning of the expression \"held fixed\" may depend on how the values of the predictor variables arise. If the experimenter directly sets the values of the predictor variables according to a study design, the comparisons of interest may literally correspond to comparisons among units whose predictor variables have been \"held fixed\" by the experimenter. Alternatively, the expression \"held fixed\" can refer to a selection that takes place in the context of data analysis. In this case, we \"hold a variable fixed\" by restricting our attention to the subsets of the data that happen to have a common value for the given predictor variable. This is the only interpretation of \"held fixed\" that can be used in an observational study.\n The notion of a \"unique effect\" is appealing when studying a complex system where multiple interrelated components influence the response variable. In some cases, it can literally be interpreted as the causal effect of an intervention that is linked to the value of a predictor variable. However, it has been argued that in many cases multiple regression analysis fails to clarify the relationships between the predictor variables and the response variable when the predictors are correlated with each other and are not assigned following a study design.[9]\n Numerous extensions of linear regression have been developed, which allow some or all of the assumptions underlying the basic model to be relaxed.\n The simplest case of a single scalar predictor variable x and a single scalar response variable y is known as simple linear regression. The extension to multiple and/or vector-valued predictor variables (denoted with a capital X) is known as multiple linear regression, also known as multivariable linear regression (not to be confused with multivariate linear regression).[10]\n Multiple linear regression is a generalization of simple linear regression to the case of more than one independent variable, and a special case of general linear models, restricted to one dependent variable. The basic model for multiple linear regression is\n for each observation \n\n\n\ni\n=\n1\n,\n\u2026\n,\nn\n\n\n{\\textstyle i=1,\\ldots ,n}\n\n.\n In the formula above we consider n observations of one dependent variable and p independent variables. Thus, Yi is the ith observation of the dependent variable, Xij is ith observation of the jth independent variable, j = 1, 2, ..., p. The values \u03b2j represent parameters to be estimated, and \u03b5i is the ith independent identically distributed normal error.\n In the more general multivariate linear regression, there is one equation of the above form for each of m > 1 dependent variables that share the same set of explanatory variables and hence are estimated simultaneously with each other:\n for all observations indexed as i = 1, ... , n and for all dependent variables indexed as j = 1, ... , m.\n Nearly all real-world regression models involve multiple predictors, and basic descriptions of linear regression are often phrased in terms of the multiple regression model. Note, however, that in these cases the response variable y is still a scalar. Another term, multivariate linear regression, refers to cases where y is a vector, i.e., the same as general linear regression.\n The general linear model considers the situation when the response variable is not a scalar (for each observation) but a vector, yi. Conditional linearity of \n\n\n\nE\n(\n\ny\n\n\u2223\n\n\nx\n\n\ni\n\n\n)\n=\n\n\nx\n\n\ni\n\n\n\nT\n\n\n\nB\n\n\n{\\displaystyle E(\\mathbf {y} \\mid \\mathbf {x} _{i})=\\mathbf {x} _{i}^{\\mathsf {T}}B}\n\n is still assumed, with a matrix B replacing the vector \u03b2 of the classical linear regression model. Multivariate analogues of ordinary least squares (OLS) and generalized least squares (GLS) have been developed. \"General linear models\" are also called \"multivariate linear models\". These are not the same as multivariable linear models (also called \"multiple linear models\").\n Various models have been created that allow for heteroscedasticity, i.e. the errors for different response variables may have different variances. For example, weighted least squares is a method for estimating linear regression models when the response variables may have different error variances, possibly with correlated errors. (See also Weighted linear least squares, and Generalized least squares.) Heteroscedasticity-consistent standard errors is an improved method for use with uncorrelated but potentially heteroscedastic errors.\n The Generalized linear model (GLM) is a framework for modeling response variables that are bounded or discrete. This is used, for example:\n Generalized linear models allow for an arbitrary link function, g, that relates the mean of the response variable(s) to the predictors: \n\n\n\nE\n(\nY\n)\n=\n\ng\n\n\u2212\n1\n\n\n(\nX\nB\n)\n\n\n{\\displaystyle E(Y)=g^{-1}(XB)}\n\n. The link function is often related to the distribution of the response, and in particular it typically has the effect of transforming between the \n\n\n\n(\n\u2212\n\u221e\n,\n\u221e\n)\n\n\n{\\displaystyle (-\\infty ,\\infty )}\n\n range of the linear predictor and the range of the response variable.\n Some common examples of GLMs are:\n Single index models[clarification needed] allow some degree of nonlinearity in the relationship between x and y, while preserving the central role of the linear predictor \u03b2\u2032x as in the classical linear regression model. Under certain conditions, simply applying OLS to data from a single-index model will consistently estimate \u03b2 up to a proportionality constant.[11]\n Hierarchical linear models (or multilevel regression) organizes the data into a hierarchy of regressions, for example where A is regressed on B, and B is regressed on C. It is often used where the variables of interest have a natural hierarchical structure such as in educational statistics, where students are nested in classrooms, classrooms are nested in schools, and schools are nested in some administrative grouping, such as a school district. The response variable might be a measure of student achievement such as a test score, and different covariates would be collected at the classroom, school, and school district levels.\n Errors-in-variables models (or \"measurement error models\") extend the traditional linear regression model to allow the predictor variables X to be observed with error. This error causes standard estimators of \u03b2 to become biased. Generally, the form of bias is an attenuation, meaning that the effects are biased toward zero.\n In a multiple linear regression model\n parameter \n\n\n\n\n\u03b2\n\nj\n\n\n\n\n{\\displaystyle \\beta _{j}}\n\n of predictor variable \n\n\n\n\nx\n\nj\n\n\n\n\n{\\displaystyle x_{j}}\n\n represents the individual effect of \n\n\n\n\nx\n\nj\n\n\n\n\n{\\displaystyle x_{j}}\n\n. It has an interpretation as the expected change in the response variable \n\n\n\ny\n\n\n{\\displaystyle y}\n\n when \n\n\n\n\nx\n\nj\n\n\n\n\n{\\displaystyle x_{j}}\n\n increases by one unit with other predictor variables held constant. When \n\n\n\n\nx\n\nj\n\n\n\n\n{\\displaystyle x_{j}}\n\n is strongly correlated with other predictor variables, it is improbable that \n\n\n\n\nx\n\nj\n\n\n\n\n{\\displaystyle x_{j}}\n\n can increase by one unit with other variables held constant. In this case, the interpretation of \n\n\n\n\n\u03b2\n\nj\n\n\n\n\n{\\displaystyle \\beta _{j}}\n\n becomes problematic as it is based on an improbable condition, and the effect of \n\n\n\n\nx\n\nj\n\n\n\n\n{\\displaystyle x_{j}}\n\n cannot be evaluated in isolation.\n For a group of predictor variables, say, \n\n\n\n{\n\nx\n\n1\n\n\n,\n\nx\n\n2\n\n\n,\n\u2026\n,\n\nx\n\nq\n\n\n}\n\n\n{\\displaystyle \\{x_{1},x_{2},\\dots ,x_{q}\\}}\n\n, a group effect \n\n\n\n\u03be\n(\n\nw\n\n)\n\n\n{\\displaystyle \\xi (\\mathbf {w} )}\n\n is defined as a linear combination of their parameters\n where \n\n\n\n\nw\n\n=\n(\n\nw\n\n1\n\n\n,\n\nw\n\n2\n\n\n,\n\u2026\n,\n\nw\n\nq\n\n\n\n)\n\n\u22ba\n\n\n\n\n{\\displaystyle \\mathbf {w} =(w_{1},w_{2},\\dots ,w_{q})^{\\intercal }}\n\n is a weight vector satisfying \n\n\n\n\n\u2211\n\nj\n=\n1\n\n\nq\n\n\n\n|\n\n\nw\n\nj\n\n\n\n|\n\n=\n1\n\n\n{\\textstyle \\sum _{j=1}^{q}|w_{j}|=1}\n\n. Because of the constraint on \n\n\n\n\n\nw\n\nj\n\n\n\n\n\n{\\displaystyle {w_{j}}}\n\n, \n\n\n\n\u03be\n(\n\nw\n\n)\n\n\n{\\displaystyle \\xi (\\mathbf {w} )}\n\n is also referred to as a normalized group effect. A group effect \n\n\n\n\u03be\n(\n\nw\n\n)\n\n\n{\\displaystyle \\xi (\\mathbf {w} )}\n\n has an interpretation as the expected change in \n\n\n\ny\n\n\n{\\displaystyle y}\n\n when variables in the group \n\n\n\n\nx\n\n1\n\n\n,\n\nx\n\n2\n\n\n,\n\u2026\n,\n\nx\n\nq\n\n\n\n\n{\\displaystyle x_{1},x_{2},\\dots ,x_{q}}\n\n change by the amount \n\n\n\n\nw\n\n1\n\n\n,\n\nw\n\n2\n\n\n,\n\u2026\n,\n\nw\n\nq\n\n\n\n\n{\\displaystyle w_{1},w_{2},\\dots ,w_{q}}\n\n, respectively, at the same time with other variables (not in the group) held constant. It generalizes the individual effect of a variable to a group of variables in that (\n\n\n\ni\n\n\n{\\displaystyle i}\n\n) if \n\n\n\nq\n=\n1\n\n\n{\\displaystyle q=1}\n\n, then the group effect reduces to an individual effect, and (\n\n\n\ni\ni\n\n\n{\\displaystyle ii}\n\n) if \n\n\n\n\nw\n\ni\n\n\n=\n1\n\n\n{\\displaystyle w_{i}=1}\n\n and \n\n\n\n\nw\n\nj\n\n\n=\n0\n\n\n{\\displaystyle w_{j}=0}\n\n for \n\n\n\nj\n\u2260\ni\n\n\n{\\displaystyle j\\neq i}\n\n, then the group effect also reduces to an individual effect.\nA group effect \n\n\n\n\u03be\n(\n\nw\n\n)\n\n\n{\\displaystyle \\xi (\\mathbf {w} )}\n\n is said to be meaningful if the underlying simultaneous changes of the \n\n\n\nq\n\n\n{\\displaystyle q}\n\n variables \n\n\n\n(\n\nx\n\n1\n\n\n,\n\nx\n\n2\n\n\n,\n\u2026\n,\n\nx\n\nq\n\n\n\n)\n\n\u22ba\n\n\n\n\n{\\displaystyle (x_{1},x_{2},\\dots ,x_{q})^{\\intercal }}\n\n is probable.\n Group effects provide a means to study the collective impact of strongly correlated predictor variables in linear regression models. Individual effects of such variables are not well-defined as their parameters do not have good interpretations. Furthermore, when the sample size is not large, none of their parameters can be accurately estimated by the least squares regression due to the multicollinearity problem. Nevertheless, there are meaningful group effects that have good interpretations and can be accurately estimated by the least squares regression. A simple way to identify these meaningful group effects is to use an all positive correlations (APC) arrangement of the strongly correlated variables under which pairwise correlations among these variables are all positive, and standardize all \n\n\n\np\n\n\n{\\displaystyle p}\n\n predictor variables in the model so that they all have mean zero and length one. To illustrate this, suppose that \n\n\n\n{\n\nx\n\n1\n\n\n,\n\nx\n\n2\n\n\n,\n\u2026\n,\n\nx\n\nq\n\n\n}\n\n\n{\\displaystyle \\{x_{1},x_{2},\\dots ,x_{q}\\}}\n\n is a group of strongly correlated variables in an APC arrangement and that they are not strongly correlated with predictor variables outside the group. Let \n\n\n\n\ny\n\u2032\n\n\n\n{\\displaystyle y'}\n\n be the centred \n\n\n\ny\n\n\n{\\displaystyle y}\n\n and \n\n\n\n\nx\n\nj\n\n\u2032\n\n\n\n{\\displaystyle x_{j}'}\n\n be the standardized \n\n\n\n\nx\n\nj\n\n\n\n\n{\\displaystyle x_{j}}\n\n. Then, the standardized linear regression model is\n Parameters \n\n\n\n\n\u03b2\n\nj\n\n\n\n\n{\\displaystyle \\beta _{j}}\n\n in the original model, including \n\n\n\n\n\u03b2\n\n0\n\n\n\n\n{\\displaystyle \\beta _{0}}\n\n, are simple functions of \n\n\n\n\n\u03b2\n\nj\n\n\u2032\n\n\n\n{\\displaystyle \\beta _{j}'}\n\n in the standardized model. The standardization of variables does not change their correlations, so \n\n\n\n{\n\nx\n\n1\n\n\u2032\n\n,\n\nx\n\n2\n\n\u2032\n\n,\n\u2026\n,\n\nx\n\nq\n\n\u2032\n\n}\n\n\n{\\displaystyle \\{x_{1}',x_{2}',\\dots ,x_{q}'\\}}\n\n is a group of strongly correlated variables in an APC arrangement and they are not strongly correlated with other predictor variables in the standardized model. A group effect of \n\n\n\n{\n\nx\n\n1\n\n\u2032\n\n,\n\nx\n\n2\n\n\u2032\n\n,\n\u2026\n,\n\nx\n\nq\n\n\u2032\n\n}\n\n\n{\\displaystyle \\{x_{1}',x_{2}',\\dots ,x_{q}'\\}}\n\n is\n and its minimum-variance unbiased linear estimator is\n where \n\n\n\n\n\n\n\n\u03b2\n^\n\n\n\n\nj\n\n\u2032\n\n\n\n{\\displaystyle {\\hat {\\beta }}_{j}'}\n\n is the least squares estimator of \n\n\n\n\n\u03b2\n\nj\n\n\u2032\n\n\n\n{\\displaystyle \\beta _{j}'}\n\n. In particular, the average group effect of the \n\n\n\nq\n\n\n{\\displaystyle q}\n\n standardized variables is\n which has an interpretation as the expected change in \n\n\n\n\ny\n\u2032\n\n\n\n{\\displaystyle y'}\n\n when all \n\n\n\n\nx\n\nj\n\n\u2032\n\n\n\n{\\displaystyle x_{j}'}\n\n in the strongly correlated group increase by \n\n\n\n(\n1\n\n/\n\nq\n)\n\n\n{\\displaystyle (1/q)}\n\nth of a unit at the same time with variables outside the group held constant. With strong positive correlations and in standardized units, variables in the group are approximately equal, so they are likely to increase at the same time and in similar amount. Thus, the average group effect \n\n\n\n\n\u03be\n\nA\n\n\n\n\n{\\displaystyle \\xi _{A}}\n\n is a meaningful effect. It can be accurately estimated by its minimum-variance unbiased linear estimator \n\n\n\n\n\n\n\n\u03be\n^\n\n\n\n\nA\n\n\n=\n\n\n1\nq\n\n\n(\n\n\n\n\n\u03b2\n^\n\n\n\n\n1\n\n\u2032\n\n+\n\n\n\n\n\u03b2\n^\n\n\n\n\n2\n\n\u2032\n\n+\n\u22ef\n+\n\n\n\n\n\u03b2\n^\n\n\n\n\nq\n\n\u2032\n\n)\n\n\n{\\textstyle {\\hat {\\xi }}_{A}={\\frac {1}{q}}({\\hat {\\beta }}_{1}'+{\\hat {\\beta }}_{2}'+\\dots +{\\hat {\\beta }}_{q}')}\n\n, even when individually none of the \n\n\n\n\n\u03b2\n\nj\n\n\u2032\n\n\n\n{\\displaystyle \\beta _{j}'}\n\n can be accurately estimated by \n\n\n\n\n\n\n\n\u03b2\n^\n\n\n\n\nj\n\n\u2032\n\n\n\n{\\displaystyle {\\hat {\\beta }}_{j}'}\n\n.\n Not all group effects are meaningful or can be accurately estimated. For example, \n\n\n\n\n\u03b2\n\n1\n\n\u2032\n\n\n\n{\\displaystyle \\beta _{1}'}\n\n is a special group effect with weights \n\n\n\n\nw\n\n1\n\n\n=\n1\n\n\n{\\displaystyle w_{1}=1}\n\n and \n\n\n\n\nw\n\nj\n\n\n=\n0\n\n\n{\\displaystyle w_{j}=0}\n\n for \n\n\n\nj\n\u2260\n1\n\n\n{\\displaystyle j\\neq 1}\n\n, but it cannot be accurately estimated by \n\n\n\n\n\n\n\n\u03b2\n^\n\n\n\n\n1\n\n\u2032\n\n\n\n{\\displaystyle {\\hat {\\beta }}'_{1}}\n\n. It is also not a meaningful effect. In general, for a group of \n\n\n\nq\n\n\n{\\displaystyle q}\n\n strongly correlated predictor variables in an APC arrangement in the standardized model, group effects whose weight vectors \n\n\n\n\nw\n\n\n\n{\\displaystyle \\mathbf {w} }\n\n are at or near the centre of the simplex \n\n\n\n\n\u2211\n\nj\n=\n1\n\n\nq\n\n\n\nw\n\nj\n\n\n=\n1\n\n\n{\\textstyle \\sum _{j=1}^{q}w_{j}=1}\n\n (\n\n\n\n\nw\n\nj\n\n\n\u2265\n0\n\n\n{\\displaystyle w_{j}\\geq 0}\n\n) are meaningful and can be accurately estimated by their minimum-variance unbiased linear estimators. Effects with weight vectors far away from the centre are not meaningful as such weight vectors represent simultaneous changes of the variables that violate the strong positive correlations of the standardized variables in an APC arrangement. As such, they are not probable. These effects also cannot be accurately estimated.\n Applications of the group effects include (1) estimation and inference for meaningful group effects on the response variable, (2) testing for \"group significance\" of the \n\n\n\nq\n\n\n{\\displaystyle q}\n\n variables via testing \n\n\n\n\nH\n\n0\n\n\n:\n\n\u03be\n\nA\n\n\n=\n0\n\n\n{\\displaystyle H_{0}:\\xi _{A}=0}\n\n versus \n\n\n\n\nH\n\n1\n\n\n:\n\n\u03be\n\nA\n\n\n\u2260\n0\n\n\n{\\displaystyle H_{1}:\\xi _{A}\\neq 0}\n\n, and (3) characterizing the region of the predictor variable space over which predictions by the least squares estimated model are accurate.\n A group effect of the original variables \n\n\n\n{\n\nx\n\n1\n\n\n,\n\nx\n\n2\n\n\n,\n\u2026\n,\n\nx\n\nq\n\n\n}\n\n\n{\\displaystyle \\{x_{1},x_{2},\\dots ,x_{q}\\}}\n\n can be expressed as a constant times a group effect of the standardized variables \n\n\n\n{\n\nx\n\n1\n\n\u2032\n\n,\n\nx\n\n2\n\n\u2032\n\n,\n\u2026\n,\n\nx\n\nq\n\n\u2032\n\n}\n\n\n{\\displaystyle \\{x_{1}',x_{2}',\\dots ,x_{q}'\\}}\n\n. The former is meaningful when the latter is. Thus meaningful group effects of the original variables can be found through meaningful group effects of the standardized variables.[12]\n In Dempster\u2013Shafer theory, or a linear belief function in particular, a linear regression model may be represented as a partially swept matrix, which can be combined with similar matrices representing observations and other assumed normal distributions and state equations. The combination of swept or unswept matrices provides an alternative method for estimating linear regression models.\n A large number of procedures have been developed for parameter estimation and inference in linear regression. These methods differ in computational simplicity of algorithms, presence of a closed-form solution, robustness with respect to heavy-tailed distributions, and theoretical assumptions needed to validate desirable statistical properties such as consistency and asymptotic efficiency.\n Some of the more common estimation techniques for linear regression are summarized below.\n Assuming that the independent variables are \n\n\n\n\n\n\n\nx\n\ni\n\n\n\u2192\n\n\n\n=\n\n[\n\n\nx\n\n1\n\n\ni\n\n\n,\n\nx\n\n2\n\n\ni\n\n\n,\n\u2026\n,\n\nx\n\nm\n\n\ni\n\n\n\n]\n\n\n\n{\\displaystyle {\\vec {x_{i}}}=\\left[x_{1}^{i},x_{2}^{i},\\ldots ,x_{m}^{i}\\right]}\n\n and the model's parameters are \n\n\n\n\n\n\n\u03b2\n\u2192\n\n\n\n=\n\n[\n\n\n\u03b2\n\n0\n\n\n,\n\n\u03b2\n\n1\n\n\n,\n\u2026\n,\n\n\u03b2\n\nm\n\n\n\n]\n\n\n\n{\\displaystyle {\\vec {\\beta }}=\\left[\\beta _{0},\\beta _{1},\\ldots ,\\beta _{m}\\right]}\n\n, then the model's prediction would be\n If \n\n\n\n\n\n\n\nx\n\ni\n\n\n\u2192\n\n\n\n\n\n{\\displaystyle {\\vec {x_{i}}}}\n\n is extended to \n\n\n\n\n\n\n\nx\n\ni\n\n\n\u2192\n\n\n\n=\n\n[\n\n1\n,\n\nx\n\n1\n\n\ni\n\n\n,\n\nx\n\n2\n\n\ni\n\n\n,\n\u2026\n,\n\nx\n\nm\n\n\ni\n\n\n\n]\n\n\n\n{\\displaystyle {\\vec {x_{i}}}=\\left[1,x_{1}^{i},x_{2}^{i},\\ldots ,x_{m}^{i}\\right]}\n\n then \n\n\n\n\ny\n\ni\n\n\n\n\n{\\displaystyle y_{i}}\n\n would become a dot product of the parameter and the independent vectors, i.e.\n In the least-squares setting, the optimum parameter vector is defined as such that minimizes the sum of mean squared loss:\n Now putting the independent and dependent variables in matrices \n\n\n\nX\n\n\n{\\displaystyle X}\n\n and \n\n\n\nY\n\n\n{\\displaystyle Y}\n\n respectively, the loss function can be rewritten as:\n As the loss function is convex, the optimum solution lies at gradient zero. The gradient of the loss function is (using Denominator layout convention):\n Setting the gradient to zero produces the optimum parameter:\n Note: The \n\n\n\n\n\n\n\u03b2\n^\n\n\n\n\n\n{\\displaystyle {\\hat {\\beta }}}\n\n obtained may indeed be the local minimum, one needs to differentiate once more to obtain the Hessian matrix and show that it is positive definite. This is provided by the Gauss\u2013Markov theorem.\n Linear least squares methods include mainly:\n Maximum likelihood estimation can be performed when the distribution of the error terms is known to belong to a certain parametric family \u0192\u03b8 of probability distributions.[15] When f\u03b8 is a normal distribution with zero mean and variance \u03b8, the resulting estimate is identical to the OLS estimate. GLS estimates are maximum likelihood estimates when \u03b5 follows a multivariate normal distribution with a known covariance matrix.\nLet's denote each data point by \n\n\n\n(\n\n\n\n\nx\n\ni\n\n\n\u2192\n\n\n\n,\n\ny\n\ni\n\n\n)\n\n\n{\\displaystyle ({\\vec {x_{i}}},y_{i})}\n\n and the regression parameters as \n\n\n\n\n\n\n\u03b2\n\u2192\n\n\n\n\n\n{\\displaystyle {\\vec {\\beta }}}\n\n, and the set of all data by \n\n\n\nD\n\n\n{\\displaystyle D}\n\n and the cost function by \n\n\n\nL\n(\nD\n,\n\n\n\n\u03b2\n\u2192\n\n\n\n)\n=\n\n\u2211\n\ni\n\n\n(\n\ny\n\ni\n\n\n\u2212\n\n\n\n\u03b2\n\u2192\n\n\n\n\n\u22c5\n\n\n\n\n\nx\n\ni\n\n\n\u2192\n\n\n\n\n)\n\n2\n\n\n\n\n{\\displaystyle L(D,{\\vec {\\beta }})=\\sum _{i}(y_{i}-{\\vec {\\beta }}\\,\\cdot \\,{\\vec {x_{i}}})^{2}}\n\n.\n As shown below the same optimal parameter that minimizes \n\n\n\nL\n(\nD\n,\n\n\n\n\u03b2\n\u2192\n\n\n\n)\n\n\n{\\displaystyle L(D,{\\vec {\\beta }})}\n\n achieves maximum likelihood too.[16] Here the assumption is that the dependent variable \n\n\n\ny\n\n\n{\\displaystyle y}\n\n is a random variable that follows a Gaussian distribution, where the standard deviation is fixed and the mean is a linear combination of \n\n\n\n\n\n\nx\n\u2192\n\n\n\n\n\n{\\displaystyle {\\vec {x}}}\n\n:\n\n\n\n\n\n\n\nH\n(\nD\n,\n\n\n\n\u03b2\n\u2192\n\n\n\n)\n\n\n\n=\n\n\u220f\n\ni\n=\n1\n\n\nn\n\n\nP\nr\n(\n\ny\n\ni\n\n\n\n|\n\n\n\n\n\nx\n\ni\n\n\n\u2192\n\n\n\n\n\n;\n\n\n\n\u03b2\n\u2192\n\n\n\n,\n\u03c3\n)\n\n\n\n\n\n\n=\n\n\u220f\n\ni\n=\n1\n\n\nn\n\n\n\n\n1\n\n\n\n2\n\u03c0\n\n\n\u03c3\n\n\n\nexp\n\u2061\n\n(\n\n\u2212\n\n\n\n\n(\n\n\ny\n\ni\n\n\n\u2212\n\n\n\n\u03b2\n\u2192\n\n\n\n\n\u22c5\n\n\n\n\n\nx\n\ni\n\n\n\u2192\n\n\n\n\n)\n\n\n2\n\n\n\n2\n\n\u03c3\n\n2\n\n\n\n\n\n\n)\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}H(D,{\\vec {\\beta }})&=\\prod _{i=1}^{n}Pr(y_{i}|{\\vec {x_{i}}}\\,\\,;{\\vec {\\beta }},\\sigma )\\\\&=\\prod _{i=1}^{n}{\\frac {1}{{\\sqrt {2\\pi }}\\sigma }}\\exp \\left(-{\\frac {\\left(y_{i}-{\\vec {\\beta }}\\,\\cdot \\,{\\vec {x_{i}}}\\right)^{2}}{2\\sigma ^{2}}}\\right)\\end{aligned}}}\n\n\n Now, we need to look for a parameter that maximizes this likelihood function. Since the logarithmic function is strictly increasing, instead of maximizing this function, we can also maximize its logarithm and find the optimal parameter that way.[16]\n \n\n\n\n\n\n\n\nI\n(\nD\n,\n\n\n\n\u03b2\n\u2192\n\n\n\n)\n\n\n\n=\nlog\n\u2061\n\n\u220f\n\ni\n=\n1\n\n\nn\n\n\nP\nr\n(\n\ny\n\ni\n\n\n\n|\n\n\n\n\n\nx\n\ni\n\n\n\u2192\n\n\n\n\n\n;\n\n\n\n\u03b2\n\u2192\n\n\n\n,\n\u03c3\n)\n\n\n\n\n\n\n=\nlog\n\u2061\n\n\u220f\n\ni\n=\n1\n\n\nn\n\n\n\n\n1\n\n\n\n2\n\u03c0\n\n\n\u03c3\n\n\n\nexp\n\u2061\n\n(\n\n\u2212\n\n\n\n\n(\n\n\ny\n\ni\n\n\n\u2212\n\n\n\n\u03b2\n\u2192\n\n\n\n\n\u22c5\n\n\n\n\n\nx\n\ni\n\n\n\u2192\n\n\n\n\n)\n\n\n2\n\n\n\n2\n\n\u03c3\n\n2\n\n\n\n\n\n\n)\n\n\n\n\n\n\n\n=\nn\nlog\n\u2061\n\n\n1\n\n\n\n2\n\u03c0\n\n\n\u03c3\n\n\n\n\u2212\n\n\n1\n\n2\n\n\u03c3\n\n2\n\n\n\n\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\n\n(\n\n\ny\n\ni\n\n\n\u2212\n\n\n\n\u03b2\n\u2192\n\n\n\n\n\u22c5\n\n\n\n\n\nx\n\ni\n\n\n\u2192\n\n\n\n\n)\n\n\n2\n\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}I(D,{\\vec {\\beta }})&=\\log \\prod _{i=1}^{n}Pr(y_{i}|{\\vec {x_{i}}}\\,\\,;{\\vec {\\beta }},\\sigma )\\\\&=\\log \\prod _{i=1}^{n}{\\frac {1}{{\\sqrt {2\\pi }}\\sigma }}\\exp \\left(-{\\frac {\\left(y_{i}-{\\vec {\\beta }}\\,\\cdot \\,{\\vec {x_{i}}}\\right)^{2}}{2\\sigma ^{2}}}\\right)\\\\&=n\\log {\\frac {1}{{\\sqrt {2\\pi }}\\sigma }}-{\\frac {1}{2\\sigma ^{2}}}\\sum _{i=1}^{n}\\left(y_{i}-{\\vec {\\beta }}\\,\\cdot \\,{\\vec {x_{i}}}\\right)^{2}\\end{aligned}}}\n\n\n The optimal parameter is thus equal to:[16]\n \n\n\n\n\n\n\n\n\n\n\narg max\n\n\n\n\u03b2\n\u2192\n\n\n\n\n\nI\n(\nD\n,\n\n\n\n\u03b2\n\u2192\n\n\n\n)\n\n\n\n=\n\n\n\narg max\n\n\n\n\u03b2\n\u2192\n\n\n\n\n\n(\n\nn\nlog\n\u2061\n\n\n1\n\n\n\n2\n\u03c0\n\n\n\u03c3\n\n\n\n\u2212\n\n\n1\n\n2\n\n\u03c3\n\n2\n\n\n\n\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\n\n(\n\n\ny\n\ni\n\n\n\u2212\n\n\n\n\u03b2\n\u2192\n\n\n\n\n\u22c5\n\n\n\n\n\nx\n\ni\n\n\n\u2192\n\n\n\n\n)\n\n\n2\n\n\n\n)\n\n\n\n\n\n\n\n=\n\n\n\narg min\n\n\n\n\u03b2\n\u2192\n\n\n\n\n\n\u2211\n\ni\n=\n1\n\n\nn\n\n\n\n\n(\n\n\ny\n\ni\n\n\n\u2212\n\n\n\n\u03b2\n\u2192\n\n\n\n\n\u22c5\n\n\n\n\n\nx\n\ni\n\n\n\u2192\n\n\n\n\n)\n\n\n2\n\n\n\n\n\n\n\n\n=\n\n\n\narg min\n\n\n\n\u03b2\n\u2192\n\n\n\n\n\nL\n(\nD\n,\n\n\n\n\u03b2\n\u2192\n\n\n\n)\n\n\n\n\n\n\n=\n\n\n\n\n\n\u03b2\n^\n\n\n\u2192\n\n\n\n\n\n\n\n\n\n{\\displaystyle {\\begin{aligned}{\\underset {\\vec {\\beta }}{\\mbox{arg max}}}\\,I(D,{\\vec {\\beta }})&={\\underset {\\vec {\\beta }}{\\mbox{arg max}}}\\left(n\\log {\\frac {1}{{\\sqrt {2\\pi }}\\sigma }}-{\\frac {1}{2\\sigma ^{2}}}\\sum _{i=1}^{n}\\left(y_{i}-{\\vec {\\beta }}\\,\\cdot \\,{\\vec {x_{i}}}\\right)^{2}\\right)\\\\&={\\underset {\\vec {\\beta }}{\\mbox{arg min}}}\\sum _{i=1}^{n}\\left(y_{i}-{\\vec {\\beta }}\\,\\cdot \\,{\\vec {x_{i}}}\\right)^{2}\\\\&={\\underset {\\vec {\\beta }}{\\mbox{arg min}}}\\,L(D,{\\vec {\\beta }})\\\\&={\\vec {\\hat {\\beta }}}\\end{aligned}}}\n\n\n In this way, the parameter that maximizes \n\n\n\nH\n(\nD\n,\n\n\n\n\u03b2\n\u2192\n\n\n\n)\n\n\n{\\displaystyle H(D,{\\vec {\\beta }})}\n\n is the same as the one that minimizes \n\n\n\nL\n(\nD\n,\n\n\n\n\u03b2\n\u2192\n\n\n\n)\n\n\n{\\displaystyle L(D,{\\vec {\\beta }})}\n\n. This means that in linear regression, the result of the least squares method is the same as the result of the maximum likelihood estimation method.[16]\n Ridge regression[17][18][19] and other forms of penalized estimation, such as Lasso regression,[5] deliberately introduce bias into the estimation of \u03b2 in order to reduce the variability of the estimate. The resulting estimates generally have lower mean squared error than the OLS estimates, particularly when multicollinearity is present or when overfitting is a problem. They are generally used when the goal is to predict the value of the response variable y for values of the predictors x that have not yet been observed. These methods are not as commonly used when the goal is inference, since it is difficult to account for the bias.\n Least absolute deviation (LAD) regression is a robust estimation technique in that it is less sensitive to the presence of outliers than OLS (but is less efficient than OLS when no outliers are present). It is equivalent to maximum likelihood estimation under a Laplace distribution model for \u03b5.[20]\n If we assume that error terms are independent of the regressors, \n\n\n\n\n\u03b5\n\ni\n\n\n\u22a5\n\n\nx\n\n\ni\n\n\n\n\n{\\displaystyle \\varepsilon _{i}\\perp \\mathbf {x} _{i}}\n\n, then the optimal estimator is the 2-step MLE, where the first step is used to non-parametrically estimate the distribution of the error term.[21]\n Linear regression is widely used in biological, behavioral and social sciences to describe possible relationships between variables. It ranks as one of the most important tools used in these disciplines.\n A trend line represents a trend, the long-term movement in time series data after other components have been accounted for. It tells whether a particular data set (say GDP, oil prices or stock prices) have increased or decreased over the period of time. A trend line could simply be drawn by eye through a set of data points, but more properly their position and slope is calculated using statistical techniques like linear regression. Trend lines typically are straight lines, although some variations use higher degree polynomials depending on the degree of curvature desired in the line.\n Trend lines are sometimes used in business analytics to show changes in data over time. This has the advantage of being simple. Trend lines are often used to argue that a particular action or event (such as training, or an advertising campaign) caused observed changes at a point in time. This is a simple technique, and does not require a control group, experimental design, or a sophisticated analysis technique. However, it suffers from a lack of scientific validity in cases where other potential changes can affect the data.\n Early evidence relating tobacco smoking to mortality and morbidity came from observational studies employing regression analysis. In order to reduce spurious correlations when analyzing observational data, researchers usually include several variables in their regression models in addition to the variable of primary interest. For example, in a regression model in which cigarette smoking is the independent variable of primary interest and the dependent variable is lifespan measured in years, researchers might include education and income as additional independent variables, to ensure that any observed effect of smoking on lifespan is not due to those other socio-economic factors. However, it is never possible to include all possible confounding variables in an empirical analysis. For example, a hypothetical gene might increase mortality and also cause people to smoke more. For this reason, randomized controlled trials are often able to generate more compelling evidence of causal relationships than can be obtained using regression analyses of observational data. When controlled experiments are not feasible, variants of regression analysis such as instrumental variables regression may be used to attempt to estimate causal relationships from observational data.\n The capital asset pricing model uses linear regression as well as the concept of beta for analyzing and quantifying the systematic risk of an investment. This comes directly from the beta coefficient of the linear regression model that relates the return on the investment to the return on all risky assets.\n Linear regression is the predominant empirical tool in economics. For example, it is used to predict consumption spending,[24] fixed investment spending, inventory investment, purchases of a country's exports,[25] spending on imports,[25] the demand to hold liquid assets,[26] labor demand,[27] and labor supply.[27]\n Linear regression finds application in a wide range of environmental science applications such as land use,[28] infectious diseases,[29] and air pollution.[30] For example, linear regression can be used to predict the changing effects of car pollution.[31] One notable example of this application in infectious diseases is the flattening the curve strategy emphasized early in the COVID-19 pandemic, where public health officials dealt with sparse data on infected individuals and sophisticated models of disease transmission to characterize the spread of COVID-19.[32]\n Linear regression is commonly used in building science field studies to derive characteristics of building occupants. In a thermal comfort field study, building scientists usually ask occupants' thermal sensation votes, which range from -3 (feeling cold) to 0 (neutral) to +3 (feeling hot), and measure occupants' surrounding temperature data. A neutral or comfort temperature can be calculated based on a linear regression between the thermal sensation vote and indoor temperature, and setting the thermal sensation vote as zero. However, there has been a debate on the regression direction: regressing thermal sensation votes (y-axis) against indoor temperature (x-axis) or the opposite: regressing indoor temperature (y-axis) against thermal sensation votes (x-axis).[33]\n Linear regression plays an important role in the subfield of artificial intelligence known as machine learning. The linear regression algorithm is one of the fundamental supervised machine-learning algorithms due to its relative simplicity and well-known properties.[34]\n Isaac Newton is credited with inventing \"a certain technique known today as linear regression analysis\" in his work on equinoxes in 1700, and wrote down the first of the two normal equations of the ordinary least squares method.[35][36] The Least squares linear regression, as a means of finding a good rough linear fit to a set of points was performed by Legendre (1805) and Gauss (1809) for the prediction of planetary movement. Quetelet was responsible for making the procedure well-known and for using it extensively in the social sciences.[37]\n",
        "doc_number": 88
    },
    {
        "url": "https://en.wikipedia.org/wiki/Association_rule_learning",
        "content": "Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.[1] In any given transaction with a variety of items, association rules are meant to discover the rules that determine how or why certain items are connected.\n Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieli\u0144ski and Arun Swami[2] introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule \n\n\n\n{\n\no\nn\ni\no\nn\ns\n,\np\no\nt\na\nt\no\ne\ns\n\n}\n\u21d2\n{\n\nb\nu\nr\ng\ne\nr\n\n}\n\n\n{\\displaystyle \\{\\mathrm {onions,potatoes} \\}\\Rightarrow \\{\\mathrm {burger} \\}}\n\n found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional pricing or product placements.\n In addition to the above example from market basket analysis, association rules are employed today in many application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\n The association rule algorithm itself consists of various parameters that can make it difficult for those without some expertise in data mining to execute, with many rules that are arduous to understand.[3]\n Following the original definition by Agrawal, Imieli\u0144ski, Swami[2] the problem of association rule mining is defined as:\n Let \n\n\n\nI\n=\n{\n\ni\n\n1\n\n\n,\n\ni\n\n2\n\n\n,\n\u2026\n,\n\ni\n\nn\n\n\n}\n\n\n{\\displaystyle I=\\{i_{1},i_{2},\\ldots ,i_{n}\\}}\n\n be a set of n binary attributes called items.\n Let \n\n\n\nD\n=\n{\n\nt\n\n1\n\n\n,\n\nt\n\n2\n\n\n,\n\u2026\n,\n\nt\n\nm\n\n\n}\n\n\n{\\displaystyle D=\\{t_{1},t_{2},\\ldots ,t_{m}\\}}\n\n be a set of transactions called the database.\n Each transaction in D has a unique transaction ID and contains a subset of the items in I.\n A rule is defined as an implication of the form:\n In Agrawal, Imieli\u0144ski, Swami[2] a rule is defined only between a set and a single item, \n\n\n\nX\n\u21d2\n\ni\n\nj\n\n\n\n\n{\\displaystyle X\\Rightarrow i_{j}}\n\n for \n\n\n\n\ni\n\nj\n\n\n\u2208\nI\n\n\n{\\displaystyle i_{j}\\in I}\n\n.\n Every rule is composed by two different sets of items, also known as itemsets, X and Y, where X is called antecedent or left-hand-side (LHS) and Y consequent or right-hand-side (RHS). The antecedent is that item that can be found in the data while the consequent is the item found when combined with the antecedent. The statement \n\n\n\nX\n\u21d2\nY\n\n\n{\\displaystyle X\\Rightarrow Y}\n\n is often read as if X then Y, where the antecedent (X ) is the if and the consequent (Y) is the then. This simply implies that, in theory, whenever X occurs in a dataset, then Y will as well.\n Association rules are made by searching data for frequent if-then patterns and by using a certain criterion under Support and Confidence to define what the most important relationships are. Support is the evidence of how frequent an item appears in the data given, as Confidence is defined by how many times the if-then statements are found true. However, there is a third criteria that can be used, it is called Lift and it can be used to compare the expected Confidence and the actual Confidence. Lift will show how many times the if-then statement is expected to be found to be true.\n Association rules are made to calculate from itemsets, which are created by two or more items. If the rules were built from the analyzing from all the possible itemsets from the data then there would be so many rules that they wouldn\u2019t have any meaning. That is why Association rules are typically made from rules that are well represented by the data.\n There are many different data mining techniques you could use to find certain analytics and results, for example, there is Classification analysis, Clustering analysis, and Regression analysis.[4] What technique you should use depends on what you are looking for with your data. Association rules are primarily used to find analytics and a prediction of customer behavior. For Classification analysis, it would most likely be used to question, make decisions, and predict behavior.[5] Clustering analysis is primarily used when there are no assumptions made about the likely relationships within the data.[5] Regression analysis Is used when you want to predict the value of a continuous dependent from a number of independent variables.[5]\n Benefits\n There are many benefits of using Association rules like finding the pattern that helps understand the correlations and co-occurrences between data sets. A very good real-world example that uses Association rules would be medicine. Medicine uses Association rules to help diagnose patients. When diagnosing patients there are many variables to consider as many diseases will share similar symptoms. With the use of the Association rules, doctors can determine the conditional probability of an illness by comparing symptom relationships from past cases.[6]\n Downsides\n However, Association rules also lead to many different downsides such as finding the appropriate parameter and threshold settings for the mining algorithm. But there is also the downside of having a large number of discovered rules. The reason is that this does not guarantee that the rules will be found relevant, but it could also cause the algorithm to have low performance. Sometimes the implemented algorithms will contain too many variables and parameters. For someone that doesn\u2019t have a good concept of data mining, this might cause them to have trouble understanding it.[7]\n Thresholds When using Association rules, you are most likely to only use Support and Confidence. However, this means you have to satisfy a user-specified minimum support and a user-specified minimum confidence at the same time. Usually, the Association rule generation is split into two different steps that needs to be applied:\n The Support Threshold is 30%, Confidence Threshold is 50%\n The Table on the left is the original unorganized data and the table on the right is organized by the thresholds. In this case Item C is better than the thresholds for both Support and Confidence which is why it is first. Item A is second because its threshold values are spot on. Item D has met the threshold for Support but not Confidence. Item B has not met the threshold for either Support or Confidence and that is why it is last.\n To find all the frequent itemsets in a database is not an easy task since it involves going through all the data to find all possible item combinations from all possible itemsets. The set of possible itemsets is the power set over I and has size \n\n\n\n\n2\n\nn\n\n\n\u2212\n1\n\n\n{\\displaystyle 2^{n}-1}\n\n , of course this means to exclude the empty set which is not considered to be a valid itemset. However, the size of the power set will grow exponentially in the number of item n that is within the power set I. An efficient search is possible by using the downward-closure property of support[2][8] (also called anti-monotonicity[9]). This would guarantee that a frequent itemset and all its subsets are also frequent and thus will have no infrequent itemsets as a subset of a frequent itemset. Exploiting this property, efficient algorithms (e.g., Apriori[10] and Eclat[11]) can find all frequent itemsets.\n To illustrate the concepts, we use a small example from the supermarket domain. Table 2 shows a small database containing the items where, in each entry, the value 1 means the presence of the item in the corresponding transaction, and the value 0 represents the absence of an item in that transaction. The set of items is \n\n\n\nI\n=\n{\n\nm\ni\nl\nk\n,\nb\nr\ne\na\nd\n,\nb\nu\nt\nt\ne\nr\n,\nb\ne\ne\nr\n,\nd\ni\na\np\ne\nr\ns\n,\ne\ng\ng\ns\n,\nf\nr\nu\ni\nt\n\n}\n\n\n{\\displaystyle I=\\{\\mathrm {milk,bread,butter,beer,diapers,eggs,fruit} \\}}\n\n.\n An example rule for the supermarket could be \n\n\n\n{\n\nb\nu\nt\nt\ne\nr\n,\nb\nr\ne\na\nd\n\n}\n\u21d2\n{\n\nm\ni\nl\nk\n\n}\n\n\n{\\displaystyle \\{\\mathrm {butter,bread} \\}\\Rightarrow \\{\\mathrm {milk} \\}}\n\n meaning that if butter and bread are bought, customers also buy milk.\n In order to select interesting rules from the set of all possible rules, constraints on various measures of significance and interest are used. The best-known constraints are minimum thresholds on support and confidence.\n Let \n\n\n\nX\n,\nY\n\n\n{\\displaystyle X,Y}\n\n be itemsets, \n\n\n\nX\n\u21d2\nY\n\n\n{\\displaystyle X\\Rightarrow Y}\n\n an association rule and T a set of transactions of a given database.\n Note: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant,[citation needed] and datasets often contain thousands or millions of transactions.\n Support is an indication of how frequently the itemset appears in the dataset.\n In our example, it can be easier to explain support by writing \n\n\n\n\nsupport\n\n=\nP\n(\nA\n\u2229\nB\n)\n=\n\n\n\n(\n\nnumber of transactions containing\u00a0\n\nA\n\n\u00a0and\u00a0\n\nB\n)\n\n\u00a0(total number of transactions)\n\n\n\n\n{\\displaystyle {\\text{support}}=P(A\\cap B)={\\frac {({\\text{number of transactions containing }}A{\\text{ and }}B)}{\\text{ (total number of transactions)}}}}\n\n [12] where A and B are separate item sets that occur at the same time in a transaction.\n Using Table 2 as an example, the itemset \n\n\n\nX\n=\n{\n\nb\ne\ne\nr\n,\nd\ni\na\np\ne\nr\ns\n\n}\n\n\n{\\displaystyle X=\\{\\mathrm {beer,diapers} \\}}\n\n has a support of 1/5=0.2 since it occurs in 20% of all transactions (1 out of 5 transactions). The argument of support of X is a set of preconditions, and thus becomes more restrictive as it grows (instead of more inclusive).[13]\n Furthermore, the itemset \n\n\n\nY\n=\n{\n\nm\ni\nl\nk\n,\nb\nr\ne\na\nd\n,\nb\nu\nt\nt\ne\nr\n\n}\n\n\n{\\displaystyle Y=\\{\\mathrm {milk,bread,butter} \\}}\n\n has a support of 1/5=0.2 as it appears in 20% of all transactions as well.\n When using antecedents and consequents, it allows a data miner to determine the support of multiple items being bought together in comparison to the whole data set. For example, Table 2 shows that if milk is bought, then bread is bought has a support of 0.4 or 40%. This because in 2 out 5 of the transactions, milk as well as bread are bought. In smaller data sets like this example, it is harder to see a strong correlation when there are few samples, but when the data set grows larger, support can be used to find correlation between two or more products in the supermarket example.\n Minimum support thresholds are useful for determining which itemsets are preferred or interesting.\n If we set the support threshold to \u22650.4 in Table 3, then the \n\n\n\n{\n\nm\ni\nl\nk\n\n}\n\u21d2\n{\n\ne\ng\ng\ns\n\n}\n\n\n{\\displaystyle \\{\\mathrm {milk} \\}\\Rightarrow \\{\\mathrm {eggs} \\}}\n\n would be removed since it did not meet the minimum threshold of 0.4. Minimum threshold is used to remove samples where there is not a strong enough support or confidence to deem the sample as important or interesting in the dataset.\n Another way of finding interesting samples is to find the value of (support)\u00d7(confidence); this allows a data miner to see the samples where support and confidence are high enough to be highlighted in the dataset and prompt a closer look at the sample to find more information on the connection between the items.\n Support can be beneficial for finding the connection between products in comparison to the whole dataset, whereas confidence looks at the connection between one or more items and another item. Below is a table that shows the comparison and contrast between support and support \u00d7 confidence, using the information from Table 4 to derive the confidence values.\n The support of X with respect to T is defined as the proportion of transactions in the dataset which contains the itemset X. Denoting a transaction by \n\n\n\n(\ni\n,\nt\n)\n\n\n{\\displaystyle (i,t)}\n\n where i is the unique identifier of the transaction and t is its itemset, the support may be written as:\n This notation can be used when defining more complicated datasets where the items and itemsets may not be as easy as our supermarket example above. Other examples of where support can be used is in finding groups of genetic mutations that work collectively to cause a disease, investigating the number of subscribers that respond to upgrade offers, and discovering which products in a drug store are never bought together.[12]\n Confidence is the percentage of all transactions satisfying X that also satisfy Y.[14]\n With respect to T, the confidence value of an association rule, often denoted as \n\n\n\nX\n\u21d2\nY\n\n\n{\\displaystyle X\\Rightarrow Y}\n\n, is the ratio of transactions containing both X and Y to the total amount of X values present, where X is the antecedent and Y is the consequent.\n Confidence can also be interpreted as an estimate of the conditional probability \n\n\n\nP\n(\n\nE\n\nY\n\n\n\n|\n\n\nE\n\nX\n\n\n)\n\n\n{\\displaystyle P(E_{Y}|E_{X})}\n\n, the probability of finding the RHS of the rule in transactions under the condition that these transactions also contain the LHS.[13][15]\n It is commonly depicted as:\n The equation illustrates that confidence can be computed by calculating the co-occurrence of transactions X and Y within the dataset in ratio to transactions containing only X. This means that the number of transactions in both  X and Y  is divided by those just in X .\n For example, Table 2 shows the rule \n\n\n\n{\n\nb\nu\nt\nt\ne\nr\n,\nb\nr\ne\na\nd\n\n}\n\u21d2\n{\n\nm\ni\nl\nk\n\n}\n\n\n{\\displaystyle \\{\\mathrm {butter,bread} \\}\\Rightarrow \\{\\mathrm {milk} \\}}\n\n which has a confidence of \n\n\n\n\n\n\n1\n\n/\n\n5\n\n\n1\n\n/\n\n5\n\n\n\n=\n\n\n0.2\n0.2\n\n\n=\n1.0\n\n\n{\\displaystyle {\\frac {1/5}{1/5}}={\\frac {0.2}{0.2}}=1.0}\n\n in the dataset, which denotes that every time a customer buys butter and bread, they also buy milk. This particular example demonstrates the rule being correct 100% of the time for transactions containing both butter and bread. The rule \n\n\n\n{\n\nf\nr\nu\ni\nt\n\n}\n\u21d2\n{\n\ne\ng\ng\ns\n\n}\n\n\n{\\displaystyle \\{\\mathrm {fruit} \\}\\Rightarrow \\{\\mathrm {eggs} \\}}\n\n, however, has a confidence of \n\n\n\n\n\n\n2\n\n/\n\n5\n\n\n3\n\n/\n\n5\n\n\n\n=\n\n\n0.4\n0.6\n\n\n=\n0.67\n\n\n{\\displaystyle {\\frac {2/5}{3/5}}={\\frac {0.4}{0.6}}=0.67}\n\n. This suggests that eggs are bought 67% of the times that fruit is brought. Within this particular dataset, fruit is purchased a total of 3 times, with two of those times consisting of egg purchases.\n For larger datasets, a minimum threshold, or a percentage cutoff, for the confidence can be useful for determining item relationships. When applying this method to some of the data in Table 2, information that does not meet the requirements are removed. Table 4 shows association rule examples where the minimum threshold for confidence is 0.5 (50%). Any data that does not have a confidence of at least 0.5 is omitted. Generating thresholds allow for the association between items to become stronger as the data is further researched by emphasizing those that co-occur the most. The table uses the confidence information from Table 3 to implement the Support \u00d7 Confidence column, where the relationship between items via their both confidence and support, instead of just one concept, is highlighted. Ranking the rules by Support \u00d7 Confidence multiples the confidence of a particular rule to its support and is often implemented for a more in-depth understanding of the relationship between the items.\n Overall, using confidence in association rule mining is great way to bring awareness to data relations. Its greatest benefit is highlighting the relationship between particular items to one another within the set, as it compares co-occurrences of items to the total occurrence of the antecedent in the specific rule. However, confidence is not the optimal method for every concept in association rule mining. The disadvantage of using it is that it does not offer multiple difference outlooks on the associations. Unlike support, for instance, confidence does not provide the perspective of relationships between certain items in comparison to the entire dataset, so while milk and bread, for example, may occur 100% of the time for confidence, it only has a support of 0.4 (40%). This is why it is important to look at other viewpoints, such as Support \u00d7 Confidence, instead of solely relying on one concept incessantly to define the relationships.\n The lift of a rule is defined as:\n \n\n\n\n\nl\ni\nf\nt\n\n(\nX\n\u21d2\nY\n)\n=\n\n\n\n\ns\nu\np\np\n\n(\nX\n\u222a\nY\n)\n\n\n\ns\nu\np\np\n\n(\nX\n)\n\u00d7\n\ns\nu\np\np\n\n(\nY\n)\n\n\n\n\n\n{\\displaystyle \\mathrm {lift} (X\\Rightarrow Y)={\\frac {\\mathrm {supp} (X\\cup Y)}{\\mathrm {supp} (X)\\times \\mathrm {supp} (Y)}}}\n\n\n or the ratio of the observed support to that expected if X and Y were independent.\n For example, the rule \n\n\n\n{\n\nm\ni\nl\nk\n,\nb\nr\ne\na\nd\n\n}\n\u21d2\n{\n\nb\nu\nt\nt\ne\nr\n\n}\n\n\n{\\displaystyle \\{\\mathrm {milk,bread} \\}\\Rightarrow \\{\\mathrm {butter} \\}}\n\n has a lift of \n\n\n\n\n\n0.2\n\n0.4\n\u00d7\n0.4\n\n\n\n=\n1.25\n\n\n{\\displaystyle {\\frac {0.2}{0.4\\times 0.4}}=1.25}\n\n.\n If the rule had a lift of 1, it would imply that the probability of occurrence of the antecedent and that of the consequent are independent of each other. When two events are independent of each other, no rule can be drawn involving those two events.\n If the lift is > 1, that lets us know the degree to which those two occurrences are dependent on one another, and makes those rules potentially useful for predicting the consequent in future data sets.\n If the lift is < 1, that lets us know the items are substitute to each other. This means that presence of one item has negative effect on presence of other item and vice versa.\n The value of lift is that it considers both the support of the rule and the overall data set.[13]\n [rede]\n The conviction of a rule is defined as \n\n\n\n\nc\no\nn\nv\n\n(\nX\n\u21d2\nY\n)\n=\n\n\n\n1\n\u2212\n\ns\nu\np\np\n\n(\nY\n)\n\n\n1\n\u2212\n\nc\no\nn\nf\n\n(\nX\n\u21d2\nY\n)\n\n\n\n\n\n{\\displaystyle \\mathrm {conv} (X\\Rightarrow Y)={\\frac {1-\\mathrm {supp} (Y)}{1-\\mathrm {conf} (X\\Rightarrow Y)}}}\n\n.[16]\n For example, the rule \n\n\n\n{\n\nm\ni\nl\nk\n,\nb\nr\ne\na\nd\n\n}\n\u21d2\n{\n\nb\nu\nt\nt\ne\nr\n\n}\n\n\n{\\displaystyle \\{\\mathrm {milk,bread} \\}\\Rightarrow \\{\\mathrm {butter} \\}}\n\n has a conviction of \n\n\n\n\n\n\n1\n\u2212\n0.4\n\n\n1\n\u2212\n0.5\n\n\n\n=\n1.2\n\n\n{\\displaystyle {\\frac {1-0.4}{1-0.5}}=1.2}\n\n, and can be interpreted as the ratio of the expected frequency that X occurs without Y (that is to say, the frequency that the rule makes an incorrect prediction) if X and Y were independent divided by the observed frequency of incorrect predictions. In this example, the conviction value of 1.2 shows that the rule \n\n\n\n{\n\nm\ni\nl\nk\n,\nb\nr\ne\na\nd\n\n}\n\u21d2\n{\n\nb\nu\nt\nt\ne\nr\n\n}\n\n\n{\\displaystyle \\{\\mathrm {milk,bread} \\}\\Rightarrow \\{\\mathrm {butter} \\}}\n\n would be incorrect 20% more often (1.2 times as often) if the association between X and Y was purely random chance.\n In addition to confidence, other measures of interestingness for rules have been proposed. Some popular measures are:\n Several more measures are presented and compared by Tan et al.[20] and by Hahsler.[21] Looking for techniques that can model what the user has known (and using these models as interestingness measures) is currently an active research trend under the name of \"Subjective Interestingness.\"\n The concept of association rules was popularized particularly due to the 1993 article of Agrawal et al.,[2] which has acquired more than 23,790 citations according to Google Scholar, as of April 2021, and is thus one of the most cited papers in the Data Mining field. However, what is now called \"association rules\" is introduced already in the 1966 paper[22] on GUHA, a general data mining method developed by Petr H\u00e1jek et al.[23]\n An early (circa 1989) use of minimum support and confidence to find all association rules is the Feature Based Modeling framework, which found all rules with \n\n\n\n\ns\nu\np\np\n\n(\nX\n)\n\n\n{\\displaystyle \\mathrm {supp} (X)}\n\n and \n\n\n\n\nc\no\nn\nf\n\n(\nX\n\u21d2\nY\n)\n\n\n{\\displaystyle \\mathrm {conf} (X\\Rightarrow Y)}\n\n greater than user defined constraints.[24]\n One limitation of the standard approach to discovering associations is that by searching massive numbers of possible associations to look for collections of items that appear to be associated, there is a large risk of finding many spurious associations. These are collections of items that co-occur with unexpected frequency in the data, but only do so by chance. For example, suppose we are considering a collection of 10,000 items and looking for rules containing two items in the left-hand-side and 1 item in the right-hand-side. There are approximately 1,000,000,000,000 such rules. If we apply a statistical test for independence with a significance level of 0.05 it means there is only a 5% chance of accepting a rule if there is no association. If we assume there are no associations, we should nonetheless expect to find 50,000,000,000 rules. Statistically sound association discovery[25][26] controls this risk, in most cases reducing the risk of finding any spurious associations to a user-specified significance level.\n Many algorithms for generating association rules have been proposed.\n Some well-known algorithms are Apriori, Eclat and FP-Growth, but they only do half the job, since they are algorithms for mining frequent itemsets. Another step needs to be done after to generate rules from frequent itemsets found in a database.\n Apriori is given by R. Agrawal and R. Srikant in 1994 for frequent item set mining and association rule learning. It proceeds by identifying the frequent individual items in the database and extending them to larger and larger item sets as long as those item sets appear sufficiently often. The name of the algorithm is Apriori because it uses prior knowledge of frequent itemset properties.\n Overview: Apriori uses a \"bottom up\" approach, where frequent subsets are extended one item at a time (a step known as candidate generation), and groups of candidates are tested against the data. The algorithm terminates when no further successful extensions are found. Apriori uses breadth-first search and a Hash tree structure to count candidate item sets efficiently. It generates candidate item sets of length \u00a0from item sets of length . Then it prunes the candidates which have an infrequent sub pattern. According to the downward closure lemma, the candidate set contains all frequent -length item sets. After that, it scans the transaction database to determine frequent item sets among the candidates.\n Example: Assume that each row is a cancer sample with a certain combination of mutations labeled by a character in the alphabet. For example a row could have {a, c} which means it is affected by mutation 'a' and mutation 'c'. \n Now we will generate the frequent item set by counting the number of occurrences of each character. This is also known as finding the support values. Then we will prune the item set by picking a minimum support threshold. For this pass of the algorithm we will pick 3. \n Since all support values are three or above there is no pruning. The frequent item set is {a}, {b}, {c}, and {d}. After this we will repeat the process by counting pairs of mutations in the input set. \n Now we will make our minimum support value 4 so only {a, d} will remain after pruning. Now we will use the frequent item set to make combinations of triplets.  We will then repeat the process by counting occurrences of triplets of mutations in the input set. \n Since we only have one item the next set of combinations of quadruplets is empty so the algorithm will stop.\n Advantages and Limitations:\n Apriori has some limitations. Candidate generation can result in large candidate sets. For example a 10^4 frequent 1-itemset will generate a 10^7 candidate 2-itemset. The algorithm also needs to frequently scan the database, to be specific n+1 scans where n is the length of the longest pattern. Apriori is slower than the Eclat algorithm. However, Apriori performs well compared to Eclat when the dataset is large. This is because in the Eclat algorithm if the dataset is too large the tid-lists become too large for memory. FP-growth outperforms the Apriori and Eclat. This is due to the FP-growth algorithm not having candidate generation or test, using a compact data structure, and only having one database scan.[27]\n Eclat[11] (alt. ECLAT, stands for Equivalence Class Transformation) is a backtracking algorithm, which traverses the frequent itemset lattice graph in a depth-first search (DFS) fashion. Whereas the breadth-first search (BFS) traversal used in the Apriori algorithm will end up checking every subset of an itemset before checking it, DFS traversal checks larger itemsets and can save on checking the support of some of its subsets by virtue of the downward-closer property. Furthermore it will almost certainly use less memory as DFS has a lower space complexity than BFS.\n To illustrate this, let there be a frequent itemset {a, b, c}. a DFS may check the nodes in the frequent itemset lattice in the following order: {a} \u2192 {a, b} \u2192 {a, b, c}, at which point it is known that {b}, {c}, {a, c}, {b, c} all satisfy the support constraint by the downward-closure property. BFS would explore each subset of {a, b, c} before finally checking it. As the size of an itemset increases, the number of its subsets undergoes combinatorial explosion.\n It is suitable for both sequential as well as parallel execution with locality-enhancing properties.[28][29]\n FP stands for frequent pattern.[30]\n In the first pass, the algorithm counts the occurrences of items (attribute-value pairs) in the dataset of transactions, and stores these counts in a 'header table'. In the second pass, it builds the FP-tree structure by inserting transactions into a trie.\n Items in each transaction have to be sorted by descending order of their frequency in the dataset before being inserted so that the tree can be processed quickly.\nItems in each transaction that do not meet the minimum support requirement are discarded.\nIf many transactions share most frequent items, the FP-tree provides high compression close to tree root.\n Recursive processing of this compressed version of the main dataset grows frequent item sets directly, instead of generating candidate items and testing them against the entire database (as in the apriori algorithm).\n Growth begins from the bottom of the header table i.e. the item with the smallest support by finding all sorted transactions that end in that item. Call this item \n\n\n\nI\n\n\n{\\displaystyle I}\n\n.\n A new conditional tree is created which is the original FP-tree projected onto \n\n\n\nI\n\n\n{\\displaystyle I}\n\n. The supports of all nodes in the projected tree are re-counted with each node getting the sum of its children counts. Nodes (and hence subtrees) that do not meet the minimum support are pruned. Recursive growth ends when no individual items conditional on \n\n\n\nI\n\n\n{\\displaystyle I}\n\n meet the minimum support threshold. The resulting paths from root to \n\n\n\nI\n\n\n{\\displaystyle I}\n\n will be frequent itemsets. After this step, processing continues with the next least-supported header item of the original FP-tree.\n Once the recursive process has completed, all frequent item sets will have been found, and association rule creation begins.[31]\n The ASSOC procedure[32] is a GUHA method which mines for generalized association rules using fast bitstrings operations. The association rules mined by this method are more general than those output by apriori, for example \"items\" can be connected both with conjunction and disjunctions and the relation between antecedent and consequent of the rule is not restricted to setting minimum support and confidence as in apriori: an arbitrary combination of supported interest measures can be used.\n OPUS is an efficient algorithm for rule discovery that, in contrast to most alternatives, does not require either monotone or anti-monotone constraints such as minimum support.[33] Initially used to find rules for a fixed consequent[33][34] it has subsequently been extended to find rules with any item as a consequent.[35] OPUS search is the core technology in the popular Magnum Opus association discovery system.\n A famous story about association rule mining is the \"beer and diaper\" story. A purported survey of behavior of supermarket shoppers discovered that customers (presumably young men) who buy diapers tend also to buy beer. This anecdote became popular as an example of how unexpected association rules might be found from everyday data. There are varying opinions as to how much of the story is true.[36] Daniel Powers says:[36]\n In 1992, Thomas Blischok, manager of a retail consulting group at Teradata, and his staff prepared an analysis of 1.2 million market baskets from about 25 Osco Drug stores. Database queries were developed to identify affinities. The analysis \"did discover that between 5:00 and 7:00 p.m. that consumers bought beer and diapers\". Osco managers did NOT exploit the beer and diapers relationship by moving the products closer together on the shelves. Multi-Relation Association Rules (MRAR): These are association rules where each item may have several relations. These relations indicate indirect relationships between the entities. Consider the following MRAR where the first item consists of three relations live in, nearby and humid: \u201cThose who live in a place which is nearby a city with humid climate type and also are younger than 20 \n\n\n\n\n\u27f9\n\n\n\n{\\displaystyle \\implies }\n\n their health condition is good\u201d. Such association rules can be extracted from RDBMS data or semantic web data.[37]\n Contrast set learning is a form of associative learning. Contrast set learners use rules that differ meaningfully in their distribution across subsets.[38][39]\n Weighted class learning is another form of associative learning where weights may be assigned to classes to give focus to a particular issue of concern for the consumer of the data mining results.\n High-order pattern discovery facilitates the capture of high-order (polythetic) patterns or event associations that are intrinsic to complex real-world data.\n[40]\n K-optimal pattern discovery provides an alternative to the standard approach to association rule learning which requires that each pattern appear frequently in the data.\n Approximate Frequent Itemset mining is a relaxed version of Frequent Itemset mining that allows some of the items in some of the rows to be 0.[41]\n Generalized Association Rules hierarchical taxonomy (concept hierarchy)\n Quantitative Association Rules categorical and quantitative data\n Interval Data Association Rules e.g. partition the age into 5-year-increment ranged\n Sequential pattern mining  discovers subsequences that are common to more than minsup (minimum support threshold) sequences in a sequence database, where minsup is set by the user. A sequence is an ordered list of transactions.[42]\n Subspace Clustering, a specific type of clustering high-dimensional data, is in many variants also based on the downward-closure property for specific clustering models.[43]\n Warmr, shipped as part of the ACE data mining suite, allows association rule learning for first order relational rules.[44]\n",
        "doc_number": 89
    },
    {
        "url": "https://en.wikipedia.org/wiki/Clustering",
        "content": "Clustering can refer to the following:\n In computing:\n In economics:\n In graph theory:\n",
        "doc_number": 90
    },
    {
        "url": "https://en.wikipedia.org/wiki/Dimensionality_reduction",
        "content": "Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable. Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics.[1]\n Methods are commonly divided into linear and nonlinear approaches.[1] Approaches can also be divided into feature selection and feature extraction.[2] Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses.\n The process of feature selection aims to find a suitable subset of the input variables (features, or attributes) for the task at hand. The three strategies are: the filter strategy (e.g., information gain), the wrapper strategy (e.g., accuracy-guided search), and the embedded strategy (features are added or removed while building the model based on prediction errors).\n Data analysis such as regression or classification can be done in the reduced space more accurately than in the original space.[3]\n Feature projection (also called feature extraction) transforms the data from the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist.[4][5] For multidimensional data, tensor representation can be used in dimensionality reduction through multilinear subspace learning.[6]\n The main linear technique for dimensionality reduction, principal component analysis, performs a linear mapping of the data to a lower-dimensional space in such a way that the variance of the data in the low-dimensional representation is maximized. In practice, the covariance (and sometimes the correlation) matrix of the data is constructed and the eigenvectors on this matrix are computed. The eigenvectors that correspond to the largest eigenvalues (the principal components) can now be used to reconstruct a large fraction of the variance of the original data. Moreover, the first few eigenvectors can often be interpreted in terms of the large-scale physical behavior of the system, because they often contribute the vast majority of the system's energy, especially in low-dimensional systems. Still, this must be proved on a case-by-case basis as not all systems exhibit this behavior. The original space (with dimension of the number of points) has been reduced (with data loss, but hopefully retaining the most important variance) to the space spanned by a few eigenvectors. [citation needed]\n NMF decomposes a non-negative matrix to the product of two non-negative ones, which has been a promising tool in fields where only non-negative signals exist,[7][8] such as astronomy.[9][10] NMF is well known since the multiplicative update rule by Lee & Seung,[7] which has been continuously developed: the inclusion of uncertainties,[9] the consideration of missing data and parallel computation,[11] sequential construction[11] which leads to the stability and linearity of NMF,[10] as well as other updates including handling missing data in digital image processing.[12]\n With a stable component basis during construction, and a linear modeling process, sequential NMF[11] is able to preserve the flux in direct imaging of circumstellar structures in astronomy,[10] as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar discs. In comparison with PCA, NMF does not remove the mean of the matrices, which leads to physical non-negative fluxes; therefore NMF is able to preserve more information than PCA as demonstrated by Ren et al.[10]\n Principal component analysis can be employed in a nonlinear way by means of the kernel trick. The resulting technique is capable of constructing nonlinear mappings that maximize the variance in the data. The resulting technique is called kernel PCA.\n Other prominent nonlinear techniques include manifold learning techniques such as Isomap, locally linear embedding (LLE),[13] Hessian LLE, Laplacian eigenmaps, and methods based on tangent space analysis.[14] These techniques construct a low-dimensional data representation using a cost function that retains local properties of the data, and can be viewed as defining a graph-based kernel for Kernel PCA.\n More recently, techniques have been proposed that, instead of defining a fixed kernel, try to learn the kernel using semidefinite programming. The most prominent example of such a technique is maximum variance unfolding (MVU). The central idea of MVU is to exactly preserve all pairwise distances between nearest neighbors (in the inner product space) while maximizing the distances between points that are not nearest neighbors.\n An alternative approach to neighborhood preservation is through the minimization of a cost function that measures differences between distances in the input and output spaces. Important examples of such techniques include: classical multidimensional scaling, which is identical to PCA; Isomap, which uses geodesic distances in the data space; diffusion maps, which use diffusion distances in the data space; t-distributed stochastic neighbor embedding (t-SNE), which minimizes the divergence between distributions over pairs of points; and curvilinear component analysis.\n A different approach to nonlinear dimensionality reduction is through the use of autoencoders, a special kind of feedforward neural networks with a bottleneck hidden layer.[15] The training of deep encoders is typically performed using a greedy layer-wise pre-training (e.g., using a stack of restricted Boltzmann machines) that is followed by a finetuning stage based on backpropagation.\n Linear discriminant analysis (LDA) is a generalization of Fisher's linear discriminant, a method used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events.\n GDA deals with nonlinear discriminant analysis using kernel function operator. The underlying theory is close to the support-vector machines (SVM) insofar as the GDA method provides a mapping of the input vectors into high-dimensional feature space.[16][17] Similar to LDA, the objective of GDA is to find a projection for the features into a lower dimensional space by maximizing the ratio of between-class scatter to within-class scatter.\n Autoencoders can be used to learn nonlinear dimension reduction functions and codings together with an inverse function from the coding to the original representation.\n T-distributed Stochastic Neighbor Embedding (t-SNE) is a nonlinear dimensionality reduction technique useful for the visualization of high-dimensional datasets. It is not recommended for use in analysis such as clustering or outlier detection since it does not necessarily preserve densities or distances well.[18]\n Uniform manifold approximation and projection (UMAP) is a nonlinear dimensionality reduction technique. Visually, it is similar to t-SNE, but it assumes that the data is uniformly distributed on a locally connected Riemannian manifold and that the Riemannian metric is locally constant or approximately locally constant.\n For high-dimensional datasets, dimension reduction is usually performed prior to applying a k-nearest neighbors (k-NN) algorithm in order to mitigate the curse of dimensionality.[19]\n Feature extraction and dimension reduction can be combined in one step, using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA), or non-negative matrix factorization (NMF) techniques to pre-process the data, followed by clustering via k-NN on feature vectors in a reduced-dimension space. In machine learning, this process is also called low-dimensional embedding.[20]\n For high-dimensional datasets (e.g., when performing similarity search on live video streams, DNA data, or high-dimensional time series), running a fast approximate k-NN search using locality-sensitive hashing, random projection,[21] \"sketches\",[22] or other high-dimensional similarity search techniques from the VLDB conference toolbox may be the only feasible option.\n A dimensionality reduction technique that is sometimes used in neuroscience is maximally informative dimensions,[23] which finds a lower-dimensional representation of a dataset such that as much information as possible about the original data is preserved.\n",
        "doc_number": 91
    },
    {
        "url": "https://en.wikipedia.org/wiki/Anomaly_detection",
        "content": "In data analysis, anomaly detection (also referred to as outlier detection and sometimes as novelty detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behavior.[1] Such examples may arouse suspicions of being generated by a different mechanism,[2] or appear inconsistent with the remainder of that set of data.[3]\n Anomaly detection finds application in many domains including cybersecurity, medicine, machine vision, statistics, neuroscience, law enforcement and financial fraud to name only a few. Anomalies were initially searched for clear rejection or omission from the data to aid statistical analysis, for example to compute the mean or standard deviation. They were also removed to better predictions from models such as linear regression, and more recently their removal aids the performance of machine learning algorithms. However, in many applications anomalies themselves are of interest and are the observations most desirous in the entire data set, which need to be identified and separated from noise or irrelevant outliers.\n Three broad categories of anomaly detection techniques exist.[1] Supervised anomaly detection techniques require a data set that has been labeled as \"normal\" and \"abnormal\" and involves training a classifier. However, this approach is rarely used in anomaly detection due to the general unavailability of labelled data and the inherent unbalanced nature of the classes. Semi-supervised anomaly detection techniques assume that some portion of the data is labelled. This may be any combination of the normal or anomalous data, but more often than not, the techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the model. Unsupervised anomaly detection techniques assume the data is unlabelled and are by far the most commonly used due to their wider and relevant application.\n Many attempts have been made in the statistical and computer science communities to define an anomaly. The most prevalent ones include the following, and can be categorised into three groups: those that are ambiguous, those that are specific to a method with pre-defined thresholds usually chosen empirically, and those that are formally defined:\n The concept of intrusion detection, a critical component of anomaly detection, has evolved significantly over time. Initially, it was a manual process where system administrators would monitor for unusual activities, such as a vacationing user's account being accessed or unexpected printer activity. This approach was not scalable and was soon superseded by the analysis of audit logs and system logs for signs of malicious behavior.[4]\n By the late 1970s and early 1980s, the analysis of these logs was primarily used retrospectively to investigate incidents, as the volume of data made it impractical for real-time monitoring. The affordability of digital storage eventually led to audit logs being analyzed online, with specialized programs being developed to sift through the data. These programs, however, were typically run during off-peak hours due to their computational intensity.[4]\n The 1990s brought the advent of real-time intrusion detection systems capable of analyzing audit data as it was generated, allowing for immediate detection of and response to attacks. This marked a significant shift towards proactive intrusion detection.[4]\n As the field has continued to develop, the focus has shifted to creating solutions that can be efficiently implemented across large and complex network environments, adapting to the ever-growing variety of security threats and the dynamic nature of modern computing infrastructures.[4]\n Anomaly detection is applicable in a very large number and variety of domains, and is an important subarea of unsupervised machine learning. As such it has applications in cyber-security, intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, detecting ecosystem disturbances, defect detection in images using machine vision, medical diagnosis and law enforcement.[5]\n Anomaly detection was proposed for intrusion detection systems (IDS) by Dorothy Denning in 1986.[6] Anomaly detection for IDS is normally accomplished with thresholds and statistics, but can also be done with soft computing, and inductive learning.[7] Types of features proposed by 1999 included profiles of users, workstations, networks, remote hosts, groups of users, and programs based on frequencies, means, variances, covariances, and standard deviations.[8]  The counterpart of anomaly detection in intrusion detection is misuse detection.\n Anomaly detection is vital in fintech for fraud prevention.[9][10]\n Preprocessing data to remove anomalies can be an important step in data analysis, and is done for a number of reasons. Statistics such as the mean and standard deviation are more accurate after the removal of anomalies, and the visualisation of data can also be improved. In supervised learning, removing the anomalous data from the dataset often results in a statistically significant increase in accuracy.[11][12]\n Anomaly detection has become increasingly vital in video surveillance to enhance security and safety.[13][14] With the advent of deep learning technologies, methods using Convolutional Neural Networks (CNNs) and Simple Recurrent Units (SRUs) have shown significant promise in identifying unusual activities or behaviors in video data.[13] These models can process and analyze extensive video feeds in real-time, recognizing patterns that deviate from the norm, which may indicate potential security threats or safety violations.[13] An important aspect for video surveillance is the developement of scalable real-time frameworks.[15][16] Such pipelines are required for processing multiple video streams with low computational resources.\n In IT infrastructure management, anomaly detection is crucial for ensuring the smooth operation and reliability of services.[17] Techniques like the IT Infrastructure Library (ITIL) and monitoring frameworks are employed to track and manage system performance and user experience.[17] Detection anomalies can help identify and pre-empt potential performance degradations or system failures, thus maintaining productivity and business process effectiveness.[17]\n Anomaly detection is critical for the security and efficiency of Internet of Things (IoT) systems.[18] It helps in identifying system failures and security breaches in complex networks of IoT devices.[18] The methods must manage real-time data, diverse device types, and scale effectively. Garbe et al.[19] have introduced a multi-stage anomaly detection framework that improves upon traditional methods by incorporating spatial clustering, density-based clustering, and locality-sensitive hashing. This tailored approach is designed to better handle the vast and varied nature of IoT data, thereby enhancing security and operational reliability in smart infrastructure and industrial IoT systems.[19]\n Anomaly detection is crucial in the petroleum industry for monitoring critical machinery.[20] Mart\u00ed et al. used a novel segmentation algorithm to analyze sensor data for real-time anomaly detection.[20] This approach helps promptly identify and address any irregularities in sensor readings, ensuring the reliability and safety of petroleum operations.[20]\n In the oil and gas sector, anomaly detection is not just crucial for maintenance and safety, but also for environmental protection.[21] Aljameel et al. propose an advanced machine learning-based model for detecting minor leaks in oil and gas pipelines, a task traditional methods may miss.[21]\n Many anomaly detection techniques have been proposed in literature.[1][22] The performance of methods usually depend on the data sets. For example, some may be suited to detecting local outliers, while others global, and methods have little systematic advantages over another when compared across many data sets.[23][24] Almost all algorithms also require the setting of non-intuitive parameters critical for performance, and usually unknown before application. Some of the popular techniques are mentioned below and are broken down into categories:\n Also referred to as frequency-based or counting-based, the simplest non-parametric anomaly detection method is to build a histogram with the training data or a set of known normal instances, and if a test point does not fall in any of the histogram bins mark it as anomalous, or assign an anomaly score to test data based on the height of the bin it falls in.[1]  The size of bins are key to the effectiveness of this technique but must be determined by the implementer.\n \nA more sophisticated technique uses kernel functions to approximate the distribution of the normal data.  Instances in low probability areas of the distribution are then considered anomalies[25]. Histogram-based Outlier Score (HBOS) uses value histograms and assumes feature independence for fast predictions.[55]\n Dynamic networks, such as those representing financial systems, social media interactions, and transportation infrastructure, are subject to constant change, making anomaly detection within them a complex task. Unlike static graphs, dynamic networks reflect evolving relationships and states, requiring adaptive techniques for anomaly detection.\n Many of the methods discussed above only yield an anomaly score prediction, which often can be explained to users as the point being in a region of low data density (or relatively low density compared to the neighbor's densities). In explainable artificial intelligence, the users demand methods with higher explainability. Some methods allow for more detailed explanations:\n",
        "doc_number": 92
    },
    {
        "url": "https://en.wikipedia.org/wiki/Time_series_analysis",
        "content": "\n In mathematics, a time series is a series of data points indexed (or listed or graphed) in time order.  Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.\n A time series is very frequently plotted via a run chart (which is a temporal line chart). Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements.\n Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values. Generally, time series data is modelled as a stochastic process. While regression analysis is often employed in such a way as to test relationships between one or more different time series, this type of analysis is not usually called \"time series analysis\", which refers in particular to relationships between different points in time within a single series.\n Time series data have a natural temporal ordering.  This makes time series analysis distinct from cross-sectional studies, in which there is no natural ordering of the observations (e.g. explaining people's wages by reference to their respective education levels, where the individuals' data could be entered in any order).  Time series analysis is also distinct from spatial data analysis where the observations typically relate to geographical locations (e.g. accounting for house prices by the location as well as the intrinsic characteristics of the houses). A stochastic model for a time series will generally reflect the fact that observations close together in time will be more closely related than observations further apart. In addition, time series models will often make use of the natural one-way ordering of time so that values for a given period will be expressed as deriving in some way from past values, rather than from future values (see time reversibility).\n Time series analysis can be applied to real-valued, continuous data, discrete numeric data, or discrete symbolic data (i.e. sequences of characters, such as letters and words in the English language[1]).\n Methods for time series analysis may be divided into two classes: frequency-domain methods and time-domain methods. The former include spectral analysis and wavelet analysis; the latter include auto-correlation and cross-correlation analysis. In the time domain, correlation and analysis can be made in a filter-like manner using scaled correlation, thereby mitigating the need to operate in the frequency domain.\n Additionally, time series analysis techniques may be divided into parametric and non-parametric methods. The parametric approaches assume that the underlying stationary stochastic process has a certain structure which can be described using a small number of parameters (for example, using an autoregressive or moving-average model). In these approaches, the task is to estimate the parameters of the model that describes the stochastic process. By contrast, non-parametric approaches explicitly estimate the covariance or the spectrum of the process without assuming that the process has any particular structure.\n Methods of time series analysis may also be divided into linear and non-linear, and univariate and multivariate.\n A time series is one type of panel data. Panel data is the general class, a multidimensional data set, whereas a time series data set is a one-dimensional panel (as is a cross-sectional dataset).  A data set may exhibit characteristics of both panel data and time series data.  One way to tell is to ask what makes one data record unique from the other records.  If the answer is the time data field, then this is a time series data set candidate.  If determining a unique record requires a time data field and an additional identifier which is unrelated to time (e.g. student ID, stock symbol, country code), then it is panel data candidate.  If the differentiation lies on the non-time identifier, then the data set is a cross-sectional data set candidate.\n There are several types of motivation and data analysis available for time series which are appropriate for different purposes.\n In the context of statistics, econometrics, quantitative finance, seismology, meteorology, and geophysics the primary goal of time series analysis is forecasting. In the context of signal processing, control engineering and communication engineering it is used for signal detection. Other applications are in data mining, pattern recognition and machine learning, where time series analysis can be used for clustering,[2][3][4] classification,[5] query by content,[6] anomaly detection as well as forecasting.[7]\n A simple way to examine a regular time series is manually with a line chart. The datagraphic shows tuberculosis deaths in the United States,[8] along with the yearly change and the percentage change from year to year. The total number of deaths declined in every year until the mid-1980s, after which there were occasional increases, often proportionately - but not absolutely - quite large.\n A study of corporate data analysts found two challenges to exploratory time series analysis: discovering the shape of interesting patterns, and finding an explanation for these patterns.[9] Visual tools that represent time series data as heat map matrices can help overcome these challenges.\n This approach may be based on harmonic analysis and filtering of signals in the frequency domain using the Fourier transform, and spectral density estimation. Its development was significantly accelerated during World War II by mathematician Norbert Wiener, electrical engineers Rudolf E. K\u00e1lm\u00e1n, Dennis Gabor and others for filtering signals from noise and predicting signal values at a certain point in time. \n An equivalent effect may be achieved in the time domain, as in a Kalman filter; see filtering and smoothing for more techniques.\n Other related techniques include:\n Curve fitting[12][13] is the process of constructing a curve, or mathematical function, that has the best fit to a series of data points,[14] possibly subject to constraints.[15][16] Curve fitting can involve either interpolation,[17][18] where an exact fit to the data is required, or smoothing,[19][20] in which a \"smooth\" function is constructed that approximately fits the data.  A related topic is regression analysis,[21][22] which focuses more on questions of statistical inference such as how much uncertainty is present in a curve that is fit to data observed with random errors. Fitted curves can be used as an aid for data visualization,[23][24] to infer values of a function where no data are available,[25] and to summarize the relationships among two or more variables.[26] Extrapolation refers to the use of a fitted curve beyond the range of the observed data,[27] and is subject to a degree of uncertainty[28] since it may reflect the method used to construct the curve as much as it reflects the observed data.\n For processes that are expected to generally grow in magnitude one of the curves in the graphic (and many others) can be fitted by estimating their parameters.\n The construction of economic time series involves the estimation of some components for some dates by interpolation between values (\"benchmarks\") for earlier and later dates. Interpolation is estimation of an unknown quantity between two known quantities (historical data), or drawing conclusions about missing information from the available information (\"reading between the lines\").[29] Interpolation is useful where the data surrounding the missing data is available and its trend, seasonality, and longer-term cycles are known. This is often done by using a related series known for all relevant dates.[30] Alternatively polynomial interpolation or spline interpolation is used where piecewise polynomial functions are fitted in time intervals such that they fit smoothly together. A different problem which is closely related to interpolation is the approximation of a complicated function by a simple function (also called regression). The main difference between regression and interpolation is that polynomial regression gives a single polynomial that models the entire data set.  Spline interpolation, however, yield a piecewise continuous function composed of many polynomials to model the data set.\n Extrapolation is the process of estimating, beyond the original observation range, the value of a variable on the basis of its relationship with another variable. It is similar to interpolation, which produces estimates between known observations, but extrapolation is subject to greater uncertainty and a higher risk of producing meaningless results.\n In general, a function approximation problem asks us to select a function among a well-defined class that closely matches (\"approximates\") a target function in a task-specific way.\nOne can distinguish two major classes of function approximation problems: First, for known target functions, approximation theory  is the branch of numerical analysis that investigates how certain known functions (for example, special functions) can be approximated by a specific class of functions (for example, polynomials or rational functions) that often have desirable properties (inexpensive computation, continuity, integral and limit values, etc.).\n Second, the target function, call it g, may be unknown; instead of an explicit formula, only a set of points (a time series) of the form (x, g(x)) is provided.  Depending on the structure of the domain and codomain of g, several techniques for approximating g may be applicable.  For example, if g is an operation on the real numbers, techniques of interpolation, extrapolation, regression analysis, and curve fitting can be used.  If the codomain (range or target set) of g is a finite set, one is dealing with a classification problem instead. A related problem of online time series approximation[31] is to summarize the data in one-pass and construct an approximate representation that can support a variety of time series queries with bounds on worst-case error.\n To some extent, the different problems (regression, classification, fitness approximation) have received a unified treatment in statistical learning theory, where they are viewed as supervised learning problems.\n In statistics, prediction is a part of statistical inference. One particular approach to such inference is known as predictive inference, but the prediction can be undertaken within any of the several approaches to statistical inference. Indeed, one description of statistics is that it provides a means of transferring knowledge about a sample of a population to the whole population, and to other related populations, which is not necessarily the same as prediction over time. When information is transferred across time, often to specific points in time, the process is known as forecasting.\n Assigning time series pattern to a specific category, for example identify a word based on series of hand movements in sign language.\n Splitting a time-series into a sequence of segments. It is often the case that a time-series can be represented as a sequence of individual segments, each with its own characteristic properties. For example, the audio signal from a conference call can be partitioned into pieces corresponding to the times during which each person was speaking. In time-series segmentation, the goal is to identify the segment boundary points in the time-series, and to characterize the dynamical properties associated with each segment. One can approach this problem using change-point detection, or by modeling the time-series as a more sophisticated system, such as a Markov jump linear system.\n Time series data may be clustered, however special care has to be taken when considering subsequence clustering.[33][34]\nTime series clustering may be split into \n Subsequence time series clustering resulted in unstable (random) clusters induced by the feature extraction using chunking with sliding windows.[35] It was found that the cluster centers (the average of the time series in a cluster - also a time series) follow an arbitrarily shifted sine pattern (regardless of the dataset, even on realizations of a random walk). This means that the found cluster centers are non-descriptive for the dataset because the cluster centers are always nonrepresentative sine waves.\n Models for time series data can have many forms and represent different stochastic processes. When modeling variations in the level of a process, three broad classes of practical importance are the autoregressive (AR) models, the integrated (I) models, and the moving-average (MA) models. These three classes depend linearly on previous data points.[36] Combinations of these ideas produce autoregressive moving-average (ARMA) and autoregressive integrated moving-average (ARIMA) models. The autoregressive fractionally integrated moving-average (ARFIMA) model generalizes the former three. Extensions of these classes to deal with vector-valued data are available under the heading of multivariate time-series models and sometimes the preceding acronyms are extended by including an initial \"V\" for \"vector\", as in VAR for vector autoregression. An additional set of extensions of these models is available for use where the observed time-series is driven by some \"forcing\" time-series (which may not have a causal effect on the observed series): the distinction from the multivariate case is that the forcing series may be deterministic or under the experimenter's control. For these models, the acronyms are extended with a final \"X\" for \"exogenous\".\n Non-linear dependence of the level of a series on previous data points is of interest, partly because of the possibility of producing a chaotic time series. However, more importantly, empirical investigations can indicate the advantage of using predictions derived from non-linear models, over those from linear models, as for example in nonlinear autoregressive exogenous models. Further references on nonlinear time series analysis: (Kantz and Schreiber),[37] and (Abarbanel)[38]\n Among other types of non-linear time series models, there are models to represent the changes of variance over time (heteroskedasticity). These models represent autoregressive conditional heteroskedasticity (ARCH) and the collection comprises a wide variety of representation (GARCH, TARCH, EGARCH, FIGARCH, CGARCH, etc.). Here changes in variability are related to, or predicted by, recent past values of the observed series. This is in contrast to other possible representations of locally varying variability, where the variability might be modelled as being driven by a separate time-varying process, as in a doubly stochastic model.\n In recent work on model-free analyses, wavelet transform based methods (for example locally stationary wavelets and wavelet decomposed neural networks) have gained favor.[39] Multiscale (often referred to as multiresolution) techniques decompose a given time series, attempting to illustrate time dependence at multiple scales. See also Markov switching multifractal (MSMF) techniques for modeling volatility evolution.\n A hidden Markov model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states. An HMM can be considered as the simplest dynamic Bayesian network. HMM models are widely used in speech recognition, for translating a time series of spoken words into text.\n Many of these models are collected in the python package sktime.\n A number of different notations are in use for time-series analysis. A common notation specifying a time series X that is indexed by the natural numbers is written\n Another common notation is\n where T is the index set.\n There are two sets of conditions under which much of the theory is built:\n Ergodicity implies stationarity, but the converse is not necessarily the case. Stationarity is usually classified into strict stationarity and wide-sense or second-order stationarity. Both models and applications can be developed under each of these conditions, although the models in the latter case might be considered as only partly specified.\n In addition, time-series analysis can be applied where the series are seasonally stationary or non-stationary. Situations where the amplitudes of frequency components change with time can be dealt with in time-frequency analysis which makes use of a time\u2013frequency representation of a time-series or signal.[40]\n Tools for investigating time-series data include:\n Time-series metrics or features that can be used for time series classification or regression analysis:[44]\n Time series can be visualized with two categories of chart: Overlapping Charts and Separated Charts. Overlapping Charts display all-time series on the same layout while Separated Charts presents them on different layouts (but aligned for comparison purpose)[48]\n",
        "doc_number": 93
    },
    {
        "url": "https://en.wikipedia.org/wiki/Computer_audition",
        "content": "Computer audition (CA) or machine listening is the general field of study of algorithms and systems for audio interpretation by machines.[1][2] Since the notion of what it means for a machine to \"hear\" is very broad and somewhat vague, computer audition attempts to bring together several disciplines that originally dealt with specific problems or had a concrete application in mind. The engineer Paris Smaragdis, interviewed in Technology Review, talks about these systems \u2014 \"software that uses sound to locate people moving through rooms, monitor machinery for impending breakdowns, or activate traffic cameras to record accidents.\"[3]\n Inspired by models of human audition, CA deals with questions of representation, transduction, grouping, use of musical knowledge and general sound semantics for the purpose of performing intelligent operations on audio and music signals by the computer. Technically this requires a combination of methods from the fields of signal processing, auditory modelling, music perception and cognition, pattern recognition, and machine learning, as well as more traditional methods of artificial intelligence for musical knowledge representation.[4][5]\n Like computer vision versus image processing, computer audition versus audio engineering deals with understanding of audio rather than processing. It also differs from problems of speech understanding by machine since it deals with general audio signals, such as natural sounds and musical recordings.\n Applications of computer audition are widely varying, and include search for sounds, genre recognition, acoustic monitoring, music transcription, score following, audio texture, music improvisation, emotion in audio and so on.\n Computer Audition overlaps with the following disciplines:\n Since audio signals are interpreted by the human ear\u2013brain system, that complex perceptual mechanism should be simulated somehow in software for \"machine listening\". In other words, to perform on par with humans, the computer should hear and understand audio content much as humans do. Analyzing audio accurately involves several fields: electrical engineering (spectrum analysis, filtering, and audio transforms); artificial intelligence (machine learning and sound classification);[6] psychoacoustics (sound perception); cognitive sciences (neuroscience and artificial intelligence);[7] acoustics (physics of sound production); and music (harmony, rhythm, and timbre). Furthermore, audio transformations such as pitch shifting, time stretching, and sound object filtering, should be perceptually and musically meaningful. For best results, these transformations require perceptual understanding of spectral models, high-level feature extraction, and sound analysis/synthesis. Finally, structuring and coding the content of an audio file (sound and metadata) could benefit from efficient compression schemes, which discard inaudible information in the sound.[8] Computational models of music and sound perception and cognition can lead to a more meaningful representation, a more intuitive digital manipulation and generation of sound and music in musical human-machine interfaces.\n The study of CA could be roughly divided into the following sub-problems:\n Computer audition deals with audio signals that can be represented in a variety of fashions, from direct encoding of digital audio in two or more channels to symbolically represented synthesis instructions. Audio signals are usually represented in terms of analogue or digital recordings. Digital recordings are samples of acoustic waveform or parameters of audio compression algorithms. One of the unique properties of musical signals is that they often combine different types of representations, such as graphical scores and sequences of performance actions that are encoded as MIDI files.\n Since audio signals usually comprise multiple sound sources, then unlike speech signals that can be efficiently described in terms of specific models (such as source-filter model), it is hard to devise a parametric representation for general audio. Parametric audio representations usually use filter banks or sinusoidal models to capture multiple sound parameters, sometimes increasing the representation size in order to capture internal structure in the signal. Additional types of data that are relevant for computer audition are textual descriptions of audio contents, such as annotations, reviews, and visual information in the case of audio-visual recordings.\n Description of contents of general audio signals usually requires extraction of features that capture specific aspects of the audio signal. Generally speaking, one could divide the features into signal or mathematical descriptors such as energy, description of spectral shape etc., statistical characterization such as change or novelty detection, special representations that are better adapted to the nature of musical signals or the auditory system, such as logarithmic growth of sensitivity (bandwidth) in frequency or octave invariance (chroma).\n Since parametric models in audio usually require very many parameters, the features are used to summarize properties of multiple parameters in a more compact or salient representation.\n Finding specific musical structures is possible by using musical knowledge as well as supervised and unsupervised machine learning methods. Examples of this include detection of tonality according to distribution of frequencies that correspond to patterns of occurrence of notes in musical scales, distribution of note onset times for detection of beat structure, distribution of energies in different frequencies to detect musical chords and so on.\n Comparison of sounds can be done by comparison of features with or without reference to time. In some cases an overall similarity can be assessed by close values of features between two sounds. In other cases when temporal structure is important, methods of dynamic time warping need to be applied to \"correct\" for different temporal scales of acoustic events. Finding repetitions and similar sub-sequences of sonic events is important for tasks such as texture synthesis and machine improvisation.\n Since one of the basic characteristics of general audio is that it comprises multiple simultaneously sounding sources, such as multiple musical instruments, people talking, machine noises or animal vocalization, the ability to identify and separate individual sources is very desirable. Unfortunately, there are no methods that can solve this problem in a robust fashion. Existing methods of source separation rely sometimes on correlation between different audio channels in multi-channel recordings. The ability to separate sources from stereo signals requires different techniques than those usually applied in communications where multiple sensors are available. Other source separation methods rely on training or clustering of features in mono recording, such as tracking harmonically related partials for multiple pitch detection. Some methods, before explicit recognition, rely on revealing structures in data without knowing the structures (like recognizing objects in abstract pictures without attributing them meaningful labels) by finding the least complex data representations, for instance describing audio scenes as generated by a few tone patterns and their trajectories (polyphonic voices) and acoustical contours drawn by a tone (chords).[9]\n Listening to music and general audio is commonly not a task directed activity. People enjoy music for various poorly understood reasons, which are commonly referred to the emotional effect of music due to creation of expectations and their realization or violation. Animals attend to signs of danger in sounds, which could be either specific or general notions of surprising and unexpected change. Generally, this creates a situation where computer audition can not rely solely on detection of specific features or sound properties and has to come up with general methods of adapting to changing auditory environment and monitoring its structure. This consists of analysis of larger repetition and self-similarity structures in audio to detect innovation, as well as ability to predict local feature dynamics.\n Among the available data for describing music, there are textual representations, such as liner notes, reviews and criticisms that describe the audio contents in words. In other cases human reactions such as emotional judgements or psycho-physiological measurements might provide an insight into the contents and structure of audio. Computer Audition tries to find relation between these different representations in order to provide this additional understanding of the audio contents.\n",
        "doc_number": 94
    },
    {
        "url": "https://en.wikipedia.org/wiki/Image_recognition",
        "content": "Computer vision tasks include methods for acquiring, processing, analyzing, and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the form of decisions.[1][2][3][4] \"Understanding\" in this context signifies the transformation of visual images (the input to the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.\n The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. Image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, 3D point clouds from LiDaR sensors, or medical scanning devices. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems.\n Subdisciplines of computer vision include scene reconstruction, object detection, event detection, activity recognition, video tracking, object recognition, 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration.\n Computer vision is an interdisciplinary field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.[5][6][7] \"Computer vision is concerned with the automatic extraction, analysis, and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding.\"[8] As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner.[9] As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems. Machine vision refers to a systems engineering discipline, especially in the context of factory automation. In more recent times, the terms computer vision and machine vision have converged to a greater degree.[10]:\u200a13\u200a\n In the late 1960s, computer vision began at universities that were pioneering artificial intelligence. It was meant to mimic the human visual system as a stepping stone to endowing robots with intelligent behavior.[11] In 1966, it was believed that this could be achieved through an undergraduate summer project,[12] by attaching a camera to a computer and having it \"describe what it saw\".[13][14]\n What distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.[11]\n The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields.[15]\nBy the 1990s, some of the previous research topics became more active than others. Research in projective 3-D reconstructions led to better understanding of camera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry. This led to methods for sparse 3-D reconstructions of scenes from multiple images. Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques. At the same time, variations of graph cut were used to solve image segmentation. This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (see Eigenface). Toward the end of the 1990s, a significant change came about with the increased interaction between the fields of computer graphics and computer vision. This included image-based rendering, image morphing, view interpolation, panoramic image stitching and early light-field rendering.[11]\n Recent work has seen the resurgence of feature-based methods used in conjunction with machine learning techniques and complex optimization frameworks.[16][17] \nThe advancement of Deep Learning techniques has brought further life to the field of computer vision. The accuracy of deep learning algorithms on several benchmark computer vision data sets for tasks ranging from classification,[18] segmentation and optical flow has surpassed prior methods. [citation needed][19]\n Solid-state physics is another field that is closely related to computer vision. Most computer vision systems rely on image sensors, which detect electromagnetic radiation, which is typically in the form of either visible, infrared or ultraviolet light. The sensors are designed using quantum physics. The process by which light interacts with surfaces is explained using physics. Physics explains the behavior of optics which are a core part of most imaging systems. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process.[11] Also, various measurement problems in physics can be addressed using computer vision, for example, motion in fluids.\n Neurobiology has greatly influenced the development of computer vision algorithms. Over the last century, there has been an extensive study of eyes, neurons, and brain structures devoted to the processing of visual stimuli in both humans and various animals. This has led to a coarse yet convoluted description of how natural vision systems operate in order to solve certain vision-related tasks. These results have led to a sub-field within computer vision where artificial systems are designed to mimic the processing and behavior of biological systems at different levels of complexity. Also, some of the learning-based methods developed within computer vision (e.g. neural net and deep learning based image and feature analysis and classification) have their background in neurobiology.  The Neocognitron, a neural network developed in the 1970s by Kunihiko Fukushima, is an early example of computer vision taking direct inspiration from neurobiology, specifically the primary visual cortex.\n Some strands of computer vision research are closely related to the study of biological vision\u2014indeed, just as many strands of AI research are closely tied with research into human intelligence and the use of stored knowledge to interpret, integrate, and utilize visual information. The field of biological vision studies and models the physiological processes behind visual perception in humans and other animals. Computer vision, on the other hand, develops and describes the algorithms implemented in software and hardware behind artificial vision systems. An interdisciplinary exchange between biological and computer vision has proven fruitful for both fields.[21]\n Yet another field related to computer vision is signal processing. Many methods for processing one-variable signals, typically temporal signals, can be extended in a natural way to the processing of two-variable signals or multi-variable signals in computer vision. However, because of the specific nature of images, there are many methods developed within computer vision that have no counterpart in the processing of one-variable signals. Together with the multi-dimensionality of the signal, this defines a subfield in signal processing as a part of computer vision.\n Robot navigation sometimes deals with autonomous path planning or deliberation for robotic systems to navigate through an environment.[22] A detailed understanding of these environments is required to navigate through them. Information about the environment could be provided by a computer vision system, acting as a vision sensor and providing high-level information about the environment and the robot\n Besides the above-mentioned views on computer vision, many of the related research topics can also be studied from a purely mathematical point of view. For example, many methods in computer vision are based on statistics, optimization or geometry. Finally, a significant part of the field is devoted to the implementation aspect of computer vision; how existing methods can be realized in various combinations of software and hardware, or how these methods can be modified in order to gain processing speed without losing too much performance. Computer vision is also used in fashion eCommerce, inventory management, patent search, furniture, and the beauty industry.[23]\n The fields most closely related to computer vision are image processing, image analysis and machine vision. There is a significant overlap in the range of techniques and applications that these cover. This implies that the basic techniques that are used and developed in these fields are similar, something which can be interpreted as there is only one field with different names. On the other hand, it appears to be necessary for research groups, scientific journals, conferences, and companies to present or market themselves as belonging specifically to one of these fields and, hence, various characterizations which distinguish each of the fields from the others have been presented. In image processing, the input is an image and the output is an image as well, whereas in computer vision, an image or a video is taken as an input and the output could be an enhanced image, an understanding of the content of an image or even behavior of a computer system based on such understanding.\n Computer graphics produces image data from 3D models, and computer vision often produces 3D models from image data.[24] There is also a trend towards a combination of the two disciplines, e.g., as explored in augmented reality.\n The following characterizations appear relevant but should not be taken as universally accepted:\n Photogrammetry also overlaps with computer vision, e.g., stereophotogrammetry vs. computer stereo vision.\n Applications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. In many computer-vision applications, computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for:\n One of the most prominent application fields is medical computer vision, or medical image processing, characterized by the extraction of information from image data to diagnose a patient. An example of this is the detection of tumours, arteriosclerosis or other malign changes, and a variety of dental pathologies; measurements of organ dimensions, blood flow, etc. are another example. It also supports medical research by providing new information: e.g., about the structure of the brain or the quality of medical treatments. Applications of computer vision in the medical area also include enhancement of images interpreted by humans\u2014ultrasonic images or X-ray images, for example\u2014to reduce the influence of noise.\n A second application area in computer vision is in industry, sometimes called machine vision, where information is extracted for the purpose of supporting a production process. One example is quality control where details or final products are being automatically inspected in order to find defects. One of the most prevalent fields for such inspection is the Wafer industry in which every single Wafer is being measured and inspected for inaccuracies or defects to prevent a computer chip from coming to market in an unusable manner. Another example is a measurement of the position and orientation of details to be picked up by a robot arm. Machine vision is also heavily used in the agricultural processes to remove undesirable foodstuff from bulk material, a process called optical sorting.[32]\n Military applications are probably one of the largest areas of computer vision[citation needed]. The obvious examples are the detection of enemy soldiers or vehicles and missile guidance. More advanced systems for missile guidance send the missile to an area rather than a specific target, and target selection is made when the missile reaches the area based on locally acquired image data. Modern military concepts, such as \"battlefield awareness\", imply that various sensors, including image sensors, provide a rich set of information about a combat scene that can be used to support strategic decisions. In this case, automatic processing of the data is used to reduce complexity and to fuse information from multiple sensors to increase reliability.\n One of the newer application areas is autonomous vehicles, which include submersibles, land-based vehicles (small robots with wheels, cars, or trucks), aerial vehicles, and unmanned aerial vehicles (UAV). The level of autonomy ranges from fully autonomous (unmanned) vehicles to vehicles where computer-vision-based systems support a driver or a pilot in various situations. Fully autonomous vehicles typically use computer vision for navigation, e.g., for knowing where they are or mapping their environment (SLAM), for detecting obstacles. It can also be used for detecting certain task-specific events, e.g., a UAV looking for forest fires. Examples of supporting systems are obstacle warning systems in cars, cameras and LiDAR sensors in vehicles, and systems for autonomous landing of aircraft. Several car manufacturers have demonstrated systems for autonomous driving of cars. There are ample examples of military autonomous vehicles ranging from advanced missiles to UAVs for recon missions or missile guidance. Space exploration is already being made with autonomous vehicles using computer vision, e.g., NASA's Curiosity and CNSA's Yutu-2 rover.\n Materials such as rubber and silicon are being used to create sensors that allow for applications such as detecting microundulations and calibrating robotic hands. Rubber can be used in order to create a mold that can be placed over a finger, inside of this mold would be multiple strain gauges. The finger mold and sensors could then be placed on top of a small sheet of rubber containing an array of rubber pins. A user can then wear the finger mold and trace a surface. A computer can then read the data from the strain gauges and measure if one or more of the pins are being pushed upward. If a pin is being pushed upward then the computer can recognize this as an imperfection in the surface. This sort of technology is useful in order to receive accurate data on imperfections on a very large surface.[33] Another variation of this finger mold sensor are sensors that contain a camera suspended in silicon. The silicon forms a dome around the outside of the camera and embedded in the silicon are point markers that are equally spaced. These cameras can then be placed on devices such as robotic hands in order to allow the computer to receive highly accurate tactile data.[34]\n Other application areas include:\n Each of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below.\n Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions.[1][2][3][4] Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.[39]\n The classical problem in computer vision, image processing, and machine vision is that of determining whether or not the image data contains some specific object, feature, or activity. Different varieties of recognition problem are described in the literature.[40]\n Currently, the best algorithms for such tasks are based on convolutional neural networks. An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge; this is a benchmark in object classification and detection, with millions of images and 1000 object classes used in the competition.[41] Performance of convolutional neural networks on the ImageNet tests is now close to that of humans.[41] The best algorithms still struggle with objects that are small or thin, such as a small ant on the stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters (an increasingly common phenomenon with modern digital cameras). By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained classes, such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this with ease.[citation needed]\n Several specialized tasks based on recognition exist, such as:\n Several tasks relate to motion estimation, where an image sequence is processed to produce an estimate of the velocity either at each points in the image or in the 3D scene or even of the camera that produces the images. Examples of such tasks are:\n Given one or (typically) more images of a scene, or a video, scene reconstruction aims at computing a 3D model of the scene. In the simplest case, the model can be a set of 3D points. More sophisticated methods produce a complete 3D surface model. The advent of 3D imaging not requiring motion or scanning, and related processing algorithms is enabling rapid advances in this field. Grid-based 3D sensing can be used to acquire 3D images from multiple angles. Algorithms are now available to stitch multiple 3D images together into point clouds and 3D models.[24]\n Image restoration comes into the picture when the original image is degraded or damaged due to some external factors like lens wrong positioning, transmission interference, low lighting or motion blurs, etc., which is referred to as noise. When the images are degraded or damaged, the information to be extracted from them also gets damaged. Therefore, we need to recover or restore the image as it was intended to be. The aim of image restoration is the removal of noise (sensor noise, motion blur, etc.) from images. The simplest possible approach for noise removal is various types of filters, such as low-pass filters or median filters. More sophisticated methods assume a model of how the local image structures look to distinguish them from noise. By first analyzing the image data in terms of the local image structures, such as lines or edges, and then controlling the filtering based on local information from the analysis step, a better level of noise removal is usually obtained compared to the simpler approaches.\n An example in this field is inpainting.\n The organization of a computer vision system is highly application-dependent. Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on whether its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions that are found in many computer vision systems.\n Image-understanding systems (IUS) include three levels of abstraction as follows: low level includes image primitives such as edges, texture elements, or regions; intermediate level includes boundaries, surfaces and volumes; and high level includes objects, scenes, or events. Many of these requirements are entirely topics for further research.\n The representational requirements in the designing of IUS for these levels are: representation of prototypical concepts, concept organization, spatial knowledge, temporal knowledge, scaling, and description by comparison and differentiation.\n While inference refers to the process of deriving new, not explicitly represented facts from currently known facts, control refers to the process that selects which of the many inference, search, and matching techniques should be applied at a particular stage of processing. Inference and control requirements for IUS are: search and hypothesis activation, matching and hypothesis testing, generation and use of expectations, change and focus of attention, certainty and strength of belief, inference and goal satisfaction.[48]\n There are many kinds of computer vision systems; however, all of them contain these basic elements: a power source, at least one image acquisition device (camera, ccd, etc.), a processor, and control and communication cables or some kind of wireless interconnection mechanism. In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories, such as camera supports, cables, and connectors.\n Most computer vision systems use visible-light cameras passively viewing a scene at frame rates of at most 60 frames per second (usually far slower).\n A few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such as structured-light 3D scanners, thermographic cameras, hyperspectral imagers, radar imaging, lidar scanners, magnetic resonance images, side-scan sonar, synthetic aperture sonar, etc. Such hardware captures \"images\" that are then processed often using the same computer vision algorithms used to process visible-light images.\n While traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realized.[49]\n Egocentric vision systems are composed of a wearable camera that automatically take pictures from a first-person perspective.\n As of 2016, vision processing units are emerging as a new class of processors to complement CPUs and graphics processing units (GPUs) in this role.[50]\n",
        "doc_number": 95
    },
    {
        "url": "https://en.wikipedia.org/wiki/Object_detection",
        "content": "Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class (such as humans, buildings, or cars) in digital images and videos.[1] Well-researched domains of object detection include face detection and pedestrian detection. Object detection has applications in many areas of computer vision, including image retrieval and video surveillance.\n It is widely used in computer vision tasks such as image annotation,[2] vehicle counting,[3] activity recognition,[4] face detection, face recognition, video object co-segmentation. It is also used in tracking objects, for example tracking a ball during a football match, tracking movement of a cricket bat, or tracking a person in a video.\n Often, the test images are sampled from a different data distribution, making the object detection task significantly more difficult.[5] To address the challenges caused by the domain gap between training and test data, many unsupervised domain adaptation approaches have been proposed.[5][6][7][8][9] A simple and straightforward solution for reducing the domain gap is to apply an image-to-image translation approach, such as cycle-GAN.[10] Among other uses, cross-domain object detection is applied in autonomous driving, where models can be trained on a vast amount of video game scenes, since the labels can be generated without manual labor.\n Every object class has its own special features that help in classifying the class \u2013 for example all circles are round.\nObject class detection uses these special features. For example, when looking for circles, objects that are at a particular distance from a point (i.e. the center) are sought. Similarly, when looking for squares, objects that are perpendicular at corners and have equal side lengths are needed.  A similar approach is used for face identification where eyes, nose, and lips can be found and features like skin color and distance between eyes can be found.\n For object localization, true positive is often measured by the thresholded intersection over union. For example, if there is a traffic sign in the image, with a bounding box drawn by a human (\"ground truth label\"), then a neural network has detected the traffic sign (a true positive) at 0.5 threshold iff it has drawn a bounding box whose IoU with the ground truth is above 0.5. Otherwise, the bounding box is a false positive.\n If there is only a single ground truth bounding box, but multiple predictions, then the IoU of each prediction is calculated. The prediction with the highest IoU is a true positive if it is above threshold, else it is a false positive. All other predicted bounding boxes are false positives. If there is no prediction with an IoU above the threshold, then the ground truth label has a false negative.\n For simultaneous object localization and classification, a true positive is one where the class label is correct, and the bounding box has an IoU exceeding the threshold.\n Simultaneous object localization and classification is benchmarked by the mean average precision (mAP). The average precision (AP) of the network for a class of objects is the area under the precision-recall curve as the IoU threshold is varied. The mAP is the average of AP over all classes.\n Methods for object detection generally fall into either neural network-based or non-neural approaches. For non-neural approaches, it becomes necessary to first define features using one of the methods below, then using a technique such as support vector machine (SVM) to do the classification. On the other hand, neural techniques are able to do end-to-end object detection without specifically defining features, and are typically based on convolutional neural networks (CNN).\n",
        "doc_number": 96
    },
    {
        "url": "https://en.wikipedia.org/wiki/Semantic_segmentation",
        "content": "\n In digital image processing and computer vision, image segmentation is the process of partitioning a digital image into multiple image segments, also known as image regions or image objects (sets of pixels). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze.[1][2] Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.\n The result of image segmentation is a set of segments that collectively cover the entire image, or a set of contours extracted from the image (see edge detection). Each of the pixels in a region are similar with respect to some characteristic or computed property,[3] such as color, intensity, or texture. Adjacent regions are significantly different with respect to the same characteristic(s).[1] When applied to a stack of images, typical in medical imaging, the resulting contours after image segmentation can be used to create 3D reconstructions with the help of geometry reconstruction algorithms like marching cubes.[4]\n Some of the practical applications of image segmentation are:\n Several general-purpose algorithms and techniques have been developed for image segmentation. To be useful, these techniques must typically be combined with a domain's specific knowledge in order to effectively solve the domain's segmentation problems.\n There are two classes of segmentation techniques.\n The simplest method of image segmentation is called the thresholding method. This method is based on a clip-level (or a threshold value) to turn a gray-scale image into a binary image.\n The key of this method is to select the threshold value (or values when multiple-levels are selected). Several popular methods are used in industry including the maximum entropy method, balanced histogram thresholding, Otsu's method (maximum variance), and k-means clustering.\n Recently, methods have been developed for thresholding computed tomography (CT) images. The key idea is that, unlike Otsu's method, the thresholds are derived from the radiographs instead of the (reconstructed) image.[21][22]\n New methods suggested the usage of multi-dimensional fuzzy rule-based non-linear thresholds. In these works decision over each pixel's membership to a segment is based on multi-dimensional rules derived from fuzzy logic and evolutionary algorithms based on image lighting environment and application.[23]\n The K-means algorithm is an iterative technique that is used to partition an image into K clusters.[24] The basic algorithm is\n In this case, distance is the squared or absolute difference between a pixel and a cluster center. The difference is typically based on pixel color, intensity, texture, and location, or a weighted combination of these factors. K can be selected manually, randomly, or by a heuristic. This algorithm is guaranteed to converge, but it may not return the optimal solution. The quality of the solution depends on the initial set of clusters and the value of K.\n The Mean Shift algorithm is a technique that is used to partition an image into an unknown apriori number of clusters. This has the advantage of not having to start with an initial guess of such parameter which makes it a better general solution for more diverse cases.\n Motion based segmentation is a technique that relies on motion in the image to perform segmentation.\n The idea is simple: look at the differences between a pair of images. Assuming the object of interest is moving, the difference will be exactly that object.\n Improving on this idea, Kenney et al. proposed interactive segmentation [2]. They use a robot to poke objects in order to generate the motion signal necessary for motion-based segmentation.\n Interactive segmentation follows the interactive perception framework proposed by Dov Katz [3] and Oliver Brock [4].\n Another technique that is based on motion is rigid motion segmentation.\n Compression based methods postulate that the optimal segmentation is the one that minimizes, over all possible segmentations, the coding length of the data.[25][26] The connection between these two concepts is that segmentation tries to find patterns in an image and any regularity in the image can be used to compress it. The method describes each segment by its texture and boundary shape. Each of these components is modeled by a probability distribution function and its coding length is computed as follows:\n For any given segmentation of an image, this scheme yields the number of bits required to encode that image based on the given segmentation. Thus, among all possible segmentations of an image, the goal is to find the segmentation which produces the shortest coding length. This can be achieved by a simple agglomerative clustering method. The distortion in the lossy compression determines the coarseness of the segmentation and its optimal value may differ for each image. This parameter can be estimated heuristically from the contrast of textures in an image. For example, when the textures in an image are similar, such as in camouflage images, stronger sensitivity and thus lower quantization is required.\n Histogram-based methods are very efficient compared to other image segmentation methods because they typically require only one pass through the pixels. In this technique, a histogram is computed from all of the pixels in the image, and the peaks and valleys in the histogram are used to locate the clusters in the image.[1] Color or intensity can be used as the measure.\n A refinement of this technique is to recursively apply the histogram-seeking method to clusters in the image in order to divide them into smaller clusters. This operation is repeated with smaller and smaller clusters until no more clusters are formed.[1][27]\n One disadvantage of the histogram-seeking method is that it may be difficult to identify significant peaks and valleys in the image.\n Histogram-based approaches can also be quickly adapted to apply to multiple frames, while maintaining their single pass efficiency. The histogram can be done in multiple fashions when multiple frames are considered. The same approach that is taken with one frame can be applied to multiple, and after the results are merged, peaks and valleys that were previously difficult to identify are more likely to be distinguishable. The histogram can also be applied on a per-pixel basis where the resulting information is used to determine the most frequent color for the pixel location. This approach segments based on active objects and a static environment, resulting in a different type of segmentation useful in video tracking.\n Edge detection is a well-developed field on its own within image processing. Region boundaries and edges are closely related,\nsince there is often a sharp adjustment in intensity at the region boundaries. Edge detection techniques have therefore been used as the base of another segmentation technique.\n The edges identified by edge detection are often disconnected. To segment an object from an image however, one needs closed region boundaries. The desired edges are the boundaries between such objects or spatial-taxons.[28][29]\n Spatial-taxons[30] are information granules,[31] consisting of a crisp pixel region, stationed at abstraction levels within a hierarchical nested scene architecture. They are similar to the Gestalt psychological designation of figure-ground, but are extended to include foreground, object groups, objects and salient object parts. Edge detection methods can be applied to the spatial-taxon region, in the same manner they would be applied to a silhouette. This method is particularly useful when the disconnected edge is part of an illusory contour[32][33]\n Segmentation methods can also be applied to edges obtained from edge detectors. Lindeberg and Li[34] developed an integrated method that segments edges into straight and curved edge segments for parts-based object recognition, based on a minimum description length (MDL) criterion that was optimized by a split-and-merge-like method with candidate breakpoints obtained from complementary junction cues to obtain more likely points at which to consider partitions into different segments.\n The detection of isolated points in an image is a fundamental part of image segmentation. This process primarily depends on the second derivative, indicating the use of the Laplacian operator. The Laplacian of a function \n\n\n\nf\n(\nx\n,\ny\n)\n\n\n{\\displaystyle f(x,y)}\n\n is given by:\n The Laplacian operator is employed such that the partial derivatives are derived from a specific equation. The second partial derivative of \n\n\n\nf\n(\nx\n,\ny\n)\n\n\n{\\displaystyle f(x,y)}\n\n with respect to \n\n\n\nx\n\n\n{\\displaystyle x}\n\n and \n\n\n\ny\n\n\n{\\displaystyle y}\n\n are given by:\n These partial derivatives are then used to compute the Laplacian as:\n This mathematical expression can be implemented by convolving with an appropriate mask. If we extend this equation to three dimensions (x,y,z), the intensity at each pixel location around a central pixel at (x, y, z) is replaced by their corresponding values. This equation becomes particularly useful when we assume that all pixels have unit spacing along each axis.\n A sphere mask has been developed for use with three-dimensional datasets. The sphere mask is designed to use only integer arithmetic during calculations, thereby eliminating the need for floating-point hardware or software.\n When applying these concepts to actual images represented as arrays of numbers, we need to consider what happens when we reach an edge or border region. The function \n\n\n\ng\n(\nx\n,\ny\n)\n\n\n{\\displaystyle g(x,y)}\n\n is defined as:\n This above equation is used to determine whether a point in the image is an isolated point based on the response magnitude \n\n\n\n\n|\n\nR\n(\nx\n,\ny\n)\n\n|\n\n\n\n{\\displaystyle |R(x,y)|}\n\n and a threshold value \n\n\n\nT\n\n\n{\\displaystyle T}\n\n. If the response magnitude is greater than or equal to the threshold, the function returns 1, indicating the presence of an isolated point; otherwise, it returns 0. This helps in the effective detection and segmentation of isolated points in the image.[35]\n The detection of isolated points has significant applications in various fields, including X-ray image processing. For instance, an original X-ray image of a turbine blade can be examined pixel-by-pixel to detect porosity in the upper-right quadrant of the blade. The result of applying an edge detector\u2019s response to this X-ray image can be approximated. This demonstrates the segmentation of isolated points in an image with the aid of single-pixel probes.[36]\n This method is a combination of three characteristics of the image: partition of the image based on histogram analysis is checked by high compactness of the clusters (objects), and high gradients of their borders. For that purpose two spaces have to be introduced: one space is the one-dimensional histogram of brightness H =\u00a0H(B); the second space is the dual 3-dimensional space of the original image itself B =\u00a0B(x,\u00a0y). The first space allows to measure how compactly the brightness of the image is distributed by calculating a minimal clustering kmin. Threshold brightness T corresponding to kmin defines the binary (black-and-white) image\u00a0\u2013 bitmap b =\u00a0\u03c6(x,\u00a0y), where \u03c6(x,\u00a0y) =\u00a00, if B(x,\u00a0y)\u00a0<\u00a0T, and \u03c6(x,\u00a0y) =\u00a01, if B(x,\u00a0y)\u00a0\u2265\u00a0T. The bitmap b is an object in dual space. On that bitmap a measure has to be defined reflecting how compact distributed black (or white) pixels are. So, the goal is to find objects with good borders. For all T the measure MDC =\u00a0G/(k\u00a0\u00d7\u00a0L) has to be calculated (where k is difference in brightness between the object and the background, L is length of all borders, and G is mean gradient on the borders). Maximum of MDC defines the segmentation.[37]\n Region-growing methods rely mainly on the assumption that the neighboring pixels within one region have similar values. The common procedure is to compare one pixel with its neighbors. If a similarity criterion is satisfied, the pixel can be set to belong to the same cluster as one or more of its neighbors. The selection of the similarity criterion is significant and the results are influenced by noise in all instances.\n The method of Statistical Region Merging[38] (SRM) starts by building the graph of pixels using 4-connectedness with edges weighted by the absolute value of the intensity difference. Initially each pixel forms a single pixel region. SRM then sorts those edges in a priority queue and decides whether or not to merge the current regions belonging to the edge pixels using a statistical predicate.\n One region-growing method is the seeded region growing method. This method takes a set of seeds as input along with the image. The seeds mark each of the objects to be segmented. The regions are iteratively grown by comparison of all unallocated neighboring pixels to the regions. The difference between a pixel's intensity value and the region's mean, \n\n\n\n\u03b4\n\n\n{\\displaystyle \\delta }\n\n, is used as a measure of similarity. The pixel with the smallest difference measured in this way is assigned to the respective region. This process continues until all pixels are assigned to a region. Because seeded region growing requires seeds as additional input, the segmentation results are dependent on the choice of seeds, and noise in the image can cause the seeds to be poorly placed.\n Another region-growing method is the unseeded region growing method. It is a modified algorithm that does not require explicit seeds. It starts with a single region \n\n\n\n\nA\n\n1\n\n\n\n\n{\\displaystyle A_{1}}\n\n\u2014the pixel chosen here does not markedly influence the final segmentation. At each iteration it considers the neighboring pixels in the same way as seeded region growing. It differs from seeded region growing in that if the minimum \n\n\n\n\u03b4\n\n\n{\\displaystyle \\delta }\n\n is less than a predefined threshold \n\n\n\nT\n\n\n{\\displaystyle T}\n\n then it is added to the respective region \n\n\n\n\nA\n\nj\n\n\n\n\n{\\displaystyle A_{j}}\n\n. If not, then the pixel is considered different from all current regions \n\n\n\n\nA\n\ni\n\n\n\n\n{\\displaystyle A_{i}}\n\n and a new region \n\n\n\n\nA\n\nn\n+\n1\n\n\n\n\n{\\displaystyle A_{n+1}}\n\n is created with this pixel.\n One variant of this technique, proposed by Haralick and Shapiro (1985),[1] is based on pixel intensities. The mean and scatter of the region and the intensity of the candidate pixel are used to compute a test statistic. If the test statistic is sufficiently small, the pixel is added to the region, and the region's mean and scatter are recomputed. Otherwise, the pixel is rejected, and is used to form a new region.\n A special region-growing method is called \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n-connected segmentation (see also lambda-connectedness). It is based on pixel intensities and neighborhood-linking paths. A degree of connectivity (connectedness) is calculated based on a path that is formed by pixels. For a certain value of \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n, two pixels are called \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n-connected if there is a path linking those two pixels and the connectedness of this path is at least \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n. \n\n\n\n\u03bb\n\n\n{\\displaystyle \\lambda }\n\n-connectedness is an equivalence relation.[39]\n Split-and-merge segmentation is based on a quadtree partition of an image. It is sometimes called quadtree segmentation.\n This method starts at the root of the tree that represents the whole image. If it is found non-uniform (not homogeneous), then it is split into four child squares (the splitting process), and so on. If, in contrast, four child squares are homogeneous, they are merged as several connected components (the merging process). The node in the tree is a segmented node. This process continues recursively until no further splits or merges are possible.[40][41] When a special data structure is involved in the implementation of the algorithm of the method, its time complexity can reach \n\n\n\nO\n(\nn\nlog\n\u2061\nn\n)\n\n\n{\\displaystyle O(n\\log n)}\n\n, an optimal algorithm of the method.[42]\n Using a partial differential equation (PDE)-based method and solving the PDE equation by a numerical scheme, one can segment the image.[43] Curve propagation is a popular technique in this category, with numerous applications to object extraction, object tracking, stereo reconstruction, etc. The central idea is to evolve an initial curve towards the lowest potential of a cost function, where its definition reflects the task to be addressed. As for most inverse problems, the minimization of the cost functional is non-trivial and imposes certain smoothness constraints on the solution, which in the present case can be expressed as geometrical constraints on the evolving curve.\n Lagrangian techniques are based on parameterizing the contour according to some sampling strategy and then evolving each element according to image and internal terms. Such techniques are fast and efficient, however the original \"purely parametric\" formulation (due to Kass, Witkin and Terzopoulos in 1987 and known as \"snakes\"), is generally criticized for its limitations regarding the choice of sampling strategy, the internal geometric properties of the curve, topology changes (curve splitting and merging), addressing problems in higher dimensions, etc.. Nowadays, efficient \"discretized\" formulations have been developed to address these limitations while maintaining high efficiency. In both cases, energy minimization is generally conducted using a steepest-gradient descent, whereby derivatives are computed using, e.g., finite differences.\n The level-set method was initially proposed to track moving interfaces by Dervieux and Thomasset[44][45] in 1979 and 1981 and was later reinvented by Osher and Sethian in 1988.[46] This has spread across various imaging domains in the late 1990s. It can be used to efficiently address the problem of curve/surface/etc. propagation in an implicit manner. The central idea is to represent the evolving contour using a signed function whose zero corresponds to the actual contour. Then, according to the motion equation of the contour, one can easily derive a similar flow for the implicit surface that when applied to the zero level will reflect the propagation of the contour. The level-set method affords numerous advantages: it is implicit, is parameter-free, provides a direct way to estimate the geometric properties of the evolving structure, allows for change of topology, and is intrinsic. It can be used to define an optimization framework, as proposed by Zhao, Merriman and Osher in 1996. One can conclude that it is a very convenient framework for addressing numerous applications of computer vision and medical image analysis.[47] Research into various level-set data structures has led to very efficient implementations of this method.\n The fast marching method has been used in image segmentation,[48] and this model has been improved (permitting both positive and negative propagation speeds) in an approach called the generalized fast marching method.[49]\n The goal of variational methods is to find a segmentation\nwhich is optimal with respect to a specific energy functional. The functionals consist of a data fitting term and a regularizing terms. A classical representative is the Potts model defined for an image \n\n\n\nf\n\n\n{\\displaystyle f}\n\n by\n A minimizer \n\n\n\n\nu\n\n\u2217\n\n\n\n\n{\\displaystyle u^{*}}\n\n is a piecewise constant image which has an optimal tradeoff between the squared L2 distance to the given image \n\n\n\nf\n\n\n{\\displaystyle f}\n\n and the total length of its jump set. The jump set of \n\n\n\n\nu\n\n\u2217\n\n\n\n\n{\\displaystyle u^{*}}\n\n defines a segmentation. The relative weight of the energies is tuned by the parameter \n\n\n\n\u03b3\n>\n0\n\n\n{\\displaystyle \\gamma >0}\n\n. The binary variant of the Potts model, i.e., if the range of \n\n\n\nu\n\n\n{\\displaystyle u}\n\n is restricted to two values, is often called Chan-Vese model.[50] An important generalization is the Mumford-Shah model[51] given by\n The functional value is the sum of the total length of the segmentation curve \n\n\n\nK\n\n\n{\\displaystyle K}\n\n, the smoothness of the approximation \n\n\n\nu\n\n\n{\\displaystyle u}\n\n, and its distance to the original image \n\n\n\nf\n\n\n{\\displaystyle f}\n\n. The weight of the smoothness penalty is adjusted by \n\n\n\n\u03bc\n>\n0\n\n\n{\\displaystyle \\mu >0}\n\n. The Potts model is often called piecewise constant Mumford-Shah model as it can be seen as the degenerate case \n\n\n\n\u03bc\n\u2192\n\u221e\n\n\n{\\displaystyle \\mu \\to \\infty }\n\n. The optimization problems are known to be NP-hard in general but near-minimizing strategies work well in practice. Classical algorithms are graduated non-convexity and Ambrosio-Tortorelli approximation.\n Graph partitioning methods are an effective tools for image segmentation since they model the impact of pixel neighborhoods on a given cluster of pixels or pixel, under the assumption of homogeneity in images. In these methods, the image is modeled as a weighted, undirected graph. Usually a pixel or a group of pixels are associated with nodes and edge weights define the (dis)similarity between the neighborhood pixels. The graph (image) is then partitioned according to a criterion designed to model \"good\" clusters. Each partition of the nodes (pixels) output from these algorithms are considered an object segment in the image; see Segmentation-based object categorization. Some popular algorithms of this category are normalized cuts,[52] random walker,[53] minimum cut,[54] isoperimetric partitioning,[55] minimum spanning tree-based segmentation,[56] and segmentation-based object categorization.\n The application of Markov random fields (MRF) for images was suggested in early 1984 by Geman and Geman.[57] Their strong mathematical foundation and ability to provide a global optimum even when defined on local features proved to be the foundation for novel research in the domain of image analysis, de-noising and segmentation. MRFs are completely characterized by their prior probability distributions, marginal probability distributions, cliques, smoothing constraint as well as criterion for updating values. The criterion for image segmentation using MRFs is restated as finding the labelling scheme which has maximum probability for a given set of features. The broad categories of image segmentation using MRFs are supervised and unsupervised segmentation.\n In terms of image segmentation, the function that MRFs seek to maximize is the probability of identifying a labelling scheme given a particular set of features are detected in the image. This is a restatement of the maximum a posteriori estimation method.\n The generic algorithm for image segmentation using MAP is given below:\n Each optimization algorithm is an adaptation of models from a variety of fields and they are set apart by their unique cost functions. The common trait of cost functions is to penalize change in pixel value as well as difference in pixel label when compared to labels of neighboring pixels.\n The iterated conditional modes (ICM) algorithm tries to reconstruct the ideal labeling scheme by changing the values of each pixel over each iteration and evaluating the energy of the new labeling scheme using the cost function given below,\n where \u03b1 is the penalty for change in pixel label and \u03b2 is the penalty for difference in label between\nneighboring pixels and chosen pixel. Here \n\n\n\nN\n(\ni\n)\n\n\n{\\displaystyle N(i)}\n\n is neighborhood of pixel i and \u03b4 is the Kronecker delta function. A major issue with ICM is that, similar to gradient descent, it has a tendency to rest over local maxima and thus not obtain a globally optimal labeling scheme.\n Derived as an analogue of annealing in metallurgy, simulated annealing (SA) uses change in pixel label over iterations and estimates the difference in energy of each newly formed graph to the initial data. If the newly formed graph is more profitable, in terms of low energy cost, given by:\n the algorithm selects the newly formed graph. Simulated annealing requires the input of temperature schedules which directly affects the speed of convergence of the system, as well as energy threshold for minimization to occur.\n A range of other methods exist for solving simple as well as higher order MRFs. They include Maximization of Posterior Marginal, Multi-scale MAP estimation,[58] Multiple Resolution segmentation[59] and more. Apart from likelihood estimates, graph-cut using maximum flow[60] and other highly constrained graph based methods[61][62] exist for solving MRFs.\n The expectation\u2013maximization algorithm is utilized to iteratively estimate the a posterior probabilities and distributions of labeling when no training data is available and no estimate of segmentation model can be formed. A general approach is to use histograms to represent the features of an image and proceed as outlined briefly in this three-step algorithm:\n 1. A random estimate of the model parameters is utilized.\n 2. E step: Estimate class statistics based on the random segmentation model defined. Using these, compute the conditional probability of belonging to a label given the feature set is calculated using naive Bayes' theorem.\n Here \n\n\n\n\u03bb\n\u2208\n\u039b\n\n\n{\\displaystyle \\lambda \\in \\Lambda }\n\n, the set of all possible labels.\n 3. M step: The established relevance of a given feature set to a labeling scheme is now used to compute the a priori estimate of a given label in the second part of the algorithm. Since the actual number of total labels is unknown (from a training data set), a hidden estimate of the number of labels given by the user is utilized in computations.\n where \n\n\n\n\u03a9\n\n\n{\\displaystyle \\Omega }\n\n is the set of all possible features.\n The watershed transformation considers the gradient magnitude of an image as a topographic surface. Pixels having the highest gradient magnitude intensities (GMIs) correspond to watershed lines, which represent the region boundaries. Water placed on any pixel enclosed by a common watershed line flows downhill to a common local intensity minimum (LIM). Pixels draining to a common minimum form a catch basin, which represents a segment.\n The central assumption of model-based approaches is that the structures of interest have a tendency towards a particular shape. Therefore, one can seek a probabilistic model that characterizes the shape and its variation. When segmenting an image, constraints can be imposed using this model as a prior.[63] Such a task may involve (i) registration of the training examples to a common pose, (ii) probabilistic representation of the variation of the registered samples, and (iii) statistical inference between the model and the image. Other important methods in the literature for model-based segmentation include active shape models and active appearance models.\n Image segmentations are computed at multiple scales in scale space and sometimes propagated from coarse to fine scales; see scale-space segmentation.\n Segmentation criteria can be arbitrarily complex and may take into account global as well as local criteria. A common requirement is that each region must be connected in some sense.\n Witkin's seminal work[64][65] in scale space included the notion that a one-dimensional signal could be unambiguously segmented into regions, with one scale parameter controlling the scale of segmentation.\n A key observation is that the zero-crossings of the second derivatives (minima and maxima of the first derivative or slope) of multi-scale-smoothed versions of a signal form a nesting tree, which defines hierarchical relations between segments at different scales. Specifically, slope extrema at coarse scales can be traced back to corresponding features at fine scales. When a slope maximum and slope minimum annihilate each other at a larger scale, the three segments that they separated merge into one segment, thus defining the hierarchy of segments.\n There have been numerous research works in this area, out of which a few have now reached a state where they can be applied either with interactive manual intervention (usually with application to medical imaging) or fully automatically. The following is a brief overview of some of the main research ideas that current approaches are based upon.\n The nesting structure that Witkin described is, however, specific for one-dimensional signals and does not trivially transfer to higher-dimensional images. Nevertheless, this general idea has inspired several other authors to investigate coarse-to-fine schemes for image segmentation. Koenderink[66] proposed to study how iso-intensity contours evolve over scales and this approach was investigated in more detail by Lifshitz and Pizer.[67] Unfortunately, however, the intensity of image features changes over scales, which implies that it is hard to trace coarse-scale image features to finer scales using iso-intensity information.\n Lindeberg[68][69] studied the problem of linking local extrema and saddle points over scales, and proposed an image representation called the scale-space primal sketch which makes explicit the relations between structures at different scales, and also makes explicit which image features are stable over large ranges of scale including locally appropriate scales for those. Bergholm proposed to detect edges at coarse scales in scale-space and then trace them back to finer scales with manual choice of both the coarse detection scale and the fine localization scale.\n Gauch and Pizer[70] studied the complementary problem of ridges and valleys at multiple scales and developed a tool for interactive image segmentation based on multi-scale watersheds. The use of multi-scale watershed with application to the gradient map has also been investigated by Olsen and Nielsen[71] and been carried over to clinical use by Dam.[72] Vincken et al.[73] proposed a hyperstack for defining probabilistic relations between image structures at different scales. The use of stable image structures over scales has been furthered by Ahuja[74][75] and his co-workers into a fully automated system. A fully automatic brain segmentation algorithm based on closely related ideas of multi-scale watersheds has been presented by Undeman and Lindeberg[76] and been extensively tested in brain databases.\n These ideas for multi-scale image segmentation by linking image structures over scales have also been picked up by Florack and Kuijper.[77] Bijaoui and Ru\u00e9[78] associate structures detected in scale-space above a minimum noise threshold into an object tree which spans multiple scales and corresponds to a kind of feature in the original signal. Extracted features are accurately reconstructed using an iterative conjugate gradient matrix method.\n In one kind of segmentation, the user outlines the region of interest with the mouse clicks and algorithms are applied so that the path that best fits the edge of the image is shown.\n Techniques like SIOX, Livewire, Intelligent Scissors or IT-SNAPS are used in this kind of segmentation. In an alternative kind of semi-automatic segmentation, the algorithms return a spatial-taxon (i.e. foreground, object-group, object or object-part) selected by the user or designated via prior probabilities.[79][80]\n Most of the aforementioned segmentation methods are based only on color information of pixels in the image. Humans use much more knowledge when performing image segmentation, but implementing this knowledge would cost considerable human engineering and computational time, and would require a huge domain knowledge database which does not currently exist. Trainable segmentation methods, such as neural network segmentation, overcome these issues by modeling the domain knowledge from a dataset of labeled pixels.\n An image segmentation neural network can process small areas of an image to extract simple features such as edges.[81] Another neural network, or any decision-making mechanism, can then combine these features to label the areas of an image accordingly. A type of network designed this way is the Kohonen map.\n Pulse-coupled neural networks (PCNNs) are neural models proposed by modeling a cat's visual cortex and developed for high-performance biomimetic image processing. In 1989, Reinhard Eckhorn introduced a neural model to emulate the mechanism of a cat's visual cortex. The Eckhorn model provided a simple and effective tool for studying the visual cortex of small mammals, and was soon recognized as having significant application potential in image processing. In 1994, the Eckhorn model was adapted to be an image processing algorithm by John L. Johnson, who termed this algorithm Pulse-Coupled Neural Network.[82] Over the past decade, PCNNs have been utilized for a variety of image processing applications, including: image segmentation, feature generation, face extraction, motion detection, region growing, noise reduction, and so on. A PCNN is a two-dimensional neural network. Each neuron in the network corresponds to one pixel in an input image, receiving its corresponding pixel's color information (e.g. intensity) as an external stimulus. Each neuron also connects with its neighboring neurons, receiving local stimuli from them. The external and local stimuli are combined in an internal activation system, which accumulates the stimuli until it exceeds a dynamic threshold, resulting in a pulse output. Through iterative computation, PCNN neurons produce temporal series of pulse outputs. The temporal series of pulse outputs contain information of input images and can be utilized for various image processing applications, such as image segmentation and feature generation. Compared with conventional image processing means, PCNNs have several significant merits, including robustness against noise, independence of geometric variations in input patterns, capability of bridging minor intensity variations in input patterns, etc.\n U-Net is a convolutional neural network which takes as input an image and outputs a label for each pixel.[83] U-Net initially was developed to detect cell boundaries in biomedical images. U-Net follows classical autoencoder architecture, as such it contains two sub-structures. The encoder structure follows the traditional stack of convolutional and max pooling layers to increase the receptive field as it goes through the layers. It is used to capture the context in the image. The decoder structure utilizes transposed convolution layers for upsampling so that the end dimensions are close to that of the input image. Skip connections are placed between convolution and transposed convolution layers of the same shape in order to preserve details that would have been lost otherwise.\n In addition to pixel-level semantic segmentation tasks which assign a given category to each pixel, modern segmentation applications include instance-level semantic segmentation tasks in which each individual in a given category must be uniquely identified, as well as panoptic segmentation tasks which combines these two tasks to provide a more complete scene segmentation.[20]\n Related images such as a photo album or a sequence of video frames often contain semantically similar objects and scenes, therefore it is often beneficial to exploit such correlations.[84] The task of simultaneously segmenting scenes from related images or video frames is termed co-segmentation,[16] which is typically used in human action localization. Unlike conventional bounding box-based object detection, human action localization methods provide finer-grained results, typically per-image segmentation masks delineating the human object of interest and its action category (e.g., Segment-Tube[17]). Techniques such as dynamic Markov Networks, CNN and LSTM are often employed to exploit the inter-frame correlations.\n There are many other methods of segmentation like multispectral segmentation or connectivity-based segmentation based on DTI images.[85][86]\n",
        "doc_number": 97
    },
    {
        "url": "https://en.wikipedia.org/wiki/Text-to-speech",
        "content": "This is an accepted version of this page \n Speech synthesis is the artificial production of human speech. A computer system used for this purpose is called a speech synthesizer, and can be implemented in software or hardware products. A text-to-speech (TTS) system converts normal language text into speech; other systems render symbolic linguistic representations like phonetic transcriptions into speech.[1] The reverse process is speech recognition.\n Synthesized speech can be created by concatenating pieces of recorded speech that are stored in a database. Systems differ in the size of the stored speech units; a system that stores phones or diphones provides the largest output range, but may lack clarity.[citation needed] For specific usage domains, the storage of entire words or sentences allows for high-quality output. Alternatively, a synthesizer can incorporate a model of the vocal tract and other human voice characteristics to create a completely \"synthetic\" voice output.[2]\n The quality of a speech synthesizer is judged by its similarity to the human voice and by its ability to be understood clearly. An intelligible text-to-speech program allows people with visual impairments or reading disabilities to listen to written words on a home computer. Many computer operating systems have included speech synthesizers since the early 1990s.[citation needed]\n A text-to-speech system (or \"engine\") is composed of two parts:[3] a front-end and a back-end. The front-end has two major tasks. First, it converts raw text containing symbols like numbers and abbreviations into the equivalent of written-out words. This process is often called text normalization, pre-processing, or tokenization. The front-end then assigns phonetic transcriptions to each word, and divides and marks the text into prosodic units, like phrases, clauses, and sentences. The process of assigning phonetic transcriptions to words is called text-to-phoneme or grapheme-to-phoneme conversion. Phonetic transcriptions and prosody information together make up the symbolic linguistic representation that is output by the front-end. The back-end\u2014often referred to as the synthesizer\u2014then converts the symbolic linguistic representation into sound. In certain systems, this part includes the computation of the target prosody (pitch contour, phoneme durations),[4] which is then imposed on the output speech.\n Long before the invention of electronic signal processing, some people tried to build machines to emulate human speech.[citation needed] There were also legends of the existence of \"Brazen Heads\", such as those involving Pope Silvester II (d. 1003 AD), Albertus Magnus (1198\u20131280), and Roger Bacon (1214\u20131294).\n In 1779, the German-Danish scientist Christian Gottlieb Kratzenstein won the first prize in a competition announced by the Russian Imperial Academy of Sciences and Arts for models he built of the human vocal tract that could produce the five long vowel sounds (in International Phonetic Alphabet notation: [a\u02d0], [e\u02d0], [i\u02d0], [o\u02d0] and [u\u02d0]).[5] There followed the bellows-operated \"acoustic-mechanical speech machine\" of Wolfgang von Kempelen of Pressburg, Hungary, described in a 1791 paper.[6] This machine added models of the tongue and lips, enabling it to produce consonants as well as vowels. In 1837, Charles Wheatstone produced a \"speaking machine\" based on von Kempelen's design, and in 1846, Joseph Faber exhibited the \"Euphonia\". In 1923, Paget resurrected Wheatstone's design.[7]\n In the 1930s, Bell Labs developed the vocoder,  which automatically analyzed speech into its fundamental tones and resonances. From his work on the vocoder, Homer Dudley developed a keyboard-operated voice-synthesizer called The Voder (Voice Demonstrator), which he exhibited at the 1939 New York World's Fair.\n Dr. Franklin S. Cooper and his colleagues at Haskins Laboratories built the Pattern playback in the late 1940s and completed it in 1950. There were several different versions of this hardware device; only one currently survives. The machine converts pictures of the acoustic patterns of speech in the form of a spectrogram back into sound. Using this device, Alvin Liberman and colleagues discovered acoustic cues for the perception of phonetic segments (consonants and vowels).\n The first computer-based speech-synthesis systems originated in the late 1950s. Noriko Umeda et al. developed the first general English text-to-speech system in 1968, at the Electrotechnical Laboratory in Japan.[8] In 1961, physicist John Larry Kelly, Jr and his colleague Louis Gerstman[9] used an IBM 704 computer to synthesize speech, an event among the most prominent in the history of Bell Labs.[citation needed] Kelly's voice recorder synthesizer (vocoder) recreated the song \"Daisy Bell\", with musical accompaniment from Max Mathews. Coincidentally, Arthur C. Clarke was visiting his friend and colleague John Pierce at the Bell Labs Murray Hill facility. Clarke was so impressed by the demonstration that he used it in the climactic scene of his screenplay for his novel 2001: A Space Odyssey,[10] where the HAL 9000 computer sings the same song as astronaut Dave Bowman puts it to sleep.[11] Despite the success of purely electronic speech synthesis, research into mechanical speech-synthesizers continues.[12][third-party source needed]\n Linear predictive coding (LPC), a form of speech coding, began development with the work of Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone (NTT) in 1966. Further developments in LPC technology were made by Bishnu S. Atal and Manfred R. Schroeder at Bell Labs during the 1970s.[13] LPC was later the basis for early speech synthesizer chips, such as the Texas Instruments LPC Speech Chips used in the Speak & Spell toys from 1978.\n In 1975, Fumitada Itakura developed the line spectral pairs (LSP) method for high-compression speech coding, while at NTT.[14][15][16] From 1975 to 1981, Itakura studied problems in speech analysis and synthesis based on the LSP method.[16] In 1980, his team developed an LSP-based speech synthesizer chip. LSP is an important technology for speech synthesis and coding, and in the 1990s was adopted by almost all international speech coding standards as an essential component, contributing to the enhancement of digital speech communication over mobile channels and the internet.[15]\n In 1975, MUSA was released, and was one of the first Speech Synthesis systems. It consisted of a stand-alone computer hardware and a specialized software that enabled it to read Italian. A second version, released in 1978, was also able to sing Italian in an \"a cappella\" style.[17]\n Dominant systems in the 1980s and 1990s were the DECtalk system, based largely on the work of Dennis Klatt at MIT, and the Bell Labs system;[18] the latter was one of the first multilingual language-independent systems, making extensive use of natural language processing methods.\n Handheld electronics featuring speech synthesis began emerging in the 1970s. One of the first was the Telesensory Systems Inc. (TSI) Speech+ portable calculator for the blind in 1976.[19][20] Other devices had primarily educational purposes, such as the Speak & Spell toy produced by Texas Instruments in 1978.[21] Fidelity released a speaking version of its electronic chess computer in 1979.[22] The first video game to feature speech synthesis was the 1980 shoot 'em up arcade game, Stratovox (known in Japan as Speak & Rescue), from Sun Electronics.[23][24] The first personal computer game with speech synthesis was Manbiki Shoujo (Shoplifting Girl), released in 1980 for the PET 2001, for which the game's developer, Hiroshi Suzuki, developed a \"zero cross\" programming technique to produce a synthesized speech waveform.[25] Another early example, the arcade version of Berzerk, also dates from 1980. The Milton Bradley Company produced the first multi-player electronic game using voice synthesis, Milton, in the same year.\n In 1976, Computalker Consultants released their CT-1 Speech Synthesizer. Designed by D. Lloyd Rice and Jim Cooper, it was an analog synthesizer built to work with microcomputers using the S-100 bus standard.[26]\n Early electronic speech-synthesizers sounded robotic and were often barely intelligible. The quality of synthesized speech has steadily improved, but as of 2016[update] output from contemporary speech synthesis systems remains clearly distinguishable from actual human speech.\n Synthesized voices typically sounded male until 1990, when Ann Syrdal, at AT&T Bell Laboratories, created a female voice.[27]\n Kurzweil predicted in 2005 that as the cost-performance ratio caused speech synthesizers to become cheaper and more accessible, more people would benefit from the use of text-to-speech programs.[28]\n The most important qualities of a speech synthesis system are naturalness and intelligibility.[29] Naturalness describes how closely the output sounds like human speech, while intelligibility is the ease with which the output is understood. The ideal speech synthesizer is both natural and intelligible. Speech synthesis systems usually try to maximize both characteristics.\n The two primary technologies generating synthetic speech waveforms are concatenative synthesis and formant synthesis. Each technology has strengths and weaknesses, and the intended uses of a synthesis system will typically determine which approach is used.\n Concatenative synthesis is based on the concatenation (stringing together) of segments of recorded speech. Generally, concatenative synthesis produces the most natural-sounding synthesized speech. However, differences between natural variations in speech and the nature of the automated techniques for segmenting the waveforms sometimes result in audible glitches in the output. There are three main sub-types of concatenative synthesis.\n Unit selection synthesis uses large databases of recorded speech. During database creation, each recorded utterance is segmented into some or all of the following: individual phones, diphones, half-phones, syllables, morphemes, words, phrases, and sentences. Typically, the division into segments is done using a specially modified speech recognizer set to a \"forced alignment\" mode with some manual correction afterward, using visual representations such as the waveform and spectrogram.[30] An index of the units in the speech database is then created based on the segmentation and acoustic parameters like the fundamental frequency (pitch), duration, position in the syllable, and neighboring phones. At run time, the desired target utterance is created by determining the best chain of candidate units from the database (unit selection). This process is typically achieved using a specially weighted decision tree.\n Unit selection provides the greatest naturalness, because it applies only a small amount of digital signal processing (DSP) to the recorded speech. DSP often makes recorded speech sound less natural, although some systems use a small amount of signal processing at the point of concatenation to smooth the waveform. The output from the best unit-selection systems is often indistinguishable from real human voices, especially in contexts for which the TTS system has been tuned. However, maximum naturalness typically require unit-selection speech databases to be very large, in some systems ranging into the gigabytes of recorded data, representing dozens of hours of speech.[31] Also, unit selection algorithms have been known to select segments from a place that results in less than ideal synthesis (e.g. minor words become unclear) even when a better choice exists in the database.[32] Recently, researchers have proposed various automated methods to detect unnatural segments in unit-selection speech synthesis systems.[33]\n Diphone synthesis uses a minimal speech database containing all the diphones (sound-to-sound transitions) occurring in a language. The number of diphones depends on the phonotactics of the language: for example, Spanish has about 800 diphones, and German about 2500. In diphone synthesis, only one example of each diphone is contained in the speech database. At runtime, the target prosody of a sentence is superimposed on these minimal units by means of digital signal processing techniques such as linear predictive coding, PSOLA[34] or MBROLA.[35] or more recent techniques such as pitch modification in the source domain using discrete cosine transform.[36] Diphone synthesis suffers from the sonic glitches of concatenative synthesis and the robotic-sounding nature of formant synthesis, and has few of the advantages of either approach other than small size. As such, its use in commercial applications is declining,[citation needed] although it continues to be used in research because there are a number of freely available software implementations. An early example of Diphone synthesis is a teaching robot, Leachim, that was invented by Michael J. Freeman.[37] Leachim contained information regarding class curricular and certain biographical information about the students whom it was programmed to teach.[38] It was tested in a fourth grade classroom in the Bronx, New York.[39][40]\n Domain-specific synthesis concatenates prerecorded words and phrases to create complete utterances. It is used in applications where the variety of texts the system will output is limited to a particular domain, like transit schedule announcements or weather reports.[41] The technology is very simple to implement, and has been in commercial use for a long time, in devices like talking clocks and calculators. The level of naturalness of these systems can be very high because the variety of sentence types is limited, and they closely match the prosody and intonation of the original recordings.[citation needed]\n Because these systems are limited by the words and phrases in their databases, they are not general-purpose and can only synthesize the combinations of words and phrases with which they have been preprogrammed. The blending of words within naturally spoken language however can still cause problems unless the many variations are taken into account. For example, in non-rhotic dialects of English the \"r\" in words like \"clear\" /\u02c8kl\u026a\u0259/ is usually only pronounced when the following word has a vowel as its first letter (e.g. \"clear out\" is realized as /\u02cckl\u026a\u0259\u0279\u02c8\u028c\u028at/). Likewise in French, many final consonants become no longer silent if followed by a word that begins with a vowel, an effect called liaison. This alternation cannot be reproduced by a simple word-concatenation system, which would require additional complexity to be context-sensitive.\n Formant synthesis does not use human speech samples at runtime. Instead, the synthesized speech output is created using additive synthesis and an acoustic model (physical modelling synthesis).[42] Parameters such as fundamental frequency, voicing, and noise levels are varied over time to create a waveform of artificial speech. This method is sometimes called rules-based synthesis; however, many concatenative systems also have rules-based components.\nMany systems based on formant synthesis technology generate artificial, robotic-sounding speech that would never be mistaken for human speech. However, maximum naturalness is not always the goal of a speech synthesis system, and formant synthesis systems have advantages over concatenative systems. Formant-synthesized speech can be reliably intelligible, even at very high speeds, avoiding the acoustic glitches that commonly plague concatenative systems. High-speed synthesized speech is used by the visually impaired to quickly navigate computers using a screen reader. Formant synthesizers are usually smaller programs than concatenative systems because they do not have a database of speech samples. They can therefore be used in embedded systems, where memory and microprocessor power are especially limited. Because formant-based systems have complete control of all aspects of the output speech, a wide variety of prosodies and intonations can be output, conveying not just questions and statements, but a variety of emotions and tones of voice.\n Examples of non-real-time but highly accurate intonation control in formant synthesis include the work done in the late 1970s for the Texas Instruments toy Speak & Spell, and in the early 1980s Sega arcade machines[43] and in many Atari, Inc. arcade games[44] using the TMS5220 LPC Chips. Creating proper intonation for these projects was painstaking, and the results have yet to be matched by real-time text-to-speech interfaces.[45]\n Articulatory synthesis consists of computational techniques for synthesizing speech based on models of the human vocal tract and the articulation processes occurring there. The first articulatory synthesizer regularly used for laboratory experiments was developed at Haskins Laboratories in the mid-1970s by Philip Rubin, Tom Baer, and Paul Mermelstein. This synthesizer, known as ASY, was based on vocal tract models developed at Bell Laboratories in the 1960s and 1970s by Paul Mermelstein, Cecil Coker, and colleagues.\n Until recently, articulatory synthesis models have not been incorporated into commercial speech synthesis systems. A notable exception is the NeXT-based system originally developed and marketed by Trillium Sound Research, a spin-off company of the University of Calgary, where much of the original research was conducted. Following the demise of the various incarnations of NeXT (started by Steve Jobs in the late 1980s and merged with Apple Computer in 1997), the Trillium software was published under the GNU General Public License, with work continuing as gnuspeech. The system, first marketed in 1994, provides full articulatory-based text-to-speech conversion using a waveguide or transmission-line analog of the human oral and nasal tracts controlled by Carr\u00e9's \"distinctive region model\".\n More recent synthesizers, developed by Jorge C. Lucero and colleagues, incorporate models of vocal fold biomechanics, glottal aerodynamics and acoustic wave propagation in the bronchi, trachea, nasal and oral cavities, and thus constitute full systems of physics-based speech simulation.[46][47]\n HMM-based synthesis is a synthesis method based on hidden Markov models, also called Statistical Parametric Synthesis. In this system, the frequency spectrum (vocal tract), fundamental frequency (voice source), and duration (prosody) of speech are modeled simultaneously by HMMs. Speech waveforms are generated from HMMs themselves based on the maximum likelihood criterion.[48]\n Sinewave synthesis is a technique for synthesizing speech by replacing the formants (main bands of energy) with pure tone whistles.[49]\n Deep learning speech synthesis uses deep neural networks (DNN) to produce artificial speech from text (text-to-speech) or spectrum (vocoder).\nThe deep neural networks are trained using a large amount of recorded speech and, in the case of a text-to-speech system, the associated labels and/or input text.\n 15.ai uses a multi-speaker model\u2014hundreds of voices are trained concurrently rather than sequentially, decreasing the required training time and enabling the model to learn and generalize shared emotional context, even for voices with no exposure to such emotional context.[50] The deep learning model used by the application is nondeterministic: each time that speech is generated from the same string of text, the intonation of the speech will be slightly different. The application also supports manually altering the emotion of a generated line using emotional contextualizers (a term coined by this project), a sentence or phrase that conveys the emotion of the take that serves as a guide for the model during inference.[51][52]\n ElevenLabs is primarily known for its browser-based, AI-assisted text-to-speech software, Speech Synthesis, which can produce lifelike speech by synthesizing vocal emotion and intonation.[53] The company states its software is built to adjust the intonation and pacing of delivery based on the context of language input used.[54] It uses advanced algorithms to analyze the contextual aspects of text, aiming to detect emotions like anger, sadness, happiness, or alarm, which enables the system to understand the user's sentiment,[55] resulting in a more realistic and human-like inflection. Other features include multilingual speech generation and long-form content creation with contextually-aware voices.[56][57]\n The DNN-based speech synthesizers are approaching the naturalness of the human voice.\nExamples of disadvantages of the method are low robustness when the data are not sufficient, lack of controllability and low performance in auto-regressive models.\n For tonal languages, such as Chinese or Taiwanese language, there are different levels of tone sandhi required and sometimes the output of speech synthesizer may result in the mistakes of tone sandhi.[58]\n In 2023, VICE reporter Joseph Cox published findings that he had recorded five minutes of himself talking and then used a tool developed by ElevenLabs to create voice deepfakes that defeated a bank's voice-authentication system.[66]\n The process of normalizing text is rarely straightforward. Texts are full of heteronyms, numbers, and abbreviations that all require expansion into a phonetic representation. There are many spellings in English which are pronounced differently based on context. For example, \"My latest project is to learn how to better project my voice\" contains two pronunciations of \"project\".\n Most text-to-speech (TTS) systems do not generate semantic representations of their input texts, as processes for doing so are unreliable, poorly understood, and computationally ineffective. As a result, various heuristic techniques are used to guess the proper way to disambiguate homographs, like examining neighboring words and using statistics about frequency of occurrence.\n Recently TTS systems have begun to use HMMs (discussed above) to generate \"parts of speech\" to aid in disambiguating homographs. This technique is quite successful for many cases such as whether \"read\" should be pronounced as \"red\" implying past tense, or as \"reed\" implying present tense. Typical error rates when using HMMs in this fashion are usually below five percent. These techniques also work well for most European languages, although access to required training corpora is frequently difficult in these languages.\n Deciding how to convert numbers is another problem that TTS systems have to address. It is a simple programming challenge to convert a number into words (at least in English), like \"1325\" becoming \"one thousand three hundred twenty-five\". However, numbers occur in many different contexts; \"1325\" may also be read as \"one three two five\", \"thirteen twenty-five\" or \"thirteen hundred and twenty five\". A TTS system can often infer how to expand a number based on surrounding words, numbers, and punctuation, and sometimes the system provides a way to specify the context if it is ambiguous.[67] Roman numerals can also be read differently depending on context. For example, \"Henry VIII\" reads as \"Henry the Eighth\", while \"Chapter VIII\" reads as \"Chapter Eight\".\n Similarly, abbreviations can be ambiguous. For example, the abbreviation \"in\" for \"inches\" must be differentiated from the word \"in\", and the address \"12 St John St.\" uses the same abbreviation for both \"Saint\" and \"Street\". TTS systems with intelligent front ends can make educated guesses about ambiguous abbreviations, while others provide the same result in all cases, resulting in nonsensical (and sometimes comical) outputs, such as \"Ulysses S. Grant\" being rendered as \"Ulysses South Grant\".\n Speech synthesis systems use two basic approaches to determine the pronunciation of a word based on its spelling, a process which is often called text-to-phoneme or grapheme-to-phoneme conversion (phoneme is the term used by linguists to describe distinctive sounds in a language). The simplest approach to text-to-phoneme conversion is the dictionary-based approach, where a large dictionary containing all the words of a language and their correct pronunciations is stored by the program. Determining the correct pronunciation of each word is a matter of looking up each word in the dictionary and replacing the spelling with the pronunciation specified in the dictionary. The other approach is rule-based, in which pronunciation rules are applied to words to determine their pronunciations based on their spellings. This is similar to the \"sounding out\", or synthetic phonics, approach to learning reading.\n Each approach has advantages and drawbacks. The dictionary-based approach is quick and accurate, but completely fails if it is given a word which is not in its dictionary. As dictionary size grows, so too does the memory space requirements of the synthesis system. On the other hand, the rule-based approach works on any input, but the complexity of the rules grows substantially as the system takes into account irregular spellings or pronunciations. (Consider that the word \"of\" is very common in English, yet is the only word in which the letter \"f\" is pronounced [v].) As a result, nearly all speech synthesis systems use a combination of these approaches.\n Languages with a phonemic orthography have a very regular writing system, and the prediction of the pronunciation of words based on their spellings is quite successful. Speech synthesis systems for such languages often use the rule-based method extensively, resorting to dictionaries only for those few words, like foreign names and loanwords, whose pronunciations are not obvious from their spellings. On the other hand, speech synthesis systems for languages like English, which have extremely irregular spelling systems, are more likely to rely on dictionaries, and to use rule-based methods only for unusual words, or words that are not in their dictionaries.\n The consistent evaluation of speech synthesis systems may be difficult because of a lack of universally agreed objective evaluation criteria. Different organizations often use different speech data. The quality of speech synthesis systems also depends on the quality of the production technique (which may involve analogue or digital recording) and on the facilities used to replay the speech. Evaluating speech synthesis systems has therefore often been compromised by differences between production techniques and replay facilities.\n Since 2005, however, some researchers have started to evaluate speech synthesis systems using a common speech dataset.[68]\n A study in the journal Speech Communication by Amy Drahota and colleagues at the University of Portsmouth, UK, reported that listeners to voice recordings could determine, at better than chance levels, whether or not the speaker was smiling.[69][70][71] It was suggested that identification of the vocal features that signal emotional content may be used to help make synthesized speech sound more natural. One of the related issues is modification of the pitch contour of the sentence, depending upon whether it is an affirmative, interrogative or exclamatory sentence. One of the techniques for pitch modification[36] uses discrete cosine transform in the source domain (linear prediction residual). Such pitch synchronous pitch modification techniques need a priori pitch marking of the synthesis speech database using techniques such as epoch extraction using dynamic plosion index applied on the integrated linear prediction residual of the voiced regions of speech.[72]  In general, prosody remains a challenge for speech synthesizers, and is an active research topic.\n Popular systems offering speech synthesis as a built-in capability.\n In the early 1980s, TI was known as a pioneer in speech synthesis, and a highly popular plug-in speech synthesizer module was available for the TI-99/4 and 4A. Speech synthesizers were offered free with the purchase of a number of cartridges and were used by many TI-written video games (games offered with speech during this promotion included Alpiner and Parsec). The synthesizer uses a variant of linear predictive coding and has a small in-built vocabulary. The original intent was to release small cartridges that plugged directly into the synthesizer unit, which would increase the device's built-in vocabulary. However, the success of software text-to-speech in the Terminal Emulator II cartridge canceled that plan.\n The Mattel Intellivision game console offered the Intellivoice Voice Synthesis module in 1982. It included the SP0256 Narrator speech synthesizer chip on a removable cartridge. The Narrator had 2kB of Read-Only Memory (ROM), and this was utilized to store a database of generic words that could be combined to make phrases in Intellivision games. Since the Orator chip could also accept speech data from external memory, any additional words or phrases needed could be stored inside the cartridge itself. The data consisted of strings of analog-filter coefficients to modify the behavior of the chip's synthetic vocal-tract model, rather than simple digitized samples.\n Also released in 1982, Software Automatic Mouth was the first commercial all-software voice synthesis program. It was later used as the basis for Macintalk. The program was available for non-Macintosh Apple computers (including the Apple II, and the Lisa), various Atari models and the Commodore 64. The Apple version preferred additional hardware that contained DACs, although it could instead use the computer's one-bit audio output (with the addition of much distortion) if the card was not present. The Atari made use of the embedded POKEY audio chip. Speech playback on the Atari normally disabled interrupt requests and shut down the ANTIC chip during vocal output. The audible output is extremely distorted speech when the screen is on. The Commodore 64 made use of the 64's embedded SID audio chip.\n Arguably, the first speech system integrated into an operating system was the circa 1983 unreleased Atari 1400XL/1450XL computers. These used the Votrax SC01 chip and a finite-state machine to enable World English Spelling text-to-speech synthesis.[74]\n The Atari ST computers were sold with \"stspeech.tos\" on floppy disk.\n The first speech system integrated into an operating system that shipped in quantity was Apple Computer's MacInTalk. The software was licensed from third-party developers Joseph Katz and Mark Barton (later, SoftVoice, Inc.) and was featured during the 1984 introduction of the Macintosh computer. This January demo required 512 kilobytes of RAM memory. As a result, it could not run in the 128 kilobytes of RAM the first Mac actually shipped with.[75] So, the demo was accomplished with a prototype 512k Mac, although those in attendance were not told of this and the synthesis demo created considerable excitement for the Macintosh. In the early 1990s Apple expanded its capabilities offering system wide text-to-speech support. With the introduction of faster PowerPC-based computers they included higher quality voice sampling. Apple also introduced speech recognition into its systems which provided a fluid command set. More recently, Apple has added sample-based voices. Starting as a curiosity, the speech system of Apple Macintosh has evolved into a fully supported program, PlainTalk, for people with vision problems. VoiceOver was for the first time featured in 2005 in Mac OS X Tiger (10.4). During 10.4 (Tiger) and first releases of 10.5 (Leopard) there was only one standard voice shipping with Mac OS X. Starting with 10.6 (Snow Leopard), the user can choose out of a wide range list of multiple voices. VoiceOver voices feature the taking of realistic-sounding breaths between sentences, as well as improved clarity at high read rates over PlainTalk. Mac OS X also includes say, a command-line based application that converts text to audible speech. The AppleScript Standard Additions includes a say verb that allows a script to use any of the installed voices and to control the pitch, speaking rate and modulation of the spoken text.\n Used in Alexa and as Software as a Service in AWS[76] (from 2017).\n The second operating system to feature advanced speech synthesis capabilities was AmigaOS, introduced in 1985. The voice synthesis was licensed by Commodore International from SoftVoice, Inc., who also developed the original MacinTalk text-to-speech system. It featured a complete system of voice emulation for American English, with both male and female voices and \"stress\" indicator markers, made possible through the Amiga's audio chipset.[77] The synthesis system was divided into a translator library which converted unrestricted English text into a standard set of phonetic codes and a narrator device which implemented a formant model of speech generation.. AmigaOS also featured a high-level \"Speak Handler\", which allowed command-line users to redirect text output to speech. Speech synthesis was occasionally used in third-party programs, particularly word processors and educational software. The synthesis software remained largely unchanged from the first AmigaOS release and Commodore eventually removed speech synthesis support from AmigaOS 2.1 onward.\n Despite the American English phoneme limitation, an unofficial version with multilingual speech synthesis was developed. This made use of an enhanced version of the translator library which could translate a number of languages, given a set of rules for each language.[78]\n Modern Windows desktop systems can use SAPI 4 and SAPI 5 components to support speech synthesis and speech recognition. SAPI 4.0 was available as an optional add-on for Windows 95 and Windows 98. Windows 2000 added Narrator, a text-to-speech utility for people who have visual impairment. Third-party programs such as JAWS for Windows, Window-Eyes, Non-visual Desktop Access, Supernova and System Access can perform various text-to-speech tasks such as reading text aloud from a specified website, email account, text document, the Windows clipboard, the user's keyboard typing, etc. Not all programs can use speech synthesis directly.[79] Some programs can use plug-ins, extensions or add-ons to read text aloud. Third-party programs are available that can read text from the system clipboard.\n Microsoft Speech Server is a server-based package for voice synthesis and recognition. It is designed for network use with web applications and call centers.\n From 1971 to 1996, Votrax produced a number of commercial speech synthesizer components.  A Votrax synthesizer was included in the first generation Kurzweil Reading Machine for the Blind.\n Text-to-speech (TTS) refers to the ability of computers to read text aloud. A TTS engine converts written text to a phonemic representation, then converts the phonemic representation to waveforms that can be output as sound. TTS engines with different languages, dialects and specialized vocabularies are available through third-party publishers.[80]\n Version 1.6 of Android added support for speech synthesis (TTS).[81]\n Currently, there are a number of applications, plugins and gadgets that can read messages directly from an e-mail client and web pages from a web browser or Google Toolbar. Some specialized software can narrate RSS-feeds. On one hand, online RSS-narrators simplify information delivery by allowing users to listen to their favourite news sources and to convert them to podcasts. On the other hand, on-line RSS-readers are available on almost any personal computer connected to the Internet. Users can download generated audio files to portable devices, e.g. with a help of podcast receiver, and listen to them while walking, jogging or commuting to work.\n A growing field in Internet based TTS is web-based assistive technology, e.g. 'Browsealoud' from a UK company and Readspeaker. It can deliver TTS functionality to anyone (for reasons of accessibility, convenience, entertainment or information) with access to a web browser. The non-profit project Pediaphon was created in 2006 to provide a similar web-based TTS interface to the Wikipedia.[82]\n Other work is being done in the context of the W3C through the W3C Audio Incubator Group with the involvement of The BBC and Google Inc.\n Some open-source software systems are available, such as:\n At the 2018 Conference on Neural Information Processing Systems (NeurIPS) researchers from Google presented the work 'Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis', which transfers learning from speaker verification to achieve text-to-speech synthesis, that can be made to sound almost like anybody from a speech sample of only 5 seconds.[85]\n Also researchers from Baidu Research presented a voice cloning system with similar aims at the 2018 NeurIPS conference,[86] though the result is rather unconvincing.\n By 2019 the digital sound-alikes found their way to the hands of criminals as Symantec researchers know of 3 cases where digital sound-alikes technology has been used for crime.[87][88]\n This increases the stress on the disinformation situation coupled with the facts that \n In March 2020, a freeware web application called 15.ai that generates high-quality voices from an assortment of fictional characters from a variety of media sources was released.[91] Initial characters included GLaDOS from Portal, Twilight Sparkle and Fluttershy from the show My Little Pony: Friendship Is Magic, and the Tenth Doctor from Doctor Who.\n A number of markup languages have been established for the rendition of text as speech in an XML-compliant format. The most recent is Speech Synthesis Markup Language (SSML), which became a W3C recommendation in 2004. Older speech synthesis markup languages include Java Speech Markup Language (JSML) and SABLE. Although each of these was proposed as a standard, none of them have been widely adopted.[citation needed]\n Speech synthesis markup languages are distinguished from dialogue markup languages. VoiceXML, for example, includes tags related to speech recognition, dialogue management and touchtone dialing, in addition to text-to-speech markup.[citation needed]\n Speech synthesis has long been a vital assistive technology tool and its application in this area is significant and widespread. It allows environmental barriers to be removed for people with a wide range of disabilities. The longest application has been in the use of screen readers for people with visual impairment, but text-to-speech systems are now commonly used by people with dyslexia and other reading disabilities as well as by pre-literate children.[92] They are also frequently employed to aid those with severe speech impairment usually through a dedicated voice output communication aid.[93] Work to personalize a synthetic voice to better match a person's personality or historical voice is becoming available.[94]  A noted application, of speech synthesis, was the Kurzweil Reading Machine for the Blind which incorporated text-to-phonetics software based on work from Haskins Laboratories and a black-box synthesizer built by Votrax.[95]\n Speech synthesis techniques are also used in entertainment productions such as games and animations. In 2007, Animo Limited announced the development of a software application package based on its speech synthesis software FineSpeech, explicitly geared towards customers in the entertainment industries, able to generate narration and lines of dialogue according to user specifications.[96] The application reached maturity in 2008, when NEC Biglobe announced a web service that allows users to create phrases from the voices of characters from the Japanese anime series Code Geass: Lelouch of the Rebellion R2.[97] 15.ai has been frequently used for content creation in various fandoms, including the My Little Pony: Friendship Is Magic fandom, the Team Fortress 2 fandom, the Portal fandom, and the SpongeBob SquarePants fandom.[citation needed]\n Text-to-speech for disability and impaired communication aids have become widely available. Text-to-speech is also finding new applications; for example, speech synthesis combined with speech recognition allows for interaction with mobile devices via natural language processing interfaces. Some users have also created AI virtual assistants using 15.ai and external voice control software.[51][52]\n Text-to-speech is also used in second language acquisition. Voki, for instance, is an educational tool created by Oddcast that allows users to create their own talking avatar, using different accents. They can be emailed, embedded on websites or shared on social media.\n Content creators have used voice cloning tools to recreate their voices for podcasts,[98][99] narration,[54] and comedy shows.[100][101][102] Publishers and authors have also used such software to narrate audiobooks and newsletters.[103][104] Another area of application is AI video creation with talking heads. Webapps and video editors like Elai.io or Synthesia allow users to create video content involving AI avatars, who are made to speak using text-to-speech technology.[105][106]\n Speech synthesis is a valuable computational aid for the analysis and assessment of speech disorders. A voice quality synthesizer, developed by Jorge C. Lucero et al. at the University of Bras\u00edlia, simulates the physics of phonation and includes models of vocal frequency jitter and tremor, airflow noise and laryngeal asymmetries.[46] The synthesizer has been used to mimic the timbre of dysphonic speakers with controlled levels of roughness, breathiness and strain.[47]\n",
        "doc_number": 98
    },
    {
        "url": "https://en.wikipedia.org/wiki/Speech-to-text",
        "content": "\nSpeech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition (ASR), computer speech recognition or speech-to-text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis.\n Some speech recognition systems require \"training\" (also called \"enrollment\") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called \"speaker-independent\"[1] systems. Systems that use training are called \"speaker dependent\".\n Speech recognition applications include voice user interfaces such as voice dialing (e.g. \"call home\"), call routing (e.g. \"I would like to make a collect call\"), domotic appliance control, search key words (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), determining speaker characteristics,[2] speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed direct voice input). Automatic pronunciation assessment is used in education such as for spoken language learning.\n The term voice recognition[3][4][5] or speaker identification[6][7][8] refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.\n From the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the worldwide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems.\n The key areas of growth were: vocabulary size, speaker independence, and processing speed.\n Raj Reddy was the first person to take on continuous speech recognition as a graduate student at Stanford University in the late 1960s. Previous systems required users to pause after each word. Reddy's system issued spoken commands for playing chess.\n Around this time Soviet researchers invented the dynamic time warping (DTW) algorithm and used it to create a recognizer capable of operating on a 200-word vocabulary.[15] DTW processed speech by dividing it into short frames, e.g. 10ms segments, and processing each frame as a single unit. Although DTW would be superseded by later algorithms, the technique carried on. Achieving speaker independence remained unsolved at this time period.\n During the late 1960s Leonard Baum developed the mathematics of Markov chains at the Institute for Defense Analysis. A decade later, at CMU, Raj Reddy's students James Baker and Janet M. Baker began using the hidden Markov model (HMM) for speech recognition.[20] James Baker had learned about HMMs from a summer job at the Institute of Defense Analysis during his undergraduate education.[21] The use of HMMs allowed researchers to combine different sources of knowledge, such as acoustics, language, and syntax, in a unified probabilistic model.\n The 1980s also saw the introduction of the n-gram language model.\n Much of the progress in the field is owed to the rapidly increasing capabilities of computers. At the end of the DARPA program in 1976, the best computer available to researchers was the PDP-10 with 4 MB ram.[28] It could take up to 100 minutes to decode just 30 seconds of speech.[29]\n Two practical products were:\n By this point, the vocabulary of the typical commercial speech recognition system was larger than the average human vocabulary.[28] Raj Reddy's former student, Xuedong Huang, developed the Sphinx-II system at CMU. The Sphinx-II system was the first to do speaker-independent, large vocabulary, continuous speech recognition and it had the best performance in DARPA's 1992 evaluation. Handling continuous speech with a large vocabulary was a major milestone in the history of speech recognition. Huang went on to found the speech recognition group at Microsoft in 1993. Raj Reddy's student Kai-Fu Lee joined Apple where, in 1992, he helped develop a speech interface prototype for the Apple computer known as Casper.\n Lernout & Hauspie, a Belgium-based speech recognition company, acquired several other companies, including Kurzweil Applied Intelligence in 1997 and Dragon Systems in 2000. The L&H speech technology was used in the Windows XP operating system. L&H was an industry leader until an accounting scandal brought an end to the company in 2001. The speech technology from L&H was bought by ScanSoft which became Nuance in 2005. Apple originally licensed software from Nuance to provide speech recognition capability to its digital assistant Siri.[34]\n In the 2000s DARPA sponsored two speech recognition programs: Effective Affordable Reusable Speech-to-Text (EARS) in 2002 and Global Autonomous Language Exploitation (GALE). Four teams participated in the EARS program: IBM, a team led by BBN with LIMSI and Univ. of Pittsburgh, Cambridge University, and a team composed of ICSI, SRI and University of Washington. EARS funded the collection of the Switchboard telephone speech corpus containing 260 hours of recorded conversations from over 500 speakers.[35] The GALE program focused on Arabic and Mandarin broadcast news speech. Google's first effort at speech recognition came in 2007 after hiring some researchers from Nuance.[36] The first product was GOOG-411, a telephone based directory service. The recordings from GOOG-411 produced valuable data that helped Google improve their recognition systems. Google Voice Search is now supported in over 30 languages.\n In the United States, the National Security Agency has made use of a type of speech recognition for keyword spotting since at least 2006.[37] This technology allows analysts to search through large volumes of recorded conversations and isolate mentions of keywords. Recordings can be indexed and analysts can run queries over the database to find conversations of interest. Some government research programs focused on intelligence applications of speech recognition, e.g. DARPA's EARS's program and IARPA's Babel program.\n In the early 2000s, speech recognition was still dominated by traditional approaches such as hidden Markov models combined with feedforward artificial neural networks.[38]\nToday, however, many aspects of speech recognition have been taken over by a deep learning method called Long short-term memory (LSTM), a recurrent neural network published by Sepp Hochreiter & J\u00fcrgen Schmidhuber in 1997.[39] LSTM RNNs avoid the vanishing gradient problem and can learn \"Very Deep Learning\" tasks[40] that require memories of events that happened thousands of discrete time steps ago, which is important for speech.\nAround 2007, LSTM trained by Connectionist Temporal Classification (CTC)[41] started to outperform traditional speech recognition in certain applications.[42] In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which is now available through Google Voice to all smartphone users.[43] Transformers, a type of neural network based solely on \"attention\", have been widely adopted in computer vision[44][45] and language modeling,[46][47] sparking the interest of adapting such models to new domains, including speech recognition.[48][49][50] Some recent papers reported superior performance levels using transformer models for speech recognition, but these models usually require large scale training datasets to reach high performance levels.\n The use of deep feedforward (non-recurrent) networks for acoustic modeling was introduced during the later part of 2009 by Geoffrey Hinton and his students at the University of Toronto and by Li Deng[51] and colleagues at Microsoft Research, initially in the collaborative work between Microsoft and the University of Toronto which was subsequently expanded to include IBM and Google (hence \"The shared views of four research groups\" subtitle in their 2012 review paper).[52][53][54] A Microsoft research executive called this innovation \"the most dramatic change in accuracy since 1979\".[55] In contrast to the steady incremental improvements of the past few decades, the application of deep learning decreased word error rate by 30%.[55] This innovation was quickly adopted across the field. Researchers have begun to use deep learning techniques for language modeling as well.\n In the long history of speech recognition, both shallow form and deep form (e.g. recurrent nets) of artificial neural networks had been explored for many years during 1980s, 1990s and a few years into the 2000s.[56][57][58]\nBut these methods never won over the non-uniform internal-handcrafting Gaussian mixture model/hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.[59] A number of key difficulties had been methodologically analyzed in the 1990s, including gradient diminishing[60] and weak temporal correlation structure in the neural predictive models.[61][62] All these difficulties were in addition to the lack of big training data and big computing power in these early days. Most speech recognition researchers who understood such barriers hence subsequently moved away from neural nets to pursue generative modeling approaches until the recent resurgence of deep learning starting around 2009\u20132010 that had overcome all these difficulties. Hinton et al. and Deng et al. reviewed part of this recent history about how their collaboration with each other and then with colleagues across four groups (University of Toronto, Microsoft, Google, and IBM) ignited a renaissance of applications of deep feedforward neural networks for speech recognition.[53][54][63][64]\n By early 2010s speech recognition, also called voice recognition[65][66][67] was clearly differentiated from speaker recognition, and speaker independence was considered a major breakthrough. Until then, systems required a \"training\" period.  A 1987 ad for a doll had carried the tagline \"Finally, the doll that understands you.\" \u2013 despite the fact that it was described as \"which children could train to respond to their voice\".[12]\n In 2017, Microsoft researchers reached a historical human parity milestone of transcribing conversational telephony speech on the widely benchmarked Switchboard task. Multiple deep learning models were used to optimize speech recognition accuracy. The speech recognition word error rate was reported to be as low as 4 professional human transcribers working together on the same benchmark, which was funded by IBM Watson speech team on the same task.[68]\n Both acoustic modeling and language modeling are important parts of modern statistically based speech recognition algorithms. Hidden Markov models (HMMs) are widely used in many systems. Language modeling is also used in many other natural language processing applications such as document classification or statistical machine translation.\n Modern general-purpose speech recognition systems are based on hidden Markov models. These are statistical models that output a sequence of symbols or quantities. HMMs are used in speech recognition because a speech signal can be viewed as a piecewise stationary signal or a short-time stationary signal. In a short time scale (e.g., 10 milliseconds), speech can be approximated as a stationary process. Speech can be thought of as a Markov model for many stochastic purposes.\n Another reason why HMMs are popular is that they can be trained automatically and are simple and computationally feasible to use. In speech recognition, the hidden Markov model would output a sequence of n-dimensional real-valued vectors (with n being a small integer, such as 10), outputting one of these every 10 milliseconds. The vectors would consist of cepstral coefficients, which are obtained by taking a Fourier transform of a short time window of speech and decorrelating the spectrum using a cosine transform, then taking the first (most significant) coefficients. The hidden Markov model will tend to have in each state a statistical distribution that is a mixture of diagonal covariance Gaussians, which will give a likelihood for each observed vector. Each word, or (for more general speech recognition systems), each phoneme, will have a different output distribution; a hidden Markov model for a sequence of words or phonemes is made by concatenating the individual trained hidden Markov models for the separate words and phonemes.\n Described above are the core elements of the most common, HMM-based approach to speech recognition. Modern speech recognition systems use various combinations of a number of standard techniques in order to improve results over the basic approach described above. A typical large-vocabulary system would need context dependency for the phonemes (so that phonemes with different left and right context would have different realizations as HMM states); it would use cepstral normalization to normalize for a different speaker and recording conditions; for further speaker normalization, it might use vocal tract length normalization (VTLN) for male-female normalization and maximum likelihood linear regression (MLLR) for more general speaker adaptation. The features would have so-called delta and delta-delta coefficients to capture speech dynamics and in addition, might use heteroscedastic linear discriminant analysis (HLDA); or might skip the delta and delta-delta coefficients and use splicing and an LDA-based projection followed perhaps by heteroscedastic linear discriminant analysis or a global semi-tied co variance transform (also known as maximum likelihood linear transform, or MLLT). Many systems use so-called discriminative training techniques that dispense with a purely statistical approach to HMM parameter estimation and instead optimize some classification-related measure of the training data. Examples are maximum mutual information (MMI), minimum classification error (MCE), and minimum phone error (MPE).\n Decoding of the speech (the term for what happens when the system is presented with a new utterance and must compute the most likely source sentence) would probably use the Viterbi algorithm to find the best path, and here there is a choice between dynamically creating a combination hidden Markov model, which includes both the acoustic and language model information and combining it statically beforehand (the finite state transducer, or FST, approach).\n A possible improvement to decoding is to keep a set of good candidates instead of just keeping the best candidate, and to use a better scoring function (re scoring) to rate these good candidates so that we may pick the best one according to this refined score. The set of candidates can be kept either as a list (the N-best list approach) or as a subset of the models (a lattice). Re scoring is usually done by trying to minimize the Bayes risk[69] (or an approximation thereof) Instead of taking the source sentence with maximal probability, we try to take the sentence that minimizes the expectancy of a given loss function with regards to all possible transcriptions (i.e., we take the sentence that minimizes the average distance to other possible sentences weighted by their estimated probability). The loss function is usually the Levenshtein distance, though it can be different distances for specific tasks; the set of possible transcriptions is, of course, pruned to maintain tractability. Efficient algorithms have been devised to re score lattices represented as weighted finite state transducers with edit distances represented themselves as a finite state transducer verifying certain assumptions.[70]\n Dynamic time warping is an approach that was historically used for speech recognition but has now largely been displaced by the more successful HMM-based approach.\n Dynamic time warping is an algorithm for measuring similarity between two sequences that may vary in time or speed. For instance, similarities in walking patterns would be detected, even if in one video the person was walking slowly and if in another he or she were walking more quickly, or even if there were accelerations and deceleration during the course of one observation. DTW has been applied to video, audio, and graphics\u00a0\u2013 indeed, any data that can be turned into a linear representation can be analyzed with DTW.\n A well-known application has been automatic speech recognition, to cope with different speaking speeds. In general, it is a method that allows a computer to find an optimal match between two given sequences (e.g., time series) with certain restrictions. That is, the sequences are \"warped\" non-linearly to match each other. This sequence alignment method is often used in the context of hidden Markov models.\n Neural networks emerged as an attractive acoustic modeling approach in ASR in the late 1980s. Since then, neural networks have been used in many aspects of speech recognition such as phoneme classification,[71] phoneme classification through multi-objective evolutionary algorithms,[72] isolated word recognition,[73] audiovisual speech recognition, audiovisual speaker recognition and speaker adaptation.\n Neural networks make fewer explicit assumptions about feature statistical properties than HMMs and have several qualities making them more attractive recognition models for speech recognition. When used to estimate the probabilities of a speech feature segment, neural networks allow discriminative training in a natural and efficient manner. However, in spite of their effectiveness in classifying short-time units such as individual phonemes and isolated words,[74] early neural networks were rarely successful for continuous recognition tasks because of their limited ability to model temporal dependencies.\n One approach to this limitation was to use neural networks as a pre-processing, feature transformation or dimensionality reduction,[75] step prior to HMM based recognition. However, more recently, LSTM and related recurrent neural networks (RNNs),[39][43][76][77] Time Delay Neural Networks(TDNN's),[78] and transformers[48][49][50] have demonstrated improved performance in this area.\n Deep neural networks and denoising autoencoders[79] are also under investigation. A deep feedforward neural network (DNN) is an artificial neural network with multiple hidden layers of units between the input and output layers.[53] Similar to shallow neural networks, DNNs can model complex non-linear relationships. DNN architectures generate compositional models, where extra layers enable composition of features from lower layers, giving a huge learning capacity and thus the potential of modeling complex patterns of speech data.[80]\n A success of DNNs in large vocabulary speech recognition occurred in 2010 by industrial researchers, in collaboration with academic researchers, where large output layers of the DNN based on context dependent HMM states constructed by decision trees were adopted.[81][82]\n[83] See comprehensive reviews of this development and of the state of the art as of October 2014 in the recent Springer book from Microsoft Research.[84] See also the related background of automatic speech recognition and the impact of various machine learning paradigms, notably including deep learning, in\nrecent overview articles.[85][86]\n One fundamental principle of deep learning is to do away with hand-crafted feature engineering and to use raw features. This principle was first explored successfully in the architecture of deep autoencoder on the \"raw\" spectrogram or linear filter-bank features,[87] showing its superiority over the Mel-Cepstral features which contain a few stages of fixed transformation from spectrograms.\nThe true \"raw\" features of speech, waveforms, have more recently been shown to produce excellent larger-scale speech recognition results.[88]\n Since 2014, there has been much research interest in \"end-to-end\" ASR. Traditional phonetic-based (i.e., all HMM-based model) approaches required separate components and training for the pronunciation, acoustic, and language model. End-to-end models jointly learn all the components of the speech recognizer. This is valuable since it simplifies the training process and deployment process. For example, a n-gram language model is required for all HMM-based systems, and a typical n-gram language model often takes several gigabytes in memory making them impractical to deploy on mobile devices.[89] Consequently, modern commercial ASR systems from Google and Apple (as of 2017[update]) are deployed on the cloud and require a network connection as opposed to the device locally.\n The first attempt at end-to-end ASR was with Connectionist Temporal Classification (CTC)-based systems introduced by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014.[90] The model consisted of recurrent neural networks and a CTC layer. Jointly, the RNN-CTC model learns the pronunciation and acoustic model together, however it is incapable of learning the language due to conditional independence assumptions similar to a HMM. Consequently, CTC models can directly learn to map speech acoustics to English characters, but the models make many common spelling mistakes and must rely on a separate language model to clean up the transcripts. Later, Baidu expanded on the work with extremely large datasets and demonstrated some commercial success in Chinese Mandarin and English.[91] In 2016, University of Oxford presented LipNet,[92] the first end-to-end sentence-level lipreading model, using spatiotemporal convolutions coupled with an RNN-CTC architecture, surpassing human-level performance in a restricted grammar dataset.[93] A large-scale CNN-RNN-CTC architecture was presented in 2018 by Google DeepMind achieving 6 times better performance than human experts.[94] In 2019, Nvidia launched two CNN-CTC ASR models, Jasper and QuarzNet, with an overall performance WER of 3%.[95][96] Similar to other deep learning applications, transfer learning and domain adaptation are important strategies for reusing and extending the capabilities of deep learning models, particularly due to the high costs of training models from scratch, and the small size of available corpus in many languages and/or specific domains.[97][98][99]\n An alternative approach to CTC-based models are attention-based models. Attention-based ASR models were introduced simultaneously by Chan et al. of Carnegie Mellon University and Google Brain and Bahdanau et al. of the University of Montreal in 2016.[100][101] The model named \"Listen, Attend and Spell\" (LAS), literally \"listens\" to the acoustic signal, pays \"attention\" to different parts of the signal and \"spells\" out the transcript one character at a time. Unlike CTC-based models, attention-based models do not have conditional-independence assumptions and can learn all the components of a speech recognizer including the pronunciation, acoustic and language model directly. This means, during deployment, there is no need to carry around a language model making it very practical for applications with limited memory. By the end of 2016, the attention-based models have seen considerable success including outperforming the CTC models (with or without an external language model).[102] Various extensions have been proposed since the original LAS model. Latent Sequence Decompositions (LSD) was proposed by Carnegie Mellon University, MIT and Google Brain to directly emit sub-word units which are more natural than English characters;[103] University of Oxford and Google DeepMind extended LAS to \"Watch, Listen, Attend and Spell\" (WLAS) to handle lip reading surpassing human-level performance.[104]\n Typically a manual control input, for example by means of a finger control on the steering-wheel, enables the speech recognition system and this is signaled to the driver by an audio prompt. Following the audio prompt, the system has a \"listening window\" during which it may accept a speech input for recognition. [citation needed]\n Simple voice commands may be used to initiate phone calls, select radio stations or play music from a compatible smartphone, MP3 player or music-loaded flash drive. Voice recognition capabilities vary between car make and model. Some of the most recent[when?] car models offer natural-language speech recognition in place of a fixed set of commands, allowing the driver to use full sentences and common phrases. With such systems there is, therefore, no need for the user to memorize a set of fixed command words.[citation needed]\n Automatic pronunciation assessment is the use of speech recognition to verify the correctness of pronounced speech,[105] as distinguished from manual assessment by an instructor or proctor.[106] Also called speech verification, pronunciation evaluation, and pronunciation scoring, the main application of this technology is computer-aided pronunciation teaching (CAPT) when combined with computer-aided instruction for computer-assisted language learning (CALL), speech remediation, or accent reduction. Pronunciation assessment does not determine unknown speech (as in dictation or automatic transcription) but instead, knowing the expected word(s) in advance, it attempts to verify the correctness of the learner's pronunciation and ideally their intelligibility to listeners,[107][108] sometimes along with often inconsequential prosody such as intonation, pitch, tempo, rhythm, and stress.[109] Pronunciation assessment is also used in reading tutoring, for example in products such as Microsoft Teams[110] and from Amira Learning.[111] Automatic pronunciation assessment can also be used to help diagnose and treat speech disorders such as apraxia.[112]\n Assessing authentic listener intelligibility is essential for avoiding inaccuracies from accent bias, especially in high-stakes assessments;[113][114][115] from words with multiple correct pronunciations;[116] and from phoneme coding errors in machine-readable pronunciation dictionaries.[117] In 2022, researchers found that some newer speech to text systems, based on end-to-end reinforcement learning to map audio signals directly into words, produce word and phrase confidence scores very closely correlated with genuine listener intelligibility.[118] In the Common European Framework of Reference for Languages (CEFR) assessment criteria for \"overall phonological control\", intelligibility outweighs formally correct pronunciation at all levels.[119]\n In the health care sector, speech recognition can be implemented in front-end or back-end of the medical documentation process. Front-end speech recognition is where the provider dictates into a speech-recognition engine, the recognized words are displayed as they are spoken, and the dictator is responsible for editing and signing off on the document. Back-end or deferred speech recognition is where the provider dictates into a digital dictation system, the voice is routed through a speech-recognition machine and the recognized draft document is routed along with the original voice file to the editor, where the draft is edited and report finalized. Deferred speech recognition is widely used in the industry currently.\n One of the major issues relating to the use of speech recognition in healthcare is that the American Recovery and Reinvestment Act of 2009 (ARRA) provides for substantial financial benefits to physicians who utilize an EMR according to \"Meaningful Use\" standards. These standards require that a substantial amount of data be maintained by the EMR (now more commonly referred to as an Electronic Health Record or EHR). The use of speech recognition is more naturally suited to the generation of narrative text, as part of a radiology/pathology interpretation, progress note or discharge summary: the ergonomic gains of using speech recognition to enter structured discrete data (e.g., numeric values or codes from a list or a controlled vocabulary) are relatively minimal for people who are sighted and who can operate a keyboard and mouse.\n A more significant issue is that most EHRs have not been expressly tailored to take advantage of voice-recognition capabilities. A large part of the clinician's interaction with the EHR involves navigation through the user interface using menus, and tab/button clicks, and is heavily dependent on keyboard and mouse: voice-based navigation provides only modest ergonomic benefits. By contrast, many highly customized systems for radiology or pathology dictation implement voice \"macros\", where the use of certain phrases \u2013 e.g., \"normal report\", will automatically fill in a large number of default values and/or generate boilerplate, which will vary with the type of the exam \u2013 e.g., a chest X-ray vs. a gastrointestinal contrast series for a radiology system.\n Prolonged use of speech recognition software in conjunction with word processors has shown benefits to short-term-memory restrengthening in brain AVM patients who have been treated with resection. Further research needs to be conducted to determine cognitive benefits for individuals whose AVMs have been treated using radiologic techniques.[citation needed]\n Substantial efforts have been devoted in the last decade to the test and evaluation of speech recognition in fighter aircraft. Of particular note have been the US program in speech recognition for the Advanced Fighter Technology Integration (AFTI)/F-16 aircraft (F-16 VISTA), the program in France for Mirage aircraft, and other programs in the UK dealing with a variety of aircraft platforms. In these programs, speech recognizers have been operated successfully in fighter aircraft, with applications including setting radio frequencies, commanding an autopilot system, setting steer-point coordinates and weapons release parameters, and controlling flight display.\n Working with Swedish pilots flying in the JAS-39 Gripen cockpit, Englund (2004) found recognition deteriorated with increasing g-loads. The report also concluded that adaptation greatly improved the results in all cases and that the introduction of models for breathing was shown to improve recognition scores significantly. Contrary to what might have been expected, no effects of the broken English of the speakers were found. It was evident that spontaneous speech caused problems for the recognizer, as might have been expected. A restricted vocabulary, and above all, a proper syntax, could thus be expected to improve recognition accuracy substantially.[120]\n The Eurofighter Typhoon, currently in service with the UK RAF, employs a speaker-dependent system, requiring each pilot to create a template. The system is not used for any safety-critical or weapon-critical tasks, such as weapon release or lowering of the undercarriage, but is used for a wide range of other cockpit functions. Voice commands are confirmed by visual and/or aural feedback. The system is seen as a major design feature in the reduction of pilot workload,[121] and even allows the pilot to assign targets to his aircraft with two simple voice commands or to any of his wingmen with only five commands.[122]\n Speaker-independent systems are also being developed and are under test for the F-35 Lightning II (JSF) and the Alenia Aermacchi M-346 Master lead-in fighter trainer. These systems have produced word accuracy scores in excess of 98%.[123]\n The problems of achieving high recognition accuracy under stress and noise are particularly relevant in the helicopter environment as well as in the jet fighter environment. The acoustic noise problem is actually more severe in the helicopter environment, not only because of the high noise levels but also because the helicopter pilot, in general, does not wear a facemask, which would reduce acoustic noise in the microphone. Substantial test and evaluation programs have been carried out in the past decade in speech recognition systems applications in helicopters, notably by the U.S. Army Avionics Research and Development Activity (AVRADA) and by the Royal Aerospace Establishment (RAE) in the UK. Work in France has included speech recognition in the Puma helicopter. There has also been much useful work in Canada. Results have been encouraging, and voice applications have included: control of communication radios, setting of navigation systems, and control of an automated target handover system.\n As in fighter applications, the overriding issue for voice in helicopters is the impact on pilot effectiveness. Encouraging results are reported for the AVRADA tests, although these represent only a feasibility demonstration in a test environment. Much remains to be done both in speech recognition and in overall speech technology in order to consistently achieve performance improvements in operational settings.\n Training for air traffic controllers (ATC) represents an excellent application for speech recognition systems. Many ATC training systems currently require a person to act as a \"pseudo-pilot\", engaging in a voice dialog with the trainee controller, which simulates the dialog that the controller would have to conduct with pilots in a real ATC situation. Speech recognition and synthesis techniques offer the potential to eliminate the need for a person to act as a pseudo-pilot, thus reducing training and support personnel. In theory, Air controller tasks are also characterized by highly structured speech as the primary output of the controller, hence reducing the difficulty of the speech recognition task should be possible. In practice, this is rarely the case. The FAA document 7110.65 details the phrases that should be used by air traffic controllers. While this document gives less than 150 examples of such phrases, the number of phrases supported by one of the simulation vendors speech recognition systems is in excess of 500,000.\n The USAF, USMC, US Army, US Navy, and FAA as well as a number of international ATC training organizations such as the Royal Australian Air Force and Civil Aviation Authorities in Italy, Brazil, and Canada are currently using ATC simulators with speech recognition from a number of different vendors.[citation needed]\n ASR is now commonplace in the field of telephony and is becoming more widespread in the field of computer gaming and simulation. In telephony systems, ASR is now being predominantly used in contact centers by integrating it with IVR systems. Despite the high level of integration with word processing in general personal computing, in the field of document production, ASR has not seen the expected increases in use.\n The improvement of mobile processor speeds has made speech recognition practical in smartphones. Speech is used mostly as a part of a user interface, for creating predefined or custom speech commands.\n People with disabilities can benefit from speech recognition programs. For individuals that are Deaf or Hard of Hearing, speech recognition software is used to automatically generate a closed-captioning of conversations such as discussions in conference rooms, classroom lectures, and/or religious services.[124]\n Students who are blind (see Blindness and education) or have very low vision can benefit from using the technology to convey words and then hear the computer recite them, as well as use a computer by commanding with their voice, instead of having to look at the screen and keyboard.[125]\n Students who are physically disabled have a Repetitive strain injury/other injuries to the upper extremities can be relieved from having to worry about handwriting, typing, or working with scribe on school assignments by using speech-to-text programs. They can also utilize speech recognition technology to enjoy searching the Internet or using a computer at home without having to physically operate a mouse and keyboard.[125]\n Speech recognition can allow students with learning disabilities to become better writers. By saying the words aloud, they can increase the fluidity of their writing, and be alleviated of concerns regarding spelling, punctuation, and other mechanics of writing.[126] Also, see Learning disability.\n The use of voice recognition software, in conjunction with a digital audio recorder and a personal computer running word-processing software has proven to be positive for restoring damaged short-term memory capacity, in stroke and craniotomy individuals.\n Speech recognition is also very useful for people who have difficulty using their hands, ranging from mild repetitive stress injuries to involve disabilities that preclude using conventional computer input devices. In fact, people who used the keyboard a lot and developed RSI became an urgent early market for speech recognition.[127][128] Speech recognition is used in deaf telephony, such as voicemail to text, relay services, and captioned telephone. Individuals with learning disabilities who have problems with thought-to-paper communication (essentially they think of an idea but it is processed incorrectly causing it to end up differently on paper) can possibly benefit from the software but the technology is not bug proof.[129] Also the whole idea of speak to text can be hard for intellectually disabled person's due to the fact that it is rare that anyone tries to learn the technology to teach the person with the disability.[130]\n This type of technology can help those with dyslexia but other disabilities are still in question. The effectiveness of the product is the problem that is hindering it from being effective. Although a kid may be able to say a word depending on how clear they say it the technology may think they are saying another word and input the wrong one. Giving them more work to fix, causing them to have to take more time with fixing the wrong word.[131]\n The performance of speech recognition systems is usually evaluated in terms of accuracy and speed.[136][137] Accuracy is usually rated with word error rate (WER), whereas speed is measured with the real time factor. Other measures of accuracy include Single Word Error Rate (SWER) and Command Success Rate (CSR).\n Speech recognition by machine is a very complex problem, however. Vocalizations vary in terms of accent, pronunciation, articulation, roughness, nasality, pitch, volume, and speed. Speech is distorted by a background noise and echoes, electrical characteristics. Accuracy of speech recognition may vary with the following:[138][citation needed]\n As mentioned earlier in this article, the accuracy of speech recognition may vary depending on the following factors:\n With discontinuous speech full sentences separated by silence are used, therefore it becomes easier to recognize the speech as well as with isolated speech. \nWith continuous speech naturally spoken sentences are used, therefore it becomes harder to recognize the speech, different from both isolated and discontinuous speech.\n Constraints are often represented by grammar. \n Speech recognition is a multi-leveled pattern recognition task.\n e.g. Known word pronunciations or legal word sequences, which can compensate for errors or uncertainties at a lower level;\n For telephone speech the sampling rate is 8000 samples per second; \n computed every 10\u00a0ms, with one 10\u00a0ms section called a frame;\n Analysis of four-step neural network approaches can be explained by further information. Sound is produced by air (or some other medium) vibration, which we register by ears, but machines by receivers. Basic sound creates a wave which has two descriptions: amplitude (how strong is it), and frequency (how often it vibrates per second).\nAccuracy can be computed with the help of word error rate (WER). Word error rate can be calculated by aligning the recognized word and referenced word using dynamic string alignment. The problem may occur while computing the word error rate due to the difference between the sequence lengths of the recognized word and referenced word.\n The formula to compute the word error rate (WER) is:\n \n\n\n\nW\nE\nR\n=\n\n\n\n(\ns\n+\nd\n+\ni\n)\n\nn\n\n\n\n\n{\\displaystyle WER={(s+d+i) \\over n}}\n\n\n where s is the number of substitutions, d is the number of deletions, i is the number of insertions, and n is the number of word references.\n While computing, the word recognition rate (WRR) is used. The formula is:\n where h is the number of correctly recognized words:\n Speech recognition can become a means of attack, theft, or accidental operation. For example, activation words like \"Alexa\" spoken in an audio or video broadcast can cause devices in homes and offices to start listening for input inappropriately, or possibly take an unwanted action.[140] Voice-controlled devices are also accessible to visitors to the building, or even those outside the building if they can be heard inside. Attackers may be able to gain access to personal information, like calendar, address book contents, private messages, and documents. They may also be able to impersonate the user to send messages or make online purchases.\n Two attacks have been demonstrated that use artificial sounds. One transmits ultrasound and attempt to send commands without nearby people noticing.[141] The other adds small, inaudible distortions to other speech or music that are specially crafted to confuse the specific speech recognition system into recognizing music as speech, or to make what sounds like one command to a human sound like a different command to the system.[142]\n Popular speech recognition conferences held each year or two include SpeechTEK and SpeechTEK Europe, ICASSP, Interspeech/Eurospeech, and the IEEE ASRU. Conferences in the field of natural language processing, such as ACL, NAACL, EMNLP, and HLT, are beginning to include papers on speech processing. Important journals include the IEEE Transactions on Speech and Audio Processing (later renamed IEEE Transactions on Audio, Speech and Language Processing and since Sept 2014 renamed IEEE/ACM Transactions on Audio, Speech and Language Processing\u2014after merging with an ACM publication), Computer Speech and Language, and Speech Communication.\n Books like \"Fundamentals of Speech Recognition\" by Lawrence Rabiner can be useful to acquire basic knowledge but may not be fully up to date (1993). Another good source can be \"Statistical Methods for Speech Recognition\" by Frederick Jelinek and \"Spoken Language Processing (2001)\" by Xuedong Huang etc., \"Computer Speech\", by Manfred R. Schroeder, second edition published in 2004, and \"Speech Processing: A Dynamic and Optimization-Oriented Approach\" published in 2003 by Li Deng and Doug O'Shaughnessey. The updated textbook Speech and Language Processing (2008) by Jurafsky and Martin presents the basics and the state of the art for ASR. Speaker recognition also uses the same features, most of the same front-end processing, and classification techniques as is done in speech recognition. A comprehensive textbook, \"Fundamentals of Speaker Recognition\" is an in depth source for up to date details on the theory and practice.[143] A good insight into the techniques used in the best modern systems can be gained by paying attention to government sponsored evaluations such as those organised by DARPA (the largest speech recognition-related project ongoing as of 2007 is the GALE project, which involves both speech recognition and translation components).\n A good and accessible introduction to speech recognition technology and its history is provided by the general audience book \"The Voice in the Machine. Building Computers That Understand Speech\" by Roberto Pieraccini (2012).\n The most recent book on speech recognition is Automatic Speech Recognition: A Deep Learning Approach (Publisher: Springer) written by Microsoft researchers D. Yu and L. Deng and published near the end of 2014, with highly mathematically oriented technical detail on how deep learning methods are derived and implemented in modern speech recognition systems based on DNNs and related deep learning methods.[84] A related book, published earlier in 2014, \"Deep Learning: Methods and Applications\" by L. Deng and D. Yu provides a less technical but more methodology-focused overview of DNN-based speech recognition during 2009\u20132014, placed within the more general context of deep learning applications including not only speech recognition but also image recognition, natural language processing, information retrieval, multimodal processing, and multitask learning.[80]\n In terms of freely available resources, Carnegie Mellon University's Sphinx toolkit is one place to start to both learn about speech recognition and to start experimenting. Another resource (free but copyrighted) is the HTK book (and the accompanying HTK toolkit). For more recent and state-of-the-art techniques, Kaldi toolkit can be used.[144] In 2017 Mozilla launched the open source project called Common Voice[145] to gather big database of voices that would help build free speech recognition project DeepSpeech (available free at GitHub),[146] using Google's open source platform TensorFlow.[147] When Mozilla redirected funding away from the project in 2020, it was forked by its original developers as Coqui STT[148] using the same open-source license.[149][150]\n Google Gboard supports speech recognition on all Android applications. It can be activated through the microphone icon.[151]\n The commercial cloud based speech recognition APIs are broadly available.\n For more software resources, see List of speech recognition software.\n",
        "doc_number": 99
    },
    {
        "url": "https://en.wikipedia.org/wiki/Machine_translation",
        "content": "\n Machine translation is use of computational techniques to translate text or speech from one language to another, including the contextual, idiomatic and pragmatic nuances of both languages.\n Early approaches were mostly rule-based or statistical. These methods have since been superseded by neural machine translation[1] and large language models.[2]\n The origins of machine translation can be traced back to the work of Al-Kindi, a ninth-century Arabic cryptographer who developed techniques for systemic language translation, including cryptanalysis, frequency analysis, and probability and statistics, which are used in modern machine translation.[3] The idea of machine translation later appeared in the 17th century. In 1629, Ren\u00e9 Descartes proposed a universal language, with equivalent ideas in different tongues sharing one symbol.[4]\n The idea of using digital computers for translation of natural languages was proposed as early as 1947 by England's A. D. Booth[5] and Warren Weaver at Rockefeller Foundation in the same year. \"The memorandum written by Warren Weaver in 1949 is perhaps the single most influential publication in the earliest days of machine translation.\"[6][7] Others followed. A demonstration was made in 1954 on the APEXC machine at Birkbeck College (University of London) of a rudimentary translation of English into French. Several papers on the topic were published at the time, and even articles in popular journals (for example an article by Cleave and Zacharov in the September 1955 issue of Wireless World).  A similar application, also pioneered at Birkbeck College at the time, was reading and composing Braille texts by computer.\n The first researcher in the field, Yehoshua Bar-Hillel, began his research at MIT (1951). A Georgetown University MT research team, led by Professor Michael Zarechnak, followed (1951) with a public demonstration of its Georgetown-IBM experiment system in 1954. MT research programs popped up in Japan[8][9] and Russia (1955), and the first MT conference was held in London (1956).[10][11]\n David G. Hays \"wrote about computer-assisted language processing as early as 1957\" and \"was project leader on computational linguistics\nat Rand from 1955 to 1968.\"[12]\n Researchers continued to join the field as the Association for Machine Translation and Computational Linguistics was formed in the U.S. (1962) and the National Academy of Sciences formed the Automatic Language Processing Advisory Committee (ALPAC) to study MT (1964). Real progress was much slower, however, and after the ALPAC report (1966), which found that the ten-year-long research had failed to fulfill expectations, funding was greatly reduced.[13] According to a 1972 report by the Director of Defense Research and Engineering (DDR&E), the feasibility of large-scale MT was reestablished by the success of the Logos MT system in translating military manuals into Vietnamese during that conflict.\n The French Textile Institute also used MT to translate abstracts from and into French, English, German and Spanish (1970); Brigham Young University started a project to translate Mormon texts by automated translation (1971).\n SYSTRAN, which \"pioneered the field under contracts from the U.S. government\"[14] in the 1960s, was used by Xerox to translate technical manuals (1978). Beginning in the late 1980s, as computational power increased and became less expensive, more interest was shown in statistical models for machine translation. MT became more popular after the advent of computers.[15] SYSTRAN's first implementation system was implemented in 1988 by the online service of the French Postal Service called Minitel.[16] Various computer based translation companies were also launched, including Trados (1984), which was the first to develop and market Translation Memory technology (1989), though this is not the same as MT. The first commercial MT system for Russian / English / German-Ukrainian was developed at Kharkov State University (1991).\n By 1998, \"for as little as $29.95\" one could \"buy a program for translating in one direction between English and a major European language of\nyour choice\" to run on a PC.[14]\n MT on the web started with SYSTRAN offering free translation of small texts (1996) and then providing this via AltaVista Babelfish,[14] which racked up 500,000 requests a day (1997).[17] The second free translation service on the web was Lernout & Hauspie's GlobaLink.[14] Atlantic Magazine wrote in 1998 that \"Systran's Babelfish and GlobaLink's Comprende\" handled\n\"Don't bank on it\" with a \"competent performance.\"[18]\n Franz Josef Och (the future head of Translation Development AT Google) won DARPA's speed MT competition (2003).[19] More innovations during this time included MOSES, the open-source statistical MT engine (2007), a text/SMS translation service for mobiles in Japan (2008), and a mobile phone with built-in speech-to-speech translation functionality for English, Japanese and Chinese (2009). In 2012, Google announced that Google Translate translates roughly enough text to fill 1 million books in one day.\n Before the advent of deep learning methods, statistical methods required a lot of rules accompanied by morphological, syntactic, and semantic annotations.\n The rule-based machine translation approach was used mostly in the creation of dictionaries and grammar programs. Its biggest downfall was that everything had to be made explicit: orthographical variation and erroneous input must be made part of the source language analyser in order to cope with it, and lexical selection rules must be written for all instances of ambiguity.\n Transfer-based machine translation was similar to interlingual machine translation in that it created a translation from an intermediate representation that simulated the meaning of the original sentence. Unlike interlingual MT, it depended partially on the language pair involved in the translation.\n Interlingual machine translation was one instance of rule-based machine-translation approaches.  In this approach, the source language, i.e. the text to be translated, was transformed into an interlingual language, i.e. a \"language neutral\" representation that is independent of any language. The target language was then generated out of the interlingua. The only interlingual machine translation system that was made operational at the commercial level was the KANT system (Nyberg and Mitamura, 1992), which was designed to translate Caterpillar Technical English (CTE) into other languages.\n Machine translation used a method based on dictionary entries, which means that the words were translated as they are by a dictionary.\n Statistical machine translation tried to generate translations using statistical methods based on bilingual text corpora, such as the Canadian Hansard corpus, the English-French record of the Canadian parliament and EUROPARL, the record of the European Parliament. Where such corpora were available, good results were achieved translating similar texts, but such corpora were rare for many language pairs. The first statistical machine translation software was CANDIDE from IBM. In 2005, Google improved its internal translation capabilities by using approximately 200 billion words from United Nations materials to train their system; translation accuracy improved.[20]\n SMT's biggest downfall included it being dependent upon huge amounts of parallel texts, its problems with morphology-rich languages (especially with translating into such languages), and its inability to correct singleton errors.\n Some work has been done in the utilization of multiparallel corpora, that is a body of text that has been translated into 3 or more languages. Using these methods, a text that has been translated into 2 or more languages may be utilized in combination to provide a more accurate translation into a third language compared with if just one of those source languages were used alone.[21][22][23]\n A deep learning-based approach to MT, neural machine translation has made rapid progress in recent years. However, the current consensus is that the so-called human parity achieved is not real, being based wholly on limited domains, language pairs, and certain test benchmarks[24] i.e., it lacks statistical significance power.[25]\n Translations by neural MT tools like DeepL Translator, which is thought to usually deliver the best machine translation results as of 2022, typically still need post-editing by a human.[26][27][28]\n Instead of training specialized translation models on parallel datasets, one can also directly prompt generative large language models like GPT to translate a text.[29][30][31] This approach is considered promising,[32] but is still more resource-intensive than specialized translation models.\n Studies using human evaluation (e.g. by professional literary translators or human readers) have systematically identified various issues with the latest advanced MT outputs.[31] Common issues include the translation of ambiguous parts whose correct translation requires common sense-like semantic language processing or context.[31] There can also be errors in the source texts, missing high-quality training data and the severity of frequency of several types of problems may not get reduced with techniques used to date, requiring some level of human active participation.\n Word-sense disambiguation concerns finding a suitable translation when a word can have more than one meaning. The problem was first raised in the 1950s by Yehoshua Bar-Hillel.[33] He pointed out that without a \"universal encyclopedia\", a machine would never be able to distinguish between the two meanings of a word.[34] Today there are numerous approaches designed to overcome this problem. They can be approximately divided into \"shallow\" approaches and \"deep\" approaches.\n Shallow approaches assume no knowledge of the text. They simply apply statistical methods to the words surrounding the ambiguous word. Deep approaches presume a comprehensive knowledge of the word. So far, shallow approaches have been more successful.[35]\n Claude Piron, a long-time translator for the United Nations and the World Health Organization, wrote that machine translation, at its best, automates the easier part of a translator's job; the harder and more time-consuming part usually involves doing extensive research to resolve ambiguities in the source text, which the grammatical and lexical exigencies of the target language require to be resolved:\n Why does a translator need a whole workday to translate five pages, and not an hour or two? ..... About 90% of an average text corresponds to these simple conditions.  But unfortunately, there's the other 10%.  It's that part that requires six [more] hours of work.  There are ambiguities one has to resolve.  For instance, the author of the source text, an Australian physician, cited the example of an epidemic which was declared during World War II in a \"Japanese prisoners of war camp\".  Was he talking about an American camp with Japanese prisoners or a Japanese camp with American prisoners?  The English has two senses.  It's necessary therefore to do research, maybe to the extent of a phone call to Australia.[36]\n The ideal deep approach would require the translation software to do all the research necessary for this kind of disambiguation on its own; but this would require a higher degree of AI than has yet been attained.  A shallow approach which simply guessed at the sense of the ambiguous English phrase that Piron mentions (based, perhaps, on which kind of prisoner-of-war camp is more often mentioned in a given corpus) would have a reasonable chance of guessing wrong fairly often.  A shallow approach that involves \"ask the user about each ambiguity\" would, by Piron's estimate, only automate about 25% of a professional translator's job, leaving the harder 75% still to be done by a human.\n One of the major pitfalls of MT is its inability to translate non-standard language with the same accuracy as standard language. Heuristic or statistical based MT takes input from various sources in standard form of a language. Rule-based translation, by nature, does not include common non-standard usages. This causes errors in translation from a vernacular source or into colloquial language. Limitations on translation from casual speech present issues in the use of machine translation in mobile devices.\n In information extraction, named entities, in a narrow sense, refer to concrete or abstract entities in the real world such as people, organizations, companies, and places that have a proper name: George Washington, Chicago, Microsoft.  It also refers to expressions of time, space and quantity such as 1 July 2011, $500.\n In the sentence \"Smith is the president of Fabrionix\" both Smith and Fabrionix are named entities, and can be further qualified via first name or other information; \"president\" is not, since Smith could have earlier held another position at Fabrionix, e.g. Vice President.\nThe term rigid designator is what defines these usages for analysis in statistical machine translation.\n Named entities must first be identified in the text; if not, they may be erroneously translated as common nouns, which would most likely not affect the BLEU rating of the translation but would change the text's human readability.[37] They may be omitted from the output translation, which would also have implications for the text's readability and message.\n Transliteration includes finding the letters in the target language that most closely correspond to the name in the source language.  This, however, has been cited as sometimes worsening the quality of translation.[38] For \"Southern California\" the first word should be translated directly, while the second word should be transliterated.  Machines often transliterate both because they treated them as one entity.  Words like these are hard for machine translators, even those with a transliteration component, to process.\n Use of a \"do-not-translate\" list, which has the same end goal \u2013 transliteration as opposed to translation.[39]  still relies on correct identification of named entities.\n A third approach is a class-based model. Named entities are replaced with a token to represent their \"class\"; \"Ted\"  and \"Erica\" would both be replaced with \"person\" class token. Then the statistical distribution and use of person names, in general, can be analyzed instead of looking at the distributions of \"Ted\" and \"Erica\" individually, so that the probability of a given name in a specific language will not affect the assigned probability of a translation. A study by Stanford on improving this area of translation gives the examples that different probabilities will be assigned to \"David is going for a walk\" and \"Ankit is going for a walk\" for English as a target language due to the different number of occurrences for each name in the training data. A frustrating outcome of the same study by Stanford (and other attempts to improve named recognition translation) is that many times, a decrease in the BLEU scores for translation will result from the inclusion of methods for named entity translation.[39]\n While no system provides the ideal of fully automatic high-quality machine translation of unrestricted text, many fully automated systems produce reasonable output.[40][41][42] The quality of machine translation is substantially improved if the domain is restricted and controlled.[43] This enables using machine translation as a tool to speed up and simplify translations, as well as producing flawed but useful low-cost or ad-hoc translations.\n Machine translation applications have also been released for most mobile devices, including mobile telephones, pocket PCs, PDAs, etc. Due to their portability, such instruments have come to be designated as mobile translation tools enabling mobile business networking between partners speaking different languages, or facilitating both foreign language learning and unaccompanied traveling to foreign countries without the need of the intermediation of a human translator.\n For example, the Google Translate app allows foreigners to quickly translate text in their surrounding via augmented reality using the smartphone camera that overlays the translated text onto the text.[44] It can also recognize speech and then translate it.[45]\n Despite their inherent limitations, MT programs are used around the world. Probably the largest institutional user is the European Commission. In 2012, with an aim to replace a rule-based MT by newer, statistical-based MT@EC, The European Commission contributed 3.072 million euros (via its ISA programme).[46]\n Machine translation has also been used for translating Wikipedia articles and could play a larger role in creating, updating, expanding, and generally improving articles in the future, especially as the MT capabilities may improve. There is a \"content translation tool\" which allows editors to more easily translate articles across several select languages.[47][48][49] English-language articles are thought to usually be more comprehensive and less biased than their non-translated equivalents in other languages.[50] As of 2022, English Wikipedia has over 6.5 million articles while the German and Swedish Wikipedias each only have over 2.5 million articles,[51] each often far less comprehensive.\n Following terrorist attacks in Western countries, including 9-11, the U.S. and its allies have been most interested in developing  Arabic machine translation programs, but also in translating Pashto and Dari languages.[citation needed] Within these languages, the focus is on key phrases and quick communication between military members and civilians through the use of mobile phone apps.[52] The Information Processing Technology Office in DARPA hosted programs like TIDES and Babylon translator. US Air Force has awarded a $1 million contract to develop a language translation technology.[53]\n The notable rise of social networking on the web in recent years has created yet another niche for the application of machine translation software \u2013 in utilities such as Facebook, or instant messaging clients such as Skype, Google Talk, MSN Messenger, etc. \u2013 allowing users speaking different languages to communicate with each other.\n Lineage W gained popularity in Japan because of its machine translation features allowing players from different countries to communicate.[54]\n Despite being labelled as an unworthy competitor to human translation in 1966 by the Automated Language Processing Advisory Committee put together by the United States government,[55] the quality of machine translation has now been improved to such levels that its application in online collaboration and in the medical field are being investigated. The application of this technology in medical settings where human translators are absent is another topic of research, but difficulties arise due to the importance of accurate translations in medical diagnoses.[56]\n Researchers caution that the use of machine translation in medicine could risk mistranslations that can be dangerous in critical situations.[57][58] Machine translation can make it easier for doctors to communicate with their patients in day to day activities, but it is recommended to only use machine translation when there is no other alternative, and that translated medical texts should be reviewed by human translators for accuracy.[59][60]\n Legal language poses a significant challenge to machine translation tools due to its precise nature and atypical use of normal words. For this reason, specialized algorithms have been developed for use in legal contexts.[61] Due to the risk of mistranslations arising from machine translators, researchers recommend that machine translations should be reviewed by human translators for accuracy, and some courts prohibit its use in formal proceedings.[62]\n The use of machine translation in law has raised concerns about translation errors and client confidentiality. Lawyers who use free translation tools such as Google Translate may accidentally violate client confidentiality by exposing private information to the providers of the translation tools.[61] In addition, there have been arguments that consent for a police search that is obtained with machine translation is invalid, with different courts issuing different verdicts over whether or not these arguments are valid.[57]\n The advancements in convolutional neural networks in recent years and in low resource machine translation (when only a very limited amount of data and examples are available for training) enabled machine translation for ancient languages, such as Akkadian and its dialects Babylonian and Assyrian.[63]\n There are many factors that affect how machine translation systems are evaluated. These factors include the intended use of the translation, the nature of the machine translation software, and the nature of the translation process.\n Different programs may work well for different purposes. For example, statistical machine translation (SMT) typically outperforms example-based machine translation (EBMT), but researchers found that when evaluating English to French translation, EBMT performs better.[64] The same concept applies for technical documents, which can be more easily translated by SMT because of their formal language.\n In certain applications, however, e.g., product descriptions written in a controlled language, a dictionary-based machine-translation system has produced satisfactory translations that require no human intervention save for quality inspection.[65]\n There are various means for evaluating the output quality of machine translation systems. The oldest is the use of human judges[66] to assess a translation's quality. Even though human evaluation is time-consuming, it is still the most reliable method to compare different systems such as rule-based and statistical systems.[67] Automated means of evaluation include BLEU, NIST, METEOR, and LEPOR.[68]\n Relying exclusively on unedited machine translation ignores the fact that communication in human language is context-embedded and that it takes a person to comprehend the context of the original text with a reasonable degree of probability. It is certainly true that even purely human-generated translations are prone to error. Therefore, to ensure that a machine-generated translation will be useful to a human being and that publishable-quality translation is achieved, such translations must be reviewed and edited by a human.[69] The late Claude Piron wrote that machine translation, at its best, automates the easier part of a translator's job; the harder and more time-consuming part usually involves doing extensive research to resolve ambiguities in the source text, which the grammatical and lexical exigencies of the target language require to be resolved. Such research is a necessary prelude to the pre-editing necessary in order to provide input for machine-translation software such that the output will not be meaningless.[70]\n In addition to disambiguation problems, decreased accuracy can occur due to varying levels of training data for machine translating programs. Both example-based and statistical machine translation rely on a vast array of real example sentences as a base for translation, and when too many or too few sentences are analyzed accuracy is jeopardized. Researchers found that when a program is trained on 203,529 sentence pairings, accuracy actually decreases.[64] The optimal level of training data seems to be just over 100,000 sentences, possibly because as training data increases, the number of possible sentences increases, making it harder to find an exact translation match.\n Flaws in machine translation have been noted for their entertainment value. Two videos uploaded to YouTube in April 2017 involve two Japanese hiragana characters \u3048\u3050 (e and gu) being repeatedly pasted into Google Translate, with the resulting translations quickly degrading into nonsensical phrases such as \"DECEARING EGG\" and \"Deep-sea squeeze trees\", which are then read in increasingly absurd voices;[71][72] the full-length version of the video currently has 6.9 million views as of March 2022.[update][73]\n In the early 2000s, options for machine translation between spoken and signed languages were severely limited. It was a common belief that deaf individuals could use traditional translators. However, stress, intonation, pitch, and timing are conveyed much differently in spoken languages compared to signed languages. Therefore, a deaf individual may misinterpret or become confused about the meaning of written text that is based on a spoken language.[74]\n Researchers Zhao, et al. (2000), developed a prototype called TEAM (translation from English to ASL by machine) that completed English to American Sign Language (ASL) translations. The program would first analyze the syntactic, grammatical, and morphological aspects of the English text. Following this step, the program accessed a sign synthesizer, which acted as a dictionary for ASL. This synthesizer housed the process one must follow to complete ASL signs, as well as the meanings of these signs. Once the entire text is analyzed and the signs necessary to complete the translation are located in the synthesizer, a computer generated human appeared and would use ASL to sign the English text to the user.[74]\n Only works that are original are subject to copyright protection, so some scholars claim that machine translation results are not entitled to copyright protection because MT does not involve creativity.[75] The copyright at issue is for a derivative work; the author of the original work in the original language does not lose his rights when a work is translated: a translator must have permission to publish a translation.[citation needed]\n",
        "doc_number": 100
    },
    {
        "url": "https://en.wikipedia.org/wiki/Question_answering",
        "content": "Question answering (QA) is a computer science discipline within the fields of information retrieval and natural language processing (NLP) that is concerned with building systems that automatically answer questions that are posed by humans in a natural language.[1]\n A question-answering implementation, usually a computer program, may construct its answers by querying a structured database of knowledge or information, usually a knowledge base. More commonly, question-answering systems can pull answers from an unstructured collection of natural language documents.\n Some examples of natural language document collections used for question answering systems include:\n Question-answering research attempts to develop ways of answering a wide range of question types, including fact, list, definition, how, why, hypothetical, semantically constrained, and cross-lingual questions.\n Another way to categorize question-answering systems is by the technical approach used. There are a number of different types of QA systems, including \n Rule-based systems use a set of rules to determine the correct answer to a question. Statistical systems use statistical methods to find the most likely answer to a question. Hybrid systems use a combination of rule-based and statistical methods.\n Two early question answering systems were BASEBALL[4] and LUNAR.[5] BASEBALL answered questions about Major League Baseball over a period of one year[ambiguous]. LUNAR answered questions about the geological analysis of rocks returned by the Apollo Moon missions. Both question answering systems were very effective in their chosen domains. LUNAR was demonstrated at a lunar science convention in 1971 and it was able to answer 90% of the questions in its domain that were posed by people untrained on the system. Further restricted-domain question answering systems were developed in the following years. The common feature of all these systems is that they had a core database or knowledge system that was hand-written by experts of the chosen domain. The language abilities of BASEBALL and LUNAR used techniques similar to ELIZA and DOCTOR, the first chatterbot programs.\n SHRDLU was a successful question-answering program developed by Terry Winograd in the late 1960s and early 1970s. It simulated the operation of a robot in a toy world (the \"blocks world\"), and it offered the possibility of asking the robot questions about the state of the world. The strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program.\n In the 1970s, knowledge bases were developed that targeted narrower domains of knowledge. The question answering systems developed to interface with these expert systems produced more repeatable[clarification needed] and valid responses to questions within an area of knowledge. These expert systems closely resembled modern question answering systems except in their internal architecture. Expert systems rely heavily on expert-constructed and organized knowledge bases, whereas many modern question answering systems rely on statistical processing of a large, unstructured, natural language text corpus.\n The 1970s and 1980s saw the development of comprehensive theories in computational linguistics, which led to the development of ambitious projects in text comprehension and question answering. One example was the Unix Consultant (UC), developed by Robert Wilensky at U.C. Berkeley in the late 1980s. The system answered questions pertaining to the Unix operating system. It had a comprehensive, hand-crafted knowledge base of its domain, and it aimed at phrasing the answer to accommodate various types of users. Another project was LILOG, a text-understanding system that operated on the domain of tourism information in a German city. The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations, but they helped the development of theories on computational linguistics and reasoning.\n Specialized natural-language question answering systems have been developed, such as EAGLi for health and life scientists.[6]\n QA systems are used in a variety of applications, including \n As of 2001[update], question-answering systems typically included a question classifier module that determined the type of question and the type of answer.[7]\n Different types of question-answering systems employ different architectures. For example, modern open-domain question answering systems may use a retriever-reader architecture. The retriever is aimed at retrieving relevant documents related to a given question, while the reader is used to infer the answer from the retrieved documents. Systems such as GPT-3, T5,[8] and BART[9] use an end-to-end[jargon] architecture in which a transformer-based[jargon] architecture stores large-scale textual data in the underlying parameters. Such models can answer questions without accessing any external knowledge sources.\n Question answering is dependent on a good search corpus; without documents containing the answer, there is little any question answering system can do. Larger collections generally mean better question answering performance, unless the question domain is orthogonal to the collection. Data redundancy in massive collections, such as the web, means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents,[10] leading to two benefits:\n Some question answering systems rely heavily on automated reasoning.[11][12]\n In information retrieval, an open-domain question answering system tries to return an answer in response to the user's question. The returned answer is in the form of short texts rather than a list of relevant documents.[13] The system finds answers by using a combination of techniques from computational linguistics, information retrieval, and knowledge representation.\n The system takes a natural language question as an input rather than a set of keywords, for example: \"When is the national day of China?\" It then transforms this input sentence into a query in its logical form. Accepting natural language questions makes the system more user-friendly, but harder to implement, as there are a variety of question types and the system will have to identify the correct one in order to give a sensible answer. Assigning a question type to the question is a crucial task; the entire answer extraction process relies on finding the correct question type and hence the correct answer type.\n Keyword extraction is the first step in identifying the input question type.[14] In some cases, words clearly indicate the question type, e.g., \"Who\", \"Where\", \"When\", or \"How many\"\u2014these words might suggest to the system that the answers should be of type \"Person\", \"Location\", \"Date\", or \"Number\", respectively. POS (part-of-speech) tagging and syntactic parsing techniques can also determine the answer type. In the example above, the subject is \"Chinese National Day\", the predicate is \"is\" and the adverbial modifier is \"when\", therefore the answer type is \"Date\". Unfortunately, some interrogative words like \"Which\", \"What\", or \"How\" do not correspond to unambiguous answer types: Each can represent more than one type. In situations like this, other words in the question need to be considered. A lexical dictionary such as WordNet can be used for understanding the context.\n Once the system identifies the question type, it uses an information retrieval system to find a set of documents that contain the correct keywords. A tagger and NP/Verb Group chunker can verify whether the correct entities and relations are mentioned in the found documents. For questions such as \"Who\" or \"Where\", a named-entity recogniser finds relevant \"Person\" and \"Location\" names from the retrieved documents. Only the relevant paragraphs are selected for ranking.[clarification needed]\n A vector space model can classify the candidate answers. Check[who?] if the answer is of the correct type as determined in the question type analysis stage. An inference technique can validate the candidate answers. A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate\u2014the more and the closer the better. The answer is then translated by parsing into a compact and meaningful representation. In the previous example, the expected output answer is \"1st Oct.\"\n An open-source, math-aware, question answering system called MathQA, based on Ask Platypus and Wikidata, was published in 2018.[15] MathQA takes an English or Hindi natural language question as input and returns a mathematical formula retrieved from Wikidata as a succinct answer, translated into a computable form that allows the user to insert values for the variables. The system retrieves names and values of variables and common constants from Wikidata if those are available. It is claimed that the system outperforms a commercial computational mathematical knowledge engine on a test set.[15] MathQA is hosted by Wikimedia at https://mathqa.wmflabs.org/. In 2022, it was extended to answer 15 math question types.[16]\n MathQA methods need to combine natural and formula language. One possible approach is to perform supervised annotation via Entity Linking. The \"ARQMath Task\" at CLEF 2020[17] was launched to address the problem of linking newly posted questions from the platform Math Stack Exchange to existing ones that were already answered by the community. Providing hyperlinks to already answered, semantically related questions helps users to get answers earlier but is a challenging problem because semantic relatedness is not trivial.[18] The lab was motivated by the fact that 20% of mathematical queries in general-purpose search engines are expressed as well-formed questions.[19] The challenge contained two separate sub-tasks. Task 1: \"Answer retrieval\" matching old post answers to newly posed questions, and Task 2: \"Formula retrieval\" matching old post formulae to new questions. Starting with the domain of mathematics, which involves formula language, the goal is to later extend the task to other domains (e.g., STEM disciplines, such as chemistry, biology, etc.), which employ other types of special notation (e.g., chemical formulae).[17][18]\n The inverse of mathematical question answering\u2014mathematical question generation\u2014has also been researched. The PhysWikiQuiz physics question generation and test engine retrieves mathematical formulae from Wikidata together with semantic information about their constituting identifiers (names and values of variables).[20] The formulae are then rearranged to generate a set of formula variants. Subsequently, the variables are substituted with random values to generate a large number of different questions suitable for individual student tests. PhysWikiquiz is hosted by Wikimedia at https://physwikiquiz.wmflabs.org/.\n Question answering systems have been extended in recent[may be outdated as of April 2023] years to encompass additional domains of knowledge[21] For example, systems have been developed to automatically answer temporal and geospatial questions, questions of definition and terminology, biographical questions, multilingual questions, and questions about the content of audio, images,[22] and video.[23] Current question answering research topics include:\n In 2011, Watson, a question answering computer system developed by IBM, competed in two exhibition matches of Jeopardy! against Brad Rutter and Ken Jennings, winning by a significant margin.[32]\nFacebook Research made their DrQA system[33] available under an open source license. This system uses Wikipedia as knowledge source.[2] The open source framework Haystack by deepset combines open-domain question answering with generative question answering and supports the domain adaptation[clarification needed] of the underlying[clarification needed] language models for industry use cases[vague].\n[34][35]\n Large Language Models (LLMs)[36] like GPT-4[37], Gemini[38] are examples of successful QA systems that are enabling more sophisticated understanding and generation of text. When coupled with Multimodal[39] QA Systems, which can process and understand information from various modalities like text, images, and audio, LLMs significantly improve the capabilities of QA systems.\n",
        "doc_number": 101
    },
    {
        "url": "https://en.wikipedia.org/wiki/Text_summarization",
        "content": "Automatic summarization is the process of shortening a set of data computationally, to create a subset (a summary) that represents the most important or relevant information within the original content. Artificial intelligence algorithms are commonly developed and employed to achieve this, specialized for different types of data.\n Text summarization is usually implemented by natural language processing methods, designed to locate the most informative sentences in a given document.[1] On the other hand, visual content can be summarized using computer vision algorithms. Image summarization is the subject of ongoing research; existing approaches typically attempt to display the most representative images from a given image collection, or generate a video that only includes the most important content from the entire collection.[2][3][4] Video summarization algorithms identify and extract from the original video content the most important frames (key-frames), and/or the most important video segments (key-shots), normally in a temporally ordered fashion.[5][6][7][8] Video summaries simply retain a carefully selected subset of the original video frames and, therefore, are not identical to the output of video synopsis algorithms, where new video frames are being synthesized based on the original video content.\n In 2022 Google Docs released an automatic summarization feature.[9]\n There are two general approaches to automatic summarization: extraction and abstraction.\n Here, content is extracted from the original data, but the extracted content is not modified in any way. Examples of extracted content include key-phrases that can be used to \"tag\" or index a text document, or key sentences (including headings) that collectively comprise an abstract, and representative images or video segments, as stated above. For text, extraction is analogous to the process of skimming, where the summary (if available), headings and subheadings, figures, the first and last paragraphs of a section, and optionally the first and last sentences in a paragraph are read before one chooses to read the entire document in detail.[10] Other examples of extraction that include key sequences of text in terms of clinical relevance (including patient/problem, intervention, and outcome).[11]\n Abstractive summarization methods generate new text that did not exist in the original text.[12] This has been applied mainly for text. Abstractive methods build an internal semantic representation of the original content (often called a language model), and then use this representation to create a summary that is closer to what a human might express. Abstraction may transform the extracted content by paraphrasing sections of the source document, to condense a text more strongly than extraction. Such transformation, however, is computationally much more challenging than extraction, involving both natural language processing and often a deep understanding of the domain of the original text in cases where the original document relates to a special field of knowledge. \"Paraphrasing\" is even more difficult to apply to images and videos, which is why most summarization systems are extractive.\n Approaches aimed at higher summarization quality rely on combined software and human effort. In Machine Aided Human Summarization, extractive techniques highlight candidate passages for inclusion (to which the human adds or removes text). In Human Aided Machine Summarization, a human post-processes software output, in the same way that one edits the output of automatic translation by Google Translate.\n There are broadly two types of extractive summarization tasks depending on what the summarization program focuses on. The first is generic summarization, which focuses on obtaining a generic summary or abstract of the collection (whether documents, or sets of images, or videos, news stories etc.). The second is  query relevant summarization, sometimes called query-based summarization, which summarizes objects specific to a query. Summarization systems are able to create both query relevant text summaries and generic machine-generated summaries depending on what the user needs.\n An example of a summarization problem is document summarization, which attempts to automatically produce an abstract from a given document. Sometimes one might be interested in generating a summary from a single source document, while others can use multiple source documents (for example, a cluster of articles on the same topic). This problem is called multi-document summarization. A related application is summarizing news articles. Imagine a system, which automatically pulls together news articles on a given topic (from the web), and concisely represents the latest news as a summary.\n Image collection summarization is another application example of automatic summarization. It consists in selecting a representative set of images from a larger set of images.[13] A summary in this context is useful to show the most representative images of results in an image collection exploration system. Video summarization is a related domain, where the system automatically creates a trailer of a long video. This also has applications in consumer or personal videos, where one might want to skip the boring or repetitive actions. Similarly, in surveillance videos, one would want to extract important and suspicious activity, while ignoring all the boring and redundant frames captured.\n At a very high level, summarization algorithms try to find subsets of objects (like set of sentences, or a set of images), which cover information of the entire set. This is also called the core-set. These algorithms model notions like diversity, coverage, information and representativeness of the summary. Query based summarization techniques, additionally model for relevance of the summary with the query. Some techniques and algorithms which naturally model summarization problems are TextRank and PageRank, Submodular set function, Determinantal point process, maximal marginal relevance (MMR) etc.\n The task is the following. You are given a piece of text, such as a journal article, and you must produce a list of keywords or key[phrase]s that capture the primary topics discussed in the text.[14] In the case of research articles, many authors provide manually assigned keywords, but most text lacks pre-existing keyphrases. For example, news articles rarely have keyphrases attached, but it would be useful to be able to automatically do so for a number of applications discussed below.\nConsider the example text from a news article:\n A keyphrase extractor might select \"Army Corps of Engineers\", \"President Bush\", \"New Orleans\", and \"defective flood-control pumps\" as keyphrases. These are pulled directly from the text. In contrast, an abstractive keyphrase system would somehow internalize the content and generate keyphrases that do not appear in the text, but more closely resemble what a human might produce, such as \"political negligence\" or \"inadequate protection from floods\". Abstraction requires a deep understanding of the text, which makes it difficult for a computer system.\nKeyphrases have many applications. They can enable document browsing by providing a short summary, improve information retrieval (if documents have keyphrases assigned, a user could search by keyphrase to produce more reliable hits than a full-text search), and be employed in generating index entries for a large text corpus.\n Depending on the different literature and the definition of key terms, words or phrases, keyword extraction is a highly related theme.\n Beginning with the work of Turney,[15] many researchers have approached keyphrase extraction as a supervised machine learning problem.\nGiven a document, we construct an example for each unigram, bigram, and trigram found in the text (though other text units are also possible, as discussed below). We then compute various features describing each example (e.g., does the phrase begin with an upper-case letter?). We assume there are known keyphrases available for a set of training documents. Using the known keyphrases, we can assign positive or negative labels to the examples. Then we learn a classifier that can discriminate between positive and negative examples as a function of the features. Some classifiers make a binary classification for a test example, while others assign a probability of being a keyphrase. For instance, in the above text, we might learn a rule that says phrases with initial capital letters are likely to be keyphrases.\nAfter training a learner, we can select keyphrases for test documents in the following manner. We apply the same example-generation strategy to the test documents, then run each example through the learner. We can determine the keyphrases by looking at binary classification decisions or probabilities returned from our learned model. If probabilities are given, a threshold is used to select the keyphrases.\nKeyphrase extractors are generally evaluated using precision and recall. Precision measures how\nmany of the proposed keyphrases are actually correct. Recall measures how many of the true\nkeyphrases your system proposed. The two measures can be combined in an F-score, which is the\nharmonic mean of the two (F\u00a0=\u00a02PR/(P\u00a0+\u00a0R) ). Matches between the proposed keyphrases and the known keyphrases can be checked after stemming or applying some other text normalization.\n Designing a supervised keyphrase extraction system involves deciding on several choices (some of these apply to unsupervised, too). The first choice is exactly how to generate examples. Turney and others have used all possible unigrams, bigrams, and trigrams without intervening punctuation and after removing stopwords. Hulth showed that you can get some improvement by selecting examples to be sequences of tokens that match certain patterns of part-of-speech tags. Ideally, the mechanism for generating examples produces all the known labeled keyphrases as candidates, though this is often not the case. For example, if we use only unigrams, bigrams, and trigrams, then we will never be able to extract a known keyphrase containing four words. Thus, recall may suffer. However, generating too many examples can also lead to low precision.\n We also need to create features that describe the examples and are informative enough to allow a learning algorithm to discriminate keyphrases from non- keyphrases. Typically features involve various term frequencies (how many times a phrase appears in the current text or in a larger corpus), the length of the example, relative position of the first occurrence, various Boolean syntactic features (e.g., contains all caps), etc. The Turney paper used about 12 such features. Hulth uses a reduced set of features, which were found most successful in the KEA (Keyphrase Extraction Algorithm) work derived from Turney's seminal paper.\n In the end, the system will need to return a list of keyphrases for a test document, so we need to have a way to limit the number. Ensemble methods (i.e., using votes from several classifiers) have been used to produce numeric scores that can be thresholded to provide a user-provided number of keyphrases. This is the technique used by Turney with C4.5 decision trees. Hulth used a single binary classifier so the learning algorithm implicitly determines the appropriate number.\n Once examples and features are created, we need a way to learn to predict keyphrases. Virtually any supervised learning algorithm could be used, such as decision trees, Naive Bayes, and rule induction. In the case of Turney's GenEx algorithm, a genetic algorithm is used to learn parameters for a domain-specific keyphrase extraction algorithm. The extractor follows a series of heuristics to identify keyphrases. The genetic algorithm optimizes parameters for these heuristics with respect to performance on training documents with known key phrases.\n Another keyphrase extraction algorithm is TextRank. While supervised methods have some nice properties, like being able to produce interpretable rules for what features characterize a keyphrase, they also require a large amount of training data. Many documents with known keyphrases are needed. Furthermore, training on a specific domain tends to customize the extraction process to that domain, so the resulting classifier is not necessarily portable, as some of Turney's results demonstrate.\nUnsupervised keyphrase extraction removes the need for training data. It approaches the problem from a different angle. Instead of trying to learn explicit features that characterize keyphrases, the TextRank algorithm[16] exploits the structure of the text itself to determine keyphrases that appear \"central\" to the text in the same way that PageRank selects important Web pages. Recall this is based on the notion of \"prestige\" or \"recommendation\" from social networks. In this way, TextRank does not rely on any previous training data at all, but rather can be run on any arbitrary piece of text, and it can produce output simply based on the text's intrinsic properties. Thus the algorithm is easily portable to new domains and languages.\n TextRank is a general purpose graph-based ranking algorithm for NLP. Essentially, it runs PageRank on a graph specially designed for a particular NLP task. For keyphrase extraction, it builds a graph using some set of text units as vertices. Edges are based on some measure of semantic or lexical similarity between the text unit vertices. Unlike PageRank, the edges are typically undirected and can be weighted to reflect a degree of similarity. Once the graph is constructed, it is used to form a stochastic matrix, combined with a damping factor (as in the \"random surfer model\"), and the ranking over vertices is obtained by finding the eigenvector corresponding to eigenvalue 1 (i.e., the stationary distribution of the random walk on the graph).\n The vertices should correspond to what we want to rank. Potentially, we could do something similar to the supervised methods and create a vertex for each unigram, bigram, trigram, etc. However, to keep the graph small, the authors decide to rank individual unigrams in a first step, and then include a second step that merges highly ranked adjacent unigrams to form multi-word phrases. This has a nice side effect of allowing us to produce keyphrases of arbitrary length. For example, if we rank unigrams and find that \"advanced\", \"natural\", \"language\", and \"processing\" all get high ranks, then we would look at the original text and see that these words appear consecutively and create a final keyphrase using all four together. Note that the unigrams placed in the graph can be filtered by part of speech. The authors found that adjectives and nouns were the best to include. Thus, some linguistic knowledge comes into play in this step.\n Edges are created based on word co-occurrence in this application of TextRank. Two vertices are connected by an edge if the unigrams appear within a window of size N in the original text. N is typically around 2\u201310. Thus, \"natural\" and \"language\" might be linked in a text about NLP. \"Natural\" and \"processing\" would also be linked because they would both appear in the same string of N words. These edges build on the notion of \"text cohesion\" and the idea that words that appear near each other are likely related in a meaningful way and \"recommend\" each other to the reader.\n Since this method simply ranks the individual vertices, we need a way to threshold or produce a limited number of keyphrases. The technique chosen is to set a count T to be a user-specified fraction of the total number of vertices in the graph. Then the top T vertices/unigrams are selected based on their stationary probabilities. A post- processing step is then applied to merge adjacent instances of these T unigrams. As a result, potentially more or less than T final keyphrases will be produced, but the number should be roughly proportional to the length of the original text.\n It is not initially clear why applying PageRank to a co-occurrence graph would produce useful keyphrases. One way to think about it is the following. A word that appears multiple times throughout a text may have many different co-occurring neighbors. For example, in a text about machine learning, the unigram \"learning\" might co-occur with \"machine\", \"supervised\", \"un-supervised\", and \"semi-supervised\" in four different sentences. Thus, the \"learning\" vertex would be a central \"hub\" that connects to these other modifying words. Running PageRank/TextRank on the graph is likely to rank \"learning\" highly. Similarly, if the text contains the phrase \"supervised classification\", then there would be an edge between \"supervised\" and \"classification\". If \"classification\" appears several other places and thus has many neighbors, its importance would contribute to the importance of \"supervised\". If it ends up with a high rank, it will be selected as one of the top T unigrams, along with \"learning\" and probably \"classification\". In the final post-processing step, we would then end up with keyphrases \"supervised learning\" and \"supervised classification\".\n In short, the co-occurrence graph will contain densely connected regions for terms that appear often and in different contexts. A random walk on this graph will have a stationary distribution that assigns large probabilities to the terms in the centers of the clusters. This is similar to densely connected Web pages getting ranked highly by PageRank. This approach has also been used in document summarization, considered below.\n Like keyphrase extraction, document summarization aims to identify the essence of a text. The only real difference is that now we are dealing with larger text units\u2014whole sentences instead of words and phrases.\n Supervised text summarization is very much like supervised keyphrase extraction. Basically, if you have a collection of documents and human-generated summaries for them, you can learn features of sentences that make them good candidates for inclusion in the summary. Features might include the position in the document (i.e., the first few sentences are probably important), the number of words in the sentence, etc. The main difficulty in supervised extractive summarization is that the known summaries must be manually created by extracting sentences so the sentences in an original training document can be labeled as \"in summary\" or \"not in summary\". This is not typically how people create summaries, so simply using journal abstracts or existing summaries is usually not sufficient. The sentences in these summaries do not necessarily match up with sentences in the original text, so it would be difficult to assign labels to examples for training. Note, however, that these natural summaries can still be used for evaluation purposes, since ROUGE-1 evaluation only considers unigrams.\n During the DUC 2001 and 2002 evaluation workshops, TNO developed a sentence extraction system for multi-document summarization in the news domain. The system was based on a hybrid system using a Naive Bayes classifier and statistical language models for modeling salience. Although the system exhibited good results, the researchers wanted to explore the effectiveness of a maximum entropy (ME) classifier for the meeting summarization task, as ME is known to be robust against feature dependencies. Maximum entropy has also been applied successfully for summarization in the broadcast news domain.\n A promising approach is adaptive document/text summarization.[17] It involves first recognizing the text genre and then applying summarization algorithms optimized for this genre. Such software has been created.[18]\n The unsupervised approach to summarization is also quite similar in spirit to unsupervised keyphrase extraction and gets around the issue of costly training data. Some unsupervised summarization approaches are based on finding a \"centroid\" sentence, which is the mean word vector of all the sentences in the document. Then the sentences can be ranked with regard to their similarity to this centroid sentence.\n A more principled way to estimate sentence importance is using random walks and eigenvector centrality. LexRank[19] is an algorithm essentially identical to TextRank, and both use this approach for document summarization. The two methods were developed by different groups at the same time, and LexRank simply focused on summarization, but could just as easily be used for keyphrase extraction or any other NLP ranking task.\n In both LexRank and TextRank, a graph is constructed by creating a vertex for each sentence in the document.\n The edges between sentences are based on some form of semantic similarity or content overlap. While LexRank uses cosine similarity of TF-IDF vectors, TextRank uses a very similar measure based on the number of words two sentences have in common (normalized by the sentences' lengths). The LexRank paper explored using unweighted edges after applying a threshold to the cosine values, but also experimented with using edges with weights equal to the similarity score. TextRank uses continuous similarity scores as weights.\n In both algorithms, the sentences are ranked by applying PageRank to the resulting graph. A summary is formed by combining the top ranking sentences, using a threshold or length cutoff to limit the size of the summary.\n It is worth noting that TextRank was applied to summarization exactly as described here, while LexRank was used as part of a larger summarization system (MEAD) that combines the LexRank score (stationary probability) with other features like sentence position and length using a linear combination with either user-specified or automatically tuned weights. In this case, some training documents might be needed, though the TextRank results show the additional features are not absolutely necessary.\n Unlike TextRank, LexRank has been applied to multi-document summarization.\n Multi-document summarization is an automatic procedure aimed at extraction of information from multiple texts written about the same topic. Resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the news aggregators performing the next step down the road of coping with information overload. Multi-document summarization may also be done in response to a question.[20][11]\n Multi-document summarization creates information reports that are both concise and comprehensive. With different opinions being put together and outlined, every topic is described from multiple perspectives within a single document. While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required. Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased. [dubious \u2013 discuss]\n Multi-document extractive summarization faces a problem of redundancy. Ideally, we want to extract sentences that are both \"central\" (i.e., contain the main ideas) and \"diverse\" (i.e., they differ from one another). For example, in a set of news articles about some event, each article is likely to have many similar sentences. To address this issue, LexRank applies a heuristic post-processing step that adds sentences in rank order, but discards sentences that are too similar to ones already in the summary. This method is called Cross-Sentence Information Subsumption (CSIS). These methods work based on the idea that sentences \"recommend\" other similar sentences to the reader. Thus, if one sentence is very similar to many others, it will likely be a sentence of great importance. Its importance also stems from the importance of the sentences \"recommending\" it. Thus, to get ranked highly and placed in a summary, a sentence must be similar to many sentences that are in turn also similar to many other sentences. This makes intuitive sense and allows the algorithms to be applied to an arbitrary new text. The methods are domain-independent and easily portable. One could imagine the features indicating important sentences in the news domain might vary considerably from the biomedical domain. However, the unsupervised \"recommendation\"-based approach applies to any domain.\n A related method is Maximal Marginal Relevance (MMR),[21] which uses a general-purpose graph-based ranking algorithm like Page/Lex/TextRank that handles both \"centrality\" and \"diversity\" in a unified mathematical framework based on absorbing Markov chain random walks (a random walk where certain states end the walk). The algorithm is called GRASSHOPPER.[22] In addition to explicitly promoting diversity during the ranking process, GRASSHOPPER incorporates a prior ranking (based on sentence position in the case of summarization).\n The state of the art results for multi-document summarization are obtained using mixtures of submodular functions. These methods have achieved the state of the art results for Document Summarization Corpora, DUC 04 - 07.[23] Similar results were achieved with the use of determinantal point processes (which are a special case of submodular functions) for DUC-04.[24]\n A new method for multi-lingual multi-document summarization that avoids redundancy generates ideograms to represent the meaning of each sentence in each document, then evaluates similarity by comparing ideogram shape and position. It does not use word frequency, training or preprocessing. It uses two user-supplied parameters: equivalence (when are two sentences to be considered equivalent?) and relevance (how long is the desired summary?).\n The idea of a submodular set function has recently emerged as a powerful modeling tool for various summarization problems. Submodular functions naturally model notions of coverage, information, representation and diversity. Moreover, several important combinatorial optimization problems occur as special instances of submodular optimization. For example, the set cover problem is a special case of submodular optimization, since the set cover function is submodular. The set cover function attempts to find a subset of objects which cover a given set of concepts. For example, in document summarization, one would like the summary to cover all important and relevant concepts in the document. This is an instance of set cover. Similarly, the facility location problem is a special case of submodular functions. The Facility Location function also naturally models coverage and diversity. Another example of a submodular optimization problem is using a determinantal point process to model diversity. Similarly, the Maximum-Marginal-Relevance procedure can also be seen as an instance of submodular optimization. All these important models encouraging coverage, diversity and information are all submodular. Moreover, submodular functions can be efficiently combined, and the resulting function is still submodular. Hence, one could combine one submodular function which models diversity, another one which models coverage and use human supervision to learn a right model of a submodular function for the problem.\n While submodular functions are fitting problems for summarization, they also admit very efficient algorithms for optimization. For example, a simple greedy algorithm admits a constant factor guarantee.[25] Moreover, the greedy algorithm is extremely simple to implement and can scale to large datasets, which is very important for summarization problems.\n Submodular functions have achieved state-of-the-art for almost all summarization problems. For example, work by Lin and Bilmes, 2012[26] shows that submodular functions achieve the best results to date on DUC-04, DUC-05, DUC-06 and DUC-07 systems for document summarization. Similarly, work by Lin and Bilmes, 2011,[27] shows that many existing systems for automatic summarization are instances of submodular functions. This was a breakthrough result establishing submodular functions as the right models for summarization problems.[citation needed]\n Submodular Functions have also been used for other summarization tasks. Tschiatschek et al., 2014 show[28] that mixtures of submodular functions achieve state-of-the-art results for image collection summarization. Similarly, Bairi et al., 2015[29] show the utility of submodular functions for summarizing multi-document topic hierarchies. Submodular Functions have also successfully been used for summarizing machine learning datasets.[30]\n Specific applications of automatic summarization include:\n The most common way to evaluate the informativeness of automatic summaries is to compare them with human-made model summaries.\n Evaluation can be intrinsic or extrinsic,[36] and inter-textual or intra-textual.[37]\n Intrinsic evaluation assesses the summaries directly, while extrinsic evaluation evaluates how the summarization system affects the completion of some other task. Intrinsic evaluations have assessed mainly the coherence and informativeness of summaries. Extrinsic evaluations, on the other hand, have tested the impact of summarization on tasks like relevance assessment, reading comprehension, etc.\n Intra-textual evaluation assess the output of a specific summarization system, while inter-textual evaluation focuses on contrastive analysis of outputs of several summarization systems.\n Human judgement often varies greatly in what it considers a \"good\" summary, so creating an automatic evaluation process is particularly difficult. Manual evaluation can be used, but this is both time and labor-intensive, as it requires humans to read not only the summaries but also the source documents. Other issues are those concerning coherence and coverage.\n The most common way to evaluate summaries is ROUGE (Recall-Oriented Understudy for Gisting Evaluation). It is very common for summarization and translation systems in NIST's Document Understanding Conferences.[2] ROUGE is a recall-based measure of how well a summary covers the content of human-generated summaries known as references. It calculates n-gram overlaps between automatically generated summaries and previously written human summaries. It is recall-based to encourage inclusion of all important topics in summaries. Recall can be computed with respect to unigram, bigram, trigram, or 4-gram matching. For example, ROUGE-1 is the fraction of unigrams that appear in both the reference summary and the automatic summary out of all unigrams in the reference summary. If there are multiple reference summaries, their scores are averaged. A high level of overlap should indicate a high degree of shared concepts between the two summaries.\n ROUGE cannot determine if the result is coherent, that is if sentences flow together in a sensibly. High-order n-gram ROUGE measures help to some degree.\n Another unsolved problem is Anaphor resolution. Similarly, for image summarization, Tschiatschek et al., developed a Visual-ROUGE score which judges the performance of algorithms for image summarization.[38]\n Domain-independent summarization techniques apply sets of general features to identify information-rich text segments. Recent research focuses on domain-specific summarization using knowledge specific to the text's domain, such as medical knowledge and ontologies for summarizing medical texts.[39]\n The main drawback of the evaluation systems so far is that we need a reference summary (for some methods, more than one), to compare automatic summaries with models. This is a hard and expensive task. Much effort has to be made to create corpora of texts and their corresponding summaries. Furthermore, some methods require manual annotation of the summaries (e.g. SCU in the Pyramid Method). Moreover, they all perform a quantitative evaluation with regard to different similarity metrics.\n The first publication in the area dates back to 1957 [40] (Hans Peter Luhn), starting with a statistical technique. Research increased significantly in 2015. Term frequency\u2013inverse document frequency had been used by 2016. Pattern-based summarization was the most powerful option for multi-document summarization found by 2016. In the following year it was surpassed by latent semantic analysis (LSA) combined with non-negative matrix factorization (NMF). Although they did not replace other approaches and are often combined with them, by 2019 machine learning methods dominated the extractive summarization of single documents, which was considered to be nearing maturity. By 2020, the field was still very active and research is shifting towards abstractive summation and real-time summarization.[41]\n Recently the rise of transformer models replacing more traditional RNN (LSTM) have provided a flexibility in the mapping of text sequences to text sequences of a different type, which is well suited to automatic summarization. This includes models such as T5[42] and Pegasus.[43]\n",
        "doc_number": 102
    },
    {
        "url": "https://en.wikipedia.org/wiki/Named-entity_recognition",
        "content": "Named-entity recognition (NER) (also known as (named) entity identification, entity chunking, and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n Most research on NER/NEE systems has been structured as taking an unannotated block of text, such as this one:\n Jim bought 300 shares of Acme Corp. in 2006. And producing an annotated block of text that highlights the names of entities:\n [Jim]Person bought 300 shares of [Acme Corp.]Organization in [2006]Time. In this example, a person name consisting of one token, a two-token company name and a temporal expression have been detected and classified.\n State-of-the-art NER systems for English produce near-human performance. For example, the best system entering MUC-7 scored 93.39% of F-measure while human annotators scored 97.60% and 96.95%.[1][2]\n Notable NER platforms include:\n In the expression named entity, the word named restricts the task to those entities for which one or many strings, such as words or phrases, stand (fairly) consistently for some referent. This is closely related to rigid designators, as defined by Kripke,[5][6] although in practice NER deals with many names and referents that are not philosophically \"rigid\". For instance, the automotive company created by Henry Ford in 1903 can be referred to as Ford or Ford Motor Company, although \"Ford\" can refer to many other entities as well (see Ford). Rigid designators include proper names as well as terms for certain biological species and substances,[7] but exclude pronouns (such as \"it\"; see coreference resolution), descriptions that pick out a referent by its properties (see also De dicto and de re), and names for kinds of things as opposed to individuals (for example \"Bank\").\n Full named-entity recognition is often broken down, conceptually and possibly also in implementations,[8] as two distinct problems: detection of names, and classification of the names by the type of entity they refer to (e.g. person, organization, or location).[9]\nThe first phase is typically simplified to a segmentation problem: names are defined to be contiguous spans of tokens, with no nesting, so that \"Bank of America\" is a single name, disregarding the fact that inside this name, the substring \"America\" is itself a name. This segmentation problem is formally similar to chunking. The second phase requires choosing an ontology by which to organize categories of things.\n Temporal expressions and some numerical expressions (e.g., money, percentages, etc.) may also be considered as named entities in the context of the NER task. While some instances of these types are good examples of rigid designators (e.g., the year 2001) there are also many invalid ones (e.g., I take my vacations in \u201cJune\u201d). In the first case, the year 2001 refers to the 2001st year of the Gregorian calendar. In the second case, the month June may refer to the month of an undefined year (past June, next June, every June, etc.). It is arguable that the definition of named entity is loosened in such cases for practical reasons. The definition of the term named entity is therefore not strict and often has to be explained in the context in which it is used.[10]\n Certain hierarchies of named entity types have been proposed in the literature. BBN categories, proposed in 2002, are used for question answering and consists of 29 types and 64 subtypes.[11] Sekine's extended hierarchy, proposed in 2002, is made of 200 subtypes.[12] More recently, in 2011 Ritter used a hierarchy based on common Freebase entity types in ground-breaking experiments on NER over social media text.[13]\n To evaluate the quality of an NER system's output, several measures have been defined. The usual measures are called precision, recall, and F1 score. However, several issues remain in just how to calculate those values.\n These statistical measures work reasonably well for the obvious cases of finding or missing a real entity exactly; and for finding a non-entity. However, NER can fail in many other ways, many of which are arguably \"partially correct\", and should not be counted as complete success or failures. For example, identifying a real entity, but: \n One overly simple method of measuring accuracy is merely to count what fraction of all tokens in the text were correctly or incorrectly identified as part of entity references (or as being entities of the correct type). This suffers from at least two problems: first, the vast majority of tokens in real-world text are not part of entity names, so the baseline accuracy (always predict \"not an entity\") is extravagantly high, typically >90%; and second, mispredicting the full span of an entity name is not properly penalized (finding only a person's first name when his last name follows might be scored as \u00bd accuracy).\n In academic conferences such as CoNLL, a variant of the F1 score has been defined as follows:[9]\n It follows from the above definition that any prediction that misses a single token, includes a spurious token, or has the wrong class, is a hard error and does not contribute positively to either precision or recall. Thus, this measure may be said to be pessimistic: it can be the case that many \"errors\" are close to correct, and might be adequate for a given purpose. For example, one system might always omit titles such as \"Ms.\" or \"Ph.D.\", but be compared to a system or ground-truth data that expects titles to be included. In that case, every such name is treated as an error. Because of such issues, it is important actually to examine the kinds of errors, and decide how important they are given one's goals and requirements.\n Evaluation models based on a token-by-token matching have been proposed.[14] Such models may be given partial credit for overlapping matches (such as using the Intersection over Union criterion). They allow a finer grained evaluation and comparison of extraction systems.\n NER systems have been created that use linguistic grammar-based techniques as well as statistical models such as machine learning. Hand-crafted grammar-based systems typically obtain better precision, but at the cost of lower recall and months of work by experienced computational linguists.[15] Statistical NER systems typically require a large amount of manually annotated training data. Semisupervised approaches have been suggested to avoid part of the annotation effort.[16][17]\n Many different classifier types have been used to perform machine-learned NER, with conditional random fields being a typical choice.[18]\n In 2001, research indicated that even state-of-the-art NER systems were brittle, meaning that NER systems developed for one domain did not typically perform well on other domains.[19] Considerable effort is involved in tuning NER systems to perform well in a new domain; this is true for both rule-based and trainable statistical systems.\n Early work in NER systems in the 1990s was aimed primarily at extraction from journalistic articles. Attention then turned to processing of military dispatches and reports. Later stages of the automatic content extraction (ACE) evaluation also included several types of informal text styles, such as weblogs and text transcripts from conversational telephone speech conversations. Since about 1998, there has been a great deal of interest in entity identification in the molecular biology, bioinformatics, and medical natural language processing communities.  The most common entity of interest in that domain has been names of genes and gene products. There has been also considerable interest in the recognition of chemical entities and drugs in the context of the CHEMDNER\ncompetition, with 27 teams participating in this task.[20]\n Despite high F1 numbers reported on the MUC-7 dataset, the problem of named-entity recognition is far from being solved. The main efforts are directed to reducing the annotations labor by employing semi-supervised learning,[16][21] robust performance across domains[22][23] and scaling up to fine-grained entity types.[12][24] In recent years, many projects have turned to crowdsourcing, which is a promising solution to obtain high-quality aggregate human judgments for supervised and semi-supervised machine learning approaches to NER.[25] Another challenging task is devising models to deal with linguistically complex contexts such as Twitter and search queries.[26]\n There are some researchers who did some comparisons about the NER performances from different statistical models such as HMM (hidden Markov model), ME (maximum entropy), and CRF (conditional random fields), and feature sets.[27] And some researchers recently proposed graph-based semi-supervised learning model for language specific NER tasks.[28]\n A recently emerging task of identifying \"important expressions\" in text and cross-linking them to Wikipedia[29][30][31] can be seen as an instance of extremely fine-grained named-entity recognition, where the types are the actual Wikipedia pages describing the (potentially ambiguous) concepts. Below is an example output of a Wikification system:\n Another field that has seen progress but remains challenging is the application of NER to Twitter and other microblogs, considered \"noisy\" due to non-standard orthography, shortness and informality of texts.[32][33] NER challenges in English Tweets have been organized by research communities to compare performances of various approaches, such as bidirectional LSTMs, Learning-to-Search, or CRFs.[34][35][36]\n",
        "doc_number": 103
    }
]